<document>

<filing_date>
2018-10-18
</filing_date>

<publication_date>
2020-12-29
</publication_date>

<priority_date>
2017-10-20
</priority_date>

<ipc_classes>
H04N21/2187,H04N21/233,H04N21/234,H04N21/25,H04N21/258,H04N21/2668,H04N21/431,H04N21/44,H04N21/45,H04N21/466,H04N21/488,H04N21/8405,H04N21/854
</ipc_classes>

<assignee>
FMR
</assignee>

<inventors>
IVERSON, CHAD
CHUN, BYUNG
FAVICCHIO, DAVID
GRIGORAS, VERONICA
HANES, PHILIPP
LACKEY, CHRISTOPHER
</inventors>

<docdb_family_id>
66170259
</docdb_family_id>

<title>
Integrated intelligent overlay for media content streams
</title>

<abstract>
Methods and systems are described for generating integrated intelligent content overlays for media content streams. A server computing device receives a video content stream from a video data source. The server extracts a corpus of machine-recognizable text from the video content stream, the corpus of machine-recognizable text corresponding to at least one of audio or closed captioning text associated with the video content stream. The server identifies one or more entity names contained in the corpus of machine-recognizable text. The server determines a set of content keywords associated with each of the identified entity names. The server generates a content overlay for the video content stream comprising one or more layers that include graphical content relating to at least one of the sets of content keywords. The server integrates the content overlay into the video content stream to generate a customized video content stream.
</abstract>

<claims>
1. A system for generating integrated intelligent content overlays for media content streams, the system comprising a server computing device with a memory that stores computer-executable instructions and a processor that executes the computer-executable instructions to: receive a video content stream from a video data source; extract a corpus of machine-recognizable text from the video content stream, the corpus of machine-recognizable text corresponding to at least one of audio or closed captioning text associated with the video content stream; identify one or more entity names contained in the corpus of machine-recognizable text, including: tokenizing the corpus of machine-recognizable text into a plurality of text fragments, tagging each token with an identifier relating to a part of speech associated with the token, selecting one or more of the tokens that are tagged with an identifier where the part of speech is a noun, and matching the selected tokens against a database of preselected entity names to identify tokens that correspond to entity names; determine a set of content keywords associated with each of the identified entity names, including determining at least a portion of the set of content keywords by mapping one or more of the entity names to a set of keywords in a mapping table; generate a content overlay for the video content stream comprising one or more layers that include graphical content relating to at least one of the sets of content keywords; and integrate the content overlay into the video content stream to generate a customized video content stream.
2. The system of claim 1, wherein the server computing device: receives a request for the video content stream from a client computing device; identifies a user of the client computing device based upon one or more attributes of the request; retrieves a user profile based upon an identity of the user, the user profile comprising one or more user attributes; and adjusts the graphical content included in the layers of the content overlay based upon the user profile.
3. The system of claim 1, wherein extracting a corpus of machine-recognizable text from the video content stream comprises: capturing an audio stream associated with the video content stream, the audio stream comprising a digital waveform corresponding to speech in the video content stream; and converting the digital waveform of the audio stream into the corpus of machine-recognizable text.
4. The system of claim 1, wherein extracting a corpus of machine-recognizable text from the video content stream comprises: capturing a closed-captioning stream associated with the video content stream, the closed-captioning stream comprising text corresponding to speech in the video content stream; and converting the text of the closed-captioning stream into the corpus of machine-recognizable text.
5. The system of claim 1, wherein determining a set of content keywords associated with each of the identified entity names comprises executing a trained word embedding model using as input the identified entity names, the trained word embedding model comprising a multidimensional vector space, to determine one or more content keywords that are within a predetermined distance of the identified entity name within the multidimensional vector space.
6. The system of claim 1, wherein determining a set of content keywords associated with each of the identified entity names comprises: executing a trained long short-term memory (LSTM) model using as input one or more sentences from the corpus of machine-recognizable text that contain the entity names to predict whether the one or more sentences relate to the identified entity names; and generating one or more content keywords using the one or more sentences, when the one or more sentences are predicted to relate to the identified entity names.
7. The system of claim 1, wherein generating a content overlay for the video content stream comprises generating one or more video frames containing the one or more layers.
8. The system of claim 7, wherein integrating the content overlay into the video content stream to generate a customized video content stream comprises inserting the one or more video frames containing the one or more layers into the video content stream to replace one or more existing video frames in the video content stream.
9. The system of claim 1, wherein extracting a corpus of machine-recognizable text from the video content stream further comprises mapping the corpus of machine-recognizable text to the video content stream based upon a timestamp.
10. The system of claim 9, wherein generating a content overlay for the video content stream further comprises synchronizing the content overlay to the video stream based upon the timestamp.
11. The system of claim 1, wherein the server computing device transmits the customized video content stream to a client computing device for display.
12. The system of claim 11, wherein the transmission of the customized video content stream to the client computing device is a streaming session.
13. The system of claim 1, wherein the video content stream comprises a live video stream or a prerecorded video file.
14. A computerized method of generating integrated intelligent content overlays for media content streams, the method comprising: receiving, by a server computing device, a video content stream from a video data source; extracting, by the server computing device, a corpus of machine-recognizable text from the video content stream, the corpus of machine-recognizable text corresponding to at least one of audio or closed captioning text associated with the video content stream; identifying, by the server computing device, one or more entity names contained in the corpus of machine-recognizable text, including: tokenizing the corpus of machine-recognizable text into a plurality of text fragments, tagging each token with an identifier relating to a part of speech associated with the token, selecting one or more of the tokens that are tagged with an identifier where the part of speech is a noun, and matching the selected tokens against a database of preselected entity names to identify tokens that correspond to entity names; determining, by the server computing device, a set of content keywords associated with each of the identified entity names, including determining at least a portion of the set of content keywords by mapping one or more of the entity names to a set of keywords in a mapping table; generating, by the server computing device, a content overlay for the video content stream comprising one or more layers that include graphical content relating to at least one of the sets of content keywords; and integrating, by the server computing device, the content overlay into the video content stream to generate a customized video content stream.
15. The method of claim 14, further comprising: receiving, by the server computing device, a request for the video content stream from a client computing device; identifying, by the server computing device, a user of the client computing device based upon one or more attributes of the request; retrieving, by the server computing device, a user profile based upon an identity of the user, the user profile comprising one or more user attributes; and adjusting, by the server computing device, the graphical content included in the layers of the content overlay based upon the user profile.
16. The method of claim 14, wherein extracting a corpus of machine-recognizable text from the video content stream comprises: capturing, by the server computing device, an audio stream associated with the video content stream, the audio stream comprising a digital waveform corresponding to speech in the video content stream; and converting, by the server computing device, the digital waveform of the audio stream into the corpus of machine-recognizable text.
17. The method of claim 14, wherein extracting a corpus of machine-recognizable text from the video content stream comprises: capturing, by the server computing device, a closed-captioning stream associated with the video content stream, the closed-captioning stream comprising text corresponding to speech in the video content stream; and converting, by the server computing device, the text of the closed-captioning stream into the corpus of machine-recognizable text.
18. The method of claim 14, wherein determining a set of content keywords associated with each of the identified entity names comprises executing a trained word embedding model using as input the identified entity names, the trained word embedding model comprising a multidimensional vector space, to determine one or more content keywords that are within a predetermined distance of the identified entity name within the multidimensional vector space.
19. The method of claim 14, wherein determining a set of content keywords associated with each of the identified entity names comprises: executing, by the server computing device, a trained long short-term memory (LSTM) model using as input one or more sentences from the corpus of machine-recognizable text that contain the entity names to predict whether the one or more sentences relate to the identified entity names; and generating, by the server computing device, one or more content keywords using the one or more sentences, when the one or more sentences are predicted to relate to the identified entity names.
20. The method of claim 14, wherein generating a content overlay for the video content stream comprises generating one or more video frames containing the one or more layers.
21. The method of claim 20, wherein inserting the content overlay into the video content stream to generate a customized video content stream comprises inserting the one or more video frames containing the one or more layers into the video content stream to replace one or more existing video frames in the video content stream.
22. The method of claim 14, wherein extracting a corpus of machine-recognizable text from the video content stream further comprises mapping the corpus of machine-recognizable text to the video content stream based upon a timestamp.
23. The method of claim 22, wherein generating a content overlay for the video content stream further comprises synchronizing the content overlay to the video stream based upon the timestamp.
24. The method of claim 14, wherein the server computing device transmits the customized video content stream to a client computing device for display.
25. The method of claim 24, wherein the transmission of the customized video content stream to the client computing device is a streaming session.
26. The method of claim 14, wherein the video content stream comprises a live video stream or a prerecorded video file.
</claims>
</document>
