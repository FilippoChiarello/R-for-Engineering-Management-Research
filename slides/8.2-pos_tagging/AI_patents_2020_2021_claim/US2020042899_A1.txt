<document>

<filing_date>
2019-10-09
</filing_date>

<publication_date>
2020-02-06
</publication_date>

<priority_date>
2014-12-04
</priority_date>

<ipc_classes>
G06N20/00,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
SAP
</assignee>

<inventors>
MALOV, DENIS
QIN BIN
AZAM FAROOQ
</inventors>

<docdb_family_id>
54782401
</docdb_family_id>

<title>
Parallel Development and Deployment for Machine Learning Models
</title>

<abstract>
Example systems and methods of developing a learning model are presented. In one example, a sample data set to train a first learning algorithm is accessed. A number of states for each input of the sample data set is determined. A subset of the inputs is selected, and the sample data set is partitioned into a number of partitions equal to a combined number of states of the selected inputs. A second learning algorithm is created for each of the partitions, wherein each second learning algorithm receives the unselected inputs. Each of the second learning algorithms is assigned to a processor and trained using the samples of the partition corresponding to that algorithm. Decision logic is generated to direct each of a plurality of operational data units as input to one of the second learning algorithms based on states of the selected inputs of the operational data unit.
</abstract>

<claims>
1. A computer-implemented method of using a plurality of processors, of a computer, in parallel to develop a learning model, the method comprising: accessing, using at least one of the processors, a database storing a sample data set to train the learning model, wherein the learning model comprises a first learning algorithm having a number of inputs and a number of outputs, and wherein each sample of the sample data set comprises a state for each of the inputs and the outputs; partitioning, using at least one of the processors, the sample data set into a number of partitions equal to a combined number of states of selected inputs; creating, using at least one of the processors, a second learning algorithm for each of the partitions; assigning each of the second learning algorithms to one of the plurality of processors; training each of the second learning algorithms on the processor assigned to the second learning algorithm using the samples of the partition corresponding to the second learning algorithm to generate trained second learning algorithms; directing each of a plurality of operational data units as input to one of the trained second learning algorithms based on a combined state of the selected inputs corresponding to the operational data unit; and generating an output for each of the plurality of the plurality of operational data units from the corresponding trained second learning algorithm.
2. The method of claim 1, wherein generating an output for each of the plurality of the plurality of operational data units from the corresponding trained second learning algorithm comprises: accessing the plurality of operational data units; and providing each of the operational data units to decision logic for execution by a corresponding one of the second learning algorithms on a corresponding one of the processors; and generating, for each of the operational data units, an output for the operational data unit from the corresponding second learning algorithm for each of the outputs of the sample data set.
3. The method of claim 1, wherein each of the partitions corresponds to a combined state of the selected inputs and each of the partitions includes the samples of the sample data set that exhibit a same combined state of the selected inputs corresponding to the partition
4. The method of claim 3 wherein the combined number of states of the selected inputs is a greatest whole number less than or equal to a number of the plurality of processors.
5. The method of claim 3, wherein the combined number of states of the selected inputs is at least a whole number greater than a number of the plurality of processors of the computing system; and the assigning of each of the second learning algorithms comprises assigning a first one of the second learning algorithms and a second one of the second learning algorithms to a same one of the plurality of processors.
6. The method of claim 5, wherein the training of each of the second learning algorithms comprises training the first one of the second learning algorithms, followed by training the second one of the second learning algorithms.
7. The method of claim 1, wherein: the first learning algorithm comprises a first artificial neural network having a first number of hidden neurons; each of the second learning algorithms comprises a second artificial neural network; and the method further comprises setting a second number of hidden neurons of at least one of the second artificial neural networks based on the first number of hidden neurons.
8. The method of claim 7, wherein the second number of hidden neurons is equal to the first number of hidden neurons based on the first number of hidden neurons being less than twice the number of inputs minus twice a number of the selected inputs.
9. The method of claim 7, wherein the second number of hidden neurons is equal to twice the number of inputs minus twice a number of the selected inputs based on the first number of hidden neurons being less than or equal to four times the number of inputs minus four times the number of selected inputs, and being greater than or equal to twice the number of inputs minus twice the number of selected inputs.
10. The method of claim 7, wherein the second number of hidden neurons is equal to the first number of hidden neurons minus twice the number of inputs, minus twice a number of the selected inputs, based on the first number of hidden neurons being greater than four times the number of inputs minus four times the number of selected inputs.
11. The method of claim 1, further comprising: determining a number of states for each of the inputs based on the sample data set by determining a number of possible states for each of the selected inputs of the sample data set.
12. The method of claim 1, further comprising: determining a number of states for each of the inputs based on the sample data set by determining a number of employed states for each of the selected inputs of the sample data set.
13. The method of claim 12, wherein directing each of the plurality of operational data units as input to one of the trained second learning algorithms comprises accessing a first operational data unit that includes a first input of the selected inputs having a state that is not employed in the sample data set, and the method further comprises: directing the first operational data unit as input to the second learning algorithm corresponding to one of the employed states of the first input.
14. The method of claim 12, wherein directing each of the plurality of operational data units as input to one of the trained second learning algorithms comprises accessing a first operational data unit that includes a first input of the selected inputs having a state that is not employed in the sample data set, and the method further comprises: directing the first operational data unit as input to at least two of the second learning algorithms, wherein each of the at least two of the second learning algorithms corresponds to one of the employed states of the first input; and calculating a weighted average of corresponding outputs of the at least two of the second learning algorithms to produce an output for the first operational data unit.
15. The method of claim 1, wherein the first learning algorithm and each of the second learning algorithms comprises an artificial neural network.
16. The method of claim 1, wherein the first learning algorithm and each of the second learning algorithms comprises a supervised learning algorithm.
17. The method of claim 1, wherein the first learning algorithm and each of the second learning algorithms comprises an unsupervised learning algorithm.
18. The method of claim 1, wherein the second algorithm comprises logic of the first learning algorithm in which the combined state of the selected inputs is equal to the combined state that corresponds to the partition, and wherein the second learning algorithm is configured to receive as input those of the inputs that are not the selected inputs.
19. A computing system for using a plurality of processors in parallel to develop a learning model, the computing system comprising: at least one processor; a memory comprising instructions which, when executed by the at least one processor, cause the computing system to perform operations comprising: accessing a database storing a sample data set to train the learning model, wherein the learning model comprises a first learning algorithm having a number of inputs and a number of outputs, and wherein each sample of the sample data set comprises a state for each of the inputs and the outputs; partitioning the sample data set into a number of partitions equal to a combined number of states of selected inputs; creating a second learning algorithm for each of the partitions; assigning each of the second learning algorithms to one of the plurality of processors; training each of the second learning algorithms on the processor assigned to the second learning algorithm using the samples of the partition corresponding to the second learning algorithm to generate trained second learning algorithms; directing each of a plurality of operational data units as input to one of the trained second learning algorithms based on a combined state of the selected inputs corresponding to the operational data unit; and generating an output for each of the plurality of the plurality of operational data units from the corresponding trained second learning algorithm.
20. A non-transitory computer-readable storage medium comprising instructions that, when executed by at least one processor of a computing system, cause the computing system to use a plurality of processors in parallel to develop a learning model by performing operations comprising: accessing, using at least one of the processors, a database storing a sample data set to train the learning model, wherein the learning model comprises a first learning algorithm having a number of inputs and a number of outputs, and wherein each sample of the sample data set comprises a state for each of the inputs and the outputs; partitioning, using at least one of the processors, the sample data set into a number of partitions equal to a combined number of states of selected inputs; creating, using at least one of the processors, a second learning algorithm for each of the partitions; assigning each of the second learning algorithms to one of the plurality of processors; training each of the second learning algorithms on the processor assigned to the second learning algorithm using the samples of the partition corresponding to the second learning algorithm to generate trained second learning algorithms; directing each of a plurality of operational data units as input to one of the trained second learning algorithms based on a combined state of the selected inputs corresponding to the operational data unit; and generating an output for each of the plurality of the plurality of operational data units from the corresponding trained second learning algorithm.
</claims>
</document>
