<document>

<filing_date>
2019-06-26
</filing_date>

<publication_date>
2020-12-01
</publication_date>

<priority_date>
2017-09-07
</priority_date>

<ipc_classes>
B25J9/16,G06K9/00,G06N20/00,G06T7/73
</ipc_classes>

<assignee>
X DEVELOPMENT
</assignee>

<inventors>
HUDSON, NICOLAS
LI, ADRIAN
EDSINGER, AARON
</inventors>

<docdb_family_id>
67220542
</docdb_family_id>

<title>
Generating and utilizing spatial affordances for an object in robotics applications
</title>

<abstract>
Methods, apparatus, systems, and computer-readable media are provided for generating spatial affordances for an object, in an environment of a robot, and utilizing the generated spatial affordances in one or more robotics applications directed to the object. Various implementations relate to applying vision data as input to a trained machine learning model, processing the vision data using the trained machine learning model to generate output defining one or more spatial affordances for an object captured by the vision data, and controlling one or more actuators of a robot based on the generated output. Various implementations additionally or alternatively relate to training such a machine learning model.
</abstract>

<claims>
1. A method implemented by one or more processors of a robot, the method comprising: receiving vision data, the vision data generated based on output from one or more vision sensors of a vision component viewing an object in an environment of a robot; applying the vision data as input to at least one trained neural network model; processing the vision data using the trained neural network model to generate output defining multiple spatial affordances for the object in the environment, wherein processing of the vision data is based on trained parameters of the trained neural network model, wherein the output is generated directly by the processing of the vision data using the trained neural network model, and wherein the multiple spatial affordances defined by the output include a first spatial affordance for a first spatial region of the object and a second spatial affordance for a second spatial region of the object; determining that the first spatial affordance is a target affordance for the object; and based on the first spatial affordance being the target affordance for the object and being defined for the first spatial region of the object: controlling one or more actuators of the robot to cause one or more components of the robot to interact with the first spatial region of the object to perform the first spatial affordance through interaction of the one or more components with the first spatial region of the object.
2. The method of claim 1, wherein the first spatial affordance further defines an affordance application parameter for the first spatial affordance for the first spatial region.
3. The method of claim 2, wherein the affordance application parameter defines at least one magnitude of force to be applied in performance of the first spatial affordance for the first spatial region.
4. The method of claim 2, wherein the affordance application parameter defines at least one direction of force to be applied in performance of the first spatial affordance for the first spatial region.
5. The method of claim 1, wherein the first spatial affordance defines a collection of affordances.
6. The method of claim 5, wherein the collection of affordances is an ordered collection of affordances.
7. The method of claim 1, wherein the first spatial affordance defines the first spatial affordance for the first spatial region based on the output including a probability, that corresponds to the first spatial region and to the first affordance, satisfying a first threshold.
8. The method of claim 1, wherein the vision data comprises a plurality of pixels or voxels, and wherein the first spatial region defines a first pixel or first voxel of the plurality of pixels or voxels and the second spatial region defines a second pixel or voxel of the plurality of pixels or voxels.
9. The method of claim 8, wherein the first spatial region defines only the first pixel or voxel and the second spatial region defines only the second pixel or voxel.
10. The method of claim 8, wherein the first spatial region defines a first collection of contiguous pixels or voxels that include the first pixel or voxel, and wherein the second spatial region defines a second collection of contiguous pixels or voxels that include the first pixel or voxel and that exclude the second pixel or voxel.
11. A robot, comprising: a vision component that comprises one or more vision sensors; actuators; an end effector; memory storing at least one trained machine learning model; one or more processors configured to: receive vision data generated by the vision component, the vision data capturing an object in an environment of the robot; apply the vision data as input to the at least one trained machine learning model; process the vision data using the trained machine learning model to generate output defining multiple spatial affordances for the object in the environment, wherein processing of the vision data is based on trained parameters of the trained neural network model, wherein the output is generated directly by the processing of the vision data using the trained neural network model, and wherein the multiple spatial affordances defined by the output include a first spatial affordance for a first spatial region of the object and a second spatial affordance for a second spatial region of the object; determine that the first spatial affordance is a target affordance for the object; and in response to the first spatial affordance being the target affordance for the object and being defined for the first spatial region of the object: control one or more of the actuators to cause performance of the first spatial affordance through interaction with the first spatial region of the object.
12. The robot of claim 11, wherein the vision data comprises a plurality of pixels or voxels, and wherein the first spatial region defines a first pixel or first voxel of the plurality of pixels or voxels and the second spatial region defines a second pixel or voxel of the plurality of pixels or voxels.
13. The robot of claim 12, wherein the first spatial region defines only the first pixel or voxel and the second spatial region defines only the second pixel or voxel.
14. The robot of claim 12, wherein the first spatial region defines a first collection of contiguous pixels or voxels that include the first pixel or voxel, and wherein the second spatial region defines a second collection of contiguous pixels or voxels that include the first pixel or voxel and that exclude the second pixel or voxel.
15. The robot of claim 11, wherein the first spatial affordance further defines an affordance application parameter for the first spatial affordance for the first spatial region.
16. The robot of claim 15, wherein the affordance application parameter defines at least one magnitude of force, and at least one direction of force, to be applied in performance of the first spatial affordance for the first spatial region.
17. The robot of claim 15, wherein the affordance application parameter defines at least one direction of force to be applied in performance of the first spatial affordance for the first spatial region.
18. The robot of claim 11, wherein the first spatial affordance defines an ordered collection of affordances.
19. The robot of claim 11, wherein the first spatial affordance defines the first spatial affordance for the first spatial region based on the output including a probability, that corresponds to the first spatial region and to the first affordance, satisfying a first threshold.
</claims>
</document>
