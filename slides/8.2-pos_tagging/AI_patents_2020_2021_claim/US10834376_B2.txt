<document>

<filing_date>
2019-06-28
</filing_date>

<publication_date>
2020-11-10
</publication_date>

<priority_date>
2017-03-29
</priority_date>

<ipc_classes>
G06Q10/08,G06Q30/06,G06T17/00,G06T3/40,H04N1/00,H04N1/195,H04N1/32,H04N13/178
</ipc_classes>

<assignee>
PLETHRON
</assignee>

<inventors>
CHU, CHARLES
MINAMI, ERIC
</inventors>

<docdb_family_id>
63671245
</docdb_family_id>

<title>
Generating a stitched image or three-dimensional model from one or more dimension extractable objects comprising spatial metadata for a captured image or video
</title>

<abstract>
The invention relates to capturing a still or moving image as well as object position and displacement data for the image, storing the image and the data as a dimension extractable object, and utilizing the dimension extractable object. In one embodiment, the dimension extractable object is used in a method of stitching a plurality of images together to form a stitched image. In another embodiment, the dimension extractable object is used in a method of generating a stitched image from a video image. In another embodiment, the dimension extractable object is used in a method of generating a three-dimensional model from a video image.
</abstract>

<claims>
1. A method of stitching a plurality of images together to form a stitched image, the method comprising: receiving, by a computing device, a plurality of dimension extractable objects; extracting, by the computing device, a set of image data and spatial metadata from each of the dimension extractable objects, wherein for each set of image data and spatial metadata, the image data corresponds to an image captured by a device and the spatial metadata comprises image position data and distance data, the distance data indicating a distance between the device and a physical object depicted in the image at a time the image data was captured; and stitching, by the computing device, images derived from the image data using the spatial metadata to form a stitched image.
2. The method of claim 1, wherein the receiving step comprises receiving a container comprising the plurality of dimension extractable objects.
3. The method of claim 1, further comprising: generating, by the computing device, a dimension extractable object comprising the stitched image.
4. The method of claim 1, further comprising: before the stitching step, performing a transformation of one or more of the images derived from the image data using a lens model stored in the spatial metadata for one or more dimension extractable objects.
5. The method of claim 1, further comprising: calculating dimensions of the physical object using the image data and spatial metadata.
6. The method of claim 5, wherein one or more of the dimensions of the physical object exceed one or more dimensions of one of more of the image data.
7. A method of generating a stitched image from a video image, the method comprising: receiving, by a computing device, a dimension extractable object; extracting, by the computing device, a video stream, a first set of metadata, and a second set of metadata from the dimension extractable object, the first set of metadata comprising metadata for the video stream; extracting a plurality of frames from the video stream, wherein each of the plurality of frames comprises an image captured by a device and the second set of metadata comprises image position data and distance data for each frame, the distance data indicating a distance between the device and a physical object depicted in the image of the frame at a time the image was captured; and stitching, by the computing device, a plurality of images from the plurality of frames using some or all of the first set of metadata and the second set of metadata to form a stitched image.
8. The method of claim 7, further comprising: generating, by the computing device, a dimension extractable object comprising the stitched image.
9. The method of claim 7, further comprising: before the stitching step, performing a transform of one or more of the images derived from the image data using a lens model stored in the spatial metadata.
10. The method of claim 7, further comprising: calculating dimensions of the physical object using one or more of the frames and some or all of the first set of metadata and the second set of metadata.
11. The method of claim 10, wherein one or more of the dimensions of the physical object exceed one or more dimensions of one or more of the frames.
12. The method of claim 7, further comprising: calculating a velocity of the physical object at the time when a plurality of the images were captured.
13. The method of claim 12, wherein the second set of metadata further comprises time metadata, accelerometer metadata for the device, and gyroscope metadata for the device and the velocity is calculated using the second set of metadata.
14. A method of generating a three-dimensional model from a video image, the method comprising: receiving, by a computing device, a dimension extractable object; extracting, by the computing device, a plurality of sets of images and metadata from the dimension extractable object, wherein the images were captured by a device and the metadata comprises image position data and distance data, the distance data indicating a distance between the device and a physical object depicted in the image at a time the image was captured; and generating a three-dimensional model, by the computing device, using the plurality of sets of images and spatial metadata.
15. The method of claim 14, further comprising: generating, by the computing device, a dimension extractable object comprising the three-dimensional model.
16. The method of claim 14, further comprising: before the step of generating a three-dimensional model, performing a transform of one or more of the images using a lens model stored in metadata associated with the one or more of the images.
17. The method of claim 14, further comprising: calculating dimensions of the physical object using the image data and metadata.
18. The method of claim 17, wherein one or more of the dimensions of the physical object exceed one or more dimensions of one of more of the image data.
19. The method of claim 14, further comprising: calculating tilt angle of the device and position of the device at the time when each image was captured.
20. The method of claim 19, wherein the metadata further comprises time metadata, accelerometer metadata for the device, and gyroscope metadata for the device and the tilt angle of the device and position of the device are calculated using the metadata.
</claims>
</document>
