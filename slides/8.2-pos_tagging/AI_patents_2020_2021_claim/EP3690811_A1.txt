<document>

<filing_date>
2020-01-09
</filing_date>

<publication_date>
2020-08-05
</publication_date>

<priority_date>
2019-01-31
</priority_date>

<ipc_classes>
G06K9/32,G06K9/40,G06K9/46,G06N3/04,G06N3/08,G06T5/00,G06T7/246
</ipc_classes>

<assignee>
STRADVISION
</assignee>

<inventors>
BOO, SUKHOON
CHO, HOJIN
JANG, TAEWOONG
JE, HONGMO
JEONG, KYUNGJOONG
KIM, HAK-KYOUNG
KIM, INSU
KIM, KYE-HYEON
KIM, YONGJOONG
NAM, WOONHYUN
RYU, WOOJU
SUNG, MYUNGCHUL
YEO, DONGHUN
</inventors>

<docdb_family_id>
68536108
</docdb_family_id>

<title>
LEARNING METHOD AND LEARNING DEVICE FOR REMOVING JITTERING ON VIDEO ACQUIRED THROUGH SHAKING CAMERA BY USING A PLURALITY OF NEURAL NETWORKS FOR FAULT TOLERANCE AND FLUCTUATION ROBUSTNESS IN EXTREME SITUATIONS, AND TESTING METHOD AND TESTING DEVICE USING THE SAME
</title>

<abstract>
A method for detecting jittering in videos generated by a shaken camera to remove the jittering on the videos using neural networks is provided for fault tolerance and fluctuation robustness in extreme situations. The method includes steps of: a computing device, generating each of t-th masks corresponding to each of objects in a t-th image; generating each of t-th object motion vectors of each of object pixels, included in the t-th image by applying at least one 2-nd neural network operation to each of the t-th masks, each of t-th cropped images, each of (t-1)-th masks, and each of (t-1)-th cropped images; and generating each of t-th jittering vectors corresponding to each of reference pixels among pixels in the t-th image by referring to each of the t-th object motion vectors. Thus, the method is used for video stabilization, object tracking with high precision, behavior estimation, motion decomposition, etc.
</abstract>

<claims>
1. A method for detecting jittering in videos generated by a shaking camera to remove the jittering on the videos using one or more neural networks, comprising steps of: (a) a computing device, if a t-th image corresponding to a t-th frame on the videos is acquired, instructing a 1-st neural network to generate each of t-th masks corresponding to each of objects in the t-th image by applying at least one 1-st neural network operation to the t-th image; (b) the computing device instructing a 2-nd neural network to generate each of t-th object motion vectors of each of object pixels, corresponding to each of the objects, included in the t-th image by applying at least one 2-nd neural network operation to (i) each of the t-th masks, (ii) each of t-th cropped images, corresponding to each of the t-th masks, which are part of the t-th image, (iii) each of (t-1)-th masks, and (iv) each of (t-1)-th cropped images, corresponding to each of the (t-1)-th masks, which are part of a (t-1)-th image; and (c) the computing device instructing a jittering estimation unit to generate each of t-th jittering vectors corresponding to each of reference pixels among pixels in the t-th image by referring to each of the t-th object motion vectors.
2. The method of Claim 1, wherein the method further comprises a step of:
(d) the computing device instructing the jittering estimation unit to generate a t-th adjusted image in which jittering of the t-th image is smoothed by referring to the t-th jittering vectors.
3. The method of Claim 1, wherein the method further comprises a step of:
(e) the computing device instructing a learning unit to calculate one or more losses by referring to (i) t-th adjusted object motion vectors optimized through a process for generating the t-th jittering vectors and (ii) the t-th object motion vectors before the optimization, and to learn at least part of one or more parameters of the 2-nd neural network by backpropagating the losses.
4. The method of Claim 1, wherein, at the step of (c), the computing device instructs the jittering estimation unit to generate the t-th jittering vectors by further referring to (i) each of t-th optical flow vectors representing a degree of movement of each of positions corresponding to each of pixels in the t-th image between a point of time when the (t-1)-th image is taken and a point of time when the t-th image is taken by the camera and (ii) each of t-th FPV vectors representing a degree of movement of each of the positions in a three dimensional space between a point of time when the (t-1)-th image is taken and a point of time when the t-th image is taken by the camera.
5. The method of Claim 4, wherein, at the step of (d), the computing device instructs the jittering estimation unit to calculate and which minimize a formula: to thereby generate the t-th jittering vectors, wherein is the t-th jittering vectors, is the t-th FPV vectors, is the t-th object motion vectors, is (t-1)-th FPV vectors, is (t-1)-th object motion vectors, λv and λo are each of weights of and oxyt-1F2 respectively.
6. The method of Claim 5, wherein the formula is valid for terms satisfying wherein is first t-th optical flow vectors, which are part of the t-th optical flow vectors, of the reference pixels, is second t-th optical flow vectors of pixels other than the reference pixels, is t-th object motion vectors, which are part of the t-th object motion vectors, of specific pixels which are the object pixels and also the reference pixels, i,j ∈ NB(x,y) at a bottom of a sigma symbol represents that information on part of the reference pixels in a grid including a certain pixel is used, and wij,xy is each of weights according to at least one location of the certain pixel in the grid.
7. The method of Claim 4, wherein the computing device instructs the jittering estimation unit to calculate and which minimize a formula: wherein is the t-th jittering vectors, is the t-th FPV vectors, is the t-th object motion vectors, is (t-k)-th FPV vectors of a (t-k)-th image, is (t-k)-th object motion vectors of the (t-k)-th image, wk is weights of and K is a certain constant.
8. The method of Claim 1, wherein, at the step of (b), the 2-nd neural network includes a (2_1)-st neural network and a (2_2)-nd neural network,
wherein the (2_1)-st neural network generates at least one intermediate feature map by performing at least one (2_1)-st neural network operation, included in the 2-nd neural network operation, which analyzes each of the t-th masks, each of the t-th cropped images, each of the (t-1)-th masks and each of the (t_1)-st cropped images, and
wherein the (2_2)-nd neural network generates the t-th object motion vectors by applying at least one (2_2)-nd neural network operation included in the 2-nd neural network operation to the intermediate feature map by referring to analysis of values inputted before acquiring the t-th image.
9. The method of Claim 8, wherein the (2_2)-nd neural network generates the t-th object motion vectors using its stored state vectors, by reflecting the analysis of values, generated by the (2_1)-st neural network, on at least part of a 1-st image to the (t-1)-th image and updates the state vectors.
10. The method of Claim 1, wherein the reference pixels are vertices disposed on boundaries of a grid cell of the grid on the t-th image.
11. The method of Claim 1, wherein, at the step of (a), the 1-st neural network includes at least one Region Proposal Network, at least one convolutional layer, at least one pooling layer, and
wherein if the Region Proposal Network generates at least one ROI, the 1-st neural network selects at least one region, corresponding to the ROI, on the t-th image by a bilinear interpolation and generates the t-th masks by applying at least one operation of the convolutional layer to said at least one region.
12. A testing method for removing jittering in videos for testing generated by a shaking camera to remove the jittering on the videos for testing using one or more neural networks, comprising steps of: (a) a testing device, on condition that a learning device has performed processes of (1) after acquiring a t-th image for training corresponding to a t-th frame for training on videos for training, instructing a 1-st neural network to generate each of t-th masks for training corresponding to each of objects for training in the t-th image for training by applying at least one 1-st neural network operation to the t-th image for training, (2) instructing a 2-nd neural network to generate each of t-th object motion vectors for training of each of object pixels for training, corresponding to each of the objects for training, included in the t-th image for training by applying at least one 2-nd neural network operation to (i) each of the t-th masks for training, (ii) each of t-th cropped images for training, corresponding to each of the t-th masks for training, which are part of the t-th image for training, (iii) each of (t-1)-th masks for training, and (iv) each of (t-1)-th cropped images for training, corresponding to each of the (t-1)-th masks for training, which are part of a (t-1)-th image for training, (3) instructing a jittering estimation unit to generate each of t-th jittering vectors for training corresponding to each of reference pixels for training among pixels in the t-th image for training by referring to each of the t-th object motion vectors for training, and (4) instructing a learning unit to calculate one or more losses by referring to (i) t-th adjusted object motion vectors for training optimized through a process for generating the t-th jittering vectors for training and (ii) the t-th object motion vectors for training before the optimization, and to learn at least part of one or more parameters of the 2-nd neural network by backpropagating the losses; if a t-th image for testing corresponding to a t-th frame for testing on the videos for testing is acquired, instructing the 1-st neural network to generate each of t-th masks for testing corresponding to each of objects for testing in the t-th image for testing by applying the 1-st neural network operation to the t-th image for testing; (b) the testing device instructing the 2-nd neural network to generate each of t-th object motion vectors for testing of each of object pixels for testing, corresponding to each of the objects for testing, included in the t-th image for testing by applying the 2-nd neural network operation to (i) each of the t-th masks for testing, (ii) each of t-th cropped images for testing, corresponding to each of the t-th masks for testing, which are part of the t-th image for testing, (iii) each of (t-1)-th masks for testing, and (iv) each of (t-1)-th cropped images for testing, corresponding to each of the (t-1)-th masks for testing, which are part of a (t-1)-th image for testing; and (c) the testing device instructing the jittering estimation unit to generate each of t-th jittering vectors for testing corresponding to each of reference pixels for testing among pixels in the t-th image for testing by referring to each of the t-th object motion vectors for testing.
13. The testing method of Claim 12, wherein the testing method further comprises a step of:
(d) the testing device instructing the jittering estimation unit to generate a t-th adjusted image for testing in which jittering of the t-th image for testing is smoothed by referring to the t-th jittering vectors for testing.
14. A computing device for detecting jittering in videos generated by a shaking camera to remove the jittering on the videos using one or more neural networks, comprising: at least one memory that stores instructions; and at least one processor configured to execute the instructions to: perform processes of: (I) if a t-th image corresponding to a t-th frame on the videos is acquired, instructing a 1-st neural network to generate each of t-th masks corresponding to each of objects in the t-th image by applying at least one 1-st neural network operation to the t-th image, (II) instructing a 2-nd neural network to generate each of t-th object motion vectors of each of object pixels, corresponding to each of the objects, included in the t-th image by applying at least one 2-nd neural network operation to (i) each of the t-th masks, (ii) each of t-th cropped images, corresponding to each of the t-th masks, which are part of the t-th image, (iii) each of (t-1)-th masks, and (iv) each of (t-1)-th cropped images, corresponding to each of the (t-1)-th masks, which are part of a (t-1)-th image, and (III) instructing a jittering estimation unit to generate each of t-th jittering vectors corresponding to each of reference pixels among pixels in the t-th image by referring to each of the t-th object motion vectors.
15. The computing device of Claim 14, wherein, at the process of (III), the processor instructs the jittering estimation unit to generate the t-th jittering vectors by further referring to (i) each of t-th optical flow vectors representing a degree of movement of each of positions corresponding to each of pixels in the t-th image between a point of time when the (t-1)-th image is taken and a point of time when the t-th image is taken by the camera and (ii) each of t-th FPV vectors representing a degree of movement of each of the positions in a three dimensional space between a point of time when the (t-1)-th image is taken and a point of time when the t-th image is taken by the camera.
</claims>
</document>
