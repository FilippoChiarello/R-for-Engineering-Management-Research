<document>

<filing_date>
2020-01-23
</filing_date>

<publication_date>
2020-07-30
</publication_date>

<priority_date>
2019-01-23
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
CHAN, WILLIAM
KIROS, JAMIE RYAN
STERN, MITCHELL THOMAS
USZKOREIT, JAKOB D.
</inventors>

<docdb_family_id>
69743906
</docdb_family_id>

<title>
GENERATING NEURAL NETWORK OUTPUTS USING INSERTION OPERATIONS
</title>

<abstract>
Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating network outputs using insertion operations.
</abstract>

<claims>
1. A method performed by one or more computers, the method comprising:
receiving a network input; and
generating a network output from the network input, wherein the network output comprises a plurality of outputs from a vocabulary of outputs arranged according to an output order, the generating comprising, at each of a plurality of generation time steps:
identifying a current partial network output that has already been generated as of the generation time step, the current partial network output comprising zero or more outputs from the vocabulary of outputs arranged according to a partial output order;
generating, using a decoder neural network conditioned on (i) at least a portion of the network input and (ii) any outputs in the current partial network output, a decoder output that defines, for each of a plurality of insertion locations, a respective score distribution over the vocabulary of outputs, wherein each insertion location is a different new location in the partial output order at which there is no output in the current partial network output;
selecting, using the decoder output, one or more of the insertion locations and, for each selected insertion location, an inserted output from the vocabulary; and generating a new partial network output that comprises (i) the zero or more outputs in the current partial network output and (ii) for each selected insertion location, the inserted output from the vocabulary inserted at the corresponding new location in the partial output order.
2. The method of claim 1, wherein the decoder neural network is an attention-based neural network that is configured to generate the decoder output by applying an attention mechanism over an encoded representation of the network input and a self-attention mechanism over the outputs in the current partial network output.
3. The method of claim 2, wherein generating the decoder output using the decoder neural network comprises:
generating a decoder input that includes the encoded representation of the network input and the outputs in the current partial network output arranged according to the partial output order.
4. The method of claim 3,
wherein generating the decoder input further comprises adding two marker outputs to the current partial network output,
wherein the decoder neural network is configured to generate a respective representation vector for each location in the partial output order after the two marker outputs have been added, and
wherein generating the decoder output comprises:
generating a respective slot representation for each insertion location by concatenating the representation vectors for each adjacent pair of locations in the partial output order; and
generating a score distribution for each insertion location from at least the slot representation for the insertion location.
5. The method of any preceding claim, wherein the vocabulary includes an end-ofsequence token.
6. The method of claim 5, wherein selecting, using the decoder output, one or more of the insertion locations and, for each selected insertion location, an inserted output from the vocabulary comprises:
determining that an insertion location - output combination with a highest score across all insertion location - output combinations does not include the end-of-sequence token; and
in response, selecting only the insertion location - output combination with a highest score across all insertion location - output combinations.
7. The method of claim 5, wherein selecting, using the decoder output, one or more of the insertion locations and, for each selected insertion location, an inserted output from the vocabulary comprises:
determining that there is at least one insertion location for which the output with the highest score is not the end-of-sequence token; and
in response, selecting only an insertion location - output combination with a highest score across all insertion location - output combinations that include an insertion location for which the output with the highest score is not the end-of-sequence token.
8. The method of claim 5, wherein selecting, using the decoder output, one or more of the insertion locations and, for each selected insertion location, an inserted output from the vocabulary comprises:
identifying, from the decoder output and for each insertion location, an output that has a highest score for the insertion location;
determining that there is at least one insertion location for which the output with the highest score is not the end-of-sequence token; and
in response, selecting each insertion location for which the output with the highest score is not the end-of-sequence token and the corresponding output that has the highest score for the insertion location.
9. The method of any preceding claim, wherein the decoder neural network is configured to generate a respective slot representation for each insertion location.
10. The method of claim 9, wherein generating the decoder output comprises:
projecting a decoder hidden state matrix generated from the slot representations using a projection matrix to generate a content-location logit matrix;
flattening the content-location logit matrix into a content-location logit vector; and applying a softmax over the content-location logit vector to generate a probability distribution over all insertion location - output combinations.
11. The method of claim 9, wherein generating the decoder output comprises:
generating a respective probability for each location by applying a softmax to a product of a decoder hidden state matrix generated from the slot representations and a learned query vector;
for each location:
projecting the slot representation for the location into a score vector that includes a respective score for each output in the vocabulary using a projection matrix;
applying a softmax over the score vector to generate an initial probability for each output in the vocabulary; and multiplying each initial probability by the probability for the location to generate a final probability for each output in the vocabulary.
12. The method of any one of claims 9-11, wherein generating the decoder output comprises:
generating a context vector by applying max pooling over the slot representations; generating a bias vector from the context vector that includes a respective bias value for each output in the vocabulary; and
generating the decoder output from the bias vector and the slot representations.
13. The method of any one of the preceding claims, wherein:
the method comprises a speech recognition method in which the network input comprises a sequence of audio data representing a spoken utterance and the network output comprises a transcription of the spoken utterance; or
the method comprises a medical diagnosis method in which the network input comprises a sequence of data from an electronic medical record and the network output comprises a sequence of one or more predicted treatments; or
the method comprises an image processing method in which the network input comprises a sequence of pixel values for an image and the network output comprises a sequence of text that describes the image; or
the method comprises a neural machine translation method in which the network input comprises a sequence of words in a first language and the network output comprises a sequence of words in a second language that represents the sequence of words in the first language, the first and second languages being different to one another.
14. One or more computer-readable storage media storing instructions that when executed by one or more computers cause the one or more computers to perform the respective operations of any one of the methods of any of the preceding claims.
15. A system comprising one or more computers and one or more storage devices storing instructions that when executed by one or more computers cause the one or more computers to perform the respective operations of any one of the methods of any one of claims 1-13.
</claims>
</document>
