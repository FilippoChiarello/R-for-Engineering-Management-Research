<document>

<filing_date>
2019-09-13
</filing_date>

<publication_date>
2020-03-19
</publication_date>

<priority_date>
2018-09-15
</priority_date>

<ipc_classes>
B25J9/16
</ipc_classes>

<assignee>
X DEVELOPMENT
</assignee>

<inventors>
KALAKRISHNAN, MRINAL
LI, ADRIAN
PASTOR SAMPEDRO, PETER
YAN, MENGYUAN
</inventors>

<docdb_family_id>
69774667
</docdb_family_id>

<title>
ACTION PREDICTION NETWORKS FOR ROBOTIC GRASPING
</title>

<abstract>
Deep machine learning methods and apparatus related to the manipulation of an object by an end effector of a robot are described herein. Some implementations relate to training an action prediction network to predict a probability density which can include candidate actions of successful grasps by the end effector given an input image. Some implementations are directed to utilization of an action prediction network to visually servo a grasping end effector of a robot to achieve a successful grasp of an object by the grasping end effector.
</abstract>

<claims>
1. A method performed by one or more processors of a robot, the method comprising: at a given iteration of visual servoing a grasping end effector of the robot, using a trained action prediction network, to grasp an object in an environment of the robot: identifying, by one or more processors, a current image captured by a vision sensor associated with a robot, the current image capturing a grasping end effector of the robot in a current pose and capturing the object; applying, by one or more of the processors, the current image as input to the trained action prediction network; generating, using the trained action prediction network, a predicted probability density of candidate actions, wherein the predicted probability density is generated using a neural density model portion of the action prediction network, wherein each of the candidate actions indicates at least a respective three-dimensional direction of movement for the grasping end effector, and wherein the predicted probability density defines, for each of the candidate actions, a respective probability that implementing the candidate action, and subsequently grasping, will result in a successful grasp of the object; selecting a given action, of the candidate actions, based on the generated predicted probability density; generating an end effector command based on the selected action; and providing the end effector command to one or more actuators of the robot.
2. The method of claim 1, wherein the neural density model is a Gaussian mixture model (GMM), wherein the predicted probability density is generated by the GMM, and wherein the predicted probability density divides candidate actions to move the grasping end effector into subpopulations for each object.
3. The method of claim 1, wherein the neural density model is a real-valued non-volume preserving (real NVP) transformation model, wherein the predicted probability density is generated by mapping the current image to a latent space using a trained real NVP transformation, and wherein the trained real NVP transformation is a bijective mapping between the current image action space and the latent space.
4. The method of claim 1, wherein the neural density model is a hybrid model, wherein the predicted probability density is generated utilizing the hybrid model by mapping the current image to a plurality of Gaussian mixtures in a latent space, and wherein each Gaussian in the Gaussian mixture of the latent space has an individual real NVP transformation.
5. The method of claim 1, wherein the action prediction network further includes a processing network portion to extract one or more features from the current image, wherein the processing network include one or more trained convolutional neural networks, a softmax layer, and one or more feed forward layers, wherein the current image is applied as input to the processing layers, wherein the processing network is upstream from the neural density model, and wherein the one or more features are applied, by one of more of the processors, as input to the neural density model.
6. The method of claim 5, further comprising: identifying, by one or more of the processors, data indicative of a target object to grasp using the end effector command, wherein the target object is one of the objects in the environment of the robot.
7. The method of claim 6, further comprising: applying, by one or more of the processors, a target object image in addition to the current image as input to the processing network, wherein the data indicative of the target object includes the target object image.
8. The method of claim 6, further comprising: applying, by the one or more processors, a target object embedding as additional input to the neural density model, wherein the data indicative of the target object includes the target object embedding.
9. The method of claim 1, further comprising an object identification network, wherein the object identification network is trained to determine if the robot successfully grasped the object with the grasping end effector using the end effector command.
10. The method of claim 1, further comprising: determining, by one or more of the processors, a normalized neural density model by normalizing a first trained neural density model using a second trained neural density model, wherein the first trained neural density model is trained using a first plurality of training instances including only successful attempts of the end effector grasping the at least one object in the environment of the robot, and wherein the second trained neural density model is trained using a second plurality of training instances including successful attempts of the end effector grasping the at least one object in the environment of the robot and unsuccessful attempts of the end effector grasping the at least one object in the environment of the robot; and generating, over the trained action prediction network including the normalized neural density layers, the predicted probability density.
11. The method of claim 1, wherein the candidate action to generate the end effector command is selected from a plurality of candidate actions by: applying the plurality of candidate actions to a trained critic model, wherein the trained critic model selects an optimal candidate action from the plurality of candidate actions, and providing the end effector command generated based on the optimal candidate action to one or more of the actuators of the robot.
12. A method of training an action prediction network, comprising: identifying, by one or more processors, a plurality of training examples generated based on sensor output from one or more robots during a plurality of grasp attempts by the robots, each of the training examples including training example input comprising: an image for a corresponding instance of time of a corresponding grasp attempt of the grasp attempts, the image capturing a robotic end effector and at least one environmental object at the corresponding instance of time, each of the training examples including training example output comprising a grasp success label indicative of success of the corresponding grasp attempt; applying, by one or more of the processors, the image to a processing network portion of the action prediction network to generate one or more features of the image, wherein the processing network includes one or more convolutional neural networks, a softmax layer, and one or more feed forward layers; applying, by the one or more processors, the one or more features of the image to a neural density model portion of the action prediction network to generate a predicted probability density of candidate actions, wherein the predicted probability density defines a plurality of candidate actions, each of the candidate actions having a respective probability that implementing the candidate action, and subsequently grasping, will result in a successful grasp of the object, wherein each of the plurality of candidate actions indicates at least a respective three-dimensional direction of movement for the grasping end effector, and wherein the neural density model portion of the action prediction network is downstream from the processing model portion of the action prediction network; determining a loss as a function of the grasp success label and the predicted probability density.
13. The method of claim 12, wherein the neural density model is a Gaussian mixture model (GMM), wherein the GMM includes: one or more linear layers which are trained to learn a set of GMM components, wherein the set of GMM components includes a center, a variance, and weights of one or more multivariate diagonal Gaussian mixtures.
14. The method of claim 12, wherein the neural density model is a real-valued non-volume preserving (real NVP) transformation model, wherein the real NVP transformation model includes: a plurality of affine coupling layers and a plurality of fully connected layers trained to learn a real NVP transformation, wherein the real NVP transformation is a bijective mapping between the current image and a latent space.
15. The method of claim 12, wherein the neural density model is a hybrid model, wherein the hybrid model includes: a plurality of affine coupling layers and a plurality of fully connected layers trained to learn a plurality of real NVP transformations, wherein each real NVP transformation is a bijective mapping between an object in the current image and a latent space, wherein each latent space includes one or more linear layers trained to learn a set of Gaussian mixture model components including a center, a variance, and weights of one or more multivariate diagonal Gaussian distributions.
16. A system comprising: a vision sensor viewing an environment; a trained action prediction network stored in one or more non-transitory computer readable media; at least one processor configured to: at a given iteration of visual servoing the a grasping end effector of a robot using the trained action prediction network to grasp the object in the environment: apply a current image capturing the grasping end effector of the robot in a current pose and the object in the environment of the robot, wherein the current image is captured by the vision sensor; generate, using the trained action prediction network, a predicted probability density of candidate actions, wherein the predicted probability density is generated using a neural density model portion of the action prediction network, wherein each of the candidate actions indicates at least a respective three-dimensional direction of movement for the grasping end effector, and wherein the predicted probability density defines, for each of the candidate actions, a respective probability that implementing the candidate action, and subsequently grasping, will result in a successful grasp of the object; select a given action, of the candidate actions, based on the generated predicted probability density; generate an end effector command based on the selected action; and provide the end effector command to one or more actuators of the robot.
</claims>
</document>
