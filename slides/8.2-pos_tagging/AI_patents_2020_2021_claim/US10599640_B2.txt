<document>

<filing_date>
2017-12-19
</filing_date>

<publication_date>
2020-03-24
</publication_date>

<priority_date>
2017-12-19
</priority_date>

<ipc_classes>
G06F16/242,G06F16/248,G06F16/51,G06F16/9535,G06K9/00,G06K9/20,G06K9/62,G06N20/00,G06N99/00
</ipc_classes>

<assignee>
AT&T INTELLECTUAL PROPERTY I (AMERICAN TELEPHONE AND TELEGRAPH COMPANY INTELLECTUAL PROPERTY I)
</assignee>

<inventors>
PRATT, JAMES
ZAVESKY, ERIC
INNES, TIMOTHY
BRADLEY, NIGEL
</inventors>

<docdb_family_id>
66814499
</docdb_family_id>

<title>
Predictive search with context filtering
</title>

<abstract>
A method may include a processing system detecting a biometric condition of a user exceeding a threshold, determining a triggering object associated with the biometric condition, where the triggering object is captured via a camera, and applying the triggering object and at least one personal parameter to a plurality of context filters. The plurality of context filters may provide a relevance topic based upon the triggering object and the at least one personal parameter. The processing system may further generate a plurality of search terms based upon the relevance topics that are determined from the plurality of context filters and present the plurality of search terms via a user interface.
</abstract>

<claims>
1. A device comprising: a processing system including at least one processor; a computer-readable medium storing instructions which, when executed by the processing system, cause the processing system to perform operations, the operations comprising: detecting a biometric condition of a user exceeding a threshold; determining a triggering object associated with the biometric condition, wherein the triggering object is captured via a camera; applying the triggering object and at least one personal parameter to a plurality of context filters, wherein each of the plurality of context filters provides a relevance topic based upon the triggering object and the at least one personal parameter; generating a plurality of search terms based upon the relevance topics that are determined from the plurality of context filters; and presenting the plurality of search terms via a user interface.
2. The device of claim 1, wherein the operations further comprise: receiving a selection of one of the plurality of search terms; applying the one of the plurality of search terms that is selected to a search engine; and presenting results from the search engine in response to the one of the plurality of search terms via the user interface.
3. The device of claim 2, wherein the operations further comprise: retraining at least one of the plurality of context filters based upon the selection.
4. The device of claim 1, wherein the operations further comprise: detecting a confirmatory condition relating to the triggering object, wherein the applying is performed in response to the detecting of the confirmatory condition.
5. The device of claim 4, wherein the confirmatory condition comprises at least one of: a verbal reference to the triggering object; a text reference to the triggering object; a gesture; or a maintenance of the triggering object within a field of view.
6. The device of claim 1, wherein the biometric condition of the user comprises at least one of: a heart rate; a blood pressure; a breathing rate; a blood oxygen concentration; a facial expression; a pupil dilation; a voice pitch; a voice tone; an activity level; a skin conductance; a body temperature; an electrocardiogram pattern; an electroencephalography scan pattern; or a diffuse optical tomography scan pattern.
7. The device of claim 1, wherein the biometric condition of the user is determined via at least one of: a heart rate monitor; a blood pressure monitor; an electrocardiogram device; a microphone; a second camera; a galvanic skin response device; an electroencephalography device; an event-related potential measurement device; or a diffuse optical tomography scanning device.
8. The device of claim 1, wherein the at least one personal parameter comprises at least one of: calendar information; location information; a communication history; a search history; a television viewing history; credit card transaction information; user preference information; or auditory information from a vicinity of the user.
9. The device of claim 1, wherein the determining the triggering object comprises: applying an image salience model to an image captured via the camera.
10. The device of claim 9, wherein the operations further comprise: retraining the image salience model based upon a user action pertaining to the plurality of search terms.
11. The device of claim 1, wherein the plurality of context filters comprises machine learning models to determine the relevance topics based upon the triggering object and the at least one personal parameter.
12. The device of claim 11, wherein the machine learning models are trained with training data associated with at least one of: the user; or a plurality of other users.
13. The device of claim 11, wherein each of the machine learning models is specific to a different theme of a plurality of themes.
14. The device of claim 13, wherein the plurality of themes includes: a family theme; a work theme; a sports theme; a relationship theme; a food theme; a shopping theme; or an entertainment theme.
15. The device of claim 1, wherein a subset of the plurality of context filters is configured to be a limited set of context filters, wherein the operations further comprise: selecting the limited set of context filters to be active context filters.
16. The device of claim 15, wherein the limited set of context filters is selected to be the active context filters based upon at least one of: a selection by the user; a time; or a location.
17. The device of claim 1, wherein the generating the plurality of search terms comprises, for each relevance topic, selecting auto-complete or auto-suggest terms associated with the relevance topic and the triggering object from a database.
18. The device of claim 1, wherein the camera is integrated with a wearable device, wherein the plurality of search terms is presented via the user interface comprising a display of the wearable device.
19. A non-transitory computer-readable medium storing instructions which, when executed by a processing system comprising at least one processor, cause the processing system to perform operations, the operations comprising: detecting a biometric condition of a user exceeding a threshold; determining a triggering object associated with the biometric condition, wherein the triggering object is captured via a camera; applying the triggering object and at least one personal parameter to a plurality of context filters, wherein each of the plurality of context filters provides a relevance topic based upon the triggering object and the at least one personal parameter; generating a plurality of search terms based upon the relevance topics that are determined from the plurality of context filters; and presenting the plurality of search terms via a user interface.
20. A method, comprising: detecting, by a processing system including at least one processor, a biometric condition of a user exceeding a threshold; determining, by the processing system, a triggering object associated with the biometric condition, wherein the triggering object is captured via a camera; applying, by the processing system, the triggering object and at least one personal parameter to a plurality of context filters, wherein each of the plurality of context filters provides a relevance topic based upon the triggering object and the at least one personal parameter; generating, by the processing system, a plurality of search terms based upon the relevance topics that are determined from the plurality of context filters; and presenting, by the processing system, the plurality of search terms via a user interface.
</claims>
</document>
