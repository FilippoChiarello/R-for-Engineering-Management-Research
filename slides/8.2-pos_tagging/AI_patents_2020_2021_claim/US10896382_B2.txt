<document>

<filing_date>
2015-08-07
</filing_date>

<publication_date>
2021-01-19
</publication_date>

<priority_date>
2014-08-07
</priority_date>

<ipc_classes>
G06N20/00,G06N7/00,G06N99/00
</ipc_classes>

<assignee>
OKINAWA INSTITUTE OF SCIENCE AND TECHNOLOGY SCHOOL CORPORATION
</assignee>

<inventors>
DOYA, KENJI
UCHIBE, EIJI
</inventors>

<docdb_family_id>
55263500
</docdb_family_id>

<title>
Inverse reinforcement learning by density ratio estimation
</title>

<abstract>
A method of inverse reinforcement learning for estimating cost and value functions of behaviors of a subject includes acquiring data representing changes in state variables that define the behaviors of the subject; applying a modified Bellman equation given by Eq. (1) to the acquired data: q(x)+gV(y)−V(x)=−ln{pi(y|x))/(p(y|x)} (1) where q(x) and V(x) denote a cost function and a value function, respectively, at state x, g represents a discount factor, and p(y|x) and pi(y|x) denote state transition probabilities before and after learning, respectively; estimating a density ratio pi(y|x)/p(y|x) in Eq. (1); estimating q(x) and V(x) in Eq. (1) using the least square method in accordance with the estimated density ratio pi(y|x)/p(y|x), and outputting the estimated q(x) and V(x).
</abstract>

<claims>
1. A computer implemented method of inverse reinforcement learning for estimating cost and value functions of behaviors of a subject, comprising: acquiring, from the subject, data representing changes in state variables that define the behaviors of the subject; applying a modified Bellman equation given by Eq. (1) to the acquired data: where q(x) and V(x) denote a cost function and a value function, respectively, at state x, V(y) denotes a value function at state y, and γ represents a discount factor, and p(y|x) and π(y|x) denote state transition probabilities before and after learning, respectively; estimating a density ratio in Eq. (1); estimating q(x) and V(x) in Eq. (1) using the least square method in accordance with the estimated density ratio and outputting, to forward reinforcement learning controllers for another subject, the estimated q(x) and V(x), wherein the method of inverse reinforcement learning predicts a preference in topic of articles that a user is likely to read from a series of articles the user selected in an Internet web surfing, and wherein said subject is the user, and said state variables that define the behaviors of the subject include topics of articles selected by the user while browsing each webpage.
2. The method according to claim 1, wherein the step of estimating the ratio includes using uLSIF (unconstrained Least Squares Importance Fitting).
3. The method according to claim 1, wherein the step of estimating the ratio includes using the Least Square Conditional Density Estimation (LSCDE).
4. The method according to claim 1, wherein the step of estimating the ratio includes using a logistic regression.
5. The method according to claim 1, wherein the step of estimating the ratio includes using a Gaussian process.
6. The method according to claim 1, wherein the step of estimating the cost function q(x) and value function V(x) includes using the least squares method with regularization.
7. A non-transitory storage medium storing instructions to cause a processor to perform an algorithm for inverse reinforcement learning for estimating cost and value functions of behaviors of a subject, said instructions causing the processor to perform the following steps: acquiring, from the subject, data representing changes in state variables that define the behaviors of the subject; applying a modified Bellman equation given by Eq. (1) to the acquired data: where q(x) and V(x) denote a cost function and a value function, respectively, at state x, V(y) denotes a value function at state y, and γ represents a discount factor, and p(y|x) and π(y|x) denote state transition probabilities before and after learning, respectively; estimating a density ratio in Eq. (1); estimating q(x) and V(x) in Eq. (1) using the least square method in accordance with the estimated density ratio and outputting, to forward reinforcement learning controllers for another subject, the estimated q(x) and V(x), wherein the algorithm of inverse reinforcement learning predicts a preference in topic of articles that a user is likely to read from a series of articles the user selected in an Internet web surfing, and wherein said subject is the user, and said state variables that define the behaviors of the subject include topics of articles selected by the user while browsing each webpage.
8. A system for inverse reinforcement learning for estimating cost and value functions of behaviors of a subject, comprising: a data acquisition unit to acquire, from the subject, data representing changes in state variables that define the behaviors of the subject; a processor with a memory, the processor and the memory are configured to: apply a modified Bellman equation given by Eq. (1) to the acquired data: where q(x) and V(x) denote a cost function and a value function, respectively, at state x, V(y) denotes a value function at state y, and γ represents a discount factor, and p(y|x) and π(y|x) denote state transition probabilities before and after learning, respectively; estimate a density ratio in Eq. (1); estimate q(x) and V(x) in Eq. (1) using the least square method in accordance with the estimated density ratio and an output interface that outputs, to forward reinforcement learning controllers for another subject, the estimated q(x) and V(x), wherein the system of inverse reinforcement learning predicts a preference in topic of articles that a user is likely to read from a series of articles the user selected in an Internet web surfing, and wherein said subject is the user, and said state variables that define the behaviors of the subject include topics of articles selected by the user while browsing each webpage.
9. A system for predicting a preference in topic of articles that a user is likely to read from a series of articles the user selected in an Internet web surfing, comprising: the system for inverse reinforcement learning implemented in a computer connected to the Internet for estimating cost and value functions of behaviors of a subject, comprising: a data acquisition unit to acquire data representing changes in state variables that define the behaviors of the subject; a processor with a memory, the processor and the memory are configured to: apply a modified Bellman equation given by Eq. (1) to the acquired data: where q(x) and V(x) denote a cost function and a value function, respectively, at state x, V(y) denotes a value function at state y, and γ represents a discount factor, and p(y|x) and π(y|x) denote state transition probabilities before and after learning, respectively; estimate a density ratio in Eq. (1); estimate q(x) and V(x) in Eq. (1) using the least square method in accordance with the estimated density ratio and an output interface that outputs the estimated q(x) and V(x), wherein said subject is the user, and said state variables that define the behaviors of the subject include topics of articles selected by the user while browsing each webpage, and wherein the processor causes an interface through which the user is browsing Internet websites to display a recommended article for the user to read in accordance with the estimated cost and value functions.
</claims>
</document>
