<document>

<filing_date>
2019-08-27
</filing_date>

<publication_date>
2020-05-07
</publication_date>

<priority_date>
2019-08-23
</priority_date>

<ipc_classes>
G06F3/01,G06F3/0488,G06K9/00,G06T19/00
</ipc_classes>

<assignee>
LG ELECTRONICS
</assignee>

<inventors>
LEE, JOONGHEE
</inventors>

<docdb_family_id>
67949337
</docdb_family_id>

<title>
XR DEVICE AND METHOD FOR CONTROLLING THE SAME
</title>

<abstract>
Disclosed are an extended reality (XR) device and a control method thereof, which are applicable to all of 5G communication technology field, a robot technology field, an autonomous driving technology field, and an AI technology field.
</abstract>

<claims>
1. A method for controlling an XR device provided with a plurality of cameras, comprising: capturing at least one real object located in a first direction through a first camera; displaying at least one virtual object and the at least one captured real object, wherein the displayed virtual object is changed based on the captured real object; in response to a first touch command recognized through a screen of the XR device, moving the at least one virtual object to a first position (i.e., a first in-screen position) contained in the screen, wherein the virtual object remains unchanged in size; capturing a user who is located in a second direction through a second camera, wherein the first direction and the second direction are different from each other; and in response to a second touch command recognized through the screen of the XR device and a motion of the user who is spaced apart from the screen of the XR device, moving the at least one virtual object to a second position (i.e., a second in-screen position) contained in the screen, wherein the virtual object is changed in size.
2. The method according to claim 1, further comprising: determining whether the XR device has moved by a predetermined range or greater by a sensor; if the XR device has not moved by the predetermined range or greater, moving the at least one virtual object to another position in response to the touch command and the user motion, wherein the virtual object is changed in size; and if the XR has moved by the predetermined range or greater, moving the at least one virtual object to the other position by reacting only to the touch command without reacting to the user motion, wherein the virtual object remains unchanged in size.
3. The method according to claim 2, further comprising: if the XR device has not moved by the predetermined range or greater, detecting change in size of a facial image of the user captured by the second camera; if reduction in the user facial image is detected, increasing the size of the virtual object; and if the enlarged facial image of the user is detected, reducing the size of the virtual object.
4. The method according to claim 3, further comprising: tracking a gaze direction of the user using the second camera.
5. The method according to claim 4, further comprising: if the tracked user gaze is directed to a specific virtual object touched by the second touch command during a first reference time period, grouping a plurality of virtual objects located in a first radius from a center point of the specific virtual object into a first group; and in response to a second touch command recognized through the screen of the XR device and a motion of the user who is spaced apart from the screen of the XR device, moving the plurality of virtual objects belonging to the first group to other positions, wherein the virtual objects are changed in size.
6. The method according to claim 5, further comprising: if the tracked user gaze is directed to a specific virtual object touched by the second touch command during a second reference time period, grouping a plurality of virtual objects located in a second radius from a center point of the specific virtual object into a second group; and in response to a second touch command recognized through the screen of the XR device and a motion of the user who is spaced apart from the screen of the XR device, moving the plurality of virtual objects belonging to the second group to other positions, wherein the virtual objects are changed in size, wherein the second reference time period is longer than the first reference time period, and the second radius is longer than the first radius.
7. The method according to claim 6, further comprising: displaying an option for cancelling some of the plurality of virtual objects belonging to the first group or the second group.
8. The method according to claim 7, further comprising: calculating a distance between each of some cancelled virtual objects and a specific virtual object touched by the second touch command; and updating a value of the first radius stored in a memory or a value of the second radius stored in the memory based on the calculation result.
9. The method according to claim 8, further comprising: determining whether at least one real object captured by the first camera is a robot connected through short range communication or 5G communication.
10. The method according to claim 9, further comprising: if the at least one real object captured by the first camera is the robot connected through short range communication or 5G communication, creating a specific virtual object corresponding to the robot; displaying the created specific virtual object; and changing the position of the specific virtual object based on at least one of the touch command and the user motion.
11. An extended reality (XR) device provided with a plurality of cameras, comprising: a first camera configured to capture at least one real object located in a first direction; a display module configured to display at least one virtual object and the at least one captured real object, wherein the displayed virtual object is changed based on the captured real object; a controller, in response to a first touch command recognized through a screen of the XR device, configured to move the at least one virtual object to a first position (i.e., a first in-screen position) contained in the screen, wherein the virtual object remains unchanged in size; and a second camera configured to capture a user who is located in a second direction, wherein the first direction and the second direction are different from each other, wherein the controller, in response to a second touch command recognized through the screen of the XR device and a motion of the user who is spaced apart from the screen of the XR device, moves the at least one virtual object to a second position (i.e., a second in-screen position) contained in the screen, wherein the virtual object is changed in size.
12. The XR device according to claim 11, further comprising: a sensor configured to determine whether the XR device has moved by a predetermined range or greater, wherein: if the XR device has not moved by the predetermined range or greater, the controller moves the at least one virtual object to other position in response to the touch command and the user motion, and if the XR has moved by the predetermined range or greater, the controller moves the at least one virtual object to other position by reacting only to the touch command without reacting to the user motion.
13. The XR device according to claim 12, wherein the controller includes: if the XR device has not moved by the predetermined range or greater, detecting change in size of a facial image of the user captured by the second camera; if reduction in the user facial image is detected, increasing the size of the virtual object; and if the enlarged facial image of the user is detected, reducing the size of the virtual object.
14. The XR device according to claim 13, wherein the second camera is configured to track a gaze direction of the user using the second camera.
15. The XR device according to claim 14, wherein the controller includes: if the tracked user gaze is directed to a specific virtual object touched by the second touch command during a first reference time period, grouping a plurality of virtual objects located in a first radius from a center point of the specific virtual object into a first group; and in response to a second touch command recognized through the screen of the XR device and a motion of the user who is spaced apart from the screen of the XR device, moving the plurality of virtual objects belonging to the first group to other positions.
16. The XR device according to claim 15, wherein the controller includes: if the tracked user gaze is directed to a specific virtual object touched by the second touch command during a second reference time period, grouping a plurality of virtual objects located in a second radius from a center point of the specific virtual object into a second group; and in response to a second touch command recognized through the screen of the XR device and a motion of the user who is spaced apart from the screen of the XR device, moving the plurality of virtual objects belonging to the second group to other positions, wherein the second reference time period is longer than the first reference time period, and the second radius is longer than the first radius.
17. The XR device according to claim 16, wherein the display module is configured to display an option for cancelling some of the plurality of virtual objects belonging to the first group or the second group.
18. The XR device according to claim 17, wherein the controller includes: calculating a distance between each of some cancelled virtual objects and a specific virtual object touched by the second touch command; and updating a value of the first radius stored in a memory or a value of the second radius stored in the memory based on the calculation result.
19. The XR device according to claim 18, wherein the controller is configured to determine whether at least one real object captured by the first camera is a robot connected through short range communication or 5G communication.
20. The XR device according to claim 19, wherein: the controller, if the at least one real object captured by the first camera is the robot connected through short range communication or 5G communication, creates a specific virtual object corresponding to the robot; and the display module displays the created specific virtual object, and displays the specific virtual object, a position of which is changed based on at least one of the touch command and the user motion.
</claims>
</document>
