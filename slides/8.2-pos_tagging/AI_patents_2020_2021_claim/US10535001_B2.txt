<document>

<filing_date>
2017-11-06
</filing_date>

<publication_date>
2020-01-14
</publication_date>

<priority_date>
2017-11-06
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,G06T7/00
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
CMIELOWSKI, LUKASZ G.
OSZAJEC, MAREK
SOBALA, WOJCIECH
CAKMAK, UMIT
</inventors>

<docdb_family_id>
66328664
</docdb_family_id>

<title>
Reducing problem complexity when analyzing 3-D images
</title>

<abstract>
A method for training a deep learning algorithm using N-dimensional data sets may be provided. Each data set comprises a plurality of N−1-dimensional data sets. The method comprises selecting a batch size and assembling an equally sized training batch. The samples are selected to be evenly distributed within said respective N-dimensional data sets. The method comprises also starting from a predetermined offset number, wherein the number of samples is equal to the selected batch size number, and feeding said training batches of N−1-dimensional samples into a deep learning algorithm for the training. Upon the training resulting in a learning rate that is below a predetermined level, selecting a different offset number for at least one of said N-dimensional data sets, and going back to the step of assembling. Upon the training resulting in a learning rate that is equal or higher than said predetermined level, the method stops.
</abstract>

<claims>
1. A method for training a deep learning algorithm using one or more N-dimensional data sets, wherein N is 2, 3 or 4, each of said N-dimensional data sets comprising a number of N−1-dimensional sub data sets, said method comprising: a) selecting a batch size number; b) assembling one or more equally sized training batches for each of said N-dimensional data sets, each of said training batches comprising a number of N−1-dimensional samples from said N-dimensional data sets, wherein said N−1-dimensional samples are selected out of said N-dimensional data sets using a jump size in order to be evenly distributed within their respective said N-dimensional data sets, starting from a predetermined offset number, wherein said number of N−1-dimensional samples is equal to said selected batch size number and is lower than said number of N−1-dimensional sub data sets, and wherein the jump size for selecting N−1-dimensional samples out of said N-dimensional data sets is determined by:
description="In-line Formulae" end="lead"?i=int((k+m−1)/(s−1)), wherein:description="In-line Formulae" end="tail"? k=a smallest number of N−1-dimensional samples available in any of said N-dimensional data sets, m=number of additional N−1-dimensional samples available in a specific one of said N-dimensional data sets, and s=number of selected N−1-dimensional samples from any of said N-dimensional data sets; c) feeding said training batches of said number of N−1-dimensional samples into a deep learning algorithm for a training; d) upon said training resulting in a learning rate that is below a predetermined level, selecting a different offset number for at least one of said N-dimensional data sets, and repeating said method from step b) onwards using an actual version of a deep learning algorithm model with one or more new ones of said training batches; and e) upon said training resulting in a learning rate that is equal or higher than said predetermined level, stopping said training using said N-dimensional data sets.
2. The method according to claim 1, wherein said deep learning algorithm is using a convolutional neural network.
3. The method according to claim 1, also comprising: upon not stopping said training in step e), adding non-neighboring randomly selected additional N−1-dimensional samples from each of said N-dimensional data sets to create an extended set of training batches; and repeating said method from step b) onwards using said actual version of a deep learning algorithm model with a new extended set of training batches.
4. The method according to claim 3, also comprising: upon said number of to be added non-neighboring randomly selected N−1-dimensional samples is smaller than a predefined percentage threshold number, selecting all N−1-dimensional samples.
5. The method according to claim 1, wherein said N-dimensions data sets comprise data required for a volumetric body visualization.
6. The method according to claim 5, wherein said volumetric body visualization is a result of an analysis using one of said techniques selected out of said group comprising X-ray, ultrasonic, MRT, CT and MRI.
7. The method according to claim 1, using an in-memory computing system for executing said method.
8. A system for training a deep learning algorithm using one or more N-dimensional data sets, wherein N is 2, 3 or 4, each of said N-dimensional data sets comprising a number of N−1-dimensional sub data sets, said system comprising: a selection unit adapted for selecting a batch size number; an assembling unit adapted for assembling one or more equally sized training batches for each of said N-dimensional data sets, each of said training batches comprising a number of N−1-dimensional samples from said N-dimensional data sets, wherein said N−1-dimensional samples are selected out of said N-dimensional data sets using a jump size in order to be evenly distributed within their respective said N-dimensional data sets, starting from a predetermined offset number, wherein said number of N−1-dimensional samples is equal to said selected batch size number and is lower than said number of N−1-dimensional sub data sets, and wherein the jump size for selecting N−1-dimensional samples out of said N-dimensional data sets is determined by:
description="In-line Formulae" end="lead"?i=int((k+m−1)/(s−1)), wherein:description="In-line Formulae" end="tail"? k=a smallest number of N−1-dimensional samples available in any of said N-dimensional data sets, m=number of additional N−1-dimensional samples available in a specific one of said N-dimensional data sets, and s=number of selected N−1-dimensional samples from any of said N-dimensional data sets; a supplying module adapted for feeding said training batches of said number of N−1-dimensional samples into a deep learning algorithm for a training; a feedback module adapted for, upon said training resulting in a learning rate that is below a predetermined level, selecting a different offset number for at least one of said N-dimensional data sets, and returning process control back to said assembling unit using an actual version of a deep learning algorithm model with one or more new ones of said training batches; and a termination detection module adapted for, upon said training resulting in a learning rate that is equal or higher than said predetermined level, stopping said training using said N-dimensional data sets.
9. The system according to claim 8, wherein said deep learning algorithm is adapted to use a convolutional neural network.
10. The system according to claim 8, wherein said feedback module is also adapted for returning process control back to said assembling module, and wherein said assembling unit is also adapted for, upon not stopping said training by said termination detection module, adding non-neighboring randomly selected additional N−1-dimensional samples from each of said N-dimensional data sets to create an extended set of training batches, and using said actual version of a deep learning algorithm model with a new extended set of training batches.
11. The system according to claim 10, wherein said assembling unit is also adapted for upon said number of to be added non-neighboring randomly selected N−1-dimensional samples is smaller than a predefined percentage threshold number, selecting all N−1-dimensional samples.
12. The system according to claim 8, wherein said N-dimensions data sets comprise data required for a volumetric body visualization.
13. The system according to claim 12, wherein said volumetric body visualization is a result of an analysis using one of said techniques selected out of said group comprising X-ray, ultrasonic, MRT, CT and MRI.
14. The system according to claim 8, comprising an in-memory computing system adapted for keeping all data required for said training a deep learning algorithm in main memory.
15. A computer program product for training a deep learning algorithm using one or more N-dimensional data sets, each of said N-dimensional data sets comprising a number of N−1-dimensional sub data sets, said computer program product comprising a computer readable storage medium having program instructions tangibly embodied therewith, said program instructions being executed by one or more computers to cause said one or more computers to: a) select a batch size number; b) assemble one or more equally sized training batches for each of said N-dimensional data sets, each of said training batches comprising a number of N−1-dimensional samples from said N-dimensional data sets, wherein said N−1-dimensional samples are selected out of said N-dimensional data sets using a jump size in order to be evenly distributed within their respective said N-dimensional data sets, starting from a predetermined offset number, wherein said number of N−1-dimensional samples is equal to said selected batch size number and lower than said number of N−1-dimensional sub data sets, and wherein the jump size for selecting N−1-dimensional samples out of said N-dimensional data sets is determined by:
description="In-line Formulae" end="lead"?i=int((k+m−1)/(s−1)), wherein:description="In-line Formulae" end="tail"? k=a smallest number of N−1-dimensional samples available in any of said N-dimensional data sets, m=number of additional N−1-dimensional samples available in a specific one of said N-dimensional data sets, and s=number of selected N−1-dimensional samples from any of said N-dimensional data sets; c) feed said training batches of said number of N−1-dimensional samples into a deep learning algorithm for a training; d) upon said training resulting in a learning rate that is below a predetermined level, select a different offset number for at least one of said N-dimensional data sets, and repeating said method from step b) onwards using an actual version of a deep learning algorithm model with one or more new ones of said training batches; and e) upon said training resulting in a learning rate that is equal or higher than said predetermined level, stop said training using said N-dimensional data sets.
</claims>
</document>
