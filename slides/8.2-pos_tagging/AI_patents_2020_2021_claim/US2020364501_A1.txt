<document>

<filing_date>
2020-06-19
</filing_date>

<publication_date>
2020-11-19
</publication_date>

<priority_date>
2017-12-21
</priority_date>

<ipc_classes>
G06K9/62,G06T5/50,G06T7/194,G06T7/40,G06T7/90
</ipc_classes>

<assignee>
TILITER PTY LTD
</assignee>

<inventors>
SAMPSON, Christopher
HERZ, Marcel
</inventors>

<docdb_family_id>
66992404
</docdb_family_id>

<title>
RETAIL CHECKOUT TERMINAL FRESH PRODUCE IDENTIFICATION SYSTEM
</title>

<abstract>
Disclosed are systems and methods including starting with a first number of images, generating a second number of images by digital operations on the first number of images, extracting features from the second number of images, and generating a classification model by training a neural network on the second number of images wherein the classification model provides a percentage likelihood of an image's categorisation, embedding the classification model in a processor and receiving an image for categorisation, wherein the processor is in communication with a POS system, the processor running the classification model to provide output to the POS system of a percentage likelihood of the image's categorisation.
</abstract>

<claims>
1. A method of image categorisation, comprising: in pre-processing: receiving a plurality of images of backgrounds in which items are to be recognised as a background image set, receiving an original number of images of items to be recognised in an original item image set, masking the background of the images in the original item image set to generate a masked item image set, digitally augmenting the masked item image set to generate an augmented masked item image set that includes a larger number of images of masked items than the masked item image set, superimposing each image of the augmented item image set on each of the images of the background image set to generate a plurality of new training images as part of a training image set, the training image set thereby providing sufficient quantitative variation to train a neural network, extracting features from the training image set, and generating a classification model by training a neural network on the training image set wherein the classification model provides a prediction of an image's categorisation; embedding the classification model in a processor; and receiving an image for categorisation, wherein the processor is in communication with a POS system, the processor running the classification model on the received image to provide output to the POS system of a prediction of the image's categorisation.
2. The method of claim 1 wherein in pre-processing, a pre-trained Convolution Neural Network (CNN) trained on large unrelated or separate data sets used as a feature detector.
3. The method of claim 1 wherein the neural network comprises a Fully-Connected Neural Network.
4. The method of claim 2 wherein the feature extraction comprises: a. the pre-trained CNN; b. Colour space histogram c. Texture features by numerical feature vector and d. Dominant Colour Segmentation.
5. The method of claim 1 wherein the POS system receives formatted communication of output by the classification model, the formatted output comprising a protocol for providing scores for the percentage likelihood of the image's category to the POS system.
6. An external system external to a Point-of-Sale (POS) system wherein the external system comprises: a processor configured for, in pre-processing: receiving a plurality of images of backgrounds in which items are to be recognised as a background image set, receiving an original number of images of items to be recognised in an original item image set, masking the background of the images in the original item image set to generate a masked item image set, digitally augmenting the masked item image set to generate an augmented masked item image set that includes a larger number of images of masked items than the masked item image set, superimposing each image of the augmented item image set on each of the images of the background image set to generate a plurality of new training images as part of a training image set, the training image set thereby providing sufficient quantitative variation to train a neural network; extracting features from the increased image set to generate a classification model by training a neural network on the increased image set; and running the classification model embedded in the processor to provide as output a prediction of the image's category; and generating a formatted communication as output comprising a protocol to the POS system, wherein the POS system receives the formatted communication of the output by a classification model of the external system.
7. The system of claim 6 wherein the classification model provides a percentage likelihood of the image's categorisation.
8. The system of claim 7 wherein in pre-processing, the neural network comprises a pre-trained Convolution Neural Network (CNN) trained on large unrelated or separate data sets used as a shape and edge detector.
9. The system of claim 7 wherein the neural network comprises a Fully-Connected Neural Network.
10. The system of claim 8 wherein the feature extraction comprises: a. The pre-trained CNN b. Colour space histogram c. Texture features by numerical feature vector and d. Dominant Colour Segmentation
11. A method for categorising products, comprising: receiving a plurality of images of backgrounds in which items are to be recognised as a background image set; receiving an original number of images of items to be recognised in an original item image set; masking the background of the images in the original item image set to generate a masked item image set; digitally augmenting the masked item image set to generate an augmented masked item image set that includes a larger number of images of masked items than the masked item image set; superimposing each image of the augmented item image set on each of the images of the background image set to generate a plurality of new training images as part of a training image set to thereby provide sufficient quantitative variation to train a neural network; feature extracting from the training image set wherein the feature extracting comprises running a pre-trained Convolution Neural Network (CNN) as a high-level edge and shape identifier; and generating a classification model by processing the second number of images by a neural network wherein the classification model provides a prediction of an image's categorisation.
12. The method of claim 11 wherein generating the classification model further comprises pre-processing feature extraction of the images in the training image set.
13. The method of claim 11 wherein the feature extraction comprises: a. The pre-trained CNN; b. Colour space histogram; c. Texture features by numerical feature vector; and d. Dominant Colour Segmentation.
14. The method of claim 11 wherein the neural network comprises a Fully-Connected Neural Network.
15. The method of claim 11 wherein the classification model is embedded in a processor that is external to a POS system and which is in communication with the POS system.
16. The method of claim 15 wherein the method includes transmitting formatted communication of output by the classification model to the POS system, the formatted output comprising a protocol for providing scores for the percentage likelihood of the image's category to the POS system.
17. A method for expanding an image data set, comprising: in pre-processing: receiving a plurality of images of settings in which items are to be recognised as a background image set; receiving an original number of images of items to be recognised in an original item image set; masking the background of the images in the original item image set to generate a masked item image set; digitally augmenting the masked item image set to generate an augmented masked item image set that includes a larger number of masked images of the items than the masked item image set; superimposing each of the images of the augmented item image set on each of the images of the background image set to generate a plurality of new training images as part of a training image set, the training image set thereby providing sufficient quantitative variation to train a neural network; extracting features from the increased image set; and processing the training image set by a neural network to generate a classification model for deployment; wherein segmentation of an image is not performed at deployment.
18. The method of claim 17 wherein at deployment, the method comprises capturing an image, extracting at least one feature from the image, and applying the extracted feature to the neural network of the classification model to generate a prediction of the image's categorisation.
19. The method of claim 17 wherein in pre-processing, the feature extraction is carried out by a pre-trained Convolution Neural Network (CNN) trained on large unrelated or separate data sets used as a shape and edge detector.
20. The method of claim 17 wherein the neural network comprises a Fully-Connected Neural Network.
21. The method of claim 19 wherein the feature extraction comprises: a. The pre-trained CNN; b. Colour space histogram; c. Texture features by numerical feature vector; and d. Dominant Colour Segmentation.
22. The method of claim 17 wherein for deployment, the classification model is embedded in a processor that is external to a POS system and which is in communication with the POS system.
23. The method of claim 22 wherein the POS system receives formatted communication of output by the classification model, the formatted output comprising a protocol for providing scores for the percentage likelihood of the image's category to the POS system.
24. A method of image categorisation, comprising: receiving a plurality of images of settings in which items are to be recognised as a background image set; receiving an original number of images of items to be recognised in an original item image set; masking the background of the images in the original item image set to generate a masked item image set; digitally augmenting the masked item image set to generate an augmented masked item image set that includes a larger number of images of masked items than the masked item image set; superimposing each image of the augmented item image set on each of the images of the background image set to generate a plurality of new training images as part of a training image set to thereby provide sufficient quantitative variation to train a neural network; extracting features from the images in the second image set in accordance with: a. a pre-trained Convolution Neural Network (CNN) trained on large unrelated or separate data sets used as a feature detector; b. Colour space histogram; c. Texture features by numerical feature vector; and d. Dominant Colour Segmentation; and training a neural network on the features extracted to generate a classification model wherein the classification model provides a prediction of an image's categorisation.
25. The method of claim 24, further comprising: embedding the classification model in a processor; and receiving an image for categorisation, wherein the processor is in communication with a POS system, the processor running the classification model to provide output to the POS system of a percentage likelihood of the image's categorisation.
26. The method of claim 25 wherein the POS system receives formatted communication of output by the classification model, the formatted output comprising a protocol for providing scores for the percentage likelihood of the image's category to the POS system.
</claims>
</document>
