<document>

<filing_date>
2018-09-12
</filing_date>

<publication_date>
2020-12-08
</publication_date>

<priority_date>
2018-09-12
</priority_date>

<ipc_classes>
G06F16/683,G10L15/00,G10L15/02,G10L15/05,G10L15/08,G10L15/18,G10L21/10
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
</assignee>

<inventors>
BHAT, VIMAL
McCormick, Manolya
Nun, Shai Ben
</inventors>

<docdb_family_id>
73653838
</docdb_family_id>

<title>
Audio locale mismatch detection
</title>

<abstract>
Systems, methods, and computer-readable media are disclosed for detecting a mismatch between the spoken language in an audio file and the audio language that is tagged as the spoken language in the audio file metadata. Example methods may include receiving a media file including spoken language metadata. Certain methods include generating an audio sample from the media file. Certain methods include generating a text translation of the audio sample based on the spoken language metadata. Certain methods include determining that the spoken language metadata does not match a spoken language in the audio sample based on the text translation. Certain methods include sending an indication that the spoken language metadata does not match the spoken language.
</abstract>

<claims>
That which is claimed is:
1. A method comprising: receiving, by one or more computer processors coupled to at least one memory, a media file comprising audio data and spoken language metadata, the spoken language metadata comprising an indication of an English language; extracting, by the one or more computer processors, an audio sample from the audio data of the media file; generating, by the one or more computer processors, a first text translation of the audio sample using a speech recognition engine based on the English language; determining, by the one or more computer processors, that the English language does not match a spoken language of the media file based on the first text translation of the audio sample; generating, by the one or more computer processors, a second text translation of the audio sample using the speech recognition engine based on a Spanish language; determining, by the one or more computer processors, that the Spanish language does match the spoken language of the media file based on the second text translation; and replacing, by the one or more computer processors, the indication of the English language in the spoken language metadata of the media file with a second indication of the Spanish language.
2. The method of claim 1, wherein determining that the English language does not match the spoken language further comprises: determining, by the one or more computer processors, that a confidence metric associated with the first text translation does not satisfy a threshold.
3. The method of claim 1, wherein determining that the English language does not match the spoken language further comprises: determining, by the one or more computer processors, an expected bigram frequency based on the English language; determining, by the one or more computer processors, an actual bigram frequency associated with the first text translation; and determining, by the one or more computer processors, that the actual bigram frequency does not match the expected bigram frequency.
4. The method of claim 1, wherein determining that the English language does not match the spoken language further comprises: receiving, by the one or more computer processors, a timed text asset associated with the media file; and determining, by the one or more computer processors, that the first text translation does not match the timed text asset.
5. A method comprising: receiving, by one or more computer processors coupled to at least one memory, a media file comprising audio data and spoken language metadata, the spoken language metadata comprising an indication of a first language; generating, by the one or more computer processors, an audio sample from the audio data of the media file; generating, by the one or more computer processors, a text translation of the audio sample based on the first language; determining, by the one or more computer processors, that the first language does not match a spoken language of the media file based on the text translation of the audio sample; determining, by the one or more computer processors, that a second language matches the spoken language of the media file; and replacing, by one or more computer processors, the indication of the first language in the spoken language metadata of the media file with a second indication of the second language.
6. The method of claim 5, wherein determining that the second language matches the spoken language of the media file comprises: generating, by the one or more computer processors, a second text translation of the audio sample based on the second language.
7. The method of claim 5, wherein replacing the indication of the first language in the spoken language metadata with the second indication of the second language comprises: generating, by the one or more computer processors, a second text translation of the audio sample based on the second language; determining, by the one or more computer processors, a first confidence metric associated with the text translation based on the first language; determining, by the one or more computer processors, a second confidence metric associated with the second text translation; and determining, by the one or more computer processors, that the second confidence metric is greater than the first confidence metric.
8. The method of claim 5, wherein determining that the first language does not match the spoken language further comprises: determining, by the one or more computer processors, a confidence metric associated with the text translation; and determining, by the one or more computer processors, that the confidence metric does not satisfy a threshold.
9. The method of claim 5, wherein determining that the first language does not match the spoken language further comprises: determining, by the one or more computer processors, an expected n-gram frequency based on the first language; determining, by the one or more computer processors, an actual n-gram frequency associated with the text translation; and determining, by the one or more computer processors, that the actual n-gram frequency does not match the expected n-gram frequency.
10. The method of claim 9, wherein the expected n-gram frequency comprises a first bigram frequency and the actual n-gram frequency comprises a second bigram frequency.
11. The method of claim 5, wherein determining that the first language does not match the spoken language further comprises: receiving, by the one or more computer processors, a timed text asset associated with the media file; and determining, by the one or more computer processors, that the text translation does not match the timed text asset.
12. The method of claim 5, wherein determining that the second language matches the spoken language of the media file comprises: generating, by the one or more computer processors, a second audio sample from the media file; and generating, by the one or more computer processors, a third text translation of the second audio sample based on the second language.
13. A device comprising: at least one memory that stores computer-executable instructions; and at least one processor configured to access the memory and execute the computer-executable instructions to: receive a media file comprising audio data and spoken language metadata, the spoken language metadata comprising an indication of a first language; generate an audio sample from the audio data of the media file; generate a text translation of the audio sample based on the first language; determine that the first language does not match a spoken language of the media file based on the text translation of the audio sample; determine that a second language matches the spoken language of the media file; and replace the indication of the first language in the spoken language metadata of the media file with a second indication of the second language.
14. The device of claim 13, wherein determining that the second language matches the spoken language of the media file comprises: generating, by the one or more computer processors, a second text translation of the audio sample based on the second language.
15. The device of claim 13, wherein replacing the indication of the first language in the spoken language metadata with the second indication of the second language comprises: generating, by the one or more computer processors, a second text translation of the audio sample based on the second language; determining, by the one or more computer processors, a first confidence metric associated with the text translation based on the first language; determining, by the one or more computer processors, a second confidence metric associated with the second text translation; and determining, by the one or more computer processors, that the second confidence metric is greater than the first confidence metric.
16. The device of claim 13, wherein determining that the first language does not match the spoken language further comprises: determining, by the one or more computer processors, a confidence metric associated with the text translation; and determining, by the one or more computer processors, that the confidence metric does not satisfy a threshold.
17. The device of claim 16, wherein the confidence metric is an average per word confidence, a standard deviation, or a mean squared error associated with the text translation.
18. The device of claim 13, wherein determining that the first language does not match the spoken language further comprises: determining, by the one or more computer processors, an expected n-gram frequency based on the first language; determining, by the one or more computer processors, an actual n-gram frequency associated with the text translation; and determining, by the one or more computer processors, that the actual n-gram frequency does not match the expected n-gram frequency.
19. The device of claim 18, wherein the expected n-gram frequency comprises a first bigram frequency and the actual n-gram frequency comprises a second bigram frequency.
20. The device of claim 13, wherein determining that the first language does not match the spoken language further comprises: receiving, by the one or more computer processors, a timed text asset associated with the media file; and determining, by the one or more computer processors, that the text translation does not match the timed text asset.
</claims>
</document>
