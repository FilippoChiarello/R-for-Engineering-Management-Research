<document>

<filing_date>
2018-09-26
</filing_date>

<publication_date>
2020-03-26
</publication_date>

<priority_date>
2018-09-26
</priority_date>

<ipc_classes>
G06K9/62,G06K9/72,G06N3/08,G06N99/00
</ipc_classes>

<assignee>
OATH CORPORATION
</assignee>

<inventors>
DE JUAN, PALOMA
PAPPU, AASISH
</inventors>

<docdb_family_id>
69883206
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR LEARNING SCENE EMBEDDINGS VIA VISUAL SEMANTICS AND APPLICATION THEREOF
</title>

<abstract>
The present teaching relates to method, system, and programming for responding to an image related query. Information related to each of a plurality of images is received, wherein the information represents concepts co-existing in the image. Visual semantics for each of the plurality of images are created based on the information related thereto. Representations of scenes of the plurality of images are obtained via machine learning, based on the visual semantics of the plurality of images, wherein the representations capture concepts associated with the scenes.
</abstract>

<claims>
We claim:
1. A method, implemented on a machine having at least one processor, storage, and a communication platform for responding to an image related query, comprising: receiving, via the communication platform, information related to each of a plurality of images, wherein the information represents concepts co-existing in the image; creating visual semantics for each of the plurality of images based on the information related thereto; and obtaining, via machine learning, representations of scenes of the plurality of images based on the visual semantics of the plurality of images, wherein the representations capture concepts associated with the scenes.
2. The method of claim 1, wherein the visual semantics created for each of the plurality of images includes an identifier of the image and one or more annotations of the concepts co-existing in the image.
3. The method of claim 2, wherein the identifier of the image provides a context of the visual semantics; and the one or more annotations specify the concepts co-existing in the image.
4. The method of claim 1, wherein the representations of scenes of the plurality of images correspond to scene embeddings.
5. The method of claim 1, wherein the representations of scenes of the plurality of images include a plurality of vectors for the annotations related to concepts co-existing in the plurality of images, identifiers of the plurality of images, and at least one combination thereof; and an artificial neural network (ANN) with a plurality of layers of nodes and connections therein connecting the nodes.
6. The method of claim 1, further comprising: receiving an image related query; obtaining a response to the image related query based on the representations obtained via machine learning.
7. The method of claim 6, wherein the image related query is directed to at least one of: a request to receive a summary of at least one concept that a query image provided with the image related query characterize; and a request to receive one or more images that are conceptually similar to the query image.
8. A system for responding to an image related query, the system comprising: a visual semantics generator implemented by a processor and configured to receive information related to each of a plurality of images, wherein the information represents concepts co-existing in the image, and create visual semantics for each of the plurality of images based on the information related thereto; and an image scene embedding training unit implemented by the processor and configured to obtain, via machine learning, representations of scenes of the plurality of images based on the visual semantics of the plurality of images, wherein the representations capture concepts associated with the scenes.
9. The system of claim 8, wherein the visual semantics created for each of the plurality of images includes an identifier of the image and one or more annotations of the concepts co-existing in the image.
10. The system of claim 9, wherein the identifier of the image provides a context of the visual semantics; and the one or more annotations specify the concepts co-existing in the image.
11. The system of claim 8, wherein the representations of scenes of the plurality of images correspond to scene embeddings.
12. The system of claim 8, wherein the representations of scenes of the plurality of images include a plurality of vectors for the annotations related to concepts co-existing in the plurality of images, identifiers of the plurality of images, and at least one combination thereof; and an artificial neural network (ANN) with a plurality of layers of nodes and connections therein connecting the nodes.
13. The system of claim 8, further comprising: a visual scene based query engine implemented by the processor and configured to receive an image related query, and obtain a response to the image related query based on the representations obtained via machine learning.
14. A machine readable and non-transitory medium having information including machine executable instructions stored thereon for responding to an image related query, wherein the information, when read by the machine, causes the machine to perform: receiving information related to each of a plurality of images, wherein the information represents concepts co-existing in the image; creating visual semantics for each of the plurality of images based on the information related thereto; and obtaining, via machine learning, representations of scenes of the plurality of images based on the visual semantics of the plurality of images, wherein the representations capture concepts associated with the scenes.
15. The medium of claim 14, wherein the visual semantics created for each of the plurality of images includes an identifier of the image and one or more annotations of the concepts co-existing in the image.
16. The medium of claim 15, wherein the identifier of the image provides a context of the visual semantics; and the one or more annotations specify the concepts co-existing in the image.
17. The medium of claim 14, wherein the representations of scenes of the plurality of images correspond to scene embeddings.
18. The medium of claim 14, wherein the representations of scenes of the plurality of images include a plurality of vectors for the annotations related to concepts co-existing in the plurality of images, identifiers of the plurality of images, and at least one combination thereof; and an artificial neural network (ANN) with a plurality of layers of nodes and connections therein connecting the nodes.
19. The medium of claim 14, wherein the information, when read by the machine, further causes the machine to perform: receiving an image related query; obtaining a response to the image related query based on the representations obtained via machine learning.
20. The medium of claim 19, wherein the image related query is directed to at least one of: a request to receive a summary of at least one concept that a query image provided with the image related query characterize; and a request to receive one or more images that are conceptually similar to the query image.
</claims>
</document>
