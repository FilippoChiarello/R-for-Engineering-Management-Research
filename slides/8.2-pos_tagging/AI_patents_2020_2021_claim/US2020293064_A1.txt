<document>

<filing_date>
2019-07-17
</filing_date>

<publication_date>
2020-09-17
</publication_date>

<priority_date>
2019-03-15
</priority_date>

<ipc_classes>
G05D1/00,G05D1/02,G06K9/00,G06K9/62,G06N3/04,G06T7/20
</ipc_classes>

<assignee>
NVIDIA CORPORATION
</assignee>

<inventors>
JANIS, PEKKA
NISTER, DAVID
PARK, MINWOO
Tong, Xin
Wu, Yue
Yang, Cheng-Chieh
</inventors>

<docdb_family_id>
72424627
</docdb_family_id>

<title>
TEMPORAL INFORMATION PREDICTION IN AUTONOMOUS MACHINE APPLICATIONS
</title>

<abstract>
In various examples, a sequential deep neural network (DNN) may be trained using ground truth data generated by correlating (e.g., by cross-sensor fusion) sensor data with image data representative of a sequences of images. In deployment, the sequential DNN may leverage the sensor correlation to compute various predictions using image data alone. The predictions may include velocities, in world space, of objects in fields of view of an ego-vehicle, current and future locations of the objects in image space, and/or a time-to-collision (TTC) between the objects and the ego-vehicle. These predictions may be used as part of a perception system for understanding and reacting to a current physical environment of the ego-vehicle.
</abstract>

<claims>
1. A method comprising: receiving image data representative of a sequence of images generated by an image sensor of a vehicle; applying the image data to a sequential deep neural network (DNN); computing, by the sequential DNN and based at least in part on the image data: a first output corresponding to a velocity, in world space, of an object depicted in one or more images of the sequence of images; and a second output corresponding to, in image space, a current location of the object and a future location of the object; and performing one or more operations by the vehicle based at least in part on at least one of the first output or the second output.
2. The method of claim 1, further comprising: computing a third output corresponding to an inverse of a time-to-collision (TTC) before the vehicle and the object are predicted to intersect; and computing the TTC using the inverse of the TTC, wherein the performing the one or more operations is further based at least in part on the third output.
3. The method of claim 1, further comprising generating a visualization of the third output, the visualization including a bounding shape corresponding to the object and a visual indicator corresponding to the bounding shape, the visual indicator representative of the TTC.
4. The method of claim 3, wherein the visual indicator includes a first type of visual indicator when the TTC is below a threshold time and a second type of visual indicator when the TTC is above the threshold time.
5. The method of claim 1, wherein the sequential DNN is trained using ground truth data generated by correlating training sensor data with training image data representative of a training sequences of images, the sensor data including at least one of LIDAR data from one or more LIDAR sensors, RADAR data from one or more RADAR sensors, SONAR data from one or more SONAR sensors, or ultrasonic data from one or more ultrasonic sensors.
6. The method of claim 5, wherein the training sensor data is correlated within the training image data using cross-sensor fusion.
7. The method of claim 1, wherein the current location includes a first pixel location within the image of a first origin of a first bounding shape corresponding to the object and the future location includes a second pixel location within the image of a second origin of a second bounding shape corresponding to the object.
8. The method of claim 7, wherein the current location further includes first dimension information corresponding to the first bounding shape and the future location further includes second dimension information corresponding to the second bounding shape.
9. The method of claim 1, wherein the current location includes a first pixel location within the image of a first origin of a first bounding shape corresponding to the object and the future location includes a translation value corresponding to a second pixel location within the image, relative to the first pixel location, of a second origin of a second bounding shape corresponding to the object.
10. The method of claim 9, wherein the current location further includes first dimension information corresponding to the first bounding shape and the future location further includes scale information relative to the first dimension information corresponding to the first bounding shape, the scale information used to determine second dimension information corresponding to the second bounding shape.
11. An autonomous vehicle system comprising: an image sensor configured to generate image data representative of a sequence of images; an inference component configured to: apply the sequence of images to a sequential deep neural network (DNN); and compute, using the sequential DNN and based at least in part on the sequence of images: a first output corresponding to a velocity, in world space, of an object depicted in one or more images of the sequence of images; and a second output corresponding to, in image space, a current location of the object and a future location of the object; and a control component configured to perform one or more operations for controlling the autonomous vehicle based at least in part on at least one of the first output or the second output.
12. The system of claim 11, wherein the inference component is further configured to: compute third output corresponding to an inverse of a time-to-collision (TTC) before the vehicle and the object are predicted to intersect; and compute the TTC using the inverse of the TTC, wherein the performing the one or more operations is further based at least in part on the third output.
13. The system of claim 11, wherein the current location is computed with respect to an image of the sequence of images and the future location of the object is computed with respect to the image and based at least in part on data representative of the object within one or more prior images of the sequence of images.
14. The system of claim 11, wherein the data includes locations and velocities of the object within the one or more prior images.
15. A method comprising: receiving sensor data corresponding to a location of an object, the sensor data including image data generated over a period of time and at least one of LIDAR data or RADAR data generated over the period of time; correlating the sensor data with the image data; based at least in part on the correlating, analyzing the sensor data that corresponds to the location of the object in the image to determine a velocity of the object at a time within the period of time; using the velocity and the location of the object to automatically generate ground truth data corresponding to an image represented by the image data; and training a sequential deep neural network (DNN) using the ground truth data.
16. The method of claim 15, further comprising: analyzing a first subset of the sensor data corresponding to the LIDAR data to determine a first velocity of the object at the time; analyzing a second subset of the sensor data corresponding to the RADAR data to determine a second velocity of the object at the time; comparing the first velocity to the second velocity; and based at least in part on the comparing, filtering out at least one of the first subset of the sensor data or the second subset of sensor data.
17. The method of claim 16, wherein the filtering out is based at least in part on a velocity difference between the first velocity and the second velocity being greater than a threshold velocity difference.
18. The method of claim 15, wherein a first location of the object in the image of the sequence of images is represented by a first bounding shape, a second location of the object in a second image of the sequence of images generated prior to the image is represented by a second bounding shape, and the method further comprises: determining a scale change between the first bounding shape and the second bounding shape; and wherein the using the location of the object to automatically generate the ground truth data includes using the scale change.
19. The method of claim 18, further comprising determining a time difference between the time and a prior time when the second image was generated, wherein the time difference is further used to automatically generate the ground truth data.
20. The method of claim 19, wherein the ground truth data that is automatically generated using the scale change and the time difference represents a ratio of the time difference to the scale change.
</claims>
</document>
