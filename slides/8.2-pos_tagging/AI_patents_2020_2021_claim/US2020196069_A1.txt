<document>

<filing_date>
2020-02-21
</filing_date>

<publication_date>
2020-06-18
</publication_date>

<priority_date>
2015-04-10
</priority_date>

<ipc_classes>
G06N3/08,G10L25/84,H04R25/00
</ipc_classes>

<assignee>
STARKEY LABORATORIES
</assignee>

<inventors>
ZHANG TAO
FITZ, KELLY
XU, BUYE
ABDOLLAHI, MOHAMMAD
</inventors>

<docdb_family_id>
55802190
</docdb_family_id>

<title>
NEURAL NETWORK-DRIVEN FREQUENCY TRANSLATION
</title>

<abstract>
Disclosed herein, among other things, are apparatus and methods for neural network-driven frequency translation for hearing assistance devices. Various embodiments include a method of signal processing an input signal in a hearing assistance device, the hearing assistance device including a receiver and a microphone. The method includes performing neural network processing to train a processor to identify acoustic features in a plurality of audio signals and predict target outputs for the plurality of audio signals, and using the trained processor to control frequency translation of the input signal.
</abstract>

<claims>
1. A system comprising: a hearing device including: a microphone configured to receive an input signal; hearing device electronics configured to perform frequency translation on the input signal to produce an output signal; and a receiver configured to provide the output signal to a wearer of the hearing device; and an external device including: a processor configured to perform machine learning to identify acoustic features in a plurality of audio signals and predict outputs for the plurality of audio signals by mapping input features of the plurality of audio signals to a set of algorithm parameters for producing frequency-lowered speech cues; and wireless communication electronics configured to transmit control signals to the hearing device, the control signals including the set of algorithm parameters, wherein the hearing device is configured to receive the control signals, and the hearing device electronics are configured to use the set of algorithm parameters to automatically recognize and translate speech sounds from the input signal to control the frequency translation of the input signal.
2. The system of claim 1, wherein the external device includes a smart phone.
3. The system of claim 1, wherein the external device includes a server in a cloud.
4. The system of claim 1, wherein the hearing device includes a hearing assistance device.
5. The system of claim 4, wherein the hearing assistance device is a hearing aid.
6. The system of claim 5, wherein the hearing aid is a behind-the-ear (BTE) hearing aid.
7. The system of claim 5, wherein the hearing aid is an in-the-ear (ITE) hearing aid.
8. The system of claim 5, wherein the hearing aid is an in-the-canal (ITC) hearing aid.
9. The system of claim 5, wherein the hearing aid is a completely-in-the-canal (CIC) hearing aid.
10. The system of claim 5, wherein the hearing aid is a receiver-in-canal (RIC) hearing aid.
11. The system of claim 5, wherein the hearing aid is a receiver-in-the-ear (RITE) hearing aid.
12. The system of claim 5, wherein the hearing aid is an invisible-in-canal (IIC) hearing aid.
13. A method of signal processing an input signal for a hearing device, the hearing device including a receiver and a microphone, the method comprising: using an external device to perform machine learning to identify acoustic features in a plurality of audio signals and predict outputs for the plurality of audio signals by mapping input features of the plurality of audio signals to a set of algorithm parameters for producing frequency-lowered speech cues; receiving control signals from the external device at the hearing device, the control signals including the set of algorithm parameters; and using the hearing device to automatically recognize and translate speech sounds from the input signal to control frequency translation of the input signal using the set of algorithm parameters.
14. The method of claim 13, wherein using an external device to perform machine learning to identify acoustic features in a plurality of audio signals and predict outputs for the plurality of audio signals includes identifying relationships between noisy speech input and frequency lowered cues.
15. The method of claim 13, wherein using an external device to perform machine learning to identify acoustic features in a plurality of audio signals and predict outputs for the plurality of audio signals includes detecting consonants in speech that are masked by background noise.
16. The method of claim 13, wherein using an external device to perform machine learning to identify acoustic features in a plurality of audio signals and predict outputs for the plurality of audio signals includes using a recurrent network to provide context.
17. The method of claim 13, wherein using an external device to perform machine learning to identify acoustic features in a plurality of audio signals and predict outputs for the plurality of audio signals includes using a decimated sequence of features to provide context.
18. The method of claim 13, wherein the hearing device includes a hearing aid.
19. The method of claim 13, wherein the external device includes a smart phone.
20. The method of claim 13, wherein the external device includes a server in a cloud.
</claims>
</document>
