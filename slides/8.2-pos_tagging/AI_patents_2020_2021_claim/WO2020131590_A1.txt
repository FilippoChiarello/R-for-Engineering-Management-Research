<document>

<filing_date>
2019-12-13
</filing_date>

<publication_date>
2020-06-25
</publication_date>

<priority_date>
2018-12-20
</priority_date>

<ipc_classes>
G06F3/01,G06F3/16
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
CARTWRIGHT, LUKE
KWOK, ALTON
NEAL, RICHARD WILLIAM
</inventors>

<docdb_family_id>
69160381
</docdb_family_id>

<title>
VOICE COMMAND EXECUTION FROM AUXILIARY INPUT
</title>

<abstract>
A computing system is provided. The computing system includes a processor of a display device configured to execute one or more programs. The processor is configured to receive, from a user, a voice command, a first auxiliary input from a first sensor, and a second auxiliary input from a second sensor. The processor is configured to, for each of a plurality of objects in the user's field of view in an environment, determine a first set of probability factors with respect to the first auxiliary input and a second set of probability factors with respect to the second auxiliary input. Each probability factor in the first and second sets indicates a likelihood that respective auxiliary inputs are directed to one of the plurality of objects. The processor is configured to determine a target object based upon the probability factors and execute the command on the target object.
</abstract>

<claims>
1. A method for use with a computing device, comprising:
at a processor of a display device:
receiving a voice command from a user;
receiving a first auxiliary input from the user from a first sensor and a second auxiliary input from the user from a second sensor;
determining a first set of probability factors for each of a plurality of objects in the user's field of view in an environment with respect to the first auxiliary input from the user and a second set of probability factors for each of the plurality of objects with respect to the second auxiliary input from the user, each probability factor in the first and second sets indicating a likelihood that respective auxiliary inputs are directed to a respective one of the plurality of objects;
determining a target object from among the plurality of objects based upon the probability factors of the first and second auxiliary inputs; and
executing the command on the target object.
2. The method of claim 1, wherein determining the target object includes:
computing a probability score for each of the plurality of objects by multiplying a respective first probability factor by a respective second probability factor; and
choosing an object with a highest probability score as the target object from among the plurality of objects.
3. The method of claim 2, wherein the object with the highest probability score is chosen as the target object when the highest probability score is above a predetermined confidence threshold above which the voice command is deemed to be sufficiently associated with an object for identification as the target object.
4. The method of claim 3, wherein determining the target object further includes, when the highest probability score is below the predetermined confidence threshold, querying the user for a disambiguation input indicating the target object.
5. The method of claim 1, wherein each of the first auxiliary input and the second auxiliary input includes one or more of a non-semantic input of the command, a pose of the user, a gesture of the user, a gaze direction of the user, and a controller input of a controller operated by the user.
6. The method of claim 1, wherein each of the first and second probability factors is determined from weights assigned to each auxiliary input type prior to receiving the voice command from the user, the weights assigned by at least one method selected from the group consisting of calculating weights based on experimental data from a sample of users, assigning weights heuristically, and training a machine learning model on a training data set.
7. The method of claim 6, wherein
training the machine learning model on the training data set to determine weights is executed in a training phase prior to receiving the voice command from the user, and
the training data set includes paired input-output data, including auxiliary inputs and voice command inputs paired with user selected target objects.
8. The method of claim 7, further comprising:
receiving the voice command, first and second auxiliary inputs, determining the first and second set of probability factors, and determining the target object occur during a run time phase, wherein
determining the first and second sets of probability factors includes inputting the first and second auxiliary inputs, respectively, into the trained machine learning model, and
determining the target object includes reading an output of the trained machine learning model that indicates a probability for each of the plurality of objects for identification of the target object;
further comprising:
determining whether the target object was correctly identified based on subsequent user input; and
updating the machine learning model based on the subsequent user input.
9. The method of claim 1, wherein the target object is a virtual object or real object.
10. The method of claim 1, further comprising prioritizing each of the first and second auxiliary inputs from the user according to a context associated with the environment.
11. A computing system, comprising:
a processor of a display device configured to execute one or more programs, the processor configured to:
receive a voice command from a user;
receive a first auxiliary input from the user from a first sensor and a second auxiliary input from the user from a second sensor;
determine a first set of probability factors for each of a plurality of objects in the user's field of view in an environment with respect to the first auxiliary input from the user and a second set of probability factors for each of the plurality of objects with respect to the second auxiliary input from the user, each probability factor in the first and second sets indicating a likelihood that respective auxiliary inputs are directed to a respective one of the plurality of objects;
determine a target object from among the plurality of objects based upon the probability factors of the first and second auxiliary inputs; and
execute the command on the target object.
12. The system of claim 11, wherein determining the target object includes:
computing a probability score for each of the plurality of objects by multiplying a respective first probability factor by a respective second probability factor; and
choosing an object with a highest probability score as the target object from among the plurality of objects.
13. The system of claim 12, wherein the object with the highest probability score is chosen as the target object when the highest probability score is above a predetermined confidence threshold above which the voice command is deemed to be sufficiently associated with an object for identification as the target object.
14. The system of claim 13, wherein determining the target object further includes the processor configured to, when the highest probability score is below the predetermined confidence threshold, query the user for a disambiguation input indicating the target object.
15. The system of claim 11, wherein each of the first auxiliary input and the second auxiliary input includes one or more of a non-semantic input of the command, a pose of the user, a gesture of the user, a gaze direction of the user, and a controller input of a controller operated by the user.
</claims>
</document>
