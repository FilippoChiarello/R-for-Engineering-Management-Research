<document>

<filing_date>
2019-06-21
</filing_date>

<publication_date>
2020-12-24
</publication_date>

<priority_date>
2019-06-21
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
SIEMENS
SIEMENS
</assignee>

<inventors>
WU, ZIYAN
ERNST, JAN
PENG, KUAN-CHUAN
KARANAM, SRIKRISHNA
</inventors>

<docdb_family_id>
67211910
</docdb_family_id>

<title>
DOMAIN ADAPTATION AND FUSION USING TASK-IRRELEVANT PAIRED DATA IN SEQUENTIAL FORM
</title>

<abstract>
A process for data classification for dual-domain data includes defining a neural network pipelines for source domain and target domain of a data classifier to learn a dual-domain task of interest. The data classifier is trained by simulating a target representation during a first training session using dual-domain task-irrelevant data pairs for input, and performing a domain adaptation in a second training session while simultaneously feeding task irrelevant data pairs and applying a first loss function, initialized with parameters of the source domain pipeline trained by the first training session. The source pipeline is shared for a joint training using a second loss function to generate a second source domain representation with task relevant data inputs. Task-relevant data in the target domain is unavailable for training the data classifier, and at least one of the training data sets is in sequential form.
</abstract>

<claims>
What is claimed is:
1. A system comprising: a memory having computer readable instructions; and one or more processors for executing the computer readable instructions, the computer readable instructions controlling the one or more processors to perform operations comprising: defining a source pipeline and a target pipeline of a data classifier for learning a dual domain task of interest, wherein each of the source pipeline and the target pipeline comprises a neural network unit; wherein available training data includes task-relevant source domain data and a plurality of task-irrelevant data pairs, each task-irrelevant data pair representing one unit of source domain data and one unit of target domain data; wherein task-relevant data in the target domain is unavailable for training the data classifier; training the data classifier, comprising: simulating a target representation during a first training session (si) of the source domain pipeline by simultaneously feeding the source domain pipeline and the target domain pipeline with a plurality of dual-domain task-irrelevant data pairs and keeping parameters fixed in the target domain pipeline, while using a first loss function to adapt a source representation of the source domain pipeline to a fixed target domain
representation of the target domain pipeline, wherein upon convergence of a loss value from the loss function to a predetermined threshold, the trained source representation is a simulated target representation; performing a domain adaptation to transfer abstract features between the source domain and the target domain by performing a second training session (s2) of the source domain pipeline against the fixed target domain pipeline while simultaneously feeding task irrelevant data pairs and applying the first loss function, wherein the second training session (s2) is initialized with the parameters of the source domain pipeline trained by the first training session (si), wherein the source pipeline is shared for a joint training using a second loss function to generate a second source domain representation using task relevant data inputs; wherein at least one of the training data sets is in sequential form.
2. The system of claim 1, wherein the data classifier training further comprises:
performing a domain fusion to generate a joint classifier by performing a third training (s3) of the source domain pipeline initialized by the state of the second source domain representation and using task relevant source domain data as input, wherein a concatenation of the source domain pipeline is performed with the target domain pipeline initialized by the state of the simulated target representation of the first training session (si), wherein a loss function drives the training from the fixed parameters of the target domain pipeline to the source domain pipeline.
3. The system of claim 2, wherein the operations further comprise:
testing the joint classifier using task-relevant source domain data for inputs to the source domain pipeline and task-relevant target domain data, wherein the joint classifier produces a prediction of a task of interest objective.
4. The system of claim 1 , wherein the neural network unit used for training data sets in sequential form is configured as a recurrent neural network (R N).
5. The system of claim 1, wherein the source domain data is sequential words of documents, and the target domain data is depth scan images that share at least one common feature with the documents.
6. A method comprising:
defining a source pipeline and a target pipeline of a data classifier for learning a dual domain task of interest, wherein each of the source pipeline and the target pipeline comprises a neural network unit; wherein available training data includes task-relevant source domain data and a plurality of task-irrelevant data pairs, each task-irrelevant data pair representing one unit of source domain data and one unit of target domain data; wherein task-relevant data in the target domain is unavailable for training the data classifier; training the data classifier, comprising: simulating a target representation during a first training session (si) of the source domain pipeline by simultaneously feeding the source domain pipeline and the target domain pipeline with a plurality of dual-domain task-irrelevant data pairs and keeping parameters fixed in the target domain pipeline, while using a first loss function to adapt a source representation of the source domain pipeline to a fixed target domain
representation of the target domain pipeline, wherein upon convergence of a loss value from the loss function to a predetermined threshold, the trained source representation is a simulated target representation; performing a domain adaptation to transfer abstract features between the source domain and the target domain by performing a second training session (s2) of the source domain pipeline against the fixed target domain pipeline while simultaneously feeding task irrelevant data pairs and applying the first loss function, wherein the second training session (s2) is initialized with the parameters of the source domain pipeline trained by the first training session (si), wherein the source pipeline is shared for a joint training using a second loss function to generate a second source domain representation using task relevant data inputs; wherein at least one of the training data sets is in sequential form.
7. The method of claim 6, wherein the data classifier training further comprises:
performing a domain fusion to generate a joint classifier by performing a third training (s3) of the source domain pipeline initialized by the state of the second source domain representation and using task relevant source domain data as input, wherein a concatenation of the source domain pipeline is performed with the target domain pipeline initialized by the state of the simulated target representation of the first training session (si), wherein a loss function drives the training from the fixed parameters of the target domain pipeline to the source domain pipeline.
8. The method of claim 7, wherein the operations further comprise:
testing the joint classifier using task-relevant source domain data for inputs to the source domain pipeline and task-relevant target domain data, wherein the joint classifier produces a prediction of a task of interest objective.
9. The method of claim 6, wherein the neural network unit used for training data sets in sequential form is configured as a recurrent neural network (R N).
10. The method of claim 6, wherein the source domain data is sequential words of documents, and the target domain data is depth scan images that share at least one common feature with the documents.
11. A computer program product for data classification comprising a computer readable storage medium having program instructions stored thereon and executable by a system comprising one or more processors, to cause the system to perform steps comprising: defining a source pipeline and a target pipeline of a data classifier for learning a dual domain task of interest, wherein each of the source pipeline and the target pipeline comprises a neural network unit; wherein available training data includes task-relevant source domain data and a plurality of task-irrelevant data pairs, each task-irrelevant data pair representing one unit of source domain data and one unit of target domain data; wherein task-relevant data in the target domain is unavailable for training the data classifier; training the data classifier, comprising: simulating a target representation during a first training session (si) of the source domain pipeline by simultaneously feeding the source domain pipeline and the target domain pipeline with a plurality of dual-domain task-irrelevant data pairs and keeping parameters fixed in the target domain pipeline, while using a first loss function to adapt a source representation of the source domain pipeline to a fixed target domain
representation of the target domain pipeline, wherein upon convergence of a loss value from the loss function to a predetermined threshold, the trained source representation is a simulated target representation; performing a domain adaptation to transfer abstract features between the source domain and the target domain by performing a second training session (s2) of the source domain pipeline against the fixed target domain pipeline while simultaneously feeding task irrelevant data pairs and applying the first loss function, wherein the second training session (s2) is initialized with the parameters of the source domain pipeline trained by the first training session (si), wherein the source pipeline is shared for a joint training using a second loss function to generate a second source domain representation using task relevant data inputs; wherein at least one of the training data sets is in sequential form.
12. The computer program product of claim 11, wherein the data classifier training further comprises:
performing a domain fusion to generate a joint classifier by performing a third training (s3) of the source domain pipeline initialized by the state of the second source domain representation and using task relevant source domain data as input, wherein a concatenation of the source domain pipeline is performed with the target domain pipeline initialized by the state of the simulated target representation of the first training session (si), wherein a loss function drives the training from the fixed parameters of the target domain pipeline to the source domain pipeline.
13. The computer program product of claim 12, wherein the operations further comprise: testing the joint classifier using task-relevant source domain data for inputs to the source domain pipeline and task-relevant target domain data, wherein the joint classifier produces a prediction of a task of interest objective.
14. The computer program product of claim 11 , wherein the neural network unit used for training data sets in sequential form is configured as a recurrent neural network (RNN).
15. The computer program product of claim 11, wherein the source domain data is sequential words of documents, and the target domain data is depth scan images that share at least one common feature with the documents.
</claims>
</document>
