<document>

<filing_date>
2019-11-15
</filing_date>

<publication_date>
2020-05-28
</publication_date>

<priority_date>
2018-11-19
</priority_date>

<ipc_classes>
B60R21/0134,B60W30/08,B60W30/14,B60W30/18,B60W40/02,B60W50/02,B60W50/14
</ipc_classes>

<assignee>
MICRON TECHNOLOGY
</assignee>

<inventors>
GIL, GOLOV
</inventors>

<docdb_family_id>
70728790
</docdb_family_id>

<title>
SENSOR FUSION TO DETERMINE RELIABILITY OF AUTONOMOUS VEHICLE OPERATION
</title>

<abstract>
A method for an autonomous vehicle includes: receiving first object data from a first sensor module; receiving second object data from a second sensor module; comparing the first object data to the second object data; determining, based on comparing the first object data to the second object data, whether the first object data corresponds to the second object data; and in response to determining that the first object data does not correspond to the second object data, performing an action for the autonomous vehicle.
</abstract>

<claims>
What is claimed is:
1. An autonomous vehicle comprising:
a first sensor module comprising a first sensor and a first processing device, wherein the first processing device evaluates sensor data from the first sensor to provide first object data;
a second sensor module comprising a second sensor and a second
processing device, wherein the second processing device evaluates sensor data from the second sensor to provide second object data; a central processing device configured to receive object data provided by sensor modules of the autonomous vehicle; and
memory storing instructions configured to instruct the central processing
device to:
receive the first object data;
receive the second object data;
make a comparison of the first object data to the second object data; determine, based on the comparison, whether the first object data corresponds to the second object data; and in response to determining that the first object data does not
correspond to the second object data, perform an action associated with operation of the autonomous vehicle.
2. The autonomous vehicle of claim 1 , wherein the first object data includes a type of object selected from a plurality of object types.
3. The autonomous vehicle of claim 2, wherein the first sensor module further includes first memory storing data for a neural network, and wherein the first processing device uses the neural network to evaluate the sensor data from the first sensor.
4. The autonomous vehicle of claim 1 , wherein making the comparison of the first object data to the second object data comprises performing a statistical correlation of the first object data to the second object data.
5. The autonomous vehicle of claim 1 , wherein the instructions are further configured to instruct the central processing device to determine a context of the autonomous vehicle based on sensor data from at least one sensor other than the first sensor and the second sensor, and wherein determining whether the first object data corresponds to the second object data is based in part on the context.
6. The autonomous vehicle of claim 1 , further comprising a host processing
device, wherein performing the action comprises sending a signal to the host processing device, and wherein sending the signal causes the host processing device to change a configuration of a vehicle system of the autonomous vehicle.
7. The autonomous vehicle of claim 1 , wherein the first sensor is a lidar or radar sensor, and the second sensor is a camera.
8. The autonomous vehicle of claim 1 , wherein the first processing device is a system on chip, a field-programmable gate array, a graphics processing unit, or an application specific integrated circuit.
9. The autonomous vehicle of claim 1 , wherein the first object data comprises a position of an object detected by the first sensor module, and the second object data comprises a position of an object detected by the second sensor module.
10. The autonomous vehicle of claim 1 , wherein determining whether the first object data corresponds to the second object data comprises comparing a number of times that the first object data matches the second object data to a threshold.
11 . The autonomous vehicle of claim 10, wherein the instructions are further configured to instruct the central processing device to determine whether a current time for the autonomous vehicle corresponds to daytime or nighttime, and wherein the threshold is a first threshold during the daytime, and is a second threshold during the nighttime.
12. The autonomous vehicle of claim 10, wherein the threshold is a predetermined percentage of a total number of comparisons.
13. The autonomous vehicle of claim 1 , wherein performing the action associated with operation of the autonomous vehicle comprises sending a signal that causes a host processing device of the autonomous vehicle to perform a diagnostic test of at least one of the first sensor module or the second sensor module.
14. The autonomous vehicle of claim 1 , wherein the first object data includes a first type of detected object, wherein the second object data includes a second type of detected object, and wherein making the comparison of the first object data to the second object data comprises comparing the first type to the second type.
15. A method for an autonomous vehicle, the method comprising:
receiving first object data from a first sensor module;
receiving second object data from a second sensor module;
comparing the first object data to the second object data;
determining, based on comparing the first object data to the second object data, whether the first object data corresponds to the second object data; and
in response to determining that the first object data does not correspond to the second object data, performing an action for the autonomous vehicle.
16. The method of claim 15, wherein the first sensor module comprises a
processing device configured to process raw data from a sensor.
17. The method of claim 15, wherein the first object data comprises a position and object type for an object detected by the first sensor module, and the second object data comprises a position and object type for an object detected by the second sensor module.
18. The method of claim 15, wherein the first object data corresponds to an object detected by a sensor of the first sensor module, the second object data corresponds to an object detected by a sensor of the second sensor module, and comparing the first object data to the second object data comprises determining a statistical match between the first object data and the second object data.
19. The method of claim 18, wherein determining whether the first object data corresponds to the second object data comprises determining whether a statistical detection relationship is maintained between the first object data and the second object data.
20. The method of claim 19, wherein performing the action for the autonomous vehicle comprises at least one of the following:
triggering a safety hazard signal;
providing an alert to a passenger;
sending a communication to a central server that monitors vehicle operation for a plurality of vehicles including the autonomous vehicle;
disabling an autonomous mode of operation for the autonomous vehicle; requiring a person to take over control of the autonomous vehicle; or automatically stopping the autonomous vehicle.
</claims>
</document>
