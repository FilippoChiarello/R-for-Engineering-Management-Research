<document>

<filing_date>
2018-10-19
</filing_date>

<publication_date>
2020-04-23
</publication_date>

<priority_date>
2018-10-19
</priority_date>

<ipc_classes>
G06F3/01,G06F3/16,G06K9/00,G06N20/00,G06N99/00,G10L15/06,G10L15/065,G10L15/22,G10L15/24,G10L15/25,G10L15/28,H04R1/04
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
BOSS, GREGORY J.
RAKSHIT, SARBAJIT K.
FOX, JEREMY R.
ANDERS, KELLEY
</inventors>

<docdb_family_id>
70279170
</docdb_family_id>

<title>
Digital assistant user interface amalgamation
</title>

<abstract>
An approach is provided that receives, from a user, an amalgamation at a digital assistant. The amalgamation includes one or more words spoken by the user that are captured by a digital microphone and a set of digital images corresponding to one or more gestures that are performed by the user with the digital images captured by a digital camera. The system then determines an action that is responsive to the amalgamation and then performs the determined action.
</abstract>

<claims>
1. A method implemented by an information handling system that includes a processor and a memory accessible by the processor, the method comprising: receiving, from a user, an amalgamation at a digital assistant, wherein the amalgamation includes one or more words spoken by the user and captured by a microphone and a set of digital images corresponding to one or more gestures performed by the user captured by a digital camera; determining an action responsive to the amalgamation; performing, by the digital assistant, the determined action.
2. The method of claim 1 further comprising: training a machine learning system, wherein the training includes the action and the corresponding amalgamation.
3. The method of claim 1 further comprising: receiving a facial expression from the user responsive to the performance of the determined action; determining, based on the facial expression, that the determined action is incorrect and responsively: receiving, from the user, one or more further amalgamations at the digital assistant; indicating a responsive action to each of the amalgamations; receiving a reactive user feedback to the indicated responsive action; identifying, based on the reactive user feedback, a selected one of the further amalgamations that corresponds to a desired action; storing the selected amalgamation and the identified desired action in a data store.
4. The method of claim 3 wherein the reactive user feedback is a facial expression of the user captured by the digital camera.
5. The method of claim 3 further comprising: receiving, from the user, a second amalgamation at the digital assistant, wherein the second amalgamation includes a second set of one or more words spoken by the user and captured by the microphone and a second set of digital images corresponding to a second set of one or more gestures performed by the user captured by a digital camera; identifying that the second amalgamation matches the stored selected amalgamation and responsively receiving the desired action from the data store; and performing, by the digital assistant, the identified desired action.
6. The method of claim 3 wherein the selected amalgamation and the identified desired action are stored in a question-answering (QA) system.
7. The method of claim 1 further comprising: determining a meaning of the gesture included in the amalgamation; determining a set of ingested words related to the received spoken words, wherein the set of ingested words correspond to the determined meaning of the gesture; and identify the action based on the set of ingested words and the meaning of the gesture.
8. An information handling system comprising: one or more processors; a memory coupled to at least one of the processors; a digital microphone accessible by at least one of the processors; a digital camera accessible by at least one of the processors; and a set of computer program instructions stored in the memory and executed by at least one of the processors in order to perform actions comprising: receiving, from a user, an amalgamation at a digital assistant, wherein the amalgamation includes one or more words spoken by the user and captured by a microphone and a set of digital images corresponding to one or more gestures performed by the user captured by a digital camera; determining an action responsive to the amalgamation; performing, by the digital assistant, the determined action.
9. The information handling system of claim 8 wherein the actions further comprise: training a machine learning system, wherein the training includes the action and the corresponding amalgamation.
10. The information handling system of claim 8 wherein the actions further comprise: receiving a facial expression from the user responsive to the performance of the determined action; determining, based on the facial expression, that the determined action is incorrect and responsively: receiving, from the user, one or more further amalgamations at the digital assistant; indicating a responsive action to each of the amalgamations; receiving a reactive user feedback to the indicated responsive action; identifying, based on the reactive user feedback, a selected one of the further amalgamations that corresponds to a desired action; storing the selected amalgamation and the identified desired action in a data store.
11. The information handling system of claim 10 wherein the reactive user feedback is a facial expression of the user captured by the digital camera.
12. The information handling system of claim 10 wherein the actions further comprise: receiving, from the user, a second amalgamation at the digital assistant, wherein the second amalgamation includes a second set of one or more words spoken by the user and captured by the microphone and a second set of digital images corresponding to a second set of one or more gestures performed by the user captured by a digital camera; identifying that the second amalgamation matches the stored selected amalgamation and responsively receiving the desired action from the data store; and performing, by the digital assistant, the identified desired action.
13. The information handling system of claim 10 wherein the selected amalgamation and the identified desired action are stored in a question-answering (QA) system.
14. The information handling system of claim 8 wherein the actions further comprise: determining a meaning of the gesture included in the amalgamation; determining a set of ingested words related to the received spoken words, wherein the set of ingested words correspond to the determined meaning of the gesture; and identify the action based on the set of ingested words and the meaning of the gesture.
15. A computer program product stored in a computer readable storage medium, comprising computer program code that, when executed by an information handling system, performs actions comprising: receiving, from a user, an amalgamation at a digital assistant, wherein the amalgamation includes one or more words spoken by the user and captured by a microphone and a set of digital images corresponding to one or more gestures performed by the user captured by a digital camera; determining an action responsive to the amalgamation; performing, by the digital assistant, the determined action.
16. The computer program product of claim 15 wherein the actions further comprise: training a machine learning system, wherein the training includes the action and the corresponding amalgamation.
17. The computer program product of claim 15 wherein the actions further comprise: receiving a facial expression from the user responsive to the performance of the determined action; determining, based on the facial expression, that the determined action is incorrect and responsively: receiving, from the user, one or more further amalgamations at the digital assistant; indicating a responsive action to each of the amalgamations; receiving a reactive user feedback to the indicated responsive action; identifying, based on the reactive user feedback, a selected one of the further amalgamations that corresponds to a desired action; storing the selected amalgamation and the identified desired action in a data store.
18. The computer program product of claim 17 wherein the reactive user feedback is a facial expression of the user captured by the digital camera.
19. The computer program product of claim 17 wherein the actions further comprise: receiving, from the user, a second amalgamation at the digital assistant, wherein the second amalgamation includes a second set of one or more words spoken by the user and captured by the microphone and a second set of digital images corresponding to a second set of one or more gestures performed by the user captured by a digital camera; identifying that the second amalgamation matches the stored selected amalgamation and responsively receiving the desired action from the data store; and performing, by the digital assistant, the identified desired action.
20. The computer program product of claim 17 wherein the selected amalgamation and the identified desired action are stored in a question-answering (QA) system.
</claims>
</document>
