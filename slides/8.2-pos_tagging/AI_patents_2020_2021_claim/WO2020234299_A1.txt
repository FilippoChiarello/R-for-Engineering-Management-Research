<document>

<filing_date>
2020-05-19
</filing_date>

<publication_date>
2020-11-26
</publication_date>

<priority_date>
2019-05-23
</priority_date>

<ipc_classes>
G06N3/00,G06N3/04,G06N3/08,G06N5/00
</ipc_classes>

<assignee>
DEEPMIND TECH LTD
</assignee>

<inventors>
BARTUNOV SERGEY
LILLICRAP TIMOTHY PAUL
OSINDERO SIMON
RAE JACK WILLIAM
</inventors>

<docdb_family_id>
70779766
</docdb_family_id>

<title>
ENERGY-BASED ASSOCIATIVE MEMORY NEURAL NETWORKS
</title>

<abstract>
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for implementing associative memory. In one aspect a system comprises an associative memory neural network to process an input to generate an output that defines an energy corresponding to the input. A reading subsystem retrieves stored information from the associative memory neural network. The reading subsystem performs operations including receiving a given, i.e. query, input and retrieving a data element from the associative memory neural network that is associated with the given input. The retrieving is performed by iteratively adjusting the given input using the associative memory neural network.
</abstract>

<claims>
1. A system comprising:
an associative memory neural network having a plurality of associative memory parameters that is configured to process an input to generate an output that defines an energy corresponding to the input; and
a reading subsystem that is configured to perform operations comprising:
receiving a given input;
retrieving a target input that is associated with the given input from the associative memory neural network by adjusting the given input using the associative memory neural network, comprising, at each of one or more iterations:
processing the given input, using the associative memory neural network and in accordance with current values of the associative memory parameters, to generate an energy corresponding to the given input;
determining a gradient of a reading objective function that depends on the energy corresponding to the given input with respect to the given input; and
using the gradient to adjust the given input; and
providing the given input after a final iteration of the one or more iterations as the target input that is associated with the given input.
2. The system of claim 1, further comprising a writing subsystem that is configured to perform operations comprising:
receiving one or more inputs to be stored;
storing the inputs using the associative memory neural network, comprising, at each of one or more iterations:
processing each of the inputs, using the associative memory neural network and in accordance with current values of the plurality of associative memory parameters, to generate a respective energy corresponding to each of the inputs;
determining gradients of a writing objective function that depends on the energy corresponding to each of the inputs with respect to a specified subset of the associative memory parameters; and using the gradients to adjust the current values of the specified subset of the associative memory parameters.
3. The system of claim 2, wherein the writing objective function depends on a respective gradient of the energy corresponding to each of the inputs with respect to the specified subset of the associative memory parameters.
4. The system of claim 3, wherein the writing objective function depends on a respective magnitude of the gradient of the energy corresponding to each of the inputs with respect to the specified subset of associative memory parameters.
5. The system of any one of claims 2-4, wherein the writing objective function depends on a difference between (i) current values of the specified subset of the associative memory parameters, and (ii) initial values of the specified subset of the associative memory parameters.
6. The system of any one of claims 2-5, wherein the specified subset of the associative memory parameters is a proper subset of the associative memory parameters.
7. The system of claim 6, wherein the specified subset of the associative memory parameters comprises only the associative memory parameters corresponding to a proper subset of a set of neural network layers of the associative memory neural network.
8. The system of claim 7, wherein the proper subset of the set of neural network layers of the associative memory neural network comprise one or more highest layers of the associative memory neural network.
9. The system of any preceding claim, wherein the associative memory neural network is configured to process inputs that comprise one or more of: image data, text data, or audio data.
10. The system of any preceding claim, wherein the associative memory neural network comprises one or more convolutional neural network layers.
11. The system of any preceding claim, wherein the associative memory neural network comprises one or more recurrent neural network layers.
12. The system of any preceding claim, wherein using the gradient to adjust the given input by the reading subsystem comprises using the gradient to adjust the given input in accordance with a gradient descent optimization rule by the reading subsystem.
13. The system of claim 12, wherein the gradient descent optimization rule is a Nesterov momentum gradient descent optimization rule.
14. The system of any preceding claim, wherein the given input received by the reading subsystem is a partially known or distorted representation of the target input that is associated with the given input.
15. The system of any one of claims 2-14, wherein:
the reading subsystem has one or more reading subsystem parameters that are used in retrieving target inputs from the associative memory neural network, the writing subsystem has one or more writing subsystem parameters that are used in storing inputs using the associative memory neural network, the reading objective function has one or more reading objective function parameters, and the writing objective function has one or more writing objective function parameters; and
the system further comprises a training subsystem that is configured to jointly train the reading subsystem parameters, the writing subsystem parameters, the reading objective function parameters, the writing objective function parameters, and the associative memory parameters based on a reconstruction error objective function using meta-learning techniques.
16. The system of claim 15, wherein the reconstruction error objective function characterizes a similarity between: (i) a training input that is stored using the associative memory neural network by the writing subsystem, and (ii) a retrieved input that is provided by the reading subsystem as the training input.
17. The system of any one of claims 15-16, wherein using reading subsystem parameters in retrieving a target input from the associative memory neural network comprises scaling the gradients of the reading objective function using reading subsystem parameters.
18. The system of any one of claims 15-17, wherein using the writing subsystem parameters to store an input using the associative memory neural network comprises scaling the gradients of the writing objective function using writing subsystem parameters.
19. The system of any one of claims 15-18, wherein the meta-learning technique is a modelagnostic meta-learning (MAML) technique.
20. The system of any preceding claim, wherein the reading subsystem is configured to perform a predetermined number of iterations to retrieve the target input that is associated with the given input.
21. The system of any one of claims 2-20, wherein the writing subsystem is configured to perform a predetermined number of iterations to store the inputs using the associative memory neural network.
22. A method performed by one or more data processing apparatus comprising operations of the respective system of any one of claims 1-21.
23. One or more non-transitory computer storage media storing instructions that when executed by one or more computers cause the one or more computers to perform operations of the respective system of any one of claims 1-21.
</claims>
</document>
