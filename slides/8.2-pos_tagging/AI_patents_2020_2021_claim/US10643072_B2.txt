<document>

<filing_date>
2019-03-14
</filing_date>

<publication_date>
2020-05-05
</publication_date>

<priority_date>
2015-08-31
</priority_date>

<ipc_classes>
G06K9/00,G06K9/46,G06K9/62,H04N5/33
</ipc_classes>

<assignee>
CAPE ANALYTICS
</assignee>

<inventors>
KOTTENSTETTE, RYAN
LORENZEN, PETER
GEDIKLI, SUAT
</inventors>

<docdb_family_id>
58188238
</docdb_family_id>

<title>
Systems and methods for analyzing remote sensing imagery
</title>

<abstract>
Disclosed systems and methods relate to remote sensing, deep learning, and object detection. Some embodiments relate to machine learning for object detection, which includes, for example, identifying a class of pixel in a target image and generating a label image based on a parameter set. Other embodiments relate to machine learning for geometry extraction, which includes, for example, determining heights of one or more regions in a target image and determining a geometric object property in a target image. Yet other embodiments relate to machine learning for alignment, which includes, for example, aligning images via direct or indirect estimation of transformation parameters.
</abstract>

<claims>
1. A method of determining heights of one or more regions in a target image, comprising: receiving, at an extractor: training images, wherein each of the training images includes one or more regions, wherein each region includes one or more pixels; a set of parameters comprising an orientation of an illumination source associated with the training images; and labels that indicate a height for each pixel of the one or more regions of each training image; creating, at the extractor, a regression model configured to determine a height of a region based on the training images, the labels, and the set of parameters; receiving, at the extractor, a target image comprising a target region; and determining, at the extractor using the regression model, a height of the target region in the target image.
2. The method of claim 1, wherein each of the training images and the target image is one of a red-green-blue, panchromatic, infrared, ultraviolet, multi-spectral, or hyperspectral image.
3. The method of claim 1, wherein receiving the training images comprises receiving the training images from one or more sensors at fixed positions.
4. The method of claim 3, wherein receiving the training images further comprises receiving orientations of the sensors relative to one another if the training images are from two or more sensors.
5. The method of claim 1, wherein receiving the training images further comprises receiving the training images from one or more sensors, and wherein the set of parameters further comprises at least one of one or more positions of the one or more sensors, time, date, latitude, or longitude.
6. The method of claim 1, wherein the training images are projected onto a same geographic surface that defines the one or more regions.
7. The method of claim 6 further comprising receiving, at the extractor, parameters related to positions of one or more sensors configured to capture the training images.
8. The method of claim 1, further comprising receiving an orientation of a target illumination source associated with the target image, wherein the height of the target region is determined based on the orientation of the target illumination source.
9. The method of claim 5, wherein the target image is received from a target sensors, the method further comprising receiving a set of target parameters, the set of target parameters comprising at least one of: a position of the target sensor, an orientation of a target illumination source associated with the target image, a target time, a target date, a target latitude, or a target longitude; wherein the height of the target region in the target image is further determined based on the set of target parameters.
10. The method of claim 1, further comprising: modelling shadows using a geometry-based model, the height of the target region, and an orientation of a target illumination source associated with the target image; and comparing the modelled shadows with shadows extracted from the target image for the target region.
11. The method of claim 1, wherein the target image comprises a nadir image.
12. The method of claim 1, wherein the target image comprises a monocular image.
13. The method of claim 1, wherein the regression model is created based on the orientation of the illumination source associated with the training images.
14. The method of claim 1, further comprising, after determining the height of the target region, determining a dense reconstruction of the target region by applying an interpolation technique to the height of the target region.
15. The method of claim 14, wherein the interpolation technique is a Markov random field.
16. The method of claim 1, wherein the labels are derived from LIDAR measurements.
17. The method of claim 16, wherein the LIDAR measurements comprise sparse 3D LIDAR points.
18. The method of claim 1, wherein the labels are derived from stereo measurements.
19. The method of claim 1, wherein the regression model comprises a convolutional neural network.
20. The method of claim 1, wherein the height of the target region comprises a heat map.
</claims>
</document>
