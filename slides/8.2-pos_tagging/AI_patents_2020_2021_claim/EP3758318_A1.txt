<document>

<filing_date>
2020-03-20
</filing_date>

<publication_date>
2020-12-30
</publication_date>

<priority_date>
2019-06-28
</priority_date>

<ipc_classes>
H04L12/721,H04L12/931,H04L12/947
</ipc_classes>

<assignee>
INTEL CORPORATION
</assignee>

<inventors>
SRINIVASAN, ARVIND
DAMA, Jonathan
HUGGAHALLI, Ramakrishna
PAPADANTONAKIS, Karl
PENARANDA CEBRIAN, Roberto
MCCORMICK, Jim
NAEIMI, Helia
SOUTHWORTH, Robert
</inventors>

<docdb_family_id>
69953779
</docdb_family_id>

<title>
SHARED MEMORY MESH FOR SWITCHING
</title>

<abstract>
Examples are described herein that relate to a mesh in a switch fabric. The mesh can include one or more buses that permit operations (e.g., read, write, or responses) to continue in the same direction, drop off to a memory, drop off a bus to permit another operation to use the bus, or receive operations that are changing direction. A latency estimate can be determined at least for operations that drop off from a bus to permit another operation to use the bus or receive and channel operations that are changing direction. An operation with a highest latency estimate (e.g., time of traversing a mesh) can be permitted to use the bus, even causing another operation, that is not to change direction, to drop off the bus and re-enter later.
</abstract>

<claims>
1. A packet switching apparatus comprising: a first node and a second node connected to the first node, wherein the first node is to permit a first operation to continue to the second node in a same direction unless a second operation is available for transfer at the first node to the second node, wherein the first node is to select the second operation based on the second operation having a highest latency estimate and wherein the same direction comprises a north-south direction, south-north direction, east-west direction, or west-east direction.
2. The apparatus of claim 1, comprising: a third node coupled to the first node and a memory device coupled to the first node, wherein the first node comprises a drop off router and a transmit queue, wherein either or both of the drop off router and the transmit queue is to sort operations to permit output of an operation with a highest latency estimate and wherein the first node is to: deliver the first operation to the memory device if the first operation has reached a destination memory device, deliver the first operation to the third node if the operation encounters a turning operation, or buffer the first operation in a queue if backpressure is applied by a node connected to the first node.
3. The apparatus of claim 2, wherein the first node is to permit an operation to continue in a same direction if the transmit queue includes no operation.
4. The apparatus of one of the previous claims, comprising a node cache and wherein an operation comprises a read response data that is written as one copy in the node cache and output using one or more reads from the node cache.
5. The apparatus of one of the previous claims, wherein an operation comprises one or more of: a write request, write response, read request, read response, data, or a packet.
6. The apparatus of one of the previous claims, comprising an egress subsystem to egress packets from the first node to a network.
7. The apparatus of any of claims 1-6, comprising a server, data center, rack, or blade.
8. A method performed using a mesh, the method comprising: receiving an operation at a node; permitting an operation to pass through to another node if the operation is to proceed in a same direction and no contention is encountered; and causing buffering of the operation in a drop off router if contention is encountered, wherein contention comprises: the operation has reached a destination memory device, the operation encounters a turning operation, or backpressure is applied by another node connected to the node.
9. The method of claim 8, comprising: queueing one or more operations in a queue for transfer to another node and prioritizing output from the queue of an operation with highest latency estimate to another node.
10. The method of claim 8 or 9, wherein an operation comprises one or more of: a write request, write response, read request, read response, data, or a packet.
11. The method of any of claims 8-10, comprising: providing separate credits for backpressure for unicast operations and for multicast operations.
12. A system comprising: at least one ingress port; a mesh; at least one egress port, wherein the mesh comprises: a first node; a second node coupled to the first node; a third node coupled to the first node; and a memory device coupled to the first node, wherein the first node comprises a drop off router to sort operations to permit output of an operation with a highest latency estimate and wherein the first node is to: permit pass-through to the second node of an operation that is to proceed in a same direction to a next node or cause buffering in a drop off router of the operation if contention is encountered, wherein contention comprises: the operation has reached a destination memory device, the operation encounters a turning operation, backpressure is applied by a node connected to the first node.
13. The system of claim 12, wherein the first node comprises a transmit queue to receive packets from the drop off router, the transmit queue to provide: output of an operation to the second node, output of an operation to the third node, or output to the memory device and wherein the transmit queue is to prioritize output of an operation with highest latency estimate to the second node or the third node.
14. The system of claim 12 or 13, comprising a node cache and wherein: an operation comprises a read response data that is written as one copy in the node cache and output using one or more reads from the node cache and a multicast edge cache to provide a cache for a port to reduce re-requests for cached data.
15. The system of any of claims 12-14, wherein an operation comprises one or more of: a write request, write response, read request, read response, data, or a packet.
</claims>
</document>
