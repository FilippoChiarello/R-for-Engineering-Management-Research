<document>

<filing_date>
2019-08-28
</filing_date>

<publication_date>
2020-03-05
</publication_date>

<priority_date>
2018-08-30
</priority_date>

<ipc_classes>
B60W40/09,G06Q30/02
</ipc_classes>

<assignee>
HONDA MOTOR COMPANY
</assignee>

<inventors>
GOTO, TAKERU
</inventors>

<docdb_family_id>
69640861
</docdb_family_id>

<title>
LEARNING DEVICE, LEARNING METHOD, AND STORAGE MEDIUM
</title>

<abstract>
A learning device includes a planner configured to generate information indicating an action of a vehicle, and a reward deriver configured to derive a plurality of individual rewards obtained by evaluating each of a plurality of pieces of information to be evaluated, which include feedback information obtained from a simulator or an actual environment by inputting information based on the information indicating the action of the vehicle to the simulator or the actual environment, and derive a reward for the action of the vehicle on the basis of the plurality of individual rewards. The planner performs reinforcement learning that optimizes the reward derived by the reward deriver.
</abstract>

<claims>
1. A learning device comprising: a planner configured to generate information indicating an action of a vehicle; and a reward deriver configured to derive a plurality of individual rewards obtained by evaluating each of a plurality of pieces of information to be evaluated, which include feedback information obtained from a simulator or an actual environment by inputting information based on the information indicating the action of the vehicle to the simulator or the actual environment, and derive a reward for the action of the vehicle on the basis of the plurality of individual rewards, wherein the planner performs reinforcement learning that optimizes the reward derived by the reward deriver.
2. The learning device according to claim 1, wherein the reward deriver derives the individual reward by applying each of rules of which evaluation characteristics are different from each other to at least a part of the plurality of pieces of information to be evaluated.
3. The learning device according to claim 2, wherein the reward deriver derives the individual reward by applying any of a plurality of reward functions of which distribution shapes of the individual rewards for a relation to a target value are different from each other to at least a part of the plurality of pieces of information to be evaluated.
4. The learning device according to claim 1, wherein the reward deriver calculates the reward for the action of the vehicle by multiplying the plurality of individual rewards with each other.
5. The learning device according to claim 1, wherein the feedback information includes at least a part of a speed, an acceleration, and a lateral position of the vehicle.
6. The learning device according to claim 1, wherein the information to be evaluated includes a risk derived on the basis of the action of the vehicle.
7. The learning device according to claim 3, wherein the plurality of reward functions include a reward function that returns a predetermined value in a case in which an input value matches a target value and returns a smaller value as an absolute value of a difference between the input value and the target value increases.
8. The learning device according to claim 3, wherein the plurality of reward functions include a reward function that returns a predetermined value in a case in which an input value matches a target value and returns a smaller value as an absolute value of a difference between the input value and the target value increases, however, a degree to which the individual reward for the difference between the input value on a side where the input value exceeds the target value is reduced and the target value is greater than a degree to which the individual reward for a difference between the input value on a side where the input value is less than the target value and the target value is reduced.
9. The learning device according to claim 3, wherein the plurality of reward functions include a reward function that returns a predetermined value in a case in which an input value is equal to or greater than a target value and returns a smaller value as an absolute value of a difference between the input value and the target value increases in a case in which the input value is less than the target value.
10. The learning device according to claim 3, wherein the plurality of reward functions include a reward function that returns a predetermined value in a case in which an input value is equal to or less than a target value and returns a smaller value as an absolute value of a difference between the input value and the target value increases in a case in which the input value is greater than the target value.
11. The learning device according to claim 3, wherein the plurality of reward functions include a reward function that returns an example of a predetermined value in a case in which an input value is within a target range and returns a smaller value as an absolute value of a difference between the input value and an upper limit or a lower limit of the target range increases.
12. The learning device according to claim 3, wherein the plurality of reward functions include a reward function that returns a larger value as an input value approaches any of two or more target values.
13. The learning device according to claim 12, wherein the reward function returning the larger value as the input value approaches any of the two or more target values returns different individual rewards according to which of the two or more target values the input value matches.
14. A learning method of causing a computer to: generate information indicating an action of a vehicle; derive a plurality of individual rewards obtained by evaluating each of a plurality of pieces of information to be evaluated, which include feedback information obtained from a simulator or an actual environment by inputting information based on the information indicating the action of the vehicle to the simulator or the actual environment; derive a reward for the action of the vehicle on the basis of the plurality of individual rewards; and perform reinforcement learning that optimizes the derived reward.
15. A computer-readable non-transitory storage medium storing a program that causes a computer to: generate information indicating an action of a vehicle; derive a plurality of individual rewards obtained by evaluating each of a plurality of pieces of information to be evaluated, which include feedback information obtained from a simulator or an actual environment by inputting information based on the information indicating the action of the vehicle to the simulator or the actual environment; derive a reward for the action of the vehicle on the basis of the plurality of individual rewards; and perform reinforcement learning that optimizes the derived reward.
</claims>
</document>
