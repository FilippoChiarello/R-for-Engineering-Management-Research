<document>

<filing_date>
2019-07-26
</filing_date>

<publication_date>
2020-12-24
</publication_date>

<priority_date>
2019-06-21
</priority_date>

<ipc_classes>
G06F9/50,G06N3/063,G06N3/10
</ipc_classes>

<assignee>
HONGFUJIN PRECISION ELECTRONICS(TIANJIN)CO.
</assignee>

<inventors>
LIU, CHENG-YUEH
</inventors>

<docdb_family_id>
73795378
</docdb_family_id>

<title>
METHOD FOR CONFIGURING DEEP LEARNING PROGRAM AND CONTAINER MANAGER
</title>

<abstract>
A container manager used in a method for configuring a deep learning program acquires deep learning program, and analyzes at least one performance indicator from the acquired deep learning program and sends the at least one performance indicator to a server. The server determines a hardware configuration and a container image according to the at least one performance indicator, generates a label containing the name of the server, the determined hardware configuration, and the container image, and sends the label to the container manager. The container manager receives the label from the server, and determines whether the label contain the name of the server, and deploys a container for the deep learning program if the label contains the name of the server.
</abstract>

<claims>
1. A container manager comprising: a processor; and a non-transitory storage medium coupled to the processor and configured to store a plurality of instructions, which cause the container manager to: acquire a deep learning program; analyze at least one performance indicator from the acquired deep learning program and send the at least one performance indicator to a server, wherein the server determines a hardware configuration and a container image according to the at least one performance indicator, generates a container image label containing the name of the server, the determined hardware configuration and the container image, and sends the container image label to the container manager; and receive the container image label from the server, and determine whether the container image label contain the name of the server, and deploy a container for the deep learning program according to the hardware configuration and the container image if the container image label contains the name of the server.
2. The container manager as recited in claim 1, wherein the plurality of instructions are further configured to cause the container manager to: use an event tracking tool to analyze the at least one performance indicator from the deep learning program.
3. The container manager as recited in claim 1, wherein the plurality of instructions are further configured to cause the container manager to: use the event tracking tool to analyze the at least one performance indicator from the deep learning program by a http web protocol function.
4. The container manager as recited in claim 2, wherein the event tracking tool can be a Swarm-Oriented Function Call Analysis, or a Flame Graph.
5. The container manager as recited in claim 1, wherein the at least one performance indicator comprises a forward propagation time, a backward time, a data replication time from a host to a graphics processing unit (GPU), and a data replication time from the host to the GPU.
6. A method for configuring deep learning program comprising: a container manager acquiring a deep learning program; the container manager analyzing at least one performance indicator from the acquired deep learning program and sending the at least one performance indicator to a server; the server determining a hardware configuration and a container image according to the at least one performance indicator, generating a container image label containing the name of the server, the determined hardware configuration and the container image, and sending the container image label to the container manager; and the container manager receiving the container image label from the server, and determining whether the container image label contain the name of the server, and deploying a container for the deep learning program according to the hardware configuration and the container image if the container image label contains the name of the server.
7. The method as recited in claim 6, further comprising: the server determining the hardware configuration and container image by a preset rule according to the at least one performance indicator to make the deep learning program have a minimum running time after utilizing the hardware configuration and container image to encapsulates the deep learning program.
8. The method as recited in claim 7, further comprising: when the forward propagation time is within a first threshold range, the server determining the hardware configuration as setting the communication mode between a CPU and a graphics processing unit (GPU) as a NVLink communication mode, and setting the container image as setting application interface; when the forward propagation time is within a second threshold range, the server determining the hardware configuration as setting the communication mode between the CPU and the GPU as a PCIex2 communication mode, and setting the container image as setting application interface.
9. The method as recited in claim 6, further comprising: the server determining the hardware configuration as setting the number of CPU's cores, memory capacity, and the number of CPUs according to the at least one performance indicator.
10. The method as recited in claim 6, further comprising: the container manager using an event tracking tool to analyze the at least one performance indicator from the deep learning program.
11. The method as recited in claim 11, further comprising: the container manager using the event tracking tool to analyze the at least one performance indicator from the deep learning program by a web protocol function.
12. The method as recited in claim 11, wherein the event tracking tool can be a Swarm-Oriented Function Call Analysis, or a Flame Graph.
13. The method as recited in claim 6, wherein the at least one performance indicator comprises a forward propagation time, a backward time, a data replication time from a host to a graphics processing unit (GPU), and a data replication time from the host to the GPU.
</claims>
</document>
