<document>

<filing_date>
2020-05-28
</filing_date>

<publication_date>
2020-09-17
</publication_date>

<priority_date>
2018-04-20
</priority_date>

<ipc_classes>
B60W10/18,B60W10/20,B60W30/09,B60W30/095,G06N3/02,G08G1/16
</ipc_classes>

<assignee>
SHENZHEN SENSETIME TECHNOLOGY COMPANY
</assignee>

<inventors>
YI, SHUAI
YU, CHENDI
LIU, WENZHI
Zeng, Xingyu
Tong, Yuanyang
</inventors>

<docdb_family_id>
63939710
</docdb_family_id>

<title>
FORWARD COLLISION CONTROL METHOD AND APPARATUS, ELECTRONIC DEVICE, PROGRAM, AND MEDIUM
</title>

<abstract>
Embodiments of the present disclosure disclose forward collision control methods and apparatuses, electronic devices, programs, and media, where the forward collision control method includes: detecting a forward suspected collision object on a road where a current traveling object is located on the basis of a neural network; predicting the collision time between the current traveling object and the suspected collision object; and performing forward collision control on the current traveling object according to the collision time, where the forward collision control includes forward collision warning and/or driving control.
</abstract>

<claims>
1. A forward collision control method, comprising: detecting, based on a neural network, a forward suspected collision object on a road where a current traveling object is located; predicting a collision time between the current traveling object and the suspected collision object; and performing, according to the collision time, forward collision control on the current traveling object, wherein the forward collision control comprises at least one of forward collision warning or driving control.
2. The method according to claim 1, wherein the current traveling object comprises: a vehicle or robot; and/or the suspected collision object comprises at least one of: a person, a vehicle, a non-motorized vehicle, a robot, or an obstruction; and/or wherein the detecting, based on a neural network, a forward suspected collision object on a road where a current traveling object is located comprises: acquiring a detection result of a lane line on the road; and determining, based on the detection result of the lane line, at least some of target objects on the road as suspected collision objects.
3. The method according to claim 2, wherein the acquiring the detection result of a lane line on the road comprises: detecting, based on the neural network, the lane line on the road to obtain the detection result of the lane line; or acquiring the detection result of the lane line from an advanced driver-assistance system.
4. The method according to claim 2, wherein the determining, based on the detection result of the lane line, at least some of target objects on the road as suspected collision objects comprises: in response to the detection result of the lane line being detected lane line information, selecting, according to the detected lane line information, at least some of target objects in the lane where the current traveling object is located as the suspected collision objects; or wherein the determining, based on the detection result of the lane line, at least some of target objects on the road as suspected collision objects comprises: in response to the detection result of the lane line being that no lane line is detected, selecting a target object having a distance from the current traveling object within a preset range as a suspected collision object.
5. The method according to claim 1, wherein the predicting the collision time between the current traveling object and the suspected collision object comprises: respectively predicting, based on detection box information of the suspected collision object in previous N frame images and the current frame image, movement information of each suspected collision object; and calculating, based on the movement information of the suspected collision object, the collision time between the current traveling object and each suspected collision object; and/or wherein the performing, according to the collision time, forward collision warning on the current traveling object comprises: comparing the collision time between the current traveling object and the suspected collision object with at least one predetermined threshold; and in response to the comparison result meeting one or more preset conditions, performing forward collision control corresponding to the met preset condition.
6. The method according to claim 5, wherein in response to multiple preset conditions being comprised in the one or more preset conditions, forward collision control degrees respectively corresponding to the multiple preset conditions are increased degree by degree.
7. The method according to claim 5, wherein the in response to the comparison result meeting one or more preset conditions, performing forward collision warning corresponding to the met preset condition comprises: in response to the collision time between a suspected collision object and the current traveling object being less than or equal to a second preset threshold and greater than a first preset threshold, performing collision warning, wherein the first preset threshold is less than the second preset threshold.
8. The method according to claim 7, wherein the in response to the collision time between a suspected collision object and the current traveling object being less than or equal to a second preset threshold and greater than a first preset threshold, performing collision warning comprises: for a first suspected collision object having the collision time less than or equal to the second preset threshold and greater than the first preset threshold, predicting, based on movement information of the first suspected collision object, whether the first suspected collision object trends to be distant from a collision region; and in response to the first suspected collision object not trending to be distant from the collision region, performing the collision warning; or wherein the in response to the collision time between a suspected collision object and the current traveling object being less than or equal to a second preset threshold and greater than a first preset threshold, performing collision warning further comprises: in response to the first suspected collision object trending to be distant from the collision region, not performing the collision warning; and/or wherein the in response to the comparison result meeting one or more preset conditions, performing forward collision warning corresponding to the met preset condition further comprises: in response to the collision time between a suspected collision object and the current traveling object being less than or equal to the first preset threshold, performing at least one of collision warning or driving control, wherein the driving control comprises at least one of: braking deceleration or changing a driving direction.
9. The method according to claim 8, wherein the in response to the collision time between a suspected collision object and the current traveling object being less than or equal to the first preset threshold, performing at least one of collision warning or driving control comprises: for a second suspected collision object having the collision time less than or equal to the first preset threshold, predicting, based on movement information of the second suspected collision object, whether the second suspected collision object trends to be distant from a collision region; and in response to the second suspected collision object not trending to be distant from the collision region, performing at least one of collision warning or driving control, wherein the driving control comprises at least one of: braking deceleration or changing a driving direction; or wherein the in response to the collision time between a suspected collision object and the current traveling object being less than or equal to the first preset threshold, performing at least one of collision warning or driving control further comprises: in response to the second suspected collision object trending to be distant from the collision region, not performing at least one of collision warning or driving control, wherein the driving control comprises at least one of: braking deceleration or changing a driving direction.
10. The method according to claim 5, further comprising: detecting, by using the neural network, a target object in the current frame image to obtain a detection box of the target object in the current frame image; predicting, based on detection boxes of the target object in previous N frame images, an object box of the target object in the current frame image, wherein the previous N frame images comprise N frame images located before the current frame image and arranged by an image acquisition order, and N is an integer greater than zero; and determining, according to the detection box and the object box of the target object in the current frame image, detection box information of the target object in the current frame image; and/or wherein in response to the suspected collision objects being persons or robots, the respectively predicting, based on detection box information of the suspected collision object in previous N frame images and the current frame image, movement information of each suspected collision object comprises: using the neural network to predict behaviors of the persons or the robots based on the detection box information in the current frame image to obtain predicted behavior information of the persons or the robots in the current frame image, wherein the predicted behavior information comprises: the movement direction, the movement state, and action classification information; and respectively determining, based on the predicted behavior information of the persons or the robots and the detection box information of the persons or the robots in the previous N frame images and the current frame image, movement information of the persons or the robots.
11. The method according to claim 10, wherein the detecting, by using the neural network, a target object in the current frame image to obtain a detection box of the target object in the current frame image comprises: extracting, by using the neural network, a feature of the current frame image; and determining, based on the extracted feature, a detection box of a target object by using the neural network.
12. The method according to claim 11, wherein the neural network comprises multiple feature layers located at different network depth, respectively; and the extracting, by using the neural network, a feature of the current frame image comprises: sequentially performing feature extraction on the current frame image input to the neural network by means of feature layers of the neural network, and performing feature fusion on a first feature which is output by a first feature layer having a network depth of i and a second feature which is output by a second feature layer having a network depth of j to obtain a fused feature as an input feature of a third feature layer having a network depth of j+1 or as the feature of the current frame image output by the neural network, wherein 1≤i<j; and/or wherein the determining, based on the extracted feature, a detection box of a target object by using the neural network comprises: determining, based on the extracted feature, candidate boxes comprising target objects by using the neural network; respectively classifying, based on features of the candidate boxes, the candidate boxes by using the neural network to obtain probability scores that the candidate boxes belong to different types of target objects; respectively using the types corresponding to the maximum probability scores in the probability scores of the candidate boxes as the types of target objects in the candidate boxes; and respectively selecting candidate boxes having an overlapping rate greater than a preset threshold and having the same type of target objects as a group of candidate boxes, and selecting, from a group of candidate boxes, a candidate box corresponding to the maximum probability score of the type of the target object as a detection box of the target object.
13. The method according to claim 10, wherein the detecting, by using the neural network, a target object in the current frame image to obtain a detection box of the target object in the current frame image comprises: in response to the current frame image being a first frame image in a frame sequence, detecting, by using the neural network, a target object in the current frame image, wherein the frame sequence comprises: M frame images arranged by the image acquisition order, and M is an integer greater than one; and/or in response to the current frame image being not the first frame image in the frame sequence, regressing, based on detection box information of the first frame image in the frame sequence, a detection box of the target object in the current frame image; and/or wherein the predicting, based on detection boxes of the target object in previous N frame images, an object box of the target object in the current frame image comprises: for a same target object in the previous N frame images, fitting, according to the sizes and the positions of the detection boxes of the same target object in the previous N frame images, the speed and the acceleration of changes in the sizes and positions of the detection boxes of the same target object in the previous N frame images; and predicting, according to the speed and the acceleration of changes in the sizes and positions of the detection boxes of the same target object in the previous N frame images, the size and the position of an object box of the same target object in the current frame image.
14. The method according to claim 13, wherein the regressing, based on detection box information of the first frame image in the frame sequence, a detection box of the target object in the current frame image comprises: taking, based on the detection box information in the first frame image, a center point of a detection box determined according to detection box information in the first frame image as a center point, and capturing, in the current frame image, a regional image having at least one of a length or width correspondingly greater than at least one of length or width of the determined detection box; detecting, by using the neural network, a target object in the regional image to obtain the detection box of the target object in the regional image; and determining, according to the detection box of the target object in the regional image and the position of the regional image in the current frame image, the detection box information of the target object in the current frame image.
15. The method according to claim 13, wherein the determining detection box information of the target object in the current frame image according to the detection box and the object box of the target object comprises: acquiring, for a first target object in the target objects which has an overlapping rate between the detection box and the object box equal to or greater than a preset threshold, average values of the detection box and the object box of the first target object in terms of the size and the position as detection box information of the first target object in the current frame image, wherein the detection box information comprises: the size and the position of the detection box.
16. The method according to claim 13, wherein the determining detection box information of the target object in the current frame image according to the detection box and the object box of the target object further comprises: for a second target object in the target objects which has an overlapping rate between the detection box and the object box less than the preset threshold, taking a predicted size and position of the object box of the second target object as the detection box information of the second target object in the current frame image.
17. The method according to claim 13, wherein the determining detection box information of the target object in the current frame image according to the detection box and the object box of the target object further comprises: for a third target object in the target objects which does not have a detection box corresponding to the object box in the current frame image, taking a predicted size and position of the object box of the third target object as detection box information of the third target object in the current frame image.
18. The method according to claim 10, wherein the predicting behaviors of persons or robots based on detection box information in the current frame image to obtain predicted behavior information of the persons or the robots in the current frame image comprises: performing, based on the detection box information in the current frame image, key point detection on a corresponding person or robot; acquiring, based on the key point detection result, the head direction and the body direction of the corresponding person or robot; acquiring, based on the head direction and the body direction of the corresponding person or robot, the movement direction of the corresponding person or robot; classifying, based on the key point detection result, actions of the corresponding person or robot to obtain action classification information of the corresponding person; and classifying, based on the key point detection result, the movement state of the corresponding person or robot to obtain movement state information of the corresponding person or robot.
19. An electronic device, comprising: a memory, configured to store computer programs; and a processor, configured to execute the compute programs stored in the memory, wherein when the computer programs are executed, the processor is configured to: detect, based on a neural network, a forward suspected collision object on a road where a current traveling object is located; predict a collision time between the current traveling object and the suspected collision object; and perform, according to the collision time, forward collision control on the current traveling object, wherein the forward collision control comprises at least one of forward collision warning or driving control.
20. A non-transitory computer-readable storage medium having computer programs stored thereon, wherein when the computer programs are executed by a processor, the processor is configured to: detecting, based on a neural network, a forward suspected collision object on a road where a current traveling object is located; predicting a collision time between the current traveling object and the suspected collision object; and performing, according to the collision time, forward collision control on the current traveling object, wherein the forward collision control comprises at least one of forward collision warning or driving control.
</claims>
</document>
