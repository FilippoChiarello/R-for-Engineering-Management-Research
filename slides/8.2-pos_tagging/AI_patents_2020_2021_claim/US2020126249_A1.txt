<document>

<filing_date>
2019-12-18
</filing_date>

<publication_date>
2020-04-23
</publication_date>

<priority_date>
2017-07-07
</priority_date>

<ipc_classes>
B64C39/02,G06T7/00,G06T7/254,G06T7/579,G06T7/70
</ipc_classes>

<assignee>
SZ DJI TECHNOLOGY COMPANY
</assignee>

<inventors>
TANG, KETAN
ZHOU, YOU
</inventors>

<docdb_family_id>
64949633
</docdb_family_id>

<title>
ATTITUDE RECOGNITION METHOD AND DEVICE, AND MOVABLE PLATFORM
</title>

<abstract>
A method for recognizing a posture includes acquiring a depth image of a scene, and obtaining point clouds of an operator based on the depth image of the scene. The method also includes separating point clouds of an arm from the point clouds of the operator and obtaining a characteristic point from the point clouds of the arm. The method further includes determining a location relationship between the characteristic point and the operator and determining a posture of the operator based on the location relationship.
</abstract>

<claims>
1. A method for recognizing a posture, comprising: acquiring a depth image of a scene, and obtaining point clouds of an operator based on the depth image of the scene; separating point clouds of an arm from the point clouds of the operator and obtaining a characteristic point from the point clouds of the arm; and determining a location relationship between the characteristic point and the operator and determining a posture of the operator based on the location relationship.
2. The method of claim 1, wherein obtaining the point clouds of the operator based on the depth image of the scene comprises: determining, based on a detection algorithm, whether a target object is included in the depth image of the scene; and determining, based on the detection algorithm, the point clouds of the operator based on determining that the target object is included in the depth image of the scene.
3. The method of claim 1, wherein obtaining the point clouds of the operator based on the depth image of the scene comprises: determining, based on a detection algorithm, point clouds of a body of the operator from the depth image of the scene; and determining a set including the point clouds of the body and point clouds connected with the point clouds of the body as the point clouds of the operator.
4. The method of claim 3, wherein separating the point clouds of the arm from the point clouds of the operator comprises: separating the point clouds of the body from the point clouds of the operator, remaining point clouds being the point clouds of the arm.
5. The method of claim 4, wherein separating the point clouds of the arm from the point clouds of the operator comprises: identifying the point clouds of the body using a frame; and determining the point clouds of the operator located outside of the frame as the point clouds of the arm.
6. The method of claim 5, before identifying the point clouds of the body using the frame, the method further comprises: calculating a location of the frame based on a location of the point clouds of the body, and calculating a size of the frame based on a range of the point clouds of the body; and determining the frame based on the location of the frame and the size of the frame.
7. The method of claim 1, wherein obtaining the point clouds of the operator based on the depth image of the scene comprises: receiving a signal, the signal comprises location information; and determining, based on the location information, a set including point clouds connected with a location indicated by the location information as the point clouds of the operator.
8. The method of claim 1, wherein after separating the point clouds of the arm from the point clouds of the operator, the method further comprises: obtaining the characteristic point from the point clouds of the arm when a number of the point clouds of the arm in the depth image of the scene is within a predetermined number.
9. The method of claim 1, wherein obtaining the characteristic point based on the point clouds of the arm comprises: determining a calibration point based on the point clouds of the operator, the calibration point comprising at least one of an edge pixel of the point clouds of the operator or an average value of the point clouds of the operator; and determining the characteristic point based on a block distance calculated between each of the point clouds of the arm and the calibration point, wherein the characteristic point comprises at least one of a characteristic point on an elbow, a characteristic point on a wrist, a characteristic point on a palm, or a characteristic point on an arm.
10. The method of claim 9, wherein determining the location relationship between the characteristic point and the operator and determining the posture of the operator based on the location relationship comprises: determining the posture of the operator as a predetermined posture based on a determination that a depth difference between the characteristic point and the calibration point is within a first range.
11. The method of claim 9, wherein determining the location relationship between the characteristic point and the operator and determining the posture of the operator based on the location relationship comprises: determining the posture of the operator as a predetermined posture based on a determination that a depth difference between the characteristic point and the calibration point in a gravity direction is within a second range.
12. A method for controlling a movable platform, comprising: acquiring at least two depth images of a scene; simulating, based on the at least two depth images of the scene, depth images of the scene at times other than times at which the at least two depth images of the scene are acquired; recognizing a target object from the acquired depth images of the scene and the simulated depth images of the scene; and calculating a location change of the target object in the at least two acquired depth images and the simulated depth images, and controlling the movable platform to move based on the location change.
13. The method of claim 12, wherein the target object comprises at least one of an operator or a characteristic part of the operator.
14. The method of claim 13, wherein recognizing the target object from the acquired depth images of the scene and the simulated depth images of the scene comprises: recognizing the operator and the characteristic part of the operator from the acquired depth images of the scene and the simulated depth images of the scene, comprising: recognizing, based on a detection algorithm, the operator from the acquired depth images and the simulated depth images; and determining the characteristic part of the operator based on the operator, the characteristic part comprising at least one of a palm, a wrist, an elbow, a leg, a foot, or a head.
15. The method of claim 13, wherein recognizing the target object from the acquired depth images of the scene and the simulated depth images of the scene comprises: recognizing the operator and the characteristic part of the operator from the acquired depth images of the scene and the simulated depth images of the scene, comprising: recognizing, based on a detection algorithm, the characteristic part from the acquired depth images and the simulated depth images; and detecting parts connected to the characteristic part of the operator to determine the operator, the characteristic part comprising at least one of a palm, a wrist, an elbow, a leg, a foot, or a head.
16. The method of claim 12, wherein simulating, based on the at least two depth images of the scene, depth images of the scene at times other than times at which the at least two depth images of the scene are acquired comprises: determining, based on a detection algorithm, whether the at least two depth images of the scene include the target object, and when the at least two depth images of the scene include the target object, simulating, based on the at least two depth images of the scene, depth images of the scene at times other than times at which the at least two depth images of the scene are acquired.
17. The method of claim 12, wherein recognizing the target object from the acquired depth images of the scene and the simulated depth images of the scene, and calculating the location change of the target object in the at least two acquired depth images and the simulated depth images, and controlling the movable platform to move based on the location change comprises: recognizing the target object from the acquired depth images of the scene and the simulated depth images of the scene, and obtaining time information of the target object; and calculating the location change of the target object in the acquired depth images and the simulated depth images and a moving velocity of the target object based on detecting at least two pairs of recognized target objects and the time information, and controlling the movable platform to move based on the location change and the moving velocity.
18. A method for recognizing an action, comprising: acquiring at least two depth images of a scene, and detecting locations of a target object in each of the depth images of the scene; calculating a moving distance and a moving direction of the target object based on times at which the depth images of the scene are acquired and the locations of the target object in the depth images, the moving direction comprising a first direction and a second direction; and determining an action of the target object is a swaying action based on a detection, within a predetermined time, that a number of movements in the first moving direction is not smaller than a first predetermined value, a number of movements in the second direction is not smaller than a second predetermined value, and the moving distance is not smaller than a predetermined distance.
19. The method of claim 18, wherein calculating the moving distance and the moving direction of the target object comprises: obtaining a distance for the target object based on a difference between the locations of the target object in two depth images of the at least two depth images, and obtaining the moving direction based on the times at which the two depth images are acquired and the locations of the target object.
20. The method of claim 18, wherein the moving distance comprises a horizontal distance between the locations of the target object in two depth images of the at least two depth images.
</claims>
</document>
