<document>

<filing_date>
2017-05-19
</filing_date>

<publication_date>
2020-03-17
</publication_date>

<priority_date>
2016-05-20
</priority_date>

<ipc_classes>
G06K9/00,G10L15/00,G10L15/02,G10L15/22,G10L15/25,G10L15/30
</ipc_classes>

<assignee>
EDUCATIONAL TESTING SERVICE
</assignee>

<inventors>
EVANINI, KEELAN
RAMANARAYANAN, VIKRAM
QIAN, YAO
SUENDERMANN-OEFT, DAVID
IVANOV, ALEXEI V.
MOLLOY, HILLARY R.
LANGE, PATRICK
TSUPRUN, EUGENE
</inventors>

<docdb_family_id>
69778907
</docdb_family_id>

<title>
Computer-implemented systems and methods for evaluating speech dialog system engagement via video
</title>

<abstract>
Systems and methods are provided providing a spoken dialog system. Output is provided from a spoken dialog system that determines audio responses to a person based on recognized speech content from the person during a conversation between the person and the spoken dialog system. Video data associated with the person interacting with the spoken dialog system is received. A video engagement metric is derived from the video data, where the video engagement metric indicates a level of the person's engagement with the spoken dialog system.
</abstract>

<claims>
It is claimed:
1. A processor-implemented method for providing a spoken dialog system, comprising: providing an engagement engine comprising a spoken dialog system configured to have a conversation with a person; providing an output from the spoken dialog system to the person, wherein the output is intended to prompt a plurality of responses from the person; capturing audio and video data of the person's responses to the output; extracting audio and video features from the audio and video data, wherein the audio and video features are indicative of the person's level of engagement with the spoken dialog system; deriving a plurality of engagement metrics from the audio and video features, wherein the plurality of engagement metrics are indicative of the level of the person's engagement with the spoken dialog system; deriving additional engagement metrics based on a quality of the person's responses; and adjusting the spoken dialog system during the conversation, based a combination of the plurality of engagement metrics derived from the audio features, the video features, and the quality of the person's responses to improve the person's level of engagement with the spoken dialog system, the adjusting comprising displaying at least one of a picture or a video when the engagement metrics indicate that the level of the person's engagement with the spoken dialog system is below a pre-defined threshold.
2. The method of claim 1, wherein the plurality of engagement metrics are not indicative of a level of correctness of any speech content received from the person.
3. The method of claim 1, wherein deriving the plurality of engagement metrics comprises calculating a scale-invariant feature transform across a plurality of frames of the video data.
4. The method of claim 3, wherein the scale-invariant feature transform tracks temporal evolution of a captured feature of the person across the plurality of frames of the video data.
5. The method of claim 4, wherein the captured feature is associated with corners or edges represented in the video data or the person's nose, mouth, eye, ear, or hand.
6. The method of claim 1, wherein deriving the plurality of engagement metrics comprises identifying a number of frames in which a captured feature of the person appears in the video data.
7. The method of claim 6, further comprising calculating a histogram of occurrences of clusters of captured features of the person in frames of the video data.
8. The method of claim 1, wherein the plurality of engagement metrics comprises multiple metrics including two or more of: conversation experience, intelligibility, system performance, and cooperation of the person.
9. The method of claim 1, wherein the plurality of engagement metrics comprises an audio engagement metric and a video engagement metric.
10. The method of claim 1, wherein the output from the spoken dialog system is provided via a computer system, wherein the video data is captured via the computer system.
11. The method of claim 9, wherein the spoken dialog system is adjusted during the conversation, based on the audio engagement metric.
12. The method of claim 1, wherein the spoken dialog system is further adjusted after the conversation, based on the plurality of engagement metrics.
13. The method of claim 9, wherein the spoken dialog system is adjusted during the conversation, based on the video engagement metric.
14. The method of claim 1, wherein the plurality of engagement metrics are used to generate a performance score for the person.
15. The method of claim 1, wherein the conversation with the person is part of an interview process for employment.
16. The method of claim 15, wherein the conversation is associated with an initial screening for the job interview process, wherein the person is a candidate for employment.
17. The method of claim 1, wherein the spoken dialog system includes a video avatar, wherein the video avatar is animated as part of the conversation.
18. The method of claim 1, wherein the spoken dialog system is a multi-modal dialog system.
19. The method of claim 1, where the plurality of engagement metrics further comprises a correctness metric, wherein the correctness metric is indicative of a level of correctness of at least a portion of speech content received from the person.
20. A computer-implemented system for providing a spoken dialog system, comprising: one or more data processors; a computer-readable medium encoded with instructions for commanding the one or more data processors to execute steps comprising: providing an engagement engine comprising a spoken dialog system configured to have a conversation with a person; providing an output from the spoken dialog system to the person, wherein the output is intended to prompt a plurality of responses from the person; capturing audio and video data of the person's responses to the output; extracting audio and video features from the audio and video data, wherein the audio and video features are indicative of the person's level of engagement with the spoken dialog system; deriving a plurality of engagement metrics from the audio and video features, wherein the plurality of engagement metrics are indicative of the level of the person's engagement with the spoken dialog system; deriving additional engagement metrics based on a quality of the person's responses; and adjusting the spoken dialog system during the conversation, based a combination of the plurality of engagement metrics derived from the audio features, the video features, and the quality of the person's responses to improve the person's level of engagement with the spoken dialog system, the adjusting comprising displaying at least one of a picture or a video when the engagement metrics indicate that the level of the person's engagement with the spoken dialog system is below a pre-defined threshold.
21. A non-transitory computer-readable medium encoded with instructions for commanding one or more data processors to execute steps of a method for providing a spoken dialog system, the steps comprising: providing an engagement engine comprising a spoken dialog system configured to have a conversation with a person; providing an output from the spoken dialog system to the person, wherein the output is intended to prompt a plurality of responses from the person; capturing audio and video data of the person's responses to the output; extracting audio and video features from the audio and video data, wherein the audio and video features are indicative of the person's level of engagement with the spoken dialog system; deriving a plurality of engagement metrics from the audio and video features, wherein the plurality of engagement metrics are indicative of the level of the person's engagement with the spoken dialog system; deriving additional engagement metrics based on a quality of the person's responses; and adjusting the spoken dialog system during the conversation, based a combination of the plurality of engagement metrics derived from the audio features, the video features, and the quality of the person's responses to improve the person's level of engagement with the spoken dialog system, the adjusting comprising displaying at least one of a picture or a video when the engagement metrics indicate that the level of the person's engagement with the spoken dialog system is below a pre-defined threshold.
22. The method of claim 9, wherein the video engagement metric is derived without consideration of the audio features, and wherein the audio engagement metric is derived without consideration of the video features.
23. The method of claim 1, wherein the adjustments made to the spoken dialog system are applied to subsequent conversations.
24. The method of claim 1, further comprising: identifying correlations between the extracted audio and video features and the plurality of engagement metrics; and applying the identified correlations to subsequent conversations.
</claims>
</document>
