<document>

<filing_date>
2019-04-19
</filing_date>

<publication_date>
2020-10-22
</publication_date>

<priority_date>
2019-04-19
</priority_date>

<ipc_classes>
G06F3/01,G06T11/00,G06T7/13,G06T7/593
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
PRICE, RAYMOND KIRK
PEKELNY, YURI
Bleyer, Michael
</inventors>

<docdb_family_id>
70285866
</docdb_family_id>

<title>
2D OBSTACLE BOUNDARY DETECTION
</title>

<abstract>
Techniques are provided to dynamically generate and render an object bounding fence in a mixed-reality scene. Initially, a sparse spatial mapping is accessed. The sparse spatial mapping beneficially includes perimeter edge data describing an object's edge perimeters. A gravity vector is also generated. Based on the perimeter edge data and the gravity vector, two-dimensional (2D) boundaries of the object are determined and a bounding fence mesh of the environment is generated. A virtual object is then rendered, where the virtual object is representative of at least a portion of the bounding fence mesh and visually illustrates a bounding fence around the object.
</abstract>

<claims>
1. A method for dynamically generating and rendering an object bounding fence in a mixed-reality scene, the method comprising: accessing a spatial mapping of an environment, the spatial mapping including perimeter edge data describing one or more perimeter edge(s) of an object located within the environment; generating a gravity vector of a head-mounted device (HMD) that is operating in the environment and that is displaying a mixed-reality scene; based on the perimeter edge data and the gravity vector, determining two-dimensional (2D) boundaries of the object within the environment; generating a bounding fence mesh of the environment, the bounding fence mesh identifying the 2D boundaries of the object within the environment; and rendering, within the mixed-reality scene, a virtual object that is representative of at least a portion of the bounding fence mesh and that visually illustrates a bounding fence around the object, wherein the bounding fence is defined by the 2D boundaries of the object to form a 2D planar area surrounding the object relative to the gravity vector.
2. The method of claim 1, wherein the perimeter edge data describes a portion, but not all, of the one or more perimeter edge(s) of the object such that the perimeter edge data constitutes incomplete data, and wherein the spatial mapping is a sparse spatial mapping as a result of including the incomplete data for the object.
3. The method of claim 1, wherein the virtual object includes a visualization of a 2D bird's eye view of the environment.
4. The method of claim 1, wherein the spatial mapping is a sparse spatial mapping and is generated using a passive stereo camera system.
5. The method of claim 4, wherein the passive stereo camera system is included as a part of a head-tracking system of the HMD.
6. The method of claim 1, wherein the gravity vector is generated based on data obtained from an inertial measurement unit (IMU).
7. (canceled)
8. The method of claim 1, wherein the bounding fence further includes a rectangular cuboid whose length and width are defined by the 2D boundaries of the object and whose height extends upwardly in an unbounded direction perpendicular to the 2D planar area and parallel to the gravity vector.
9. The method of claim 1, wherein the bounding fence further includes a rectangular cuboid whose length and width are defined by the 2D boundaries of the object and whose height extends upwardly in an bounded direction perpendicular to the 2D planar area and parallel to the gravity vector, the height extending at least to a height of the object such that the rectangular cuboid entirely envelopes the object.
10. The method of claim 1, wherein a buffer is provided between the bounding fence and the 2D boundaries of the object such that an area defined by the bounding fence is larger than an area defined by the 2D boundaries of the object.
11. A computer system comprising: one or more processor(s); and one or more computer-readable hardware storage device(s) having stored thereon computer-executable instructions that are executable by the one or more processor(s) to cause the computer system to dynamically generate and render an object bounding fence in a mixed-reality scene by causing the computer system to at least: access a sparse spatial mapping of an environment, the sparse spatial mapping including perimeter edge data describing one or more perimeter edge(s) of an object located within the environment; generate a gravity vector of a head-mounted device (HMD) that is operating in the environment and that is displaying a mixed-reality scene; based on the perimeter edge data and the gravity vector, determine two-dimensional (2D) boundaries of the object within the environment; generate a bounding fence mesh of the environment, the bounding fence mesh identifying the 2D boundaries of the object within the environment; and render, within the mixed-reality scene, a virtual object that is representative of at least a portion of the bounding fence mesh and that visually illustrates a bounding fence around the object, wherein a second virtual object is rendered in the mixed-reality scene, the second virtual object operating as a second bounding fence for a second object included in the environment.
12. (canceled)
13. The computer system of claim 11, wherein the second virtual object is visually distinguished from the virtual object.
14. The computer system of claim 13, wherein visually distinguishing the second virtual object from the virtual object is performed with at least one of a different color, texture, or animation.
15. The computer system of claim 13, wherein visually distinguishing the second virtual object from the virtual object is based on determined types of the object and the second object.
16. The computer system of claim 11, wherein rendering the virtual object is performed only in response to a triggering event.
17. The computer system of claim 16, wherein the triggering event occurs when the HMD is determined to be located within a pre-established threshold distance to the object in the environment.
18. The computer system of claim 11, wherein the computer system is one of the following: a virtual-reality system or an augmented-reality system.
19. A head-mounted device (HMD) comprising: a wearable display; one or more processor(s); and one or more computer-readable hardware storage device(s) having stored thereon computer-executable instructions that are executable by the one or more processor(s) to cause the HMD to dynamically generate and render an object bounding fence in a mixed-reality scene by causing the HMD to at least: access a sparse spatial mapping of an environment, the sparse spatial mapping including perimeter edge data describing one or more perimeter edge(s) of an object located within the environment; generate a gravity vector of the HMD, which is operating in the environment and which is displaying a mixed-reality scene on the wearable display; based on the perimeter edge data and the gravity vector, determine two-dimensional (2D) boundaries of the object within the environment; generate a bounding fence mesh of the environment, the bounding fence mesh identifying the 2D boundaries of the object within the environment; and render, within the mixed-reality scene on the wearable display, a virtual object that is representative of at least a portion of the bounding fence mesh and that visually illustrates a bounding fence around the object.
20. The HMD of claim 19, wherein the sparse spatial mapping is generated using one of the following: a motion stereo camera system, a passive stereo camera system, an active stereo camera system, an active time-of-flight, or an active structured light camera.
</claims>
</document>
