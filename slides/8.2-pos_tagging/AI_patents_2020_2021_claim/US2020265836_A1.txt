<document>

<filing_date>
2019-03-30
</filing_date>

<publication_date>
2020-08-20
</publication_date>

<priority_date>
2019-02-15
</priority_date>

<ipc_classes>
G06F3/16,G06N3/08,G10L15/16,G10L15/22
</ipc_classes>

<assignee>
WIPRO
</assignee>

<inventors>
KAR, SIBSAMBHU
</inventors>

<docdb_family_id>
72042351
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR CONTROLLING DEVICES THROUGH VOICE INTERACTION
</title>

<abstract>
A method and a system for controlling devices through voice interaction are disclosed. In an embodiment, the method may include identifying at least one feature of a target device and an action to be performed on the at least one feature, based on an intent and an object determined from a voice input received from a user. The method may further include determining a correspondence between the at least one feature and the action to be performed using a trained neural network. The method may further include comparing a current operational state of the at least one feature with an operational threshold of the at least one feature. The method may further include performing the action on the at least one feature based on the determined correspondence, when the current operational state is within limits of the operational threshold.
</abstract>

<claims>
1. A method for controlling devices through voice interaction, the method comprises: identifying, by a controlling device, at least one feature of a target device and an action to be performed on the feature, based on an intent and an object determined from a received voice input; determining, by the controlling device, a correspondence between the feature and the action to be performed using a trained neural network, wherein the trained neural network is pre-trained based on a correspondence between a plurality of prior actions and a plurality of features associated with the target device; comparing, by the controlling device, a current operational state of the feature with an operational threshold of the feature; and performing, by the controlling device, the action on the feature based on the determined correspondence, when the current operational state is within one or more limits of the operational threshold.
2. The method of claim 1 further comprising: converting, by the controlling device, the voice input received from the user to text; and determining, by the controlling device, each of the intent and the object based on processing of the text by a Long Short Term Memory (LSTM) model.
3. The method of claim 2, wherein the text is provided to the LSTM model in the form of sequence of words using word embeddings and the LSTM model is trained based on the prior actions and each of the prior actions is associated with a probability of execution.
4. The method of claim 1 further comprising determining, by the controlling device, from an image associated with the target device, and by a convoluting neural network (CNN), the feature of the target device, wherein the CNN is trained to identify features from the target device using at least one training image associated with the target device.
5. The method of claim 4, wherein the image comprises at least one of a blueprint of the target device, a drawing of the target device, or a layout of the target device and the method further comprises updating, by the controlling device, the image associated with the target device, when the current operational state is within the limits of the operational threshold.
6. The method of claim 1 further comprising: establishing, by the controlling device, non-performance of the action on the feature, when the current operational state is outside the limits of the operational threshold; and outputting, by the controlling device, an alert regarding non-performance of the action, wherein the alert comprises details associated with the non-performance of the action.
7. A controlling device, comprising: a processor; and a memory communicatively coupled to the processor and storing instructions that, when executed by the processor, cause the processor to: identify at least one feature of a target device and an action to be performed on the feature, based on an intent and an object determined from a received voice input; determine a correspondence between the feature and the action to be performed using a trained neural network, wherein the trained neural network is pre-trained based on a correspondence between a plurality of prior actions and a plurality of features associated with the target device; compare a current operational state of the feature with an operational threshold of the feature; and perform the action on the feature based on the determined correspondence, when the current operational state is within one or more limits of the operational threshold.
8. The controlling device of claim 7, wherein the instructions, when executed by the processor, further cause the processor to: convert the voice input received from the user to text; and determine each of the intent and the object based on processing of the text by a Long Short Term Memory (LSTM) model.
9. The controlling device of claim 8, wherein the text is provided to the LSTM model in the form of sequence of words using word embeddings and the LSTM model is trained based on the prior actions and each of the prior actions is associated with a probability of execution.
10. The controlling device of claim 7, wherein the instructions, when executed by the processor, further cause the processor to determine, from an image associated with the target device and by a convoluting neural network (CNN), the feature of the target device, wherein the CNN is trained to identify features from the target device using at least one training image associated with the target device.
11. The controlling device of claim 10, wherein the image comprises at least one of a blueprint of the target device, a drawing of the target device, or a layout of the target device and the instructions, when executed by the processor, further cause the processor to update the image associated with the target device, when the current operational state is within the limits of the operational threshold.
12. The controlling device of claim 7, wherein the instructions, when executed by the processor, further cause the processor to: establish non-performance of the action on the feature, when the current operational state is outside the limits of the operational threshold; and output an alert regarding non-performance of the action, wherein the alert comprises details associated with the non-performance of the action.
13. A non-transitory computer readable medium having stored thereon instructions for controlling devices through voice interaction comprising executable code which when executed by one or more processors, causes the one or more processors to: identify at least one feature of a target device and an action to be performed on the feature, based on an intent and an object determined from a received voice input; determine a correspondence between the feature and the action to be performed using a trained neural network, wherein the trained neural network is pre-trained based on a correspondence between a plurality of prior actions and a plurality of features associated with the target device; compare a current operational state of the feature with an operational threshold of the feature; and perform the action on the feature based on the determined correspondence, when the current operational state is within one or more limits of the operational threshold.
14. The non-transitory computer-readable medium of claim 13, wherein the executable code, when executed by the processors, further causes the processors to: convert the voice input received from the user to text; and determine each of the intent and the object based on processing of the text by a Long Short Term Memory (LSTM) model.
15. The non-transitory computer-readable medium of claim 14, wherein the text is provided to the LSTM model in the form of sequence of words using word embeddings and the LSTM model is trained based on the prior actions and each of the prior actions is associated with a probability of execution.
16. The non-transitory computer-readable medium of claim 13, wherein the executable code, when executed by the processors, further causes the processors to determine, from an image associated with the target device and by a convoluting neural network (CNN), the feature of the target device, wherein the CNN is trained to identify features from the target device using at least one training image associated with the target device.
17. The non-transitory computer-readable medium of claim 16, wherein the image comprises at least one of a blueprint of the target device, a drawing of the target device, or a layout of the target device and the executable code, when executed by the processors, further causes the processors to update the image associated with the target device, when the current operational state is within the limits of the operational threshold.
18. The non-transitory computer-readable medium of claim 13, wherein the executable code, when executed by the processors, further causes the processors to: establish non-performance of the action on the feature, when the current operational state is outside the limits of the operational threshold; and output an alert regarding non-performance of the action, wherein the alert comprises details associated with the non-performance of the action.
</claims>
</document>
