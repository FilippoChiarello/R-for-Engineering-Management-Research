<document>

<filing_date>
2019-10-23
</filing_date>

<publication_date>
2020-12-31
</publication_date>

<priority_date>
2019-06-26
</priority_date>

<ipc_classes>
G06N20/00,G10L15/18
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
JIN, HONGXIA
RAY, AVIK
</inventors>

<docdb_family_id>
74043724
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR NATURAL LANGUAGE UNDERSTANDING
</title>

<abstract>
An electronic device for natural language understanding includes at least one memory and at least one processor coupled to the at least one memory. The at least one processor is configured to process an utterance using a trained model. The at least one processor is also configured to replace a first portion of the utterance with a first token, where the first token represents a semantic role of the first portion of the utterance based on a slot vocabulary. The at least one processor is further configured to determine a slot value in the utterance based on the first token. In addition, the at least one processor is configured to perform a task corresponding to the utterance based on the determined slot value.
</abstract>

<claims>
1. A method, comprising: processing an utterance using a trained model; replacing a first portion of the utterance with a first token, wherein the first token represents a semantic role of the first portion of the utterance based on a slot vocabulary; determining a slot value in the utterance based on the first token; and performing a task corresponding to the utterance based on the determined slot value.
2. The method of claim 1, wherein: the trained model is trained using training data including tokens and non-tokens for learning one or more semantic relationships between the tokens and the non-tokens; and the slot vocabulary includes data different from the training data and is domain-dependent.
3. The method of claim 1, further comprising: replacing at least a second portion of the utterance with a second token prior to processing the utterance using the trained model.
4. The method of claim 3, further comprising: determining the second token based on string-matching of the second portion of the utterance and a content in the slot vocabulary.
5. The method of claim 1, wherein replacing the first portion of the utterance with the first token includes: identifying that a slot associated with the first portion of the utterance matches a predefined slot type; and replacing, based on the identification that the slot matches the predefined slot type, the first portion of the utterance with the first token, wherein the first token corresponds to the predefined slot type.
6. The method of claim 1, wherein replacing the first portion of the utterance with the first token includes: determining the utterance includes the first token; and modifying the utterance such that the first portion of the utterance is replaced as part of the first token based on a measurement.
7. The method of claim 1, further comprising: determining a parsing score for the processed utterance, wherein the parsing score is based on one or more slot entropy scores associated with the processed utterance; selecting the processed utterance from among one or more other processed utterances based on the parsing score for the processed utterance; and performing the task based on the selected processed utterance.
8. An electronic device, comprising: at least one memory; and at least one processor coupled to the at least one memory, wherein the at least one processor is configured to: process an utterance using a trained model; replace a first portion of the utterance with a first token, wherein the first token represents a semantic role of the first portion of the utterance based on a slot vocabulary; determine a slot value in the utterance based on the first token; and perform a task corresponding to the utterance based on the determined slot value.
9. The electronic device of claim 8, wherein: the trained model is trained using training data including tokens and non-tokens for learning one or more semantic relationships between the tokens and the non-tokens; and the slot vocabulary includes data different from the training data and is domain-dependent.
10. The electronic device of claim 8, wherein the at least one processor is further configured to replace at least a second portion of the utterance with a second token prior to processing the utterance using the trained model.
11. The electronic device of claim 10, wherein the at least one processor is further configured to determine the second token based on string matching of the second portion of the utterance and a content in the slot vocabulary.
12. The electronic device of claim 8, wherein, to replace the first portion of the utterance with the first token, the at least one processor is configured to: identify that a slot associated with the first portion of the utterance matches a predefined slot type; and replace, based on the identification that the slot matches the predefined slot type, the first portion of the utterance with the first token, wherein the first token corresponds to the predefined slot type.
13. The electronic device of claim 8, wherein, to replace the first portion of the utterance with the first token, the at least one processor is configured to: determine the utterance includes the first token; and modify the utterance such that the first portion of the utterance is replaced as part of the first token based on a measurement.
14. The electronic device of claim 8, wherein the at least one processor is further configured to: determine a parsing score for the processed utterance, wherein the parsing score is based on one or more slot entropy scores associated with the processed utterance; select the processed utterance from among one or more other processed utterances based on the parsing score for the processed utterance; and perform the task based on the selected processed utterance.
15. A non-transitory computer readable medium embodying a computer program, the computer program comprising instructions that when executed cause at least one processor of an electronic device to: process an utterance using a trained model; replace a first portion of the utterance with a first token, wherein the first token represents a semantic role of the first portion of the utterance based on a slot vocabulary; determine a slot value in the utterance based on the first token; and perform a task corresponding to the utterance based on the determined slot value.
16. The non-transitory computer readable medium of claim 15, wherein the instructions when executed further cause the at least one processor to replace at least a second portion of the utterance with a second token prior to processing the utterance using the trained model.
17. The non-transitory computer readable medium of claim 16, wherein the instructions when executed further cause the at least one processor to determine the second token based on string-matching of the second portion of the utterance and a content in the slot vocabulary.
18. The non-transitory computer readable medium of claim 15, wherein the instructions that when executed cause the at least one processor to replace the first portion of the utterance with the first token further cause the processor to: identify that a slot associated with the first portion of the utterance matches a predefined slot type; and replace, based on the identification that the slot matches the predefined slot type, the first portion of the utterance with the first token, wherein the first token corresponds to the predefined slot type.
19. The non-transitory computer readable medium of claim 15, wherein the instructions that when executed cause the at least one processor to replace the first portion of the utterance with the first token further cause the processor to: determine the utterance includes the first token; and modify the utterance such that the first portion of the utterance is replaced as part of the first token based on a measurement.
20. The non-transitory computer readable medium of claim 15, wherein the instructions when executed further cause the at least one processor to: determine a parsing score for the processed utterance, wherein the parsing score is based on one or more slot entropy scores associated with the processed utterance; select the processed utterance from among one or more other processed utterances based on the parsing score for the processed utterance; and perform the task based on the selected processed utterance.
21. A method for training a model for natural language understanding (NLU), the method comprising: providing an NLU training dataset and a slot vocabulary; generating a partially delexicalized training dataset using the NLU training dataset and the slot vocabulary; generating a combined training dataset by combining the NLU training dataset and the partially delexicalized training dataset; and training a model using the combined training dataset.
22. The method of claim 21, wherein the slot vocabulary includes one or more tokens, each token corresponding to a word or phrase.
23. The method of claim 22, wherein generating the partially delexicalized training dataset includes replacing at least one word or phrase in the NLU training dataset with at least one of the one or more tokens.
24. The method of claim 23, wherein the at least one of the one or more tokens is associated with the at least one word or phrase.
25. The method of claim 23, wherein replacing the at least one word or phrase in the NLU training dataset with the at least one of the one or more tokens includes selecting the at least one word to be replaced randomly from the NLU training dataset.
</claims>
</document>
