<document>

<filing_date>
2018-12-21
</filing_date>

<publication_date>
2021-01-26
</publication_date>

<priority_date>
2017-12-22
</priority_date>

<ipc_classes>
G02B27/01,G06F3/01,G06K9/00,G06T15/06,G06T15/08,G06T15/20,G06T15/40,G06T17/20,G06T19/00,G06T7/00,G06T7/11,G06T7/187,G06T7/50,G06T7/593
</ipc_classes>

<assignee>
MAGIC LEAP
</assignee>

<inventors>
ZHANG, YIFU
WEI, XIAOLIN
MIN, JIANYUAN
MOLYNEAUX, DAVID GEOFFREY
STEINBRÃœCKER, FRANK THOMAS
WU, ZHONGLE
</inventors>

<docdb_family_id>
66950474
</docdb_family_id>

<title>
Method of occlusion rendering using raycast and live depth
</title>

<abstract>
An augmented reality/mixed reality system that provides a more immersive user experience. That experience is provided with increased speed of update for occlusion data by using depth sensor data augmented with lower-level reconstruction data. When operating in real-time dynamic environments, changes in the physical world can be reflected quickly in the occlusion data. Occlusion rendering using live depth data augmented with lower-level 3D reconstruction data, such as a raycast point cloud, can greatly reduce the latency for visual occlusion processing. Generating occlusion data in this way may provide faster operation of an XR system using less computing resources and enabling the system to be packaged in a battery operated wearable device.
</abstract>

<claims>
1. A method of operating a computing system to render a virtual object in a scene comprising a physical object, the method comprising: determining depth information from a sensor worn by a user, the depth information indicating a distance between the user and a portion of a surface of a physical object within a field of view of the sensor in an environment, wherein determining the depth information comprises: determining data for 3D (three-dimensional) reconstruction at least by using a raycast into the 3D reconstruction of a volumetric model for the environment; determining depth data based at least in part upon the data determined for the 3D reconstruction; generating a depth map based at least in part on the depth information; determining the 3D reconstruction for the environment before generation of occlusion data based at least in part upon the depth data and the depth map; and selecting a portion of the data for the 3D reconstruction at least by scanning the portion of the surface to fill a hole in the occlusion data; maintaining surface information for the portion of the surface in the depth map, the depth map being generated based at least in part on the depth information and comprising a plurality of values, each value indicating a respective distance to a point of the physical object, the surface information comprising the distance, such that the surface information is updated as the depth information changes; incrementally modifying the volumetric model for the environment based at least in part upon the surface information for the portion of the surface, wherein the volumetric model comprises a 3D shape that represents the portion of the surface, and the 3D shape comprises a 3D mesh or a 3D volume; culling a set of 3D shapes into a culled set of 3D shapes for representing the environment at least by projecting information pertaining to one or more 3D shapes representing a portion of the environment to the depth map; and rendering at least a portion of the volumetric model of the environment based at least in part upon the surface information, a location of the virtual object in a scene, the volumetric model for the environment, and the culled 3D set of shapes.
2. The method of claim 1, incrementally modifying the volumetric model for the environment comprising: determining a first set of elements that models the portion of the surface of the physical object, wherein the user is located at a first user location when the portion of the surface is appears the field of view of the sensor; adding the first set of elements into the volumetric model for the environment; determining a second set of elements that models an additional portion of the surface of the physical object, wherein the user is located at a second user location when the additional portion of the surface appears within the field of view of the sensor; and modifying the volumetric model for the environment at least by adding the second set of elements into the volumetric model for the environment.
3. The method of claim 1, wherein maintaining the surface information as the depth map comprises generating a portion of the depth map base on stereo depth information from a stereo camera, the stereo depth information corresponds to regions of the physical object selected based on the depth information, and the depth information is a sequence of depth images at a frame rate of at least 30 frames per second.
4. The method of claim 1, wherein maintaining the surface information as the depth map comprises generating the depth map based at least in part on low-level data of a 3D reconstruction of the physical object, and the low-level data of the 3D reconstruction of the physical object comprises a raycast point cloud.
5. The method of claim 1, incrementally modifying the volumetric model for the environment comprising: detecting a change in a region in the environment around the sensor at least by processing an image captured by the computing system or by processing the depth information; generating an indication of the change in the region; and in response to detection of the change, triggering incremental modification of the volumetric model for the environment.
6. The method of claim 1, wherein maintaining the surface information as the depth map comprises generating the depth map based at least in part on low-level data of a 3D representation of the scene comprising the physical object.
7. The method of claim 1, wherein: the computing system comprises a remote service and an application, wherein the remote service is external to the computing system; the surface information is generated by the remote service; and the application computes one or more portions of the virtual object to render based at least in part on the surface information received from the remote service.
8. The method of claim 1, wherein maintaining the surface information comprises: processing the depth information to generate a first depth map, the first depth map comprising a plurality of pixels, each pixel indicating a distance to a point of the physical object; selectively acquiring low-level data of a 3D reconstruction of the physical object; and generating the depth map based at least in part on the first depth map and the selectively-acquired low-level data of the 3D reconstruction of the physical object, wherein selectively acquiring the low-level data of the 3D reconstruction of the physical object comprises casting rays from a virtual camera to the 3D reconstruction of the physical object.
9. The method of claim 8, wherein processing the depth information to generate the depth map comprises: determining quality metrics for regions of the first depth map; identifying a hole in the first depth map based on the quality metrics; and removing the hole out of the first depth map.
10. The method of claim 1, incrementally modifying a volumetric model further comprising representing a portion of the environment as a set of one or more bricks at least by culling an initial set of multiple bricks into the set of one or more bricks based at least in part upon a frustum, wherein a brick comprises multiple elements, and the frustum is determined from at least the field of view.
11. The method of claim 10, representing the portion of the environment as the set of one or more bricks comprising: identifying the initial set of multiple bricks for the environment; culling the initial set of the multiple bricks into the set of one or more bricks based at least in part upon the frustum of an image sensor of the computing system; and culling the initial set of the multiple bricks into the set of one or more bricks at least by projecting a silhouette of a brick in the initial set of multiple bricks into the depth information based at least in part upon a location of the brick relative to the portion of the surface.
12. The method of claim 9, wherein of removing the hole out of the first depth map comprises removing one or more regions in the first depth map that are spaced from the location of the virtual object beyond a threshold distance.
13. The method of claim 10, further comprising: identifying a first brick having a first interface and a second brick having a second interface from the set of one or more bricks, wherein the first interface is adjacent to the second interface; identifying a first set of elements and a second set of elements respectively representing the first brick and the second brick; and constructing a first interface region at the first interface and a second interface region at the second interface.
14. The method of claim 13, further comprising: determining one or more first fused elements for the first interface region and one or more second fused elements for the second interface region; modifying the first brick at least by changing the first set of elements into a first modified set of elements, without modifying the second brick; and representing the one or more second fused elements for the second interface region at least by combining at least some of first data for the first brick or the first interface region with at least some of second data for the second brick or the second interface region.
15. The method of claim 14, wherein casting rays comprises casting first multiple rays at one or more boundary regions of the 3D reconstruction of the physical object based at least in part upon an eye gaze of the user, and second multiple rays in one or more center regions of the 3D reconstruction of the physical object, wherein the first multiple rays are denser than the second multiple rays.
16. The method of claim 1, wherein computing portions of the virtual object to render comprises simulating interaction of the virtual object with the physical object based at least in part on the surface information, the 3D mesh or the 3D volume for the 3D shape comprises a topologically connected mesh for a first portion of the 3D mesh and a topologically disconnected mesh for a second portion of the 3D mesh.
17. An electronic system portable by a user comprising: a processor configured to determine depth information about one or more physical objects in a scene from a sensor worn by a user, the depth information indicating a distance between the user and a portion of a physical object within a field of view of the sensor in an environment, wherein the depth information is transmitted to a service, wherein the processor configured to determine the depth information is further configured to: determine data for 3D (three-dimensional) reconstruction at least by using a raycast into the 3D reconstruction of a volumetric model for the environment; determine depth data based at least in part upon the data determined for the 3D reconstruction; generate a depth map based at least in part on the depth information; determine the 3D reconstruction for the environment before generation of occlusion data based at least in part upon the depth data and the depth map; and select a portion of the data for the 3D reconstruction at least by scanning the portion of the surface to fill a hole in the occlusion data; the processor further configured to: maintain surface information for the portion of the surface in the depth map, the depth map being generated based at least in part on the depth information and comprising a plurality of values, each value indicating a respective distance to a point of the physical object, the surface information comprising the distance, such that the surface information is updated as depth information changes; incrementally modify the volumetric model for the environment based at least in part upon the surface information for the portion of the surface, wherein the volumetric model comprises a 3D shape that represents the portion of the surface, and the 3D shape comprises a 3D mesh or a 3D volume; cull a set of 3D shapes into a culled set of 3D shapes for representing the environment at least by projecting information pertaining to one or more 3D shapes representing a portion of the environment to the depth map; and an application configured to execute first computer executable instructions to render a virtual object in the scene, wherein the application receives from the service a depth buffer of the surface information, and the application renders the virtual object based at least in part upon the surface information and a location of the virtual object in a scene, the volumetric model for the environment, and the culled 3D set of shapes.
18. The electronic system of claim 17, wherein the processor configured to incrementally modify the volumetric model for the environment is further configured to: determine a first set of elements that models the portion of the surface of the physical object, wherein the user is located at a first user location when the portion of the surface is appears the field of view of the sensor; add the first set of elements into the volumetric model for the environment; determine a second set of elements that models an additional portion of the surface of the physical object, wherein the user is located at a second user location when the additional portion of the surface appears within the field of view of the sensor; and modify the volumetric model for the environment at least by adding the second set of elements into the volumetric model for the environment.
19. The electronic system of claim 17, wherein the processor configured to incrementally modify the volumetric model for the environment is further configured to: detecting a change in a region in the environment around the sensor at least by processing an image captured by the computing system or by processing the depth information; generating an indication of the change in the region; and in response to detection of the change, triggering incremental modification of the volumetric model for the environment.
20. The electronic system of claim 1, wherein the processor configured to incrementally modify the volumetric model for the environment is further configured to: represent a portion of the environment as a set of one or more bricks at least by culling an initial set of multiple bricks into the set of one or more bricks based at least in part upon a frustum, wherein a brick comprises multiple elements, and the frustum is determined from at least the field of view.
21. The method of claim 1, wherein the processor is further configured to: identify a first brick having a first interface and a second brick having a second interface from the set of one or more bricks, wherein the first interface is adjacent to the second interface; identify a first set of elements and a second set of elements respectively representing the first brick and the second brick; construct a first interface region at the first interface and a second interface region at the second interface; determine one or more first fused elements for the first interface region and one or more second fused elements for the second interface region; modify the first brick at least by changing the first set of elements into a first modified set of elements, without modifying the second brick; and represent the one or more second fused elements for the second interface region at least by combining at least some of first data for the first brick or the first interface region with at least some of second data for the second brick or the second interface region.
</claims>
</document>
