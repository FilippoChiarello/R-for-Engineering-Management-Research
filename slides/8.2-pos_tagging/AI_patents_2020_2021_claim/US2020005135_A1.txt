<document>

<filing_date>
2018-06-29
</filing_date>

<publication_date>
2020-01-02
</publication_date>

<priority_date>
2018-06-29
</priority_date>

<ipc_classes>
G06N3/04,G06N3/063,G06N3/08
</ipc_classes>

<assignee>
AMD (ADVANCED MICRO DEVICES)
</assignee>

<inventors>
CHE SHUAI
</inventors>

<docdb_family_id>
69055207
</docdb_family_id>

<title>
OPTIMIZING INFERENCE FOR DEEP-LEARNING NEURAL NETWORKS IN A HETEROGENEOUS SYSTEM
</title>

<abstract>
Systems, methods, and devices for deploying an artificial neural network (ANN). Candidate ANNs are generated for performing an inference task based on specifications of a target inference device. Trained ANNs are generated by training the candidate ANNs to perform the inference task on an inference device conforming to the specifications. Characteristics describing the trained ANNs performance of the inference task on a device conforming to the specifications are determined. Profiles that reflect the characteristics of each trained ANN are stored. The stored profiles are queried based on requirements of an application to select an ANN from among the trained ANNs. The selected ANN is deployed on an inference device conforming to the target inference device specifications. Input data is communicated to the deployed ANN from the application. An output is generated using the deployed ANN, and the output is communicated to the application.
</abstract>

<claims>
1. A method for deploying an artificial neural network (ANN), the method comprising: generating candidate ANNs for performing an inference task based on specifications of a target inference device; generating trained ANNs by training the candidate ANNs to perform the inference task on an inference device conforming to the specifications; determining characteristics describing the trained ANNs performance of the inference task on a device conforming to the specifications; storing profiles of the trained ANNs, the profiles reflecting the characteristics of each trained ANN; querying the stored profiles based on requirements of an application to select an ANN from among the trained ANNs; deploying the selected ANN on an inference device conforming to the target inference device specifications.
2. The method of claim 1, wherein the specifications of the target inference device comprise the architecture, components, bit width, device types, memory structures, memory types, or memory capacity of the target inference device.
3. The method of claim 1, further comprising: communicating input data to the deployed ANN from the application; generating an output using the deployed ANN; and communicating the output data to the application.
4. The method of claim 1, wherein the characteristics of the trained ANNs comprise a memory capacity requirement, inference time, latency, accuracy, number of layers, type of layer, type of activation function, or ANN topology.
5. The method of claim 1, further comprising receiving, in response to querying the stored profiles, an indication of one or more ANNs having a stored profile that satisfies the requirement.
6. The method of claim 1, wherein the application provides data to the deployed ANN and receives an inference from the deployed ANN based on the data.
7. The method of claim 1, wherein the requirements of the application comprise a maximum latency, a maximum time to inference, a maximum memory capacity, a maximum power, or a device constraint.
8. The method of claim 1, wherein deploying the selected ANN on the inference device comprises loading the selected ANN into at least one memory of the inference device.
9. The method of claim 8, wherein the memory into which the selected ANN is loaded is determined based on the profile of the selected ANN.
10. A method for generating an artificial neural network (ANN), the method comprising: inputting specifications of a target inference device to an ANN generation device; generating candidate ANNs, by the ANN generation device, based on the specifications; generating trained ANNs, by the ANN generation device, by training the candidate ANNs to perform an inference task; generating profiles of the trained ANNs, wherein the profiles indicate characteristics of the trained ANNs; storing the profiles to be queried based on the requirements and to return a profile of one of the trained ANNs having characteristics satisfying requirements of an application, for deployment on a target inference device.
11. The method of claim 10, wherein the specifications of the target inference device comprise the architecture, components, bit width, device types, memory structures, memory types, or memory capacity of the target inference device.
12. The method of claim 10, wherein the inference task comprises image recognition.
13. The method of claim 10, wherein the characteristics of the trained ANNs comprise a memory capacity requirement, inference time, latency, accuracy, number of layers, type of layer, type of activation function, or ANN topology.
14. The method of claim 10, wherein the requirements of the application comprise a maximum latency, a maximum time to inference, a maximum memory capacity, a maximum power, or a device constraint.
15. The method of claim 10, wherein deploying the selected ANN on the inference device comprises loading the selected ANN into at least one memory of the inference device.
16. The method of claim 10, wherein the memory into which the selected ANN is loaded is determined based on the profile of the selected ANN.
17. A method for deploying an artificial neural network (ANN), the method comprising: querying stored profiles, based on requirements of an application, to select an ANN, the profiles reflecting characteristics of a plurality of ANNs trained to perform an inference task on an inference device conforming to specifications of a target inference device; and deploying the selected ANN on an inference device conforming to the target inference device specifications.
18. The method of claim 17, wherein the specifications of the target inference device comprise the architecture, components, bit width, device types, memory structures, memory types, or memory capacity of the target inference device.
19. The method of claim 17, further comprising: communicating input data to the deployed ANN from the application; generating an output using the deployed ANN; and communicating the output data to the application.
20. The method of claim 17, further comprising receiving, in response to querying the stored profiles, an indication of one or more ANNs having a stored profile that satisfies the requirement.
21. The method of claim 17, wherein the application provides data to the deployed ANN and receives an inference from the deployed ANN based on the data.
22. The method of claim 17, wherein the requirements of the application comprise a maximum latency, a maximum time to inference, a maximum memory capacity, a maximum power, or a device constraint.
23. The method of claim 17, wherein deploying the selected ANN on the inference device comprises loading the selected ANN into at least one memory of the inference device.
24. The method of claim 23, wherein the memory into which the selected ANN is loaded is determined based on the profile of the selected ANN.
25. A device for generating an artificial neural network (ANN), the device comprising: an input interface configured to input specifications of a target inference device; processing circuitry configured to generate candidate ANNs based on the specifications; processing circuitry configured to generate trained ANNs by training the candidate ANNs to perform an inference task; profiling circuitry configured to generate profiles of the trained ANNs which reflect the characteristics of each trained ANN, and to store the profiles to be queried based on the requirements and to return a profile of one of the trained ANNs having characteristics satisfying requirements of an application, for deployment on a target inference device.
26. The device of claim 25, wherein the specifications of the target inference device comprise the architecture, components, bit width, device types, memory structures, memory types, or memory capacity of the target inference device.
27. The device of claim 25, wherein the characteristics of the trained ANNs comprise a memory capacity requirement, inference time, latency, accuracy, number of layers, type of layer, type of activation function, or ANN topology.
28. The device of claim 25, wherein the requirements of the application comprise a maximum latency, a maximum time to inference, a maximum memory capacity, a maximum power, or a device constraint.
29. A device for deploying an artificial neural network (ANN) to perform an inference task, the device comprising: an input interface configured to input inference task requirements of an application; querying circuitry configured to query stored profiles based on the requirements, the profiles reflecting characteristics of ANNs trained to perform an inference task on a target inference device; the querying circuitry further configured to select an ANN based on the query; and deployment circuitry configured to deploy the selected ANN on an inference device conforming to specifications of the target inference device.
30. The device of claim 29, wherein the specifications of the target inference device comprise the architecture, components, bit width, device types, memory structures, memory types, or memory capacity of the target inference device.
31. The device of claim 29, wherein querying the stored profiles comprises matching the requirements to characteristics of the trained ANNs.
32. The device of claim 31, wherein the characteristics comprise a memory capacity requirement, inference time, latency, accuracy, number of layers, type of layer, type of activation function, or ANN topology.
33. The device of claim 29, wherein the querying circuitry is configured to receive, in response to the query, an indication of one or more ANNs having a stored profile that satisfies the requirements.
34. The device of claim 29, wherein the application provides data to the deployed ANN and receives an inference from the deployed ANN based on the data.
35. The device of claim 29, wherein the requirements of the application comprise a maximum latency, a maximum time to inference, a maximum memory capacity, a maximum power, or a device constraint.
36. The device of claim 29, wherein deploying the selected ANN on the inference device comprises loading the selected ANN into at least one memory of the inference device.
37. The device of claim 36, wherein the memory into which the selected ANN is loaded is determined based on the profile of the selected ANN.
</claims>
</document>
