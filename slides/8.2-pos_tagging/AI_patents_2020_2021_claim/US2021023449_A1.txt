<document>

<filing_date>
2019-05-24
</filing_date>

<publication_date>
2021-01-28
</publication_date>

<priority_date>
2018-05-25
</priority_date>

<ipc_classes>
A63F13/52,A63F13/5378,A63F13/86
</ipc_classes>

<assignee>
GUANGZHOU HUYA INFORMATION TECHNOLOGY COMPANY
</assignee>

<inventors>
LIU, LU
Wu, Xiaodong
</inventors>

<docdb_family_id>
64006021
</docdb_family_id>

<title>
GAME SCENE DESCRIPTION METHOD AND APPARATUS, DEVICE, AND STORAGE MEDIUM
</title>

<abstract>
A game scene description method and apparatus, a device, and a storage medium is provided. The method includes: obtaining at least one video frame from a game live video stream; capturing a game map region image in the at least one video frame; inputting the game map region image into a first target detection model to obtain a display region of a game element on the game map region image; inputting an image of the display region of the game element into a classification model to obtain a state of the game element; and forming description information of a game scene displayed by the at least one video frame by the display region and the state of the game element.
</abstract>

<claims>
1. A game scene description method, comprising: acquiring at least one video frame in a game live broadcast video stream; capturing a game map area image in the at least one video frame; inputting the game map area image to a first target detection model to obtain a display area of a game element in the game map area image; inputting an image of the display area of the game element to a classification model to obtain a state of the game element; and forming description information of a game scene displayed by the at least one video frame by adopting the display area and the state of the game element.
2. The method of claim 1, wherein capturing the game map area image in the at least one video frame comprises: inputting the at least one video frame to a second target detection model to obtain a game map detection area in the at least one video frame; correcting the game map detection area by performing feature matching on a route feature in the game map detection area and a reference feature, to obtain a game map correction area; and in a case where a deviation distance of a game map correction area of one video frame of the at least one video frame relative to a game map detection area of the one video frame exceeds a deviation threshold, capturing an image of the game map detection area in the one video frame.
3. The method of claim 2, further comprising: in a case where the deviation distance of the game map correction area of the one video frame relative to the game map detection area of the one video frame does not exceed the deviation threshold, capturing an image of the game map correction area in the one video frame.
4. The method of claim 2, wherein before inputting the at least one video frame to the second target detection model, the method further comprises: acquiring a plurality of sample video frames, wherein the plurality of sample video frames and the at least one video frame correspond to a same game type; and constituting a second training sample set by the plurality of sample video frames and a display area of a game map in the plurality of sample video frames, and training the second target detection model by using the second training sample set.
5. The method of claim 1, before inputting the game map area image to the first target detection model to obtain the display area of the game element in the game map area image, the method further comprises: acquiring a plurality of game map sample images, wherein the plurality of game map sample images and the game map area image correspond to a same game type; and constituting a first training sample set by the plurality of game map sample images and a display area of a game element in the plurality of game map sample images, and training the first target detection model by using the first training sample set.
6. The method of claim 1, wherein the first target detection model comprises a feature map generation sub-model, a grid segmentation sub-model, and a positioning sub-model; wherein inputting the game map area image to the first target detection model to obtain the display area of the game element in the game map area image comprises: inputting the game map area image to the feature map generation sub-model to generate a feature map of the game map area image; inputting the feature map to the grid segmentation sub-model to segment the feature map into a plurality of grids, wherein a difference between a size of each of the plurality of grids and a minimum size of the game element is within a preset size range; inputting the plurality of grids to the positioning sub-model to obtain a matching degree between each of the plurality of grids and a feature of a respective one of a plurality of types of game elements; and determining an area corresponding to a grid with a maximum matching degree as a display area of a corresponding type of game elements in the game map area image by adopting a non-maximum value suppression algorithm.
7. The method of claim 1, wherein forming the description information of the game scene displayed by the at least one video frame by adopting the display area and the state of the game element comprises: obtaining description information of a game scene displayed by one video frame of the at least one video frame according to a correspondence between the description information and a display area and a state of the game element in the one video frame; or, wherein forming the description information of the game scene displayed by the at least one video frame by adopting the display area and the state of the game element comprises: obtaining a display area change trend from a display area of the game element in a plurality of video frames, and obtaining a state change trend from a state of the game element in the plurality of video frames; and obtaining description information of a game scene displayed by the plurality of video frames according to a correspondence between the description information and the display area change trend and the state change trend of the game element.
8. A game scene description apparatus, comprising: an acquisition module, which is configured to acquire at least one video frame in a game live broadcast video stream; a capturing module, which is configured to capture a game map area image in the at least one video frame; a display area recognition module, which is configured to input the game map area image to a first target detection model to obtain a display area of a game element in the game map area image; a state recognition module, which is configured to input an image of the display area of the game element to a classification model to obtain a state of the game element; and a forming module, which is configured to form description information of a game scene displayed by the at least one video frame by adopting the display area and the state of the game element.
9. An electronic device, comprising: at least one processor; and a memory, which is configured to store at least one program; wherein the at least one program, when executed by the at least one processor, causes the at least one processor to implement a game scene description method, wherein the game scene description method comprises: acquiring at least one video frame in a game live broadcast video stream: capturing a game map area image in the at least one video frame; inputting the game map area image to a first target detection model to obtain a display area of a game element in the game map area image: inputting an image of the display area of the game element to a classification model to obtain a state of the game element; and forming description information of a game scene displayed by the at least one video frame by adopting the display area and the state of the game element.
10. A computer-readable storage medium, storing a computer program, wherein the computer program, when executed by a processor, implements the game scene description method of claim 1.
11. The electronic device of claim 9, wherein capturing the game map area image in the at least one video frame comprises: inputting the at least one video frame to a second target detection model to obtain a game map detection area in the at least one video frame; correcting the game map detection area by performing feature matching on a route feature in the game map detection area and a reference feature, to obtain a game map correction area; and in a case where a deviation distance of a game map correction area of one video frame of the at least one video frame relative to a game map detection area of the one video frame exceeds a deviation threshold, capturing an image of the game map detection area in the one video frame.
12. The electronic device of claim 11, further comprising: in a case where the deviation distance of the game map correction area of the one video frame relative to the game map detection area of the one video frame does not exceed the deviation threshold, capturing an image of the game map correction area in the one video frame.
13. The electronic device of claim 11, wherein before inputting the at least one video frame to the second target detection model, the method further comprises: acquiring a plurality of sample video frames, wherein the plurality of sample video frames and the at least one video frame correspond to a same game type; and constituting a second training sample set by the plurality of sample video frames and a display area of a game map in the plurality of sample video frames, and training the second target detection model by using the second training sample set.
14. The electronic device of claim 9, before inputting the game map area image to the first target detection model to obtain the display area of the game element in the game map area image, the method further comprises: acquiring a plurality of game map sample images, wherein the plurality of game map sample images and the game map area image correspond to a same game type; and constituting a first training sample set by the plurality of game map sample images and a display area of a game element in the plurality of game map sample images, and training the first target detection model by using the first traning sample set.
15. The electronic device of claim 9, wherein the first target detection model comprises a feature map generation sub-model, a grid segmentation sub-model, and a positioning sub-model; wherein inputting the game map area image to the first target detection model to obtain the display area of the game element in the game map area image comprises: inputting the game map area image to the feature map generation sub-model to generate a feature map of the game map area image; inputting the feature map to the grid segmentation sub-model to segment the feature map into a plurality of grids, wherein a difference between a size of each of the plurality of grids and a minimum size of the game element is within a preset size range; inputting the plurality of grids to the positioning sub-model to obtain a matching degree between each of the plurality of grids and a feature of a respective one of a plurality of types of game elements; and determining an area corresponding to a grid with a maximum matching degree as a display area of a corresponding type of game elements in the game map area image by adopting a non-maximum value suppression algorithm.
16. The electronic device of claim 9, wherein forming the description information of the game scene displayed by the at least one video frame by adopting the display area and the state of the game element comprises: obtaining description information of a game scene displayed by one video frame of the at least one video frame according to a correspondence between the description information and a display area and a state of the game element in the one video frame; or, wherein forming the description information of the game scene displayed by the at least one video frame by adopting the display area and the state of the game element comprises: obtaining a display area change trend from a display area of the game element in a plurality of video frames, and obtaining a state change trend from a state of the game element in the plurality of video frames; and obtaining description information of a game scene displayed by the plurality of video frames according to a correspondence between the description information and the display area change trend and the state change trend of the game element.
</claims>
</document>
