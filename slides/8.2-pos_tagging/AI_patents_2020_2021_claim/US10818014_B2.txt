<document>

<filing_date>
2018-07-27
</filing_date>

<publication_date>
2020-10-27
</publication_date>

<priority_date>
2018-07-27
</priority_date>

<ipc_classes>
G06T7/11,G06T7/194,G06T7/73,G11B27/031
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
COHEN, SCOTT
PRICE, BRIAN
XU NING
</inventors>

<docdb_family_id>
69177488
</docdb_family_id>

<title>
Image object segmentation based on temporal information
</title>

<abstract>
A temporal object segmentation system determines a location of an object depicted in a video. In some cases, the temporal object segmentation system determines the object's location in a particular frame of the video based on information indicating a previous location of the object in a previous video frame. For example, an encoder neural network in the temporal object segmentation system extracts features describing image attributes of a video frame. A convolutional long-short term memory neural network determines the location of the object in the frame, based on the extracted image attributes and information indicating a previous location in a previous frame. A decoder neural network generates an image mask indicating the object's location in the frame. In some cases, a video editing system receives multiple generated masks for a video, and modifies one or more video frames based on the locations indicated by the masks.
</abstract>

<claims>
1. A method of generating multiple masks for an object depicted in a video, the method including one or more processing devices performing operations comprising: receiving (i) a video file having multiple frames in which an object is at least partially visible and (ii) a first mask corresponding to a first frame of the multiple frames, wherein the first mask indicates a first location of the object in the first frame; extracting, from the first frame, a location feature map indicating the first location of the object, by applying an initializer subnetwork to the first frame and the first mask, wherein the initializer subnetwork is trained to determine the first location of the object based on the first mask; extracting, from a second frame of the multiple frames, an image feature map indicating attributes of the second frame, by applying an encoder subnetwork to the second frame, wherein the encoder subnetwork is trained to determine the attributes of the second frame based on the second frame; extracting a difference feature map indicating a second location of the object, by applying a convolutional long-short term memory ("LSTM") subnetwork to the location feature map and to the image feature map, wherein the convolutional LSTM subnetwork is trained to determine the second location based on the location feature map and the image feature map; generating, based on the difference feature map, a second mask indicating the second location of the object, by applying a decoder subnetwork to the difference feature map; selecting a training mask that indicates a known location of the object in the second frame; determining an error based on a comparison of the generated second mask with the training mask, wherein the error indicates a variation between the second location of the object and the known location of the object; and modifying, based on the determined error, one or more of the initializer subnetwork, the encoder subnetwork, the convolutional LSTM subnetwork, or the decoder subnetwork.
2. The method of claim 1, wherein the location feature map extracted by the initializer subnetwork is received as a hidden state by the convolutional LSTM subnetwork.
3. The method of claim 1, the operations further comprising: generating, subsequent to generating the second mask and based on a third frame of the multiple frames, a third mask indicating a third location of the object in the third frame, wherein generating the third mask further comprises: extracting, from the third frame, an additional image feature map indicating attributes of the third frame, by applying the encoder subnetwork to the third frame; extracting an additional difference feature map indicating a third location of the object, by applying the convolutional LSTM subnetwork to the difference feature map and the additional image feature map; and generating, based on the additional difference feature map, the third mask indicating the third location of the object, by applying the decoder subnetwork to the additional difference feature map.
4. The method of claim 1, further comprising extracting, from the first frame and by applying the initializer subnetwork, a group of multiple feature maps, wherein the location feature map is included in the group of multiple feature maps.
5. The method of claim 1, wherein the attributes of the second frame indicate graphical or semantic image content of the second frame, including one or more of: edges, colors, gradients, or subject matter depicted in the second frame.
6. The method of claim 1, further comprising providing the first mask and the second mask to a video editing system that is capable of modifying the video based on the first location and the second location of the object.
7. A non-transitory computer-readable medium embodying program code for generating multiple masks for an object depicted in a video, the program code comprising instructions which, when executed by a processor, cause the processor to perform operations comprising: receiving (i) a video file including multiple frames in which the object is at least partially visible and (ii) a first mask corresponding to a first frame of the multiple frames, wherein the first mask indicates a first location of the object in the first frame; extracting, from the first frame, a location feature map indicating the first location of the object, by applying an initializer subnetwork to the first frame and the first mask, wherein the initializer subnetwork is trained to determine the first location of the object based on the first mask; extracting, from a second frame of the multiple frames, an image feature map indicating attributes of the second frame, by applying an encoder subnetwork to the second frame, wherein the encoder subnetwork is trained to determine the attributes of the second frame based on the second frame; extracting a difference feature map indicating a second location of the object, by applying a convolutional long-short term memory ("LSTM") subnetwork to the location feature map and to the image feature map, wherein the convolutional LSTM subnetwork is trained to determine the second location based on the location feature map and the image feature map; generating, based on the difference feature map, a second mask indicating the second location of the object, by applying a decoder subnetwork to the difference feature map; selecting a training mask that indicates a known location of the object in the second frame; determining an error based on a comparison of the generated second mask with the training mask, wherein the error indicates a variation between the second location of the object and the known location of the object; and modifying, based on the determined error, one or more of the initializer subnetwork, the encoder subnetwork, the convolutional LSTM subnetwork, or the decoder subnetwork.
8. The non-transitory computer-readable medium of claim 7, wherein the location feature map extracted by the initializer subnetwork is received as a hidden state by the convolutional LSTM subnetwork.
9. The non-transitory computer-readable medium of claim 7, the operations further comprising: generating, subsequent to generating the second mask and based on a third frame of the multiple frames, a third mask indicating a third location of the object in the third frame, wherein generating the third mask further comprises: extracting, from the third frame, an additional image feature map indicating attributes of the third frame, by applying the encoder subnetwork to the third frame; extracting an additional difference feature map indicating the third location of the object, by applying the convolutional LSTM subnetwork to the difference feature map and the additional image feature map; and generating, based on the additional difference feature map, the third mask indicating the third location of the object, by applying the decoder subnetwork to the additional difference feature map.
10. The non-transitory computer-readable medium of claim 9, wherein the difference feature map is received as a hidden state by the convolutional LSTM subnetwork.
11. The non-transitory computer-readable medium of claim 7, further comprising providing the first mask and the second mask to a video editing system that is capable of modifying the video based on the first location and the second location of the object.
12. An object segmentation system for generating a group of masks for an object depicted in a video, the object segmentation system comprising: a memory device storing instructions which, when executed by a processor, implement a mask extraction subnetwork, the mask extraction subnetwork including an encoder subnetwork, a convolutional long-short term memory ("LSTM") subnetwork, and a decoder subnetwork; a means for receiving (i) a video file including multiple frames in which the object is at least partially visible and (ii) a initializing mask, wherein the initializing mask indicates an initial location of the object in a first frame of the multiple frames; a means for extracting, from the first frame, a first image feature map indicating attributes of the first frame by applying the encoder subnetwork to the first frame and the initializing mask, wherein the encoder subnetwork is trained to determine the attributes based on the first frame and the initializing mask; a means for extracting a difference feature map indicating a first location of the object, by applying the convolutional LSTM subnetwork to the first image feature map, wherein the convolutional LSTM subnetwork is trained to determine the first location of the object based on the first image feature map; a means for generating, based on the difference feature map, a first mask indicating the first location of the object, by applying the decoder subnetwork to the difference feature map; a means for selecting a training mask associated with the first frame, the training mask indicating a known location of the object in the first frame; a means for determining an error based on a comparison of the generated first mask with the training mask associated with the first frame, wherein the error indicates a variation between the first location of the object and the known location of the object; and a means for modifying, based on the determined error, the object segmentation system.
13. The system of claim 12, wherein the first image feature map indicates image attributes and location attributes.
14. The system of claim 12, wherein the convolutional LSTM subnetwork is trained to determine the first location of the object based on the first image feature map based on memory information received as a hidden state by the convolutional LSTM subnetwork.
15. The system of claim 12, further comprising: a means for generating, based on the first mask and a second frame of the multiple frames, a second mask indicating a second location of the object in the second frame, wherein the means for generating the second mask further comprises: a means for extracting, from the second frame, a second image feature map indicating additional attributes of the second frame, by applying the encoder subnetwork to the second frame and the first mask; a means for extracting an additional difference feature map indicating the second location of the object, by applying the convolutional LSTM subnetwork to the difference feature map and the second image feature map; and a means for generating, based on the additional difference feature map, the second mask indicating the second location of the object, by applying the decoder subnetwork to the additional difference feature map.
16. The system of claim 12, further comprising a means for extracting, from the first frame and by applying the encoder subnetwork, a group of multiple feature maps, wherein the first image feature map is included in the group of multiple feature maps.
17. The system of claim 12, further comprising providing the generated first mask to a video editing system that is capable of modifying the video based on the first location of the object.
</claims>
</document>
