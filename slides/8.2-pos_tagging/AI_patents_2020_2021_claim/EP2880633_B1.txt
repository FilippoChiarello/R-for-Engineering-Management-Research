<document>

<filing_date>
2013-07-31
</filing_date>

<publication_date>
2020-10-28
</publication_date>

<priority_date>
2012-07-31
</priority_date>

<ipc_classes>
G06T13/40
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
CHEN, JIAWEN
IZADI, SHAHRAM
FITZGIBBON, Andrew William
</inventors>

<docdb_family_id>
48948551
</docdb_family_id>

<title>
ANIMATING OBJECTS USING THE HUMAN BODY
</title>

<abstract>
Methods of animating objects using the human body are described. In an embodiment, a deformation graph is generated from a mesh which describes the object. Tracked skeleton data is received which is generated from sensor data and the tracked skeleton is then embedded in the graph. Subsequent motion which is captured by the sensor result in motion of the tracked skeleton and this motion is used to define transformations on the deformation graph. The transformations are then applied to the mesh to generate an animation of the object which corresponds to the captured motion. In various examples, the mesh is generated by scanning an object and the deformation graph is generated using orientation-aware sampling such that nodes can be placed close together within the deformation graph where there are sharp corners or other features with high curvature in the object.
</abstract>

<claims>
1. A method of animating an object comprising: generating, by a processor, a deformation graph automatically from an input mesh defining the object (112), the generating comprising: defining a plurality of node positions from the input mesh using sampling (502); creating a nearest neighbor graph by connecting each node to a number of nearest neighbor nodes (504); the sampling and the creating of the nearest neighbor graph both using a 5D orientation-aware distance D(p,q) which is defined as: where: where p and q are samples with normals np and nq, ; receiving body tracking data defining positions of one or more points on a body (114); mapping points on a skeleton to points on the deformation graph using the body tracking data (116) to generate a tracked skeleton; transforming the deformation graph in real-time by computing a plurality of transformations (118) on the deformation graph based on motion of the tracked skeleton; and dynamically applying the plurality of transformations to the input mesh to render a corresponding animation of the object (120).
2. A method according to claim 1, further comprising creating the input mesh by: generating a 3D volumetric reconstruction of a scene scanned by a user with a depth camera (401); segmenting an object from the 3D volumetric reconstruction of the scene (402); and extracting a geometric isosurface from the segmented portion of the 3D volumetric reconstruction (403).
3. A method according to any of the preceding claims, wherein the deformation graph is generated automatically from the input mesh using orientation-aware sampling (502).
4. A method according to any of the preceding claims, wherein generating a deformation graph automatically from an input mesh defining the object comprises:
adding additional edges to the nearest neighbor graph based on analysis of connected components in the nearest neighbor graph (506).
5. A method according to claim 4, wherein skeleton is an animal skeleton.
6. A method according to claim 4 or 5, wherein adding additional edges to the nearest neighbor graph based on analysis of connected components in the nearest neighbor graph comprises: identifying any connected components in the nearest neighbor graph (562); forming a weighted graph comprising nodes corresponding to each identified connected component and weights defined based on a minimum distance between the connected components in the nearest neighbor graph (563); computing a minimum spanning tree on the weighted graph (564); and adding an additional edge to the nearest neighbor graph where there is an edge between the connected components in the spanning tree (565).
7. A method according to any of claims 4-6, wherein transforming the deformation graph in real-time by computing a plurality of transformations on the deformation graph based on motion of the tracked skeleton comprises treating the additional edges as links with greater rigidity than other connections between nodes in the nearest neighbor graph.
8. A method according to any of the preceding claims, wherein mapping points on a skeleton to points on the deformation graph comprises: providing visual feedback to a user showing alignment of the tracked body and the input mesh (600); dynamically generating a proposed mapping between points on a skeleton and a number of nearest nodes in the input mesh: and in response to a verbal command received from the user (708), storing the proposed mapping (710), and wherein the stored mapping is used in dynamically applying the plurality of transformations to the mesh.
9. A system (300) comprising: an input for receiving an input mesh defining an object; a pre-processing module (308) arranged to generate a deformation graph automatically from the input mesh; and a warp module (310) arranged to receive the deformation graph, the input mesh and body tracking data defining positions of one or more points on a body, to map points on a skeleton to points on the deformation graph using the body tracking data to generate a tracked skeleton, to compute a series of transformations on the deformation graph in real-time based on motion of the tracked skeleton, and to apply the series of translations to the input mesh to generate an animation of the object; wherein the pre-processing module is arranged to define a plurality of node positions from the input mesh using sampling (502) and create a nearest neighbor graph by connecting each node to a number of nearest neighbor graph both using a 5D orientation-aware distance D(p,q) which is defined as: where: where p and q are samples with normals np and nq,.
10. A system according to claim 9, further comprising:
a skeleton tracker module (312) arranged receive data from a sensor (314) and to generate the body tracking data from the received data.
</claims>
</document>
