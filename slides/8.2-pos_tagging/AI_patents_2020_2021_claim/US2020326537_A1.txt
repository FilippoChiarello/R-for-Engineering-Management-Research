<document>

<filing_date>
2020-01-21
</filing_date>

<publication_date>
2020-10-15
</publication_date>

<priority_date>
2019-04-11
</priority_date>

<ipc_classes>
G02B27/00,G02B27/01,G06F3/01
</ipc_classes>

<assignee>
Hypergiant Industries, Inc.
</assignee>

<inventors>
BUSEY, ANDREW THOMAS
LAMM, BENJAMIN EDWARD
Haab, Daniel
Amato, Greg
Saltzgiver, Davis
</inventors>

<docdb_family_id>
72747866
</docdb_family_id>

<title>
GESTURE CONTROL OF HEADS-UP DISPLAY
</title>

<abstract>
An apparatus for providing gesture recognition that may be used to control the display of data in a heads-up display (HUD) is disclosed. The HUD may be a HUD associated with a visor on a user's helmet or another aeronautical helmet. An image capture system (e.g., camera) coupled to the helmet (or spacesuit) may be used to capture images of gestures being made by the user. Recognition of the gestures may then be used to control the display of data in the HUD. Control of the display of data may include controlling the selection of data that is displayed or turning on/off the HUD display in an on-demand capacity.
</abstract>

<claims>
1. An apparatus comprising: a wearable element configured to be worn by a user; an image capture device coupled to the wearable element, wherein the image capture device is configured to capture one or more two-dimensional images in a field of view of the user; a digital heads-up display (HUD) screen coupled to the wearable element, wherein the digital HUD screen is positioned to display images of data in the field of view of the user; and a processor circuit that includes one or more processing cores; memory storing program instructions executable by the processor circuit to: receive a plurality of two-dimensional images captured by the image capture device; and recognize, based on at least one trained machine learning algorithm, gestures made by the user in the received two-dimensional images.
2. The apparatus of claim 1, wherein the program instructions executable by the processor circuit include instructions to control at least one characteristic in the digital HUD screen based on a recognized gesture made by the user.
3. The apparatus of claim 2, wherein the at least one characteristic in the digital HUD screen includes a characteristic related to the images of data displayed in the field of view of the user.
4. The apparatus of claim 1, wherein at least one image of data displayed in the field of view of the user includes data from a sensor positioned on the apparatus.
5. The apparatus of claim 1, wherein the image capture device is a single image capture device.
6. The apparatus of claim 1, wherein the image capture device is a single image capture device, and wherein the program instructions executable by the processor circuit include instructions to: recognize, based on the at least one trained machine learning algorithm, at least one gesture made by the user by assessing motion of a hand of the user in the received two-dimensional images; and control at least one characteristic in the digital HUD screen based on the at least one recognized gesture made by the user.
7. The apparatus of claim 1, wherein the wearable element is a helmet apparatus configured to be attached to a spacesuit.
8. The apparatus of claim 7, wherein the helmet apparatus and the spacesuit, when attached, provide a substantially sealed, breathable environment for the user inside the helmet apparatus and the spacesuit.
9. The apparatus of claim 1, wherein the image capture device includes an optical camera.
10. A non-transitory computer-readable medium having instructions stored thereon that are executable by a computing device to perform operations comprising: capturing, by an image capture device coupled to a wearable element worn by a user, one or more two-dimensional images in a field of view of the user; receiving, by a computer system coupled to the wearable element, the two-dimensional images captured by the image capture device; assessing the received two-dimensional images, based on at least one trained machine learning algorithm, to recognize at least one gesture made by the user; and displaying, on a digital heads-up display (HUD) screen coupled to the wearable element, images of data, wherein the digital HUD screen displays the images of data in the field of view of the user, and wherein at least one characteristic of the images of data is displayed based on the at least one recognized gesture made by the user.
11. The non-transitory computer-readable medium of claim 10, wherein assessing the received two-dimensional images includes assessing motion of a hand of the user in the received two-dimensional images.
12. The non-transitory computer-readable medium of claim 10, wherein the at least one characteristic of the images of data includes data for a device coupled to the wearable element.
13. The non-transitory computer-readable medium of claim 10, wherein the at least one characteristic of the images of data includes a selection of data for display on the digital HUD screen.
14. The non-transitory computer-readable medium of claim 10, wherein recognizing the at least one gesture made by the user includes classifying the at least one gesture into a gesture classification category.
15. The non-transitory computer-readable medium of claim 10, wherein assessing the received two-dimensional images is implemented substantially continuously.
16. The non-transitory computer-readable medium of claim 10, wherein the at least one trained machine learning algorithm is trained by: accessing a training data set that includes images of a plurality of gestures by a user corresponding to one or more classification categories and known labels for one or more subsets of the training data set, wherein the images are two-dimensional images; and training the machine learning algorithm to generate a predictive score indicative of whether an unclassified gesture corresponds to at least one gesture classification category based on the two-dimensional images and the known labels.
17. A method, comprising: accessing a training data set that includes images of a plurality of gestures by a user corresponding to one or more classification categories and known labels for one or more subsets of the training data set, wherein the images are two-dimensional images; and training a machine learning module to generate a predictive score indicative of whether an unclassified gesture corresponds to at least one gesture classification category based on the two-dimensional images and the known labels.
18. The method of claim 17, wherein training the machine learning module includes determining one or more classifiers for implementation in the machine learning module.
19. The method of claim 18, further comprising: implementing the classifiers into a memory associated with a processor circuit that includes one or more processing cores, wherein the processor circuit is configured to be coupled to an apparatus comprising: a wearable element configured to be worn by a user; an image capture device coupled to the wearable element, wherein the image capture device is configured to capture one or more images in a field of view of the user; and a digital heads-up display (HUD) screen coupled to the wearable element, wherein the digital HUD screen is positioned to display images of data in the field of view of the user; wherein the memory stores program instructions executable by the processor circuit to: receive images captured by the image capture device; and recognize, from the received images, gestures made by the user based on the classifiers.
20. The method of claim 17, wherein the predictive score is a probability that the unclassified gesture corresponds to the at least one gesture classification category.
</claims>
</document>
