<document>

<filing_date>
2018-11-14
</filing_date>

<publication_date>
2020-09-15
</publication_date>

<priority_date>
2018-11-14
</priority_date>

<ipc_classes>
G06F40/242,G06N3/04,G06N3/08,G10L15/00,G10L15/08,G10L15/16,G10L15/22
</ipc_classes>

<assignee>
SRI INTERNATIONAL
</assignee>

<inventors>
FRANCO, HORACIO
MITRA, VIKRAMJIT
VAN HOUT, JULIEN
YILMAZ, EMRE
</inventors>

<docdb_family_id>
70551763
</docdb_family_id>

<title>
Time-frequency convolutional neural network with bottleneck architecture for query-by-example processing
</title>

<abstract>
A computing system determines whether a reference audio signal contains a query. A time-frequency convolutional neural network (TFCNN) comprises a time and frequency convolutional layers and a series of additional layers, which include a bottleneck layer. The computation engine applies the TFCNN to samples of a query utterance at least through the bottleneck layer. A query feature vector comprises output values of the bottleneck layer generated when the computation engine applies the TFCNN to the samples of the query utterance. The computation engine also applies the TFCNN to samples of the reference audio signal at least through the bottleneck layer. A reference feature vector comprises output values of the bottleneck layer generated when the computation engine applies the TFCNN to the samples of the reference audio signal. The computation engine determines at least one detection score based on the query feature vector and the reference feature vector.
</abstract>

<claims>
1. A computing system for determining whether a reference audio signal contains a query, the computing system comprising: a computer-readable storage medium configured to: store samples of the reference audio signal, each of the samples of the reference audio signal corresponding to a different combination of a time band and a frequency band; and store data representing a time-frequency convolutional neural network (TFCNN), the TFCNN comprising: a time convolutional layer that applies first filters to first input feature vectors, the first input feature vectors being sets of the samples that correspond to a same frequency band and different time bands; a frequency convolutional layer that applies second filters to second input feature vectors, the second input feature vectors being sets of the samples that correspond to a same time band and different frequency bands; and a series of additional layers, wherein: the series of additional layers includes an input layer, an output layer, and a series of hidden layers between the input layer and the output layer, input to the input layer comprises output of the time convolutional layer and output of the frequency convolutional layer, and the series of hidden layers includes a bottleneck layer that includes fewer neurons than a hidden layer that precedes the bottleneck layer in the series of hidden layers; a computation engine that comprises circuitry configured to apply the TFCNN to samples of the query utterance at least through the bottleneck layer, the TFCNN is trained to discriminate phonetic classes, and wherein a query feature vector comprises output values of the bottleneck layer generated when the computation engine applies the TFCNN to the samples of the query utterance; wherein the computation engine comprises circuitry configured to apply the TFCNN to the samples of the reference audio signal at least through the bottleneck layer, wherein a reference feature vector comprises output values of the bottleneck layer generated when the computation engine applies the TFCNN to the samples of the reference audio signal; and wherein the computation engine comprises circuitry configured to determine, based on the query feature vector and the reference feature vector, a detection score corresponding to a level of confidence that the reference audio signal contains the query utterance.
2. The computing system of claim 1, wherein the computation engine comprises circuitry configured to: generate, based on the detection score, an indication that the reference audio signal contains the query utterance; and output the indication.
3. The computing system of claim 1, wherein the circuitry of the computation engine configured to determine the detection score is configured to determine the detection score based on an accumulated distance of a path determined by applying dynamic time warping to the query feature vector and the reference feature vector.
4. The computing system of claim 3, wherein the circuitry of the computation engine configured to determine the detection score is configured to: generate a joint distance matrix, wherein matrix frames in the joint distance matrix correspond to different combinations of features of the query feature vector and features of the reference feature vector, each of the matrix frames of the joint distance matrix contains a value indicating a distance between features in the combination corresponding to the matrix frame; and determine a best path through the matrix frame of the joint distance matrix, wherein a total of distances indicated by the matrix frame along the best path is less than totals of distances indicated by matrix frames along other evaluated paths through the matrix frames of the joint distance matrix, wherein the detection score is based on the total of distances indicated by the cells along the best path.
5. The computing system of claim 1, wherein: the TFCNN comprises a first max pooling layer, wherein inputs of the first max pooling layer are outputs of the time convolutional layer, the first max pooling layer groups the inputs into a first set of sub-regions, outputs of the first max pooling layer include a maximum input into each sub-region of the first set of sub-regions, and the outputs of the first max pooling layer are the inputs to the input layer of the series of additional layers, and the TFCNN comprises a second max pooling layer, wherein inputs of the second max pooling layer are outputs of the frequency convolutional layer, the second max pooling layer groups the inputs into a second set of sub-regions, outputs of the second max pooling layer include a maximum input into each of sub-region of the second set of sub-regions, and the outputs of the second max pooling layer are the inputs to the input layer of the series of additional layers.
6. The computing system of claim 1, wherein the query utterance is an in-domain keyword template, the reference audio signal is in a set of reference audio signals, the detection score corresponding to the level of confidence that the reference audio signal contains the query utterance is a Query by Example (QbE)-based detection score for the reference audio signal, and the computation engine further comprises circuitry configured to: apply an Automatic Speech Recognition (ASR)-based keyword spotting process to the set of reference audio signals to determine ASR-based detection scores for the reference audio signals; identify, based on the ASR-based detection scores for the reference audio signals, a set of top results among the set of reference audio signals; determine similarity measures between the top results using dynamic time warping; identify a subset of the top results based on the similarity measures; combine reference audio signals in the subset of the top results into the in-domain keyword template; and determine a final detection score for the reference audio signal based on the QbE-based detection score for the reference audio signal and the ASR-based detection score for the reference audio signal.
7. The computing system of claim 1, wherein a segment of the reference audio stream starting at the reference audio signal is a detected search segment, the computation engine further comprises circuitry configured to: include a plurality of query exemplars as entries in a dictionary; align the detected search segment to a longest of the query exemplars; determine weights for the entries in the dictionary, wherein the determined weights minimize a first reconstruction error, the first reconstruction error quantifying a deviation between the detected search segment and a weighted sum of the entries in the dictionary in which the entries in the dictionary are weighted according to the weights; generate a merged query exemplar based on the weighted sum of the entries in the dictionary; calculate a second reconstruction error, wherein the second reconstruction error quantifies a deviation between the merged query exemplar and the detected search segment; and determine, based on the second reconstruction error, a detection score for the detected search segment.
8. A method for determining whether a reference audio signal contains a query, the method comprising: storing samples of the reference audio signal, each of the samples of the reference audio signal corresponding to a different combination of a time band and a frequency band; and storing data representing a time-frequency convolutional neural network (TFCNN), the TFCNN comprising: a time convolutional layer that applies first filters to first input feature vectors, the first input feature vectors being sets of the samples that correspond to a same frequency and different times; a frequency convolutional layer that applies second filters to second input feature vectors, the second input feature vectors being sets of the samples that correspond to a same time and different frequencies; and a series of additional layers, wherein: the series of additional layers including an input layer, an output layer, and a series of hidden layers between the input layer and the output layer, input to the input layer comprises output of the time convolutional layer and output of the frequency convolutional layer, the series of hidden layers includes a bottleneck layer that includes fewer neurons than a hidden layer that precedes the bottleneck layer in the series of hidden layers; applying the TFCNN to samples of a query utterance at least through the bottleneck layer, wherein the TFCNN is trained to discriminate phonetic classes and a query feature vector comprises output values of the bottleneck layer generated when the computation engine applies the TFCNN to the samples of the query utterance, the query utterance being an example of an audio signal that contains the query; applying the TFCNN to samples of the reference audio signal at least through the bottleneck layer, wherein a reference feature vector comprises output values of the bottleneck layer generated when the computation engine applies the TFCNN to the samples of the reference audio signal; and determining, based on the query feature vector and the reference feature vector, a detection score corresponding to a level of confidence that the reference audio signal contains the query.
9. The method of claim 8, further comprising: generating, based on the detection score, an indication that the audio frame of the reference audio signal contains the query; and outputting the indication.
10. The method of claim 8, wherein: determining the detection score comprises determining the detection score based on an accumulated distance of a path determined by applying dynamic time warping to the query feature vector and the reference feature vector.
11. The method of claim 10, wherein determining the detection score comprises: generating a joint distance matrix, wherein matrix frames in the joint distance matrix correspond to different combinations of features of the query feature vector and features of the reference feature vector, each of the matrix frames of the joint distance matrix contains a value indicating a distance between features in the combination corresponding to the matrix frame; and determining a best path through the matrix frame of the joint distance matrix, wherein a total of distances indicated by the matrix frame along the best path is less than totals of distances indicated by matrix frames along other evaluated paths through the matrix frames of the joint distance matrix, wherein the detection score is based on the total of distances indicated by the cells along the best path.
12. The method of claim 8, wherein: the TFCNN comprises a first max pooling layer, wherein inputs of the first max pooling layer are outputs of the time convolutional layer, the first max pooling layer groups the inputs into a first set of sub-regions, outputs of the first max pooling layer include a maximum input into each sub-region of the first set of sub-regions, and the outputs of the first max pooling layer are the inputs to the input layer of the series of additional layers, and the TFCNN comprises a second max pooling layer, wherein inputs of the second max pooling layer are outputs of the frequency convolutional layer, the second max pooling layer groups the inputs into a second set of sub-regions, outputs of the second max pooling layer include a maximum input into each of sub-region of the second set of sub-regions, and the outputs of the second max pooling layer are the inputs to the input layer of the series of additional layers.
13. The method of claim 8, wherein the query utterance is an in-domain keyword template, the reference audio signal is in a set of reference audio signals, the detection score corresponding to the level of confidence that the audio frame of the reference audio signal contains the query is a Query by Example (QbE)-based detection score for the reference audio signal, and the method further comprises: applying an Automatic Speech Recognition (ASR)-based keyword spotting process to the set of reference audio signals to determine ASR-based detection scores for the reference audio signals; identifying, based on the ASR-based detection scores for the reference audio signals, a set of top results among the set of reference audio signals; determining similarity measures between the top results using dynamic time warping; identifying a subset of the top results based on the similarity measures; combining reference audio signals in the subset of the top results into the in-domain keyword template; and determining a final detection score for the reference audio signal based on the QbE-based detection score for the reference audio signal and the ASR-based detection score for the reference audio signal.
14. The method of claim 8, wherein a segment of the reference audio stream starting at the audio frame of the reference audio signal is a detected search segment, the method further comprising: including a plurality of query exemplars as entries in a dictionary; aligning the detected search segment to a longest of the query exemplars; determining weights for the entries in the dictionary, wherein the determined weights minimize a first reconstruction error, the first reconstruction error quantifying a deviation between the detected search segment and a weighted sum of the entries in the dictionary in which the entries in the dictionary are weighted according to the weights; generating a merged query exemplar based on the weighted sum of the entries in the dictionary; calculating a second reconstruction error, wherein the second reconstruction error quantifies a deviation between the merged query exemplar and the detected search segment; and determining, based on the second reconstruction error, a detection score for the detected search segment.
15. A non-transitory computer-readable data storage medium having instructions stored thereon that, when executed, cause a computing system to: store samples of the reference audio signal, each of the samples of the reference audio signal corresponding to a different combination of a time band and a frequency band; and store data representing a time-frequency convolutional neural network (TFCNN), the TFCNN comprising: a time convolutional layer that applies first filters to first input feature vectors, the first input feature vectors being sets of the samples that correspond to a same frequency band and different time bands; a frequency convolutional layer that applies second filters to second input feature vectors, the second input feature vectors being sets of the samples that correspond to a same time band and different frequency bands; and a series of additional layers, wherein: the series of additional layers including an input layer, an output layer, and a series of hidden layers between the input layer and the output layer, input to the input layer comprises output of the time convolutional layer and output of the frequency convolutional layer, the series of hidden layers includes a bottleneck layer that includes fewer neurons than a hidden layer that precedes the bottleneck layer in the series of hidden layers; apply the TFCNN to samples of a query utterance at least through the bottleneck layer, wherein the TFCNN is trained to discriminate phonetic classes and a query feature vector comprises output values of the bottleneck layer generated when the computation engine applies the TFCNN to the samples of the query utterance, the query utterance being an example of an audio signal that contains the query; apply the TFCNN to the samples of the reference audio signal at least through the bottleneck layer, wherein a reference feature vector comprises output values of the bottleneck layer generated when the computation engine applies the TFCNN to the samples of the reference audio signal; and determine, based on the query feature vector and the reference feature vector, a detection score corresponding to a level of confidence that the reference audio signal contains the query.
16. The non-transitory computer-readable data storage medium of claim 15, wherein execution of the instructions causes the computing system to: generate, based on the detection score, an indication that the reference audio signal contains the query; output the indication.
17. The non-transitory computer-readable data storage medium of claim 15, wherein execution of the instructions causes the computing system to determine the detection score based on an accumulated distance of a path determined by applying dynamic time warping to the query feature vector and the reference feature vector.
18. The non-transitory computer-readable data storage medium of claim 17, wherein execution of the instructions causes the computing system to: generate a joint distance matrix, wherein matrix frames in the joint distance matrix correspond to different combinations of features of the query feature vector and features of the reference feature vector, each of the matrix frames of the joint distance matrix contains a value indicating a distance between features in the combination corresponding to the matrix frame; and determine a best path through the matrix frame of the joint distance matrix, wherein a total of distances indicated by the matrix frame along the best path is less than totals of distances indicated by matrix frames along other evaluated paths through the matrix frames of the joint distance matrix, wherein the detection score is based on the total of distances indicated by the cells along the best path.
19. The non-transitory computer-readable data storage medium of claim 15, wherein: the TFCNN comprises a first max pooling layer, wherein inputs of the first max pooling layer are outputs of the time convolutional layer, the first max pooling layer groups the inputs into a first set of sub-regions, outputs of the first max pooling layer include a maximum input into each sub-region of the first set of sub-regions, and the outputs of the first max pooling layer are the inputs to the input layer of the series of additional layers, and the TFCNN comprises a second max pooling layer, wherein inputs of the second max pooling layer are outputs of the frequency convolutional layer, the second max pooling layer groups the inputs into a second set of sub-regions, outputs of the second max pooling layer include a maximum input into each of sub-region of the second set of sub-regions, and the outputs of the second max pooling layer are the inputs to the input layer of the series of additional layers.
20. The non-transitory computer-readable storage data storage medium of claim 15, wherein a segment of the reference audio stream is a detected search segment and execution of the instructions further causes the computing system to: include a plurality of query exemplars as entries in a dictionary; align the detected search segment to a longest of the query exemplars; determine weights for the entries in the dictionary, wherein the determined weights minimize a first reconstruction error, the first reconstruction error quantifying a deviation between the detected search segment and a weighted sum of the entries in the dictionary in which the entries in the dictionary are weighted according to the weights; generate a merged query exemplar based on the weighted sum of the entries in the dictionary; calculate a second reconstruction error, wherein the second reconstruction error quantifies a deviation between the merged query exemplar and the detected search segment; and determine, based on the second reconstruction error, a detection score for the detected search segment.
</claims>
</document>
