<document>

<filing_date>
2015-06-30
</filing_date>

<publication_date>
2020-02-04
</publication_date>

<priority_date>
2015-06-30
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,G06N99/00
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
ASENTE, PAUL JOHN
MECH, RADOMIR
MILLER GAVIN STUART PETER
YUMER, MEHMET ERSIN
</inventors>

<docdb_family_id>
57684282
</docdb_family_id>

<title>
Procedural modeling using autoencoder neural networks
</title>

<abstract>
An intuitive object-generation experience is provided by employing an autoencoder neural network to reduce the dimensionality of a procedural model. A set of sample objects are generated using the procedural model. In embodiments, the sample objects may be selected according to visual features such that the sample objects are uniformly distributed in visual appearance. Both procedural model parameters and visual features from the sample objects are used to train an autoencoder neural network, which maps a small number of new parameters to the larger number of procedural model parameters of the original procedural model. A user interface may be provided that allows users to generate new objects by adjusting the new parameters of the trained autoencoder neural network, which outputs procedural model parameters. The output procedural model parameters may be provided to the procedural model to generate the new objects.
</abstract>

<claims>
1. One or more computer storage media storing computer-useable instructions that, when used by one or more computing devices, cause the one or more computing devices to perform operations comprising: generating a set of sample image objects using a procedural model having a plurality of procedural model parameters for generating image objects; providing procedural model parameter inputs used to generate each sample image object and visual features of each sample image object as training data; training an autoencoder neural network using the training data to map the plurality of procedural model parameters to a smaller number of new parameters; and generating a new image object by: receiving user input to adjust one or more of the new parameters of the trained autoencoder neural network to cause the trained autoencoder neural network to output a set of new procedural model parameter inputs, and providing the set of new procedural model parameter inputs to the procedural model to cause the procedural model to generate the new image object.
2. The one or more computer storage media of claim 1, wherein the set of sample image objects is generated by: generating a categorization tree using the procedural model to generate image objects to populate the categorization tree; sampling the categorization tree to obtain image objects uniformly distributed with respect to visual appearance; and iteratively adding additional image objects to the categorization tree, computing a new categorization tree, and sampling the new categorization tree until a criterion is satisfied.
3. The one or more computer storage media of claim 2, wherein sampling the categorization tree employs a probability distribution based on individual selection probabilities computed for each leaf node of the categorization tree as follows: wherein di is a depth of a node i starting from a root node and si is a number of siblings.
4. The one or more computer storage media of claim 2, wherein the criterion is:
description="In-line Formulae" end="lead"?Et−Et-1<w(Et-1−Et-2)description="In-line Formulae" end="tail"? wherein Et is potential energy in the categorization tree at iteration t, and 0<w<1 is a weighting constant, wherein potential energy is computed as: where S is the set of image objects in the categorization tree and ei is a potential energy of image object i defined as:
description="In-line Formulae" end="lead"?ei=min|xi−xj|i,j∈S,i≠j description="In-line Formulae" end="tail"? wherein xi and xj are visual features of image objects i and j respectively.
5. The one or more computer storage media of claim 1, wherein the visual features from the sample image objects are based on shape.
6. The one or more computer storage media of claim 5, wherein the sample image objects are 3D objects and the visual feature for each sample image object is generated by: computing a plurality of 2D renders of the sample image object; extracting a silhouette-based histogram from each 2D render; and concatenating the silhouette-based histograms to form a feature vector for the sample image object.
7. The one or more computer storage media of claim 5, wherein the sample image objects are 2D objects and the visual feature for each sample image object comprises a feature vector based on a texton histogram of a shape of the sample image object.
8. The one or more computer storage media of claim 1, wherein the autoencoder neural network comprises a five hidden layer symmetric autoencoder neural network.
9. The one or more computer storage media of claim 1, wherein the operations further comprise generating a user interface that exposes the new parameters from the autoencoder neural network, the new parameters being adjustable via user interface elements to generate the new image object.
10. The one or more computer storage media of claim 9, wherein a decoding part of the autoencoder neural network is employed to generate the new image object, and wherein the new parameters adjustable via the user interface elements correspond with nodes at a bottleneck layer of the autoencoder neural network.
11. A computerized method comprising: providing a procedural model having a plurality of procedural model parameters for generating image objects; generating a set of sample image objects using the procedural model; providing procedural model parameter inputs used to generate each sample image object and visual features of each sample image object as training data; training an autoencoder neural network using the training data, the autoencoder neural network mapping the plurality of procedural model parameters to a set of new parameters, the new parameters being fewer in number than the procedural model parameters; generating a user interface that exposes the set of new parameters via user interface elements allowing a user to adjust the new parameters: receiving input via the user interface adjusting at least a portion of the new parameters to provide new parameter inputs; providing the new parameter inputs to the autoencoder neural network to cause the autoencoder neural network to ouput new procedural model parameter inputs; and providing the new procedural model parameter inputs to the procedural model to cause the procedural model to generate an output image object.
12. The method of claim 11, wherein the set of sample image objects is generated by: generating a categorization tree using the procedural model to generate image objects to populate the categorization tree; sampling the categorization tree to obtain image objects uniformly distributed with respect to visual appearance; and iteratively adding additional image objects to the categorization tree, computing a new categorization tree, and sampling the new categorization tree until a criterion is satisfied.
13. The method of claim 12, wherein sampling the categorization tree employs a probability distribution based on individual selection probabilities computed for each leaf node of the categorization tree as follows: wherein di is a depth of a node i starting from a root node and si is a number of siblings.
14. The method of claim 12, wherein the criterion is:
description="In-line Formulae" end="lead"?Et−Et-1<w(Et-1−Et-2)description="In-line Formulae" end="tail"? wherein Et is potential energy in the categorization tree at iteration t, and 0<w<1 is a weighting constant, wherein potential energy is computed as: wherein S is a set of image objects in the categorization tree and ei is the potential energy of image object i defined as:
description="In-line Formulae" end="lead"?ei=min|xi−xj|i,j∈S,i≠j description="In-line Formulae" end="tail"? wherein xi and xj are visual features of image objects i and j respectively.
15. The method of claim 11, wherein the visual features from the sample image objects are based on shape.
16. The method of claim 15, wherein the sample image objects are 3D objects and the visual feature for each sample image object is generated by: computing a plurality of 2D renders of the sample image object; extracting a silhouette-based histogram from each 2D render; and concatenating the silhouette-based histograms to form a feature vector for the sample image object.
17. The method of claim 15, wherein the sample image objects are 2D image objects and the visual feature for each sample image object comprises a feature vector based on a texton histogram of a shape of the sample image object.
18. The method of claim 11, wherein the autoencoder neural network comprises a five hidden layer symmetric autoencoder neural network.
19. A computerized method comprising: generating a set of sample image objects using a procedural model having a plurality of procedural model parameters for generating the set of sample image objects by: generating a categorization tree using the procedural model to generate image objects to populate the categorization tree, sampling the categorization tree to obtain image objects uniformly distributed with respect to visual appearance, and iteratively adding additional image objects to the categorization tree, computing a new categorization tree, and sampling the new categorization tree until a criterion is satisfied; providing procedural model parameter inputs used to generate each sample image object and visual features of each sample image object as training data training an autoencoder neural network using the training data comprising the procedural model parameters and the visual features from the sample image objects to map the plurality of procedural model parameters to a set of new parameters; and generating a user interface that exposes the set of new parameters from the autoencoder neural network, the user interface including user interface elements allowing a user to adjust the new parameters to generate new image objects; receiving input via the user interface adjusting at least a portion of the new parameters to provide new parameter inputs; providing the new parameter inputs to the autoencoder neural network to cause the autoencoder neural network to output new procedural model parameter inputs; and providing the new procedural model parameter inputs to the procedural model to cause the procedural model generate an output image object.
</claims>
</document>
