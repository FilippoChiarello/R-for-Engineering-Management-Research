<document>

<filing_date>
2018-01-04
</filing_date>

<publication_date>
2020-10-13
</publication_date>

<priority_date>
2018-01-04
</priority_date>

<ipc_classes>
G06F17/16,G06K9/00,G06K9/62
</ipc_classes>

<assignee>
SHUTTERFLY
SHUTTERFLY
</assignee>

<inventors>
MOUSSAFFI, OMER MOSHE
</inventors>

<docdb_family_id>
67059736
</docdb_family_id>

<title>
High precision additive pattern recognition for image and other applications
</title>

<abstract>
A computer-implemented method includes selecting a kernel and kernel parameters for a first Support Vector Machine (SVM) model, testing the first SVM model on a feature matrix T of n feature vectors of length m to produce false positive (FP) data set and false negative (FN) data set by a computer processor, wherein n and m are integer numbers, automatically removing feature vectors corresponding to the FN data set from the feature matrix T by the computer processor to produce a feature matrix T_best of size (n-size(FN))*m, retraining the first SVM model on the feature matrix T_best to produce a second SVM model, and checking if a ratio (T_best sample number)/(SVM support vector number) is above a threshold for the second SVM model on T_best. If the ratio is above the threshold, SVM predictions is performed using the second SVM model on the feature matrix T_best.
</abstract>

<claims>
1. A computer-implemented method, comprising: selecting a kernel and kernel parameters for a first Support Vector Machine (SVM) model; testing the first SVM model on a feature matrix T to produce false positive (FP) data set and false negative (FN) data set by a computer processor, wherein the feature matrix T includes n feature vectors of length m, wherein n and m are integer numbers; automatically removing feature vectors corresponding to the FN data set from the feature matrix T by the computer processor to produce a feature matrix T_best of size (n-size(FN))*m; retraining the first SVM model on the feature matrix T_best to produce a second SVM model; checking if a ratio (T_best sample number)/(SVM support vector number) is above a threshold for the second SVM model on T_best; and if the ratio is above the threshold, performing SVM predictions using the second SVM model on the feature matrix T_best.
2. The computer-implemented method of claim 1, further comprising: if the ratio (T_best sample number)/(SVM model support vector number) is not above the threshold for the SVM model on T_best, repeating selecting, testing, automatically removing, retraining, and checking to find a third SVM model having (T_best sample number)/(SVM model support vector number) ratio that exceeds the threshold.
3. The computer-implemented method of claim 2, further comprising: performing SVM predictions using the third SVM model on the feature matrix T_best.
4. The computer-implemented method of claim 2, further comprising: if a SVM model having (T_best sample number)/(SVM model support vector number) ratio that exceeds the threshold is not found, selecting a fourth SVM model that yields a highest (T_best sample number)/(SVM model support vector number) ratio in repeating selecting, testing, automatically removing, retraining, and checking; and performing SVM predictions using the fourth SVM model on the feature matrix T_best.
5. The computer-implemented method of claim 1, wherein the threshold for the (T_best sample number)/(SVM model support vector number) ratio is ten or higher.
6. The computer-implemented method of claim 1, further comprising: applying Grid Search on the feature matrix T to find optimal kernel parameters before testing the first SVM model on a feature matrix T.
7. The computer-implemented method of claim 1, wherein the feature vectors are based on faces or objects in images, the computer-implemented method further comprising: classifying the objects and the faces in the images by performing SVM predictions on the feature matrix T_best.
8. The computer-implemented method of claim 7, further comprising: automatically creating designs for photo products based on the objects and faces classified by performing SVM predictions on the feature matrix T_best.
9. The computer-implemented method of claim 1, further comprising: selecting a feature vector v from the FN data set to add back to the feature matrix T_best to produce a feature matrix T_v; retraining the second SVM model on the feature matrix T_v to produce a fifth SVM model; performing SVM predictions on the feature matrix T_v using the fifth SVM model; calculating a benefit function to produce a benefit function value, wherein the benefit function is dependent of differences between true positives, true negatives, and numbers of support vectors generated on T_best and T_v respectively by the second SVM model and the fifth SVM model; adding the feature vector v to T_best if the benefit function value meets a predetermined criterion; and performing SVM prediction using the feature matrix T_best.
10. The computer-implemented method of claim 9, further comprising: repeating selecting a feature vector v, training the second SVM model, performing SVM predictions, calculating a benefit function, and adding the feature vector v to T_best by selecting and adding a different feature vector from the FN data set to feature matrix T_best, wherein SVM predictions are performed using the feature matrix T_best that gives a highest best function value.
11. The computer-implemented method of claim 10, wherein all the feature vectors corresponding to the FN data set are evaluated by calculating a corresponding benefic function and determined to be added to the T_best or not depending on a value of the corresponding benefic function.
12. The computer-implemented method of claim 9, wherein the feature vector v is not added to the feature matrix T_best if the value of the benefit function does not meet the predetermined criterion.
13. The computer-implemented method of claim 9, wherein the feature vectors are based on faces or objects in images, the computer-implemented method further comprising: classifying the objects and the faces in the images by performing SVM predictions on the feature matrix T_best.
</claims>
</document>
