<document>

<filing_date>
2018-04-19
</filing_date>

<publication_date>
2020-08-27
</publication_date>

<priority_date>
2017-09-13
</priority_date>

<ipc_classes>
G06F3/01,G06K9/00,G06K9/62
</ipc_classes>

<assignee>
VISUALCAMP COMPANY
</assignee>

<inventors>
LEE, TAE HEE
SEOK, YOON CHAN
</inventors>

<docdb_family_id>
66037021
</docdb_family_id>

<title>
EYE TRACKING METHOD AND USER TERMINAL PERFORMING SAME
</title>

<abstract>
A user terminal according to an embodiment of the present invention includes a capturing device for capturing a face image of a user, and an eye tracking unit for, on the basis of a configured rule, acquiring, from the face image, a vector representing the direction that the face of the user is facing, and a pupil image of the user, and performing eye tracking of the user by inputting, in a configured deep learning model, the face image, the vector and the pupil image.
</abstract>

<claims>
1. A user terminal comprising: an imaging device configured to capture a face image of a user; and an eye tracking unit configured to acquire a vector representing a direction that a face of the user faces and an ocular image of the user from the face image on the basis of set rules and track a gaze of the user by inputting the face image, the vector, and the ocular image to a set deep learning model.
2. The user terminal of claim 1, further comprising a training data collection unit configured to collect training data including a face image of a viewer captured at a time point of receiving a set action and location information of a set point when the set action is received from the viewer who looks at the set point in a screen, wherein the eye tracking unit trains the deep learning model with the training data and tracks the gaze of the user using the deep learning model which has learned the training data.
3. The user terminal of claim 2, wherein, when the viewer touches the point, the training data collection unit collects the training data at a time point at which the touch is made.
4. The user terminal of claim 3, wherein the training data collection unit collects the training data by operating the imaging device at the time point at which the viewer touches the point.
5. The user terminal of claim 3, wherein the training data collection unit transmits the training data collected at the time point at which the viewer touches the point to a server.
6. The user terminal of claim 3, wherein, when the viewer touches the point while the imaging device is operating, the training data collection unit separately collects the training data at the time point at which the touch is made and time points a set time before and after the time point at which the touch is made.
7. The user terminal of claim 3, wherein the training data collection unit changes a visual element of the point after the viewer touches the point so that a gaze of the viewer remains at the point even after the touch.
8. The user terminal of claim 2, wherein the training data collection unit displays set text at the point and collects, when the viewer speaks, the training data at a time point at which an utterance of the viewer is started.
9. The user terminal of claim 1, wherein the eye tracking unit acquires ocular location coordinates and face location coordinates of the user from the face image on the basis of the rules and additionally inputs the ocular location coordinates and the face location coordinates to the deep learning model together with the vector representing the direction that the face of the user faces.
10. The user terminal of claim 1, further comprising a content providing unit configured to display advertising content on the screen, wherein the eye tracking unit determines whether the user is watching the advertising content on the basis of a detected gaze of the user and a location of the advertising content in the screen; and the content providing unit changes the location of the advertising content in the screen by considering the location of the advertising content in the screen and a time period for which the user has watched the advertising content.
11. An eye tracking method comprising: capturing, by an imaging device, a face image of a user; acquiring, by an eye tracking unit, a vector representing a direction in that a face of the user faces and an ocular image of the user from the face image on the basis of set rules; and inputting, by the eye tracking unit, the face image, the vector, and the ocular image to a set deep learning model to track a gaze of the user.
12. The eye tracking method of claim 11, further comprising: when a set action is received from a viewer who looks at a set point in a screen, collecting, by a training data collection unit, training data including a face image of the viewer captured at a time point of receiving the set action and location information of the set point; and training, by the eye tracking unit, the deep learning model with the training data, wherein the tracking of the gaze of the user comprises tracking the gaze of the user by using the deep learning model which has learned the training data.
13. The eye tracking method of claim 12, wherein the collecting of the training data comprises, when the viewer touches the point, collecting the training data at a time point at which the touch is made.
14. The eye tracking method of claim 13, wherein the collecting of the training data comprises collecting the training data by operating the imaging device at the time point at which the viewer touches the point.
15. The eye tracking method of claim 13, further comprising transmitting, by the training data collection unit, the training data collected at the time point at which the viewer touches the point to a server.
16. The eye tracking method of claim 13, wherein the collecting of the training data comprises, when the viewer touches the point while the imaging device is operating, separately collecting the training data at the time point at which the touch is made and time points a set time before and after the time point at which the touch is made.
17. The eye tracking method of claim 13, further comprising changing, by the training data collection unit, a visual element of the point after the viewer touches the point so that a gaze of the viewer remains at the point even after the touch.
18. The eye tracking method of claim 12, wherein the collecting of the training data comprises displaying set text at the point and collecting, when the viewer speaks, the training data at a time point at which an utterance of the viewer is started.
19. The eye tracking method of claim 11, further comprising acquiring, by the eye tracking unit, ocular location coordinates and face location coordinates of the user from the face image on the basis of the rules, wherein the tracking of the gaze of the user comprises additionally inputting the ocular location coordinates and the face location coordinates to the deep learning model together with the vector representing the direction that the face of the user faces.
20. The eye tracking method of claim 11, further comprising: displaying, by a content providing unit, advertising content on the screen; determining, by the eye tracking unit, whether the user is watching the advertising content on the basis of a detected gaze of the user and a location of the advertising content in the screen; and changing, by the content providing unit, the location of the advertising content in the screen by considering the location of the advertising content in the screen and a time period for which the user has watched the advertising content.
</claims>
</document>
