<document>

<filing_date>
2019-07-09
</filing_date>

<publication_date>
2020-01-16
</publication_date>

<priority_date>
2018-07-10
</priority_date>

<ipc_classes>
G06N3/08,H04L12/58
</ipc_classes>

<assignee>
TATA CONSULTANCY SERVICES
</assignee>

<inventors>
SHROFF, GAUTAM
AGARWAL, PUNEET
VIG, LOVEKESH
KHURANA, PRERNA
</inventors>

<docdb_family_id>
67220710
</docdb_family_id>

<title>
METHOD AND SYSTEM FOR RESOLVING ABSTRACT ANAPHORA USING HIERARCHICALLY-STACKED RECURRENT NEURAL NETWORK (RNN)
</title>

<abstract>
Conversational systems are required to be capable of handling more sophisticated interactions than providing factual answers only. Such interactions are handled by resolving abstract anaphoric references in conversational systems which includes antecedent fact references and posterior fact references. The present disclosure resolves abstract anaphoric references in conversational systems using hierarchically stacked neural networks. In the present disclosure, a deep hierarchical maxpool network based model is used to obtain a representation of each utterance received from users and a representation of one or more generated sequences of utterances. The obtained representations are further used to identify contextual dependencies with in the one or more generated sequences which helps in resolving abstract anaphoric references in conversational systems. Further, a response for an incoming sequence of utterances is retrieved based on classification of incoming sequence of utterances into one or more pre-created responses. The proposed model takes lesser time to retrain.
</abstract>

<claims>
1. A processor-implemented method, comprising: receiving, in a multi-turn retrieval chat-bot, a plurality of consecutive utterances comprising of at least a sub-set of utterances indicative of anaphoric reference to specific entities or facts comprised in past utterances, in a multi-turn retrieval chat-bot; generating one or more sequences of the plurality of consecutive utterances and obtaining one or more pre-created corresponding responses from the database for each of the one or more generated sequences; training a Deep Hierarchical Maxpool Network (DHMN) based model, using the one or more generated sequences, and the plurality of consecutive utterances, to (a) obtain a representation for each of (i) the one or more generated sequences, and (ii) the plurality of consecutive utterances, and (b) identify contextual dependencies within the one or more generated sequences using each representation to resolve anaphoric references; updating, using a Character to Word Encoder (CWE) network comprised in the DHMN based model, the representation of the plurality of consecutive utterances based on a presence or an absence of discrepancies in one or more utterances comprised in an incoming sequence of utterances; and classifying, using the trained DHMN based model and the identified contextual dependencies, the incoming sequence of utterances, based on at least one of (i) the updated representation, and (ii) the representation of the one or more generated sequences, into at least one of the one or more pre-created corresponding answers.
2. The method of claim 1, further comprising dynamically optimizing the DHMN based model based on at least one of the updated representation and the incoming sequence of utterances being classified into at least one of the one or more pre-created corresponding answers.
3. The method of claim 1, wherein the updated representation is based on a similarity score between a predicted word embedding and an actual word embedding of the words comprised in the each of the plurality of utterances, wherein the similarity score is computed using a loss function.
4. The method of claim 1, wherein updating the representation by Character to Word Encoder (CWE) network comprises refraining misclassification of utterances.
5. The method of claim 1, further comprising upon receiving the plurality of consecutive utterances, determining one or more actions to be performed for the plurality of consecutive utterances.
6. A system, comprising: a memory; one or more communication interfaces; and one or more hardware processors coupled to said memory through said one or more communication interfaces, wherein said one or more hardware processors are configured to: receive, in a multi-turn retrieval chat-bot, a plurality of consecutive utterances comprising of at least a sub-set of utterances indicative of anaphoric reference to specific entities or facts comprised in past utterances, in a multi-turn retrieval chat-bot; generate one or more sequences of the plurality of consecutive utterances and obtain one or more pre-created corresponding responses from the database for each of the one or more generated sequences; train, a Deep Hierarchical Maxpool Network (DHMN) based model, using the one or more generated sequences, the plurality of consecutive utterances, and the one or more relevant past utterances to (a) obtain a representation for each of (i) the one or more generated sequences, and (ii) the plurality of consecutive utterances, and (b) identify contextual dependencies within the one or more generated sequences using each representation to resolve anaphoric references; update, using a Character to Word Encoder (CWE) network comprised in the DHMN based model, the representation of the plurality of consecutive utterances based on a presence or an absence of discrepancies in one or more utterances comprised in an incoming sequence of utterances; and classify, using the trained DHMN based model and the identified contextual dependencies, the incoming sequence of utterances, based on at least one of (i) the updated representation, and (ii) the representation of the one or more generated sequences, into at least one of the one or more pre-created corresponding answers.
7. The system of claim 6, the one or more hardware processors are further configured to dynamically optimize the DHMN based model based on at least one of the updated representation and the incoming sequence of utterances being classified into at least one of the one or more pre-created corresponding answers.
8. The system of claim 6, wherein the updated representation is based on a similarity score between a predicted word embedding and an actual word embedding of the words comprised in the each of the plurality of utterances, wherein the similarity score is computed using a loss function.
9. The system of claim 6, wherein the representation of the plurality of consecutive utterances is updated by the Character to Word Encoder (CWE) network to refrain misclassification of utterances.
10. The system of claim 6, wherein the one or more hardware processors are further configured by the instructions to determine one or more actions to be performed for the plurality of consecutive utterances being received.
11. One or more non-transitory machine readable information storage media storing instructions which, when executed by one or more hardware processors, cause the one or more hardware processors to perform operations comprising: receiving, in a multi-turn retrieval chat-bot, a plurality of consecutive utterances comprising of at least a sub-set of utterances indicative of anaphoric reference to specific entities or facts comprised in past utterances, in a multi-turn retrieval chat-bot; generating one or more sequences of the plurality of consecutive utterances and obtaining one or more pre-created corresponding responses from the database for each of the one or more generated sequences; training a Deep Hierarchical Maxpool Network (DHMN) based model, using the one or more generated sequences, and the plurality of consecutive utterances, to (a) obtain a representation for each of (i) the one or more generated sequences, and (ii) the plurality of consecutive utterances, and (b) identify contextual dependencies within the one or more generated sequences using each representation to resolve anaphoric references; updating, using a Character to Word Encoder (CWE) network comprised in the DHMN based model, the representation of the plurality of consecutive utterances based on a presence or an absence of discrepancies in one or more utterances comprised in an incoming sequence of utterances; and classifying, using the trained DHMN based model and the identified contextual dependencies, the incoming sequence of utterances, based on at least one of (i) the updated representation, and (ii) the representation of the one or more generated sequences, into at least one of the one or more pre-created corresponding answers.
12. The one or more non-transitory machine readable media of claim 11, wherein the operations further comprise dynamically optimizing the DHMN based model based on at least one of the updated representation and the incoming sequence of utterances being classified into at least one of the one or more pre-created corresponding answers.
13. The one or more non-transitory machine readable media of claim 11, wherein the updated representation is based on a similarity score between a predicted word embedding and an actual word embedding of the words comprised in the each of the plurality of utterances, wherein the similarity score is computed using a loss function.
14. The one or more non-transitory machine readable media of claim 11, wherein updating the representation by Character to Word Encoder (CWE) network comprises refraining misclassification of utterances.
15. The one or more non-transitory machine readable media of claim 11, wherein the operations further comprise, upon receiving the plurality of consecutive utterances, determining one or more actions to be performed for the plurality of consecutive utterances.
</claims>
</document>
