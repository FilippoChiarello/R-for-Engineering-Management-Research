<document>

<filing_date>
2016-06-29
</filing_date>

<publication_date>
2020-08-05
</publication_date>

<priority_date>
2015-11-05
</priority_date>

<ipc_classes>
G06T7/10,G06T7/20,G06T7/50
</ipc_classes>

<assignee>
FACEBOOK
</assignee>

<inventors>
PALURI, BALAMANOHAR
BOURDEV, LUBOMIR
TRAN, DU LE HONG
FERGUS, ROBERT D.
</inventors>

<docdb_family_id>
56360184
</docdb_family_id>

<title>
SYSTEMS AND METHODS FOR PROCESSING CONTENT USING CONVOLUTIONAL NEURAL NETWORKS
</title>

<abstract>
Systems, methods, and non-transitory computer-readable media can obtain a set of video frames at a first resolution. Process the set of video frames using a convolutional neural network to output one or more signals, the convolutional neural network including (i) a set of two-dimensional convolutional layers and (ii) a set of three-dimensional convolutional layers, wherein the processing causes the set of video frames to be reduced to a second resolution. Process the one or more signals using a set of three-dimensional de-convolutional layers of the convolutional neural network. Obtain one or more outputs corresponding to the set of video frames from the convolutional neural network.
</abstract>

<claims>
1. A computer-implemented method comprising: obtaining (602), by a computing system (400), a set of video frames (405) of a given length at a first I resolution, wherein the length corresponds to a number of frames and the resolution corresponds to a width and height of each frame in pixels; processing, by the computing system (400), the set of video frames of the given length using a convolutional neural network (402) to output one or more signals corresponding to the set of video frames, the convolutional neural network including (i) a set of two-dimensional convolutional layers, (ii) a set of three-dimensional convolutional layers, and (iii) a set of three-dimensional de-convolutional layers, wherein the three-dimensional convolutional layers reduce the set of video frames to a second resolution, and wherein the three-dimensional de-convolutional layers (422, 428, 434) upsample the set of video frames to the length and resolution of the video content provided to the convolutional neural network, and wherein the signals outputted from the two-dimensional convolutional layers are inputted to said three-dimensional convolutional layers, and the signals outputted by the three-dimensional convolutional layers are inputted to said three-dimensional deconvolutional layers; and obtaining (608), by the computing system (400), the one or more outputted signals corresponding to the set of video frames of the given length from the convolutional neural network, the outputted signals providing information corresponding to one or more voxels in the set of video frames, wherein each voxel is a pixel at a specific time.
2. The computer-implemented method of claim 1, wherein obtaining the one or more outputs corresponding to the set of video frames further comprises: obtaining, by the computing system, one or more respective feature descriptors for one or more voxels in the set of video frames, wherein each feature descriptor references a recognized scene, object, or action; and/or
obtaining, by the computing system, a respective optical flow for one or more voxels in the set of video frames, wherein the optical flow for a voxel describes at least a predicted direction and magnitude of the voxel; and/or obtaining, by the computing system, a respective depth measurement for one or more voxels in the set of video frames.
3. The computer-implemented method of claim 1 or 2, wherein processing the one or more signals using the set of three-dimensional de-convolutional layers of the convolutional neural network further comprises:
inputting, by the computing system, at least a portion of signals produced by the set of three-dimensional convolutional layers to the set of three-dimensional de-convolutional layers, the three-dimensional de-convolutional layers being trained to apply at least one three-dimensional de-convolutional operation to the portion of signals.
4. The computer-implemented method of claim 3, wherein the at least one three-dimensional de-convolutional operation is based at least on one or more three-dimensional filters to de-convolve the portion of signals, and wherein the three-dimensional de-convolutional operation causes the representation of the video content to be increased in signal size.
5. The computer-implemented method of any of claims 1 to 4, wherein processing the set of video frames using a convolutional neural network to output one or more signals, the convolutional neural network including (i) a set of two-dimensional convolutional layers and (ii) a set of three-dimensional convolutional layers further comprises:
inputting, by the computing system, a representation of the set of video frames to the set of two-dimensional convolutional layers to output a set of first signals, the two-dimensional convolutional layers being trained to apply at least one two-dimensional convolutional operation to the representation of the video content; and inputting, by the computing system, at least a portion of the set of first signals to the set of three-dimensional convolutional layers to output a set of second signals, the three-dimensional convolutional layers being trained to apply at least one three-dimensional convolutional operation to the set of first signals.
6. The computer-implemented method of claim 5, wherein the at least one two-dimensional convolutional operation is based at least on one or more two-dimensional filters to convolve the representation of the video content, and wherein the two-dimensional convolutional operation causes the representation of the video content to be reduced in signal size.
7. The computer-implemented method of claim 5 or 6, wherein the at least one three-dimensional convolutional operation is based at least on one or more three-dimensional filters to convolve the set of first signals, and wherein the three-dimensional convolutional operation causes the representation of the video content to be reduced in signal size.
8. The computer-implemented method of any of claims 1 to 7, wherein the set of video frames includes more than two video frames.
9. A system comprising: at least one processor; and a memory storing instructions that, when executed by the at least one processor, cause the system (400) to perform: obtaining (602) a set of video frames (405) of a given length at a first resolution, wherein the length corresponds to a number of frames and the resolution corresponds to a width and height of each frame in pixels; processing the set of video frames of the given length using a convolutional neural network (402) to output one or more signals corresponding to the set of video frames, the convolutional neural network including (i) a set of two-dimensional convolutional layers, (ii) a set of three-dimensional convolutional layers, and (iii) a set of three-dimensional de-convolutional layers, wherein the three-dimensional convolutional layers reduce the set of video frames to a second resolution, and wherein the three-dimensional de-convolutional layers (422, 428, 434) upsample the set of video frames to the length and resolution of the video content provided to the convolutional neural network, and wherein the signals outputted from the two-dimensional convolutional layers are inputted to said three-dimensional convolutional layers, and the signals outputted by the three-dimensional convolutional layers are inputted to said three-dimensional deconvolutional layers; and obtaining (608) the one or more outputted signals corresponding to the set of video frames of the given length from the convolutional neural network, the outputted signals providing information corresponding to one or more voxels in the set of video frames, wherein each voxel is a pixel at a specific time.
10. The system of claim 9, wherein obtaining the one or more outputs corresponding to the set of video frames further causes the system to perform: obtaining one or more respective feature descriptors for one or more voxels in the set of video frames, wherein each feature descriptor references a recognized scene, object, or action; and/or obtaining a respective optical flow for one or more voxels in the set of video frames, wherein the optical flow for a voxel describes at least a predicted direction and magnitude of the voxel; and/or obtaining a respective optical flow for one or more voxels in the set of video frames, wherein the optical flow for a voxel describes at least a predicted direction and magnitude of the voxel.
11. The system of claim 9 or 10, wherein processing the one or more signals us-ing the set of three-dimensional de-convolutional layers of the convolutional neural network further causes the system to perform:
inputting, by the computing system, at least a portion of signals produced by the set of three-dimensional convolutional layers to the set of three-dimensional de-convolutional layers, the three-dimensional de-convolutional layers being trained to apply at least one three-dimensional de-convolutional operation to the portion of signals.
12. A non-transitory computer-readable storage medium including instructions that, when executed by at least one processor of a computing system, cause the computing system (400) to perform: obtaining (602) a set of video frames (405) of a given length at a first resolution, wherein the length corresponds to a number of frames and the resolution corresponds to a width and height of each frame in pixels; processing the set of video frames of the given length using a convolutional neural network (402) to output one or more signals corresponding to the set of video frames, the convolutional neural network including (i) a set of two-dimensional convolutional layers, (ii) a set of three-dimensional convolutional layers, and (iii) a set of three-dimensional de-convolutional layers, wherein the three-dimensional convolutional layers reduce the set of video frames to a second resolution, and wherein the three-dimensional de-convolutional layers (422, 428, 434) upsample the set of video frames to the length and resolution of the video content provided to the convolutional neural network, and wherein the signals outputted from the two-dimensional convolutional layers are inputted to said three-dimensional convolutional layers, and the signals outputted by the three-dimensional convolutional layers are inputted to said three-dimensional deconvolutional layers; and obtaining (608) the one or more outputted signals corresponding to the set of video frames of the given length from the convolutional neural network, the outputted signals providing information corresponding to one or more voxels in the set of video frames, wherein each voxel is a pixel at a specific time.
13. The non-transitory computer-readable storage medium of claim 12, wherein obtaining the one or more outputs corresponding to the set of video frames further causes the computing system to perform: obtaining one or more respective feature descriptors for one or more voxels in the set of video frames, wherein each feature descriptor references a recognized scene, object, or action; and/or obtaining a respective optical flow for one or more voxels in the set of video frames, wherein the optical flow for a voxel describes at least a predicted direction and magnitude of the voxel; and/or obtaining a respective optical flow for one or more voxels in the set of video frames, wherein the optical flow for a voxel describes at least a predicted direction and magnitude of the voxel.
14. The non-transitory computer-readable storage medium of claim 12 or 13, wherein processing the one or more signals using the set of three-dimensional de-convolutional layers of the convolutional neural network further causes the computing system to perform: inputting, by the computing system, at least a portion of signals produced by the set of three-dimensional convolutional layers to the set of three-dimensional deconvolutional layers, the three-dimensional de-convolutional layers being trained to apply at least one three-dimensional de-convolutional operation to the portion of signals.
</claims>
</document>
