<document>

<filing_date>
2018-11-30
</filing_date>

<publication_date>
2020-12-08
</publication_date>

<priority_date>
2018-11-30
</priority_date>

<ipc_classes>
G06F3/0481,G06K9/00,G06T7/246,G06T7/73,H04L12/58,H04L29/06
</ipc_classes>

<assignee>
SNAP
</assignee>

<inventors>
LI, YUNCHENG
LUO, LINJIE
ZHANG, NING
Nie, Xuecheng
</inventors>

<docdb_family_id>
73653643
</docdb_family_id>

<title>
Efficient human pose tracking in videos
</title>

<abstract>
Systems, devices, media and methods are presented for a human pose tracking framework. The human pose tracking framework may identify a message with video frames, generate, using a composite convolutional neural network, joint data representing joint locations of a human depicted in the video frames, the generating of the joint data by the composite convolutional neural network done by a deep convolutional neural network operating on one portion of the video frames, a shallow convolutional neural network operating on a another portion of the video frames, and tracking the joint locations using a one-shot learner neural network that is trained to track the joint locations based on a concatenation of feature maps and a convolutional pose machine. The human pose tracking framework may store, the joint locations, and cause presentation of a rendition of the joint locations on a user interface of a client device.
</abstract>

<claims>
1. A method comprising: identifying, using one or more processors, a multimodal message comprising a plurality of video frames, the plurality of video frames comprising a first set of video frames and a second set of video frames; generating, using a composite convolutional neural network, joint data representing a plurality of joint locations of a human depicted in the plurality of video frames, the generating of the joint data by the composite convolutional neural network comprising: operating on the first set of video frames using a deep convolutional neural network; operating on the second set of video frames using a shallow convolutional neural network; and tracking the plurality of joint locations using a one-shot learner neural network that is trained to track the plurality of joint locations based on a concatenation of: feature maps comprising temporal information corresponding to the plurality of video frames; and a convolutional pose machine trained to produce pose estimation results corresponding to the plurality of joint locations in the plurality of video frames; storing, using the one or more processors, the plurality of the joint locations of the human depicted in the plurality of video frames; and causing presentation of a rendition of the plurality of joint locations of the human on a user interface of a client device.
2. The method of claim 1 wherein the feature maps are produced by the deep convolutional neural network and the shallow convolutional neural network.
3. The method of claim 1 wherein the first set of video frames comprises an initial video frame and the second set of video frames comprises subsequent video frames which follow the initial video frame.
4. The method of claim 1 wherein a number of layers in the deep convolutional neural network is at least five.
5. The method of claim 1 wherein the one-shot learner neural network directly outputs a template of key points, the template of key points corresponding to the plurality of joint locations of the human.
6. The method of claim 5 wherein the one-shot learner neural network outputs the template of key points to a correlation filter.
7. The method of claim 1, wherein the rendition is a character icon of the human in the plurality of video frames.
8. A system comprising: a processor; and a memory storing instructions that, when executed by the processor, configure the apparatus to: identify, a multimodal message comprising a plurality of video frames, the plurality of video frames comprising a first set of video frames and a second set of video frames; generate, using a composite convolutional neural network, joint data representing a plurality of joint locations of a human depicted in the plurality of video frames, the generating of the joint data by the composite convolutional neural network comprising: operating on the first set of video frames using a deep convolutional neural network; operating on the second set of video frames using a shallow convolutional neural network; and tracking the plurality of joint locations using a one-shot learner neural network that is trained to track the plurality of joint locations based on a concatenation of: feature maps comprising temporal information corresponding to the plurality of video frames; and a convolutional pose machine trained to produce pose estimation results corresponding to the plurality of joint locations in the plurality of video frames; store, the plurality of the joint locations of the human depicted in the plurality of video frames; and cause presentation of a rendition of the plurality of joint locations of the human on a user interface of a client device.
9. The system of claim 8 wherein the feature maps are produced by the deep convolutional neural network and the shallow convolutional network.
10. The system of claim 8 wherein the first set of video frames comprises an initial video frame and the second set of video frames comprises subsequent video frames which follow the initial video frame.
11. The system of claim 8 wherein a number of layers in the deep convolutional neural network is at least five.
12. The system of claim 8 wherein the deep convolutional neural network contains at least one deconvolution layer.
13. The system of claim 8 wherein the one-shot learner neural network directly outputs a plurality of correlation filters, the correlation filters corresponding to the plurality of joint locations of the human.
14. The system of claim 8, wherein the rendition is a character icon of a user associated with the client device.
15. A non-transitory computer-readable storage medium, the computer-readable storage medium including instructions that when executed by a computer, cause the computer to: identify, a multimodal message comprising a plurality of video frames, the plurality of; video frames comprising a first set of video frames and a second set of video frames; generate, using a composite convolutional neural network, joint data representing a plurality of joint locations of a human depicted in the plurality of video frames, the generating of the joint data by the composite convolutional neural network comprising: operating on the first set of video frames using a deep convolutional neural network; operating on the second set of video frames using a shallow convolutional neural network; and tracking the plurality of joint locations using a one-shot learner neural network that is trained to track the plurality of joint locations based on a concatenation of: feature maps comprising temporal information corresponding to the plurality of video frames; and a convolutional pose machine trained to produce pose estimation results corresponding to the plurality of joint locations in the plurality of video frames; store, the plurality of the joint locations of the human depicted in the plurality of video frames; and cause presentation of a rendition of the plurality of joint locations of the human on a user interface of a client device.
</claims>
</document>
