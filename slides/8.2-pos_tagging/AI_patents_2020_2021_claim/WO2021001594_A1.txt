<document>

<filing_date>
2020-06-10
</filing_date>

<publication_date>
2021-01-07
</publication_date>

<priority_date>
2019-07-03
</priority_date>

<ipc_classes>
G06N20/00,G06N3/08,G06T9/00,H04N19/102,H04N19/192
</ipc_classes>

<assignee>
NOKIA TECHNOLOGIES
</assignee>

<inventors>
CRICRI, FRANCESCO
AYTEKIN, CAGLAR
</inventors>

<docdb_family_id>
74101251
</docdb_family_id>

<title>
NEURAL NETWORK FOR VARIABLE BIT RATE COMPRESSION
</title>

<abstract>
Example embodiments provide a neural data compression network which may be configured to output variable bit rate codes and a decompression network capable of decompressing the variable bit rate codes. This is achieved based on training an encoder network to be capable of outputting variable size activations. Output activations may be divided into a plurality of blocks and a subset of the blocks may be selected based on a desired quality level. A decoder network may be trained as part of an auto-encoder comprising the encoder network. Apparatuses, methods, and computer programs are disclosed.
</abstract>

<claims>
1. An apparatus, comprising:
at least one processor; and
at least one memory including computer program code; the at least one memory and the computer program code configured to, with the at least one processor, cause the apparatus at least to:
receive an input at a neural network comprising a plurality of blocks;
receive an indication of a quality level;
determine a subset of the blocks based on the indication of the quality level; and
provide an output from the neural network based on the received input and the subset of the blocks.
2. The apparatus according to claim 1, wherein a block comprises a predetermined number of weights or filters.
3. The apparatus according to claim 1 or claim 2, wherein the quality level indicates a number of the blocks for providing the output.
4. The apparatus according to any preceding claim, wherein the neural network comprises N blocks and wherein a quality level Q indicates providing an output from first Q blocks in an order of blocks and not providing an output from last N-Q blocks in the order of blocks.
5. The apparatus according to any preceding claim, wherein the apparatus is further caused to:
train an ith block based on keeping parameters of i-1 previous blocks substantially frozen, setting last N-i blocks to zero, and updating parameters of the ith block, wherein N is a number of blocks at the neural network.
6. The apparatus according to any preceding claim, wherein the plurality of blocks are non-overlapping and/or wherein the plurality of blocks are located at an output layer of the neural network.
7. The apparatus according to claim 6, wherein training the ith block further comprises updating parameters of one or more preceding layers of the neural network.
8. The apparatus according to any preceding claim, wherein the neural network comprises an encoder network of an auto encoder neural network, and wherein the input comprises image data, video data, audio data, or a representation of another neural network.
9. A method, comprising:
receiving an input at a neural network comprising a plurality of blocks;
receiving an indication of a quality level;
determining a subset of the blocks based on the indication of the quality level; and
providing an output from the neural network based on the received input and the subset of the blocks.
10. The method according to claim 9, wherein a block comprises a predetermined number of weights or filters.
11. The method according to claim 9 or claim 10, wherein the quality level indicates a number of the blocks for providing the output. 12. The method according to any of claims 9 to 11, wherein the neural network comprises N blocks and wherein a quality level Q indicates providing an output from first Q blocks in an order of blocks and not providing an output from last N-Q blocks in the order of blocks.
13. The method according to any of claims 9 to 12, further comprising :
training an ith block based on keeping parameters of i-1 previous blocks substantially frozen, setting last N-i blocks to zero, and updating parameters of the ith block, wherein N is a number of blocks in the neural network.
14. A computer program comprising program code configured to cause an apparatus at least to:
receive an input at a neural network comprising a plurality of blocks;
receive an indication of a quality level;
determine a subset of the blocks based on the indication of the quality level; and
provide an output from the neural network based on the received input and the subset of the blocks.
15. An apparatus comprising:
a decoder neural network obtainable by a training method comprising:
training an ith block of an encoder neural network of an auto-encoder based on keeping parameters of i-1 previous blocks substantially frozen, setting last N-i blocks to zero, and updating parameters of the ith block, wherein N is a number of blocks in the encoder neural network, and wherein training the ith block further comprises updating parameters of the decoder neural network.
</claims>
</document>
