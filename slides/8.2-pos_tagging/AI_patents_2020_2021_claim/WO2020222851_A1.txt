<document>

<filing_date>
2019-06-03
</filing_date>

<publication_date>
2020-11-05
</publication_date>

<priority_date>
2019-05-02
</priority_date>

<ipc_classes>
G10L15/26,G10L21/06,G10L21/10,G10L25/78,G10L25/84
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
COHEN, DANIEL
HUME, THOMAS WEEDON
BAR, NADAV
SCHLESINGER, BENJAMIN
KEMLER, BRIAN
BLEUEL, Nicole Kiana
LUIPOLD, Heather Patricia
ROCARD, Kevin
LAURENT, Eric
LEE, Justin Wooyoung
BERRY, Robert James
RAMANOVICH, Michelle
VAN DEMAN, Kelsie Hope
TRIPALDI, Anthony Felice
BLOCK, Asa Jonas Ivry
PITARO, Stefanie Bianca
BURFORD, Elliott Charles
</inventors>

<docdb_family_id>
66913088
</docdb_family_id>

<title>
AUTOMATICALLY CAPTIONING AUDIBLE PARTS OF CONTENT ON A COMPUTING DEVICE
</title>

<abstract>
A computing device is described for automatically captioning content. The computing device generates captions directly from audio data being output from content sources, unlike other captioning systems which often rely on information contained in audio signals being sent to speakers. The computing device may analyze metadata to determine whether the audio data is suitable for captioning or whether the audio data is some other type of audio data. Responsive to identifying audio data for captioning, the computing device can quickly generate a description of audible sounds interpreted from the audio data. The computing device can provide the descriptions of audible content visually no matter the application source of the audible content and regardless whether the computing device ever produces audio of the content.
</abstract>

<claims>
What is claimed is:
1. A computer-implemented method comprising:
obtaining, by a processor of a computing device from an audio mixer of the computing device, audio data output from an application executing at the computing device, the audio data comprising data indicative of audible parts of content;
determining, by the processor using the audio data, whether the audio data is of a type that is suitable for captioning;
responsive to determining that the audio data is of a type that is suitable for captioning, determining, by the processor, a description of the audible parts of the content; and
while outputting visual parts of the content for display, outputting, by the processor and for display, the description of the audible parts of the content.
2. The method of claim 1, wherein the data indicative of the audible parts of the content is non-metadata and the audio data further includes metadata, wherein determining whether the audio data is of a type that is suitable for captioning comprises determining, by the processor using the metadata, whether the audio data is of a type that is suitable for captioning.
3. The method of any of claims 1 and 2, wherein the description of the audible parts of the content comprises a transcription of spoken audio from the audible parts of the content.
4. The method of any of claims 1-3, wherein the description of the audible parts of the content comprises a description of non-spoken audio from the audible parts of the content.
5. The method of claim 4, wherein the non-spoken audio comprises a noise from a particular source and the description of the noise from the particular source comprises an indication of the particular source.
6. The method of claim 5, wherein:
the noise comprises an animal noise from an animal source, or
the noise comprises an environmental noise from a non-animal source.
7. The method of any of claims 1-6, wherein determining the description of the audible parts of the content comprises executing, by the processor of the computing device, a machine-learned model that is trained to determine descriptions from the audio data to determine the description of the audible parts of the content.
8. The method of claim 7, wherein the machine-learned model comprises an end-to-end Recurrent-Neural-Network-Transducer Automatic Speech Recognition Model.
9. The method of any of claims 1-8, wherein the data indicative of the audible parts of the content comprises unannotated data that has not been annotated for captioning.
10. The method of any of claims 1-9, wherein the description comprises text indicating nonspoken audio extracted from the audible parts of the content.
11. The method of any of claims 1-10, wherein the description comprises text identifying a human and a non-human source for different portions of the audible parts of the content.
12. The method of any of claims 1-11, wherein outputting the description of the audible parts of the content comprises outputting, by the processor and for display, as a persistent element apart from visual parts of the content and apart from a graphical user interface of the application, the description of the audible parts of the content.
13. The method of clause 12, further comprising:
responsive to receiving, by the processor, a user input associated with the persistent element, modifying a size of the persistent element to display previous or subsequent descriptions generated from the audible parts of the content.
14. A computing device comprising at least one processor configured to perform any of the methods of clauses 1-13.
15. A computer-readable storage medium comprising instructions that, when executed, configure a processor of a computing device to perform any of the methods of clauses 1-13.
</claims>
</document>
