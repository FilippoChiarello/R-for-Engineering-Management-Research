<document>

<filing_date>
2019-03-21
</filing_date>

<publication_date>
2020-06-23
</publication_date>

<priority_date>
2019-03-21
</priority_date>

<ipc_classes>
G06K9/00,G06K9/62,G06T15/50,G06T19/00
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
SUNKAVALLI, KALYAN
CARR, NATHAN
HADAP, SUNIL
GARON, MATHIEU
</inventors>

<docdb_family_id>
69171827
</docdb_family_id>

<title>
Dynamically estimating lighting parameters for positions within augmented-reality scenes using a neural network
</title>

<abstract>
This disclosure relates to methods, non-transitory computer readable media, and systems that use a local-lighting-estimation-neural network to estimate lighting parameters for specific positions within a digital scene for augmented reality. For example, based on a request to render a virtual object in a digital scene, a system uses a local-lighting-estimation-neural network to generate location-specific-lighting parameters for a designated position within the digital scene. In certain implementations, the system also renders a modified digital scene comprising the virtual object at the designated position according to the parameters. In some embodiments, the system generates such location-specific-lighting parameters to spatially vary and adapt lighting conditions for different positions within a digital scene. As requests to render a virtual object come in real (or near real) time, the system can quickly generate different location-specific-lighting parameters that accurately reflect lighting conditions at different positions within a digital scene in response to render requests.
</abstract>

<claims>
We claim:
1. A non-transitory computer readable medium storing instructions thereon that, when executed by at least one processor, cause a computer system to: identify a request to render a virtual object at a designated position within a digital scene; extract a global feature map from the digital scene utilizing a first set of network layers of a local-lighting-estimation-neural network; generate a local position indicator for the designated position within the digital scene; modify the global feature map for the digital scene based on the local position indicator for the designated position; generate location-specific-lighting parameters for the designated position based on the modified global feature map utilizing a second set of network layers of the local-lighting-estimation-neural network; and based on the request, render a modified digital scene comprising the virtual object at the designated position illuminated according to the location-specific-lighting parameters.
2. The non-transitory computer readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computer system to generate the location-specific-lighting parameters for the designated position by generating location-specific-spherical-harmonic coefficients indicating lighting conditions for an object at the designated position.
3. The non-transitory computer readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computer system to: identify a position-adjustment request to move the virtual object from the designated position within the digital scene to a new designated position within the digital scene; generate a new local position indicator for the new designated position within the digital scene; modify the global feature map for the digital scene based on the new local position indicator for the new designated position to form a new modified global feature map; generate new location-specific-lighting parameters for the new designated position based on the new modified global feature map utilizing the second set of network layers; and based on the position-adjustment request, render an adjusted digital scene comprising the virtual object at the new designated position illuminated according to the new location-specific-lighting parameters.
4. The non-transitory computer readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computer system to: identify a perspective-adjustment request to render the digital scene from a different point of view; and based on the perspective-adjustment request, render the modified digital scene from the different point of view comprising the virtual object at the designated position illuminated according to the location-specific-lighting parameters.
5. The non-transitory computer readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computer system to generate the local position indicator for the designated position by identifying local position coordinates representing the designated position within the digital scene.
6. The non-transitory computer readable medium of claim 5, further comprising instructions that, when executed by the at least one processor, cause the computer system to: modify the global feature map to generate the modified global feature map by: generating a masking feature map from the local position coordinates; multiplying the global feature map and the masking feature map for the local position coordinates to generate a masked-dense-feature map; and concatenating the global feature map and the masked-dense-feature map to form a combined feature map; and generate the location-specific-lighting parameters by providing the combined feature map to the second set of network layers.
7. The non-transitory computer readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computer system to: identify an adjustment of lighting conditions for the designated position within the digital scene; extract a new global feature map from the digital scene utilizing the first set of network layers of the local-lighting-estimation-neural network; modify the new global feature map for the digital scene based on the local position indicator for the designated position; generate new location-specific-lighting parameters for the designated position based on the new modified global feature map utilizing the second set of network layers; and based on the adjustment of lighting conditions, render an adjusted digital scene comprising the virtual object at the designated position illuminated according to the new location-specific-lighting parameters.
8. The non-transitory computer readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computer system to: identify a perspective-adjustment request to render the virtual object at the designated position within the digital scene from a different point of view; generate a new local position indicator for the designated position within the digital scene from the different point of view; modify the global feature map for the digital scene based on the new local position indicator for the designated position from the different point of view to form a new modified global feature map; generate new location-specific-lighting parameters for the designated position from the different point of view based on the new modified global feature map utilizing the second set of network layers; and based on the perspective-adjustment request, render an adjusted digital scene comprising the virtual object at the designated position illuminated according to the new location-specific-lighting parameters.
9. The non-transitory computer readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computer system to: generate the local position indicator for the designated position by: selecting first pixels corresponding to the designated position from a first feature map corresponding to a first layer of the first set of network layers; and selecting second pixels corresponding to the designated position from a second feature map corresponding to a second layer of the first set of network layers; modify the global feature map to generate the modified global feature map by: combining features for the first pixels and the second pixels corresponding to the designated position to generate a hyper column map; and concatenating the global feature map and the hyper column map to form a combined feature map; and generate the location-specific-lighting parameters by providing the combined feature map to the second set of network layers.
10. A system comprising: at least one processor; at least one non-transitory computer readable medium comprising digital training scenes and ground-truth-lighting parameters for positions within the digital training scenes; a local-lighting-estimation-neural network; and instructions that, when executed by at least one processor, cause the system to train the local-lighting-estimation-neural network by: extracting a global-feature-training map from a digital training scene of the digital training scenes utilizing a first set of network layers of the local-lighting-estimation-neural network; generating a local-position-training indicator for a designated position within the digital training scene; modifying the global-feature-training map for the digital training scene based on the local-position-training indicator for the designated position; generating location-specific-lighting-training parameters for the designated position based on the modified global-feature-training map utilizing a second set of network layers of the local-lighting-estimation-neural network; and modifying network parameters of the local-lighting-estimation-neural network based on a comparison of the location-specific-lighting-training parameters with a set of ground-truth-lighting parameters for the designated position within the digital training scene.
11. The system of claim 10, wherein the digital training scene comprises a three-dimensional-digital model or a digital viewpoint image of a realistic scene.
12. The system of claim 10, further comprising instructions that, when executed by the at least one processor, cause the system to: generate the location-specific-lighting-training parameters for the designated position by generating location-specific-spherical-harmonic-training coefficients indicating lighting conditions at the designated position; and determine the set of ground-truth-lighting parameters for the designated position by determining a set of ground-truth-location-specific-spherical-harmonic coefficients indicating lighting conditions at the designated position.
13. The system of claim 12, further comprising instructions that, when executed by the at least one processor, cause the system to generate the location-specific-spherical-harmonic-training coefficients of degree five for each color channel.
14. The system of claim 12, further comprising instructions that, when executed by the at least one processor, cause the system to determine the set of ground-truth-location-specific-spherical-harmonic coefficients by: identifying positions within the digital training scene; generating a cube map for each position within the digital training scene; and projecting the cube map for each position within the digital training scene to the set of ground-truth-location-specific-spherical-harmonic coefficients.
15. The system of claim 10, further comprising instructions that, when executed by the at least one processor, cause the system to generate the local-position-training indicator for the designated position by identifying local-position-training coordinates representing the designated position within the digital training scene.
16. The system of claim 15, further comprising instructions that, when executed by the at least one processor, cause the system to: modify the global-feature-training map to generate the modified global-feature-training map by: generating a masking-feature-training map from the local-position-training coordinates; multiplying the global-feature-training map and the masking-feature-training map for the local-position-training coordinates to generate a masked-dense-feature-training map; and concatenating the global-feature-training map and the masked-dense-feature-training map to form a combined-feature-training map; and generate the location-specific-lighting-training parameters by providing the combined-feature-training map to the second set of network layers.
17. The system of claim 10, further comprising instructions that, when executed by the at least one processor, cause the system to generate the local-position-training indicator for the designated position by: selecting a first training pixel corresponding to the designated position from a first feature-training map corresponding to a first layer of the local-lighting-estimation-neural network; and selecting a second training pixel corresponding to the designated position from a second feature-training map corresponding to a second layer of the local-lighting-estimation-neural network.
18. The system of claim 10, wherein the first set of network layers of the local-lighting-estimation-neural network comprises lower layers of a densely connected convolutional network and the second set of network layers of the local-lighting-estimation-neural network comprises convolutional layers and fully connected layers.
19. In a digital medium environment for rendering augmented-reality scenes, a computer-implemented method for estimating lighting conditions for virtual objects, comprising: accessing digital training scenes and ground-truth-lighting parameters for positions within the digital training scenes; performing a step for training a local-lighting-estimation-neural network utilizing global-feature-training maps for the digital training scenes and local-position-training indicators for designated positions within the digital training scenes; identifying a request to render a virtual object at a designated position within a digital scene; performing a step for generating location-specific-lighting parameters for the designated position by utilizing the trained local-lighting-estimation-neural network; and based on the request, render a modified digital scene comprising the virtual object at the designated position illuminated according to the location-specific-lighting parameters.
20. The computer-implemented method of claim 19, further comprising: receiving the request to render the virtual object at the designated position from a mobile device; and based on receiving the request from the mobile device, rendering, within a graphical user interface of the mobile device, the modified digital scene comprising the virtual object at the designated position illuminated according to the location-specific-lighting parameters.
</claims>
</document>
