<document>

<filing_date>
2020-03-17
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2019-03-28
</priority_date>

<ipc_classes>
G01C21/32,G01S19/42,G06K9/00,G06K9/62,H04N5/225
</ipc_classes>

<assignee>
NEXAR
</assignee>

<inventors>
RIPPA, SHMUEL
LAVY, LEV YITZHAK
BROSH, ELIAHU
Levi, Elad
Herzig, Roei
</inventors>

<docdb_family_id>
72605612
</docdb_family_id>

<title>
LOCALIZATION AND MAPPING METHODS USING VAST IMAGERY AND SENSORY DATA COLLECTED FROM LAND AND AIR VEHICLES
</title>

<abstract>
A system for training simultaneous localization and mapping (SLAM) models, including a camera, mounted in a vehicle and in communication with an image server via a cellular connection, that captures images labeled with a geographic position system location and a timestamp, and uploads them to an image server, a storage device that stores geographical maps and images, and indexes the images geographically with reference to the geographical maps, an images server that receives uploaded images, labels the uploaded images with a GPS location and a timestamp, and stores the uploaded images on the storage device, and a training server that trains a SLAM model using images labeled with a GPS location and a timestamp, wherein the SLAM model (i) receives an image as input and predicts the image location as output, and/or (ii) receives an image having error as input and predicts a local correction for the image as output.
</abstract>

<claims>
1. A system for training simultaneous localization and mapping (SLAM) models, comprising: at least one camera mounted in a vehicle and in communication with an image server via a cellular connection, that captures images labeled with a geographic position system (GPS) location and a timestamp, and uploads them to the image server; a storage device that stores geographical maps and images, and indexes the images geographically with reference to the geographical maps; an images server that receives uploaded images, labels the uploaded images with a GPS location and a timestamp, and stores the uploaded images on said storage device; and a training server that trains a SLAM model using images labeled with a GPS location and a timestamp, wherein the SLAM model (i) receives an image as input and predicts the image location as output, and/or (ii) receives an image having error as input and predicts a local correction for the image as output.
2. The system of claim 1 further comprising at least one sensor mounted in the vehicle that sends low bandwidth sensor data to said image server, wherein said at least one sensor is a member of the group consisting of a lidar sensor, a radar sensor, an external camera, an accelerometer, a magnetometer, a phone accelerometer, a gyroscope and a barometer, wherein said training server uses the sensor data to train the SLAM model to identify elements comprising turns, speed bumps and potholes.
3. The system of claim 1 wherein said training server processes sensor data as time series data and separates that sensor data to time intervals labelled with time and location, and/or processes sensor data as spatial data and separates the sensor data according to distance into regions labelled with time and location.
4. The system of claim 1 wherein said training server maps sensor data to road conditions.
5. The system of claim 1 wherein said at least one camera is in communication with a mobile phone via wired or wireless communication, and said at least one camera transmits its captured images to a mobile phone that is connected to said image server.
6. The system of claim 1 wherein said at least one camera captures a video sequence of images with a time series of GPS and timestamp data, wherein the video sequence of images is a time-lapse or street-lapse sequence with images separated by time intervals or distance intervals, respectively, and wherein said image server synchronizes the images with the GPS and timestamp data.
7. The system of claim 1 wherein said training server trains the SLAM model to generate vehicle dynamic motion relative to a vehicle coordinate system.
8. The system of claim 1 wherein said training server detects one or more objects in an image and determines the locations of the one or more objects relative to the vehicle.
9. The system of claim 8 wherein said training server determines from which locations the one or more objects may be seen, at which angles of view.
10. The system of claim 1 wherein said training server updates maps based on images uploaded to said image server, and detects changes in updated maps vis-Ã -vis non-updated maps.
11. The system of claim 1 further comprising an annotation tool that enables a person to mark and upload to said image server a vehicle route for an uploaded image or sequence of images, by marking points in an aerial map, and wherein said training server determines ground truth points corresponding to the marked points.
12. The system of claim 1 wherein the SLAM model accepts as input raster or vector road images and/or aerial road images, and wherein said training server trains the SLAM model to position uploaded images relative to the road images.
13. The system of claim 12 wherein the aerial road images are pre-processed as a tiled map, at least some of the map tiles comprising one or more members of the group consisting of roads, lanes, crossings, road markings, road edges, road signs, bridges, sidewalks, parking lots, buildings and trees, and wherein said training server trains the SLAM model to determine which map tile covers a current location of the vehicle.
14. A method for training simultaneous localization and mapping (SLAM) models, comprising: capturing a sequence of road images by a vehicle dashboard camera, while the vehicle is moving; labeling the captured road images with time and GPS location; creating feature representations of the captured road images; further creating a feature representation of a map tile from aerial imagery, the map tile covering the area where the vehicle is traveling; and triangulating vehicle location and correcting GPS errors, comprising training a SLAM model to use feature representations of the road images and of the map tile.
15. The method of claim 14, wherein said labelling also labels the collected road images with inertial measurement data based on inertial measurement unit (IMU) sensors, the IMU sensors comprising members of the group consisting of a mobile phone, a vehicle controller area network (CAN) bus, an on-board diagnostics (OBD) module, and electronic wearables.
16. The method of claim 14, further comprising correcting localization error, comprising compressing the map tile via a trained compression model that compresses a map tile to a feature vector, comprising: providing a road image to the compression model together with the map tile; and training the compression model to fit the road image to the map tile, thereby generating a compressed vector representation of the map tile comprising correlated aerial and road view information.
17. The method of claim 14, further comprising: generating a full geographic area of compressed vector/pixel level segmentation of the map tile, comprising: annotating the map tile using segmentation and vector annotation of lanes, road borders, road features, and road crossings; and training the localization model on the annotated data; and continually updating the compressed vector/pixel level segmentation of the map tile, from up-to-date camera road images.
18. The method of claim 14, further comprising training a SLAM model to predict vehicle location from a sequence of images and GPS data, based on the compressed map tiles and/or the compressed road images.
19. The method of claim 14, further comprising using the collected camera road images to train the SLAM model to predict if a map tile is outdated in some areas, and to update an aerial raster image tile in road areas that are visible in both road and aerial views.
20. The method of claim 14, further comprising: running localization in real time on the mobile phone, via an external camera on the vehicle that uses the mobile phone camera or that streams video to the mobile phone; and transmitting the localization to other vehicles via vehicle to vehicle (V2V) communication, to enable advanced driver-assistance system (ADAS) warnings across vehicles to be accurate, and to enable warnings to be generated and dispatched to proper drivers in real time according to the location of the drivers' vehicles.
</claims>
</document>
