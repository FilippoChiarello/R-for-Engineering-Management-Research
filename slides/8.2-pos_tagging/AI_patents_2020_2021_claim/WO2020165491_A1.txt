<document>

<filing_date>
2020-01-29
</filing_date>

<publication_date>
2020-08-20
</publication_date>

<priority_date>
2019-02-15
</priority_date>

<ipc_classes>
G06K9/62,G06N20/00,G06N3/08,H03M7/40
</ipc_classes>

<assignee>
NOKIA TECHNOLOGIES
</assignee>

<inventors>
CRICRI, FRANCESCO
AYTEKIN, CAGLAR
</inventors>

<docdb_family_id>
72044760
</docdb_family_id>

<title>
APPARATUS AND A METHOD FOR NEURAL NETWORK COMPRESSION
</title>

<abstract>
There is provided an apparatus comprising means for training a neural network, wherein the training comprises applying a loss function configured to increase sparsity of a weight tensor of the neural network and to cause a plurality of non-zero elements of the weight tensor to be substantially equal to each other;and means for entropy coding the weight tensor to obtain a compressed neural network.
</abstract>

<claims>
1. An apparatus comprising
means for training a neural network, wherein the training comprises applying a loss function configured to increase sparsity of a weight tensor of the neural network and to cause a plurality of non-zero elements of the weight tensor to be substantially equal to each other; and
means for entropy coding the weight tensor to obtain a compressed neural network.
2. The apparatus according to claim 1 , wherein the means are further configured to perform
providing the compressed neural network for transmission.
3. The apparatus according to claim 1 or 2, wherein the loss function comprises at least one critical point, and wherein the loss function at the critical point corresponds to a sparse weight tensor, and wherein a plurality of non-zero elements of the sparse weight tensor are substantially equal to each other.
4. The apparatus according to any of the claims 1 to 3, wherein the loss function comprises a compression loss defined by an L1 norm of the weight tensor divided by an L2 norm of the weight tensor.
5. The apparatus according to any of the claims 1 to 4, wherein at least a portion of the elements of the sparse weight tensor are substantially equal to zero.
6. The apparatus according to any of the claims 1 to 5, wherein the loss function comprises a plurality of critical points comprising a first critical point and a second critical point, and wherein
a first weight tensor corresponding to a first value of the loss function at a first critical point has a first number of substantially zero elements;
a second weight tensor corresponding to a second value of the loss function at a second critical point has a second number of substantially zero elements, wherein the first number is higher than the second number; and wherein the first value of the loss function is lower than the second value of the loss function.
7. The apparatus according to any of the claims 1 to 6, wherein the loss function comprises a compression loss and a task-specific loss.
8. The apparatus according to any of the claims 1 to 7, wherein the means are further configured to perform
quantizing the weight tensor.
9. The apparatus according to claim 8, wherein the quantizing comprises approximating quantization by introducing additive noise to the weight tensor during training, wherein the additive noise level is defined by a first hyperparameter.
10. The apparatus according to claim 8 or 9, wherein the quantizing is performed after training according to a set of hyperparameters comprising a first hyperparameter defining the additive noise level;
a second hyperparameter defining a lower limit of a weight range; and a third hyperparameter defining an upper limit of the weight range.
11. The apparatus according to any of the claims 1 to 10, wherein the means are further configured to perform
initializing the neural network randomly by applying a mapping function arranged such that the initialization falls into non-saturated region of the mapping function.
12. The apparatus according to claim 11 , wherein the means are further configured to perform
adaptively changing weight initialization given the mapping function.
13. The apparatus according to any of the claims 1 to 10, wherein the means are further configured to perform
initializing the neural network from a given seed by applying a mapping function arranged such that the seed falls into non-saturated region of the mapping function.
14. The apparatus according to 1 1 or 13, wherein the means are further configured to perform
adaptively changing the mapping function according to given weight initialization.
15. The apparatus of any preceding claim wherein the means comprises at least one processor; at least one memory including computer program code; the at least one memory and the computer program code configured to, with the at least one processor, cause the performance of the apparatus.
16. A method comprising
training a neural network, wherein the training comprises applying a loss function configured to increase sparsity of a weight tensor of the neural network and to cause a plurality of non-zero elements of the weight tensor to be substantially equal to each other; and
entropy coding the weight tensor to obtain a compressed neural network.
17. A computer program product comprising computer program code configured to, when executed on at least one processor, cause an apparatus or a system to:
train a neural network, wherein the training comprises applying a loss function configured to increase sparsity of a weight tensor of the neural network and to cause a plurality of non-zero elements of the weight tensor to be substantially equal to each other; and
entropy code the weight tensor to obtain a compressed neural network.
</claims>
</document>
