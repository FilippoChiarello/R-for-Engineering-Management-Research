<document>

<filing_date>
2020-07-01
</filing_date>

<publication_date>
2020-10-22
</publication_date>

<priority_date>
2016-12-15
</priority_date>

<ipc_classes>
G06K9/00,G06K9/46,G06K9/62,G06K9/66,G06N20/00,G06N3/04,G06N3/08,G06T5/00,H04N19/126,H04N19/13,H04N19/149,H04N19/154,H04N19/167,H04N19/172,H04N19/18,H04N19/196,H04N19/33,H04N19/44,H04N19/48,H04N19/91
</ipc_classes>

<assignee>
WAVEONE
</assignee>

<inventors>
BOURDEV, LUBOMIR
RIPPEL, OREN
</inventors>

<docdb_family_id>
62561751
</docdb_family_id>

<title>
DEEP LEARNING BASED ADAPTIVE ARITHMETIC CODING AND CODELENGTH REGULARIZATION
</title>

<abstract>
A deep learning based compression (DLBC) system applies trained models to compress binary code of an input image to a target codelength. For a set of binary codes representing the quantized coefficents of an input image, the DLBC system applies a first model that is trained to predict feature probabilities based on the context of each bit of the binary codes. The DLBC system compresses the binary code via adaptive arithmetic coding based on the determined probability of each bit. The compressed binary code represents a balance between a reconstruction quality of a reconstruction of the input image and a target compression ratio of the compressed binary code.
</abstract>

<claims>
1. An encoder stored on a computer readable storage medium, wherein the encoder is manufactured by a process comprising: obtaining training data including one or more training images; for a compression model including an encoding portion and a decoding portion, and for each training image in the one or more training images: generating a reconstructed image of the training image by applying the encoding portion to the training image to generate a tensor for the training image, and applying the decoding portion to the tensor to generate the reconstructed image, determining a loss function, the loss function including: a reconstruction loss indicating a difference between the training image and the reconstructed image, and a codelength regularization loss that increases when a codelength of a code generated by compressing information in the tensor increases, and decreases when the codelength of the code decreases; repeatedly backpropagating one or more terms obtained from the loss function to update a set of parameters for the encoding portion and the decoding portion, and stopping the backpropagation after the loss function satisfies a predetermined criteria; and storing the set of parameters of the encoding portion on the computer readable storage medium as parameters of the encoder.
2. The encoder of claim 1, wherein the process further comprises: quantizing coefficients of the tensor for the training image to obtain a quantized tensor for the training image.
3. The encoder of claim 2, wherein the codelength regularization loss includes a term representing a magnitude of coefficients in the quantized tensor for the training image.
4. The encoder of claim 2, wherein the codelength regularization loss includes a term representing deviations between a coefficient in the quantized tensor for the training image and a set of neighboring coefficients.
5. The encoder of claim 4, wherein the process further comprises: decomposing the quantized tensor to a binary tensor of one or more bitplanes such that the coefficient of the quantized tensor is represented by a set of binary values at the one or more bitplanes, and wherein the set of neighboring coefficients for the coefficient are binary values positioned adjacent to a binary value for the coefficient in a same bitplane.
6. The encoder of claim 2, wherein the process further comprises: decomposing the quantized tensor to a binary tensor of one or more bitplanes, wherein the codelength regularization loss includes a term representing a change between a codelength of a code generated by compressing the binary tensor when a binary value in the binary tensor is flipped.
7. The encoder of claim 1, wherein the process further comprises: determining whether the codelength of the code is above or lower than a target codelength, and scaling up the codelength regularization loss if the codelength of the code is above the target codelength and scaling down the codelength regularization loss if the codelength of the code is lower than the target codelength.
8. The encoder of claim 1, wherein the training images are frames of a video.
9. The encoder of claim 1, wherein the training images are residual frames of a video.
10. A decoder stored on a computer readable storage medium, wherein the decoder is manufactured by a process comprising: obtaining training data including one or more training images; for a compression model including an encoding portion and a decoding portion, and for each training image in the one or more training images: generating a reconstructed image of the training image by applying the encoding portion to the training image to generate a tensor for the training image, and applying the decoding portion to the tensor to generate the reconstructed image, determining a loss function, the loss function including: a reconstruction loss indicating a difference between the training image and the reconstructed image, and a codelength regularization loss that increases when a codelength of a code generated by compressing information in the tensor increases, and decreases when the codelength of the code decreases; repeatedly backpropagating one or more terms obtained from the loss function to update a set of parameters for the encoding portion and the decoding portion, and stopping the backpropagation after the loss function satisfies a predetermined criteria; and storing the set of parameters of the decoding portion on the computer readable storage medium as parameters of the decoder.
11. The decoder of claim 10, wherein the process further comprises: quantizing coefficients of the tensor for the training image to obtain a quantized tensor for the training image.
12. The decoder of claim 11, wherein the codelength regularization loss includes a term representing a magnitude of coefficients in the quantized tensor for the training image.
13. The decoder of claim 11, wherein the codelength regularization loss includes a term representing deviations between a coefficient in the quantized tensor for the training image and a set of neighboring coefficients.
14. The decoder of claim 13, wherein the process further comprises: decomposing the quantized tensor to a binary tensor of one or more bitplanes such that the coefficient of the quantized tensor is represented by a set of binary values at the one or more bitplanes, and wherein the set of neighboring coefficients are binary values positioned adjacent to a binary value for the coefficient in a same bitplane.
15. The decoder of claim 11, wherein the process further comprises: decomposing the quantized tensor to a binary tensor of one or more bitplanes, wherein the codelength regularization loss includes a term representing a change between a codelength of a code generated by compressing the binary tensor when a binary value in the binary tensor is flipped.
16. The decoder of claim 10, wherein the process further comprises: determining whether the codelength of the code is above or lower than a target codelength, and scaling up the codelength regularization loss if the codelength of the code is above the target codelength and scaling down the codelength regularization loss if the codelength of the code is lower than the target codelength.
17. The decoder of claim 10, wherein the training images are frames of a video.
18. The decoder of claim 10, wherein the training images are residual frames of a video.
</claims>
</document>
