<document>

<filing_date>
2019-11-13
</filing_date>

<publication_date>
2020-05-28
</publication_date>

<priority_date>
2018-11-19
</priority_date>

<ipc_classes>
G06F11/30,G06F11/34
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
KAPADIA, PARAS
PIASECZNY, MICHAL
RAJENDERAN, AMOG
SALIBA, HANI
</inventors>

<docdb_family_id>
69165556
</docdb_family_id>

<title>
VETO-BASED MODEL FOR MEASURING PRODUCT HEALTH
</title>

<abstract>
The performance of a cloud-based software product over time is determined by collecting telemetry data representing whether different features of online sessions of the software product are operating properly. The telemetry data represents shared performance metrics of the software product across different participants and components participating in an online session. The collected telemetry data is correlated with session identifiers identifying the online session from which the telemetry data was collected. The telemetry data for an online session is processed to establish a unit of failure when the telemetry data indicates that the online session operated outside of predefined performance metrics. The unit of failure is a function of vetoes applied to a candidate list of online sessions indicating that the online session may have problems. The performance of the software product may be determined as a function of the unit of failure over time.
</abstract>

<claims>
1. A system for determining the performance of a cloud-based software product over time, comprising:
processing circuitry including at least one processor; and
a memory device having instructions stored therein, wherein the instructions, which when executed by the processing circuitry, configure the at least one processor to: collect telemetry data representing whether different features of online sessions of the software product are operating properly, where the telemetry data represents shared performance metrics of the software product across different participants and components participating in an online session;
correlate the collected telemetry data with session identifiers identifying the online session from which the telemetry data was collected;
process the telemetry data for the online session to establish a unit of failure when the telemetry data indicates that the online session operated outside of predefined performance metrics; and
determine the performance of the software product as a function of the unit of failure over time.
2. A system as in claim 1, further comprising an interface device through which a user may define user experience metrics for the software product, specify telemetry data needed for measuring performance of the software product and provide a basis for combining the telemetry data into the shared performance metrics, wherein the unit of failure represents top Quality of Experience issues impacting the shared performance metrics.
3. A system as in claim 1, wherein the processing circuitry is further configured to create at least one shared performance metric that begins with a candidate list of online sessions of the software product potentially with performance outside of the predefined performance metrics, wherein the candidate list of online sessions contains a session identifier, metadata about the online sessions, and why each online session was chosen as part of a candidate list of online sessions.
4. A system as in claim 3, wherein the processing circuitry is further configured to apply vetoes to the candidate list, where the vetoes are signals provided by at least one of components and owners of the software product that identify a problem and include session identifiers when at least one of the components and a service area behave outside of the predefined performance metrics.
5. A system as in claim 4, wherein the processing circuitry is further configured to combine a number of online sessions potentially invalidated with a multitude of vetoes to generate the unit of failure.
6. A system as in claim 1, wherein the processing circuitry is further configured to identify online sessions that have been marked as possibly not operating properly by vetoes and determining whether the identified online sessions satisfy the unit of failure, wherein performance of the software product is calculated as a function of the vetoes.
7. A system as in claim 6, wherein the processing circuitry is further configured to generate a veto-based health table for the software product from processing of the vetoes, further comprising visualization software that generates dashboard views representative of user experiences with the software product from values stored in the veto-based health table.
8. A system as in claim 6, wherein the processing circuitry is further configured to functionally join telemetry event data from an online session that has been vetoed with corresponding data from a server log using the session identifier for the online session to generate a data table, and to recategorize the vetoed session to an owner of the server log data for service issues relating to the vetoed session.
9. A computer-implemented method of determining the performance of a cloud-based software product over time, comprising:
collecting telemetry data representing whether different features of online sessions of the software product are operating properly, where the telemetry data represents shared performance metrics of the software product across different participants and components participating in an online session;
correlating the collected telemetry data with session identifiers identifying the online session from which the telemetry data was collected;
processing the telemetry data for an online session to establish a unit of failure when the telemetry data indicates that the online session operated outside of predefined performance metrics; and
determining the performance of the software product as a function of the unit of failure over time.
10. A method as in claim 9, wherein collecting telemetry data comprises creating at least one shared performance metric that begins with a candidate list of online sessions of the software product potentially with performance outside of the predefined performance metrics, wherein the candidate list of online sessions contains a session identifier, metadata about the online sessions, and why each online session was chosen as part of a candidate list of online sessions, and wherein processing the telemetry data comprises applying vetoes to the candidate list, where the vetoes are signals provided by at least one of components and owners of the software product that identify a problem and include session identifiers when at least one of the components and a service area behave outside of the predefined performance metrics.
11. A method as in claim 9, wherein processing the telemetry data comprises identifying online sessions that have been marked as possibly not operating properly by vetoes and determining whether the identified online sessions satisfy the unit of failure, wherein performance of the software product is calculated as a function of the vetoes.
12. A method as in claim 11, further comprising:
generating a veto-based health table for the software product from processing of the vetoes; and
generating, using visualization software, dashboard views representative of user experiences with the software product from values stored in the veto-based health table.
13. A method as in claim 11, further comprising creating a final veto calculation table for the software product as a series of joins between data from online sessions and column data resulting from vetoes of the online sessions and aggregating data in the final veto calculation table by unique session identifier to provide a data collection correlated with the vetoes for visualization as an indication of the function of the software product over time.
14. A method as in claim 11, wherein processing the telemetry data comprises functionally joining telemetry event data from an online session that has been vetoed with corresponding data from a server log using the session identifier for the online session to generate a data table, and recategorizing the vetoed session to an owner of the server log data for service issues relating to the vetoed session.
15. A machine-readable medium having instructions stored thereon that when executed by one or more processors cause the one or more processors to implement a method of determining the performance of a cloud-based software product over time, the method including:
collecting telemetry data representing whether different features of online sessions of the software product are operating properly, where the telemetry data represents shared performance metrics of the software product across different participants and components participating in an online session; correlating the collected telemetry data with session identifiers identifying the online session from which the telemetry data was collected;
processing the telemetry data for the online session to establish a unit of failure when the telemetry data indicates that the online session operated outside of predefined performance metrics; and
determining the performance of the software product as a function of the unit of failure over time.
</claims>
</document>
