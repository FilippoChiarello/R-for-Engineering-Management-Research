<document>

<filing_date>
2020-02-26
</filing_date>

<publication_date>
2020-08-27
</publication_date>

<priority_date>
2019-02-26
</priority_date>

<ipc_classes>
G06K9/00,G06T11/20,G06T7/00,G06T7/73,H04N5/225,H04N5/235,H04N5/247
</ipc_classes>

<assignee>
HYUNDAI MOBIS COMPANY
</assignee>

<inventors>
CHO, HEUNG RAE
</inventors>

<docdb_family_id>
72140364
</docdb_family_id>

<title>
OBJECT DETECTION APPARATUS AND METHOD FOR VEHICLE
</title>

<abstract>
An object detection apparatus includes a first camera unit, a second camera unit, and a control unit. The first camera unit includes one or more cameras, and is configured to capture an image around a vehicle. The second camera unit includes one or more cameras, and is configured to capture an image of an area ahead of the vehicle. The control unit is configured to: determine a displacement of a feature point positioned in a common area from the image acquired via the first camera unit; determine a pixel displacement of the feature point in the image acquired via the second camera unit; and determine distance information to an object recognized in the image captured via the second camera unit based on the displacement of the feature point and the pixel displacement of the feature point.
</abstract>

<claims>
1. An object detection apparatus comprising: a first camera unit comprising one or more cameras, the first camera unit being configured to capture an image around a vehicle; a second camera unit comprising one or more cameras, the second camera unit being configured to capture an image of an area ahead of the vehicle; and a control unit configured to: determine a displacement of a feature point positioned in a common area from the image acquired via the first camera unit; determine a pixel displacement of the feature point in the image acquired via the second camera unit; and determine distance information to an object recognized in the image captured via the second camera unit based on the displacement of the feature point and the pixel displacement of the feature point.
2. The object detection apparatus of claim 1, wherein the control unit is configured to: determine per-pixel distance information of the image captured via the second camera unit based on the displacement of the feature point and the pixel displacement of the feature point; and determine the distance information to the recognized object using the per-pixel distance information.
3. The object detection apparatus of claim 1, wherein the control unit is configured to store per-pixel distance information of the image acquired via the first camera unit as predetermined information.
4. The object detection apparatus of claim 1, wherein the common area is an area in a shooting area of the first camera unit overlapping a shooting area of the second camera unit.
5. The object detection apparatus of claim 1, wherein: the first camera unit comprises a surround view monitor (SVM) camera; and the second camera unit comprises a multi-function camera (MFC).
6. The object detection apparatus of claim 1, further comprising: a lamp module configured to illuminate the common area, wherein the control unit is configured to generate the feature point in the common area via the lamp module.
7. The object detection apparatus of claim 1, further comprising: an illumination sensor configured to detect illuminance outside the vehicle; and a lamp module configured to illuminate the common area, wherein the control unit is configured to generate the feature point in the common area via the lamp module in response to the illuminance satisfying a preset reference value.
8. The object detection apparatus of claim 1, wherein, to generate a bounding box for the object, the control unit is configured to: couple images of adjacent cameras among the cameras of the first and second camera units to form a coupled image; perform object recognition on the coupled image; and generate the bounding box based on the object recognition.
9. An object detection method for a vehicle, comprising: determining, by a control unit, a displacement of a feature point positioned in a common area of an image acquired via a first camera unit configured to capture an image around the vehicle; determining, by the control unit, a pixel displacement of the feature point in an image acquired via a second camera unit configured to capture an image of an area ahead of the vehicle; and determining, by the control unit, distance information to an object recognized in the image captured via the second camera unit based on the displacement of the feature point and the pixel displacement of the feature point.
10. The object detection method of claim 9, wherein determining the distance information to the object comprises: determining, by the control unit, per-pixel distance information of the image captured via the second camera unit based on the displacement of the feature point and the pixel displacement of the feature point; and determining, by the control unit, the distance information to the object using the per-pixel distance information.
11. The object detection method of claim 9, further comprising: causing, at least in part, a lamp module to generate the feature point in the common area before determining the displacement of the feature point.
12. The object detection method of claim 11, further comprising: detecting, by the control unit, illuminance outside the vehicle via an illuminance sensor before generating the feature point, wherein the control unit is configured to cause, at least in part, the lamp module to generate the feature point in response to the illuminance satisfying a preset reference value.
13. The object detection method of claim 9, further comprising: coupling, by the control unit, images of adjacent cameras among cameras of the first and second camera units; and generating, by the control unit, a bounding box for an object recognized in the coupled image via object recognition.
14. The object detection method of claim 13, further comprising: causing, at least in part, a lamp module to illuminate the entire common area.
15. An apparatus, comprising: at least one processor; and at least one memory comprising one or more sequences of one or more instructions configured to, in response to being executed via the at least one processor, cause the apparatus at least to: determine displacement of a feature point via one or more first images, the one or more first images corresponding to an area around a vehicle; determine pixel displacement of the feature point via one or more second images, the one or more second images corresponding to an area in a path of travel of the vehicle; and determine distance information to an object in the second image based on the displacement of the feature point and the pixel displacement of the feature point.
16. The apparatus of claim 15, wherein the feature point is located in an area common to the one or more first images and the one or more second images.
17. The apparatus of claim 15, wherein the at least one memory and the one or more sequences of one or more instructions are configured to, in response to being executed via the at least one processor, cause the apparatus at least to: determine per-pixel distance information via the one or more second images based on the displacement of the feature point and the pixel displacement of the feature point; and determine the distance information to the object based on the per-pixel distance information.
18. The apparatus of claim 15, wherein the at least one memory and the one or more sequences of one or more instructions are further configured to, in response to being executed via the at least one processor, cause the apparatus at least to illuminate a portion of the area in the path of travel of the vehicle to generate the feature point.
19. The apparatus of claim 18, wherein: the at least one memory and the one or more sequences of one or more instructions are further configured to, in response to being executed via the at least one processor, cause the apparatus at least to detect illuminance of an ambient environment; and generation of the feature point is in response to satisfaction of a predetermined condition by the illuminance.
20. The apparatus of claim 15, wherein the at least one memory and the one or more sequences of one or more instructions are further configured to, in response to being executed via the at least one processor, cause the apparatus at least to: couple a first image among the one or more first images with a second image among the one or more second images; and generate bounding box information with respect to the object.
</claims>
</document>
