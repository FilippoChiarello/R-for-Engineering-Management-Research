<document>

<filing_date>
2020-07-21
</filing_date>

<publication_date>
2020-11-05
</publication_date>

<priority_date>
2015-01-26
</priority_date>

<ipc_classes>
G10L17/04
</ipc_classes>

<assignee>
Verint Systems Ltd.
</assignee>

<inventors>
SHAPIRA, IDO
GORODETSKI, ALEX
WEIN, RON
SIDI, OANA
</inventors>

<docdb_family_id>
56432728
</docdb_family_id>

<title>
WORD-LEVEL BLIND DIARIZATION OF RECORDED CALLS WITH ARBITRARY NUMBER OF SPEAKERS
</title>

<abstract>
Disclosed herein are methods of diarizing audio data using first-pass blind diarization and second-pass blind diarization that generate speaker statistical models, wherein the first pass-blind diarization is on a per-frame basis and the second pass-blind diarization is on a per-word basis, and methods of creating acoustic signatures for a common speaker based only on the statistical models of the speakers in each audio session.
</abstract>

<claims>
1. A method of blind diarization of audio data having a first-pass blind diarization process and a second-pass blind diarization process, the method comprising: identifying non-speech segments in audio data and segmenting the audio data into a plurality of utterance that are separated by the identified non-speech segments, representing each utterance as an utterance model representative of a plurality of feature vectors of each utterance; clustering the utterance models; and constructing a plurality of speaker models from the clustered utterance models.
2. The method of claim 1, further comprising: constructing a first hidden Markov model (HMM) of the plurality of speaker models; decoding a sequence of identified speaker models that best corresponds to the utterances of the audio data; for each segment, decoding the segment using a decoder, wherein the decoder outputs words and non-speech symbols; and for each segment, analyzing the sequence of output words and non-speech symbols from the decoder for the segment, wherein non-speech parts are discarded and the segment is refined resulting in sub-segments comprising words.
3. The method of claim 2, wherein the decoder comprises a large-vocabulary continuous speech recognition (LVCSR) decoder.
4. The method of claim 2, further comprising: constructing a second plurality of speaker models using the subsegments; by feeding the resulting sub-segments into a clustering algorithm; and constructing a hidden Markov model (HMM) HMM of the second plurality of speaker models.
5. The method of claim 4, wherein constructing the second plurality of speaker models using the subsegments comprises feeding the sub-segments into a clustering algorithm.
6. The method of claim 4, further comprising decoding a best path corresponding to the sequence of output words in the HMM by applying a Viterbi algorithm that performs word-level segmentation.
7. The method of claim 6, wherein decoding the best path corresponding to the sequence of output words in the HMM comprises decoding the best path by applying a Viterbi algorithm that performs word-level segmentation.
8. The method of claim 1, further comprising segmenting the audio data using a voice-activity-detector (VAD).
9. A system comprising: at least one processor; and at least one non-transitory computer-readable storage medium storing instructions that, when executed by the at least one processor, cause the system to: identify non-speech segments in audio data and segmenting the audio data into a plurality of utterance that are separated by the identified non-speech segments, represent each utterance as an utterance model representative of a plurality of feature vectors of each utterance; cluster the utterance models; and construct a plurality of speaker models from the clustered utterance models.
10. The system of claim 9, further comprising instructions that, when executed by the at least one processor, cause the system to: constructing a first hidden Markov model (HMM) of the plurality of speaker models; decoding a sequence of identified speaker models that best corresponds to the utterances of the audio data; for each segment, decoding the segment using a decoder, wherein the decoder outputs words and non-speech symbols; and for each segment, analyzing the sequence of output words and non-speech symbols from the decoder for the segment, wherein non-speech parts are discarded and the segment is refined resulting in sub-segments comprising words.
11. The system of claim 10, wherein the decoder comprises a large-vocabulary continuous speech recognition (LVCSR) decoder.
12. The system of claim 10, further comprising instructions that, when executed by the at least one processor, cause the system to: construct a second plurality of speaker models using the subsegments; by feeding the resulting sub-segments into a clustering algorithm; and construct a hidden Markov model (HMM) HMM of the second plurality of speaker models.
13. The system of claim 12, wherein constructing the second plurality of speaker models using the subsegments comprises feeding the sub-segments into a clustering algorithm.
14. The system of claim 12, further comprising decoding a best path corresponding to the sequence of output words in the HMM by applying a Viterbi algorithm that performs word-level segmentation.
15. The system of claim 14, wherein decoding the best path corresponding to the sequence of output words in the HMM comprises decoding the best path by applying a Viterbi algorithm that performs word-level segmentation.
16. The system of claim 9, further comprising segmenting the audio data using a voice-activity-detector (VAD).
17. A non-transitory computer-readable storage medium storing instructions that, when executed by at least one processor, cause the at least one processor to: identify non-speech segments in audio data and segmenting the audio data into a plurality of utterance that are separated by the identified non-speech segments, represent each utterance as an utterance model representative of a plurality of feature vectors of each utterance; cluster the utterance models; and construct a plurality of speaker models from the clustered utterance models.
18. The computer-readable storage medium of claim 17, further comprising instructions that, when executed by the at least one processor, cause the at least one processor to: constructing a first hidden Markov model (HMM) of the plurality of speaker models; decoding a sequence of identified speaker models that best corresponds to the utterances of the audio data; for each segment, decoding the segment using a decoder, wherein the decoder outputs words and non-speech symbols; and for each segment, analyzing the sequence of output words and non-speech symbols from the decoder for the segment, wherein non-speech parts are discarded and the segment is refined resulting in sub-segments comprising words.
19. The computer-readable storage medium of claim 18, wherein the decoder comprises a large-vocabulary continuous speech recognition (LVCSR) decoder.
20. The computer-readable storage medium of claim 17, further comprising instructions that, when executed by the at least one processor, cause the at least one processor to: construct a second plurality of speaker models using the subsegments; by feeding the resulting sub-segments into a clustering algorithm; and construct a hidden Markov model (HMM) HMM of the second plurality of speaker models.
</claims>
</document>
