<document>

<filing_date>
2019-05-20
</filing_date>

<publication_date>
2020-11-26
</publication_date>

<priority_date>
2019-05-20
</priority_date>

<ipc_classes>
G06F11/30,G06F11/34,G06F9/50,G06F9/54,H04L12/26
</ipc_classes>

<assignee>
AT&T INTELLECTUAL PROPERTY I (AMERICAN TELEPHONE AND TELEGRAPH COMPANY INTELLECTUAL PROPERTY I)
</assignee>

<inventors>
BHORKAR, ABHIJEET
JIANG, BAOFENG
MALBOUBI, MEHDI
</inventors>

<docdb_family_id>
73456706
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR LOW LATENCY EDGE COMPUTING
</title>

<abstract>
Aspects of the subject disclosure may include, for example, a method in which a processing system receives data at an edge node of a network that also includes regional nodes and central nodes. The processing system also determines a latency criterion associated with an application for processing the data; the application corresponds to an application programming interface. The method also includes processing the data in accordance with the application, monitoring a latency associated with the processing, and determining whether the latency meets the latency criterion. The processing system dynamically assigns data processing resources so that the latency meets the latency criterion; the resources include computation, network and storage resources of the edge node, a central node, and a regional node in communication with the edge node and the central node. Other embodiments are disclosed.
</abstract>

<claims>
1. A method comprising: receiving, by a processing system including a processor, data at an edge node of a plurality of edge nodes of a network, the network further comprising a plurality of regional nodes and a plurality of central nodes; determining, by the processing system, a latency criterion associated with an application for processing the data, the application utilizing an application programming interface (API), information regarding the API being stored in a database accessible to at least one of the edge node, a regional node of the plurality of regional nodes in communication with the edge node, and a central node of the plurality of central nodes in communication with the regional node; monitoring, by the processing system, a latency associated with the processing of the data by the application; determining, by the processing system, whether the latency satisfies the latency criterion; and responsive to the latency satisfying the latency criterion, assigning, by the processing system, resources to assist the application in reducing the latency associated with the processing of the data, the resources comprising computation resources, storage resources, network resources, or a combination thereof obtained from the central node, the regional node, the edge node, or any combination thereof.
2. The method of claim 1, further comprising: partitioning, by the processing system, the API into a plurality of base APIs including a first base API and a second base API, wherein the first base API has associated therewith a first latency less than a second latency associated with the second base API; and assigning, by the processing system, the edge node to the first base API and the regional node to the second base API.
3. The method of claim 1, wherein the API is accessible to a third party.
4. The method of claim 1, further comprising: by the processing system, measuring a key performance indicator (KPI) of the network, accessing the KPI, or both; and predicting, by the processing system, a network performance metric based on the KPI.
5. The method of claim 1, wherein the API comprises a plurality of base APIs, and wherein the database comprises a list of data objects to be accessed via the respective base APIs.
6. The method of claim 5, further comprising triggering, by the processing system, a procedure for reducing the latency comprising placement of the data objects at one or more of the edge node, the regional node, and the central node.
7. The method of claim 5, further comprising triggering, by the processing system, a procedure for reducing the latency comprising placement of the base APIs at one or more of the edge node, the regional node, and the central node.
8. The method of claim 5, further comprising triggering, by the processing system, an iterative procedure for reducing the latency comprising placement of the data objects and the base APIs at one or more of the edge node, the regional node, and the central node.
9. The method of claim 1, wherein the data is streamed from the edge node to the regional node for processing at the regional node.
10. The method of claim 1, wherein the received data is accessible to a third party from the edge node, the regional node, the central node or a combination thereof.
11. A device comprising: a processing system including a processor; and a memory that stores executable instructions that, when executed by the processing system, facilitate performance of operations comprising: receiving data at an edge node of a plurality of edge nodes of a network, the network further comprising a plurality of regional nodes and a plurality of central nodes; determining a latency criterion associated with an application for processing the data, the application utilizing an application programming interface (API), information regarding the API being stored in a database accessible to at least one of the edge node, a regional node of the plurality of regional nodes in communication with the edge node, and a central node of the plurality of central nodes in communication with the regional node; measuring a key performance indicator (KPI) of the network; monitoring a latency associated with the processing of the data by the application; determining whether the latency satisfies the latency criterion; and responsive to the latency satisfying the latency criterion, assigning resources to assist the application in reducing the latency associated with the processing of the data, the resources comprising computation resources, storage resources, network resources, or a combination thereof obtained from the central node, the regional node, the edge node, or any combination thereof.
12. The device of claim 11, wherein the operations further comprise: partitioning the API into a plurality of base APIs including a first base API and a second base API, wherein the first base API has associated therewith a first latency less than a second latency associated with the second base API; and assigning the edge node to the first base API and the regional node to the second base API.
13. The device of claim 11, wherein the operations further comprise predicting the latency based on the KPI.
14. The device of claim 11, wherein the API comprises a plurality of base APIs, and wherein the database comprises a list of data objects to be accessed via the respective base APIs.
15. The device of claim 14, wherein the operations further comprise triggering a procedure for reducing the latency comprising placement of the data objects at one or more of the edge node, the regional node, and the central node.
16. The device of claim 14, wherein the operations further comprise triggering a procedure for reducing the latency comprising placement of the base APIs at one or more of the edge node, the regional node, and the central node.
17. A machine-readable medium comprising executable instructions that, when executed by a processing system including a processor, facilitate performance of operations comprising: receiving data at an edge node of a plurality of edge nodes of a network, the network further comprising a plurality of regional nodes and a plurality of central nodes; determining a latency criterion associated with an application for processing the data, the application utilizing an application programming interface (API), information regarding the API being stored in a database accessible to at least one of the edge node, a regional node of the plurality of regional nodes in communication with the edge node and a central node of the plurality of central nodes in communication with the regional node, the API comprising a plurality of base APIs, the database comprising a list of data objects to be accessed via the respective base APIs; monitoring a latency associated with the processing of the data by the application; determining whether the latency satisfies the latency criterion; and responsive to the latency satisfying the latency criterion, assigning resources to assist the application in reducing the latency associated with the processing of the data, the resources comprising computation resources, storage resources, network resources, or a combination thereof obtained from the central node, the regional node, the edge node, or any combination thereof.
18. The machine-readable medium of claim 17, wherein the operations further comprise triggering a procedure for reducing the latency comprising placement of the data objects at one or more of the edge node, the regional node, and the central node.
19. The machine-readable medium of claim 17, wherein the operations further comprise triggering a procedure for reducing the latency comprising placement of the base APIs at one or more of the edge node, the regional node, and the central node.
20. The machine-readable medium of claim 17, wherein the operations further comprise triggering an iterative procedure for reducing the latency comprising placement of the data objects and the base APIs at one or more of the edge node, the regional node, and the central node.
</claims>
</document>
