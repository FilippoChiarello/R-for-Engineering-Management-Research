<document>

<filing_date>
2020-06-09
</filing_date>

<publication_date>
2020-12-24
</publication_date>

<priority_date>
2019-06-17
</priority_date>

<assignee>
DMAI
</assignee>

<inventors>
ZHANG, Wanyi
</inventors>

<docdb_family_id>
74037544
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR PERSONALIZED AND MULTIMODAL CONTEXT AWARE HUMAN MACHINE DIALOGUE
</title>

<abstract>
The present teaching relates to method, system, medium, and implementations for human machine dialogue. An utterance is received from a user engaged in the human machine dialogue on a topic in a dialogue scene. Multimodal surround information related to the human machine dialogue is obtained and analyzed to track multimodal context of the human machine dialogue. The operation for spoken language understanding of the utterance is conducted, in a context aware manner based on the tracked multimodal context, to determine semantics of the utterance.
</abstract>

<claims>
WE CLAIM:
1. A method implemented on at least one machine including at least one processor, memory, and communication platform capable of connecting to a network for human machine dialogue, comprising:
receiving an utterance from a user engaged in the human machine dialogue on a topic in a dialogue scene;
obtaining multimodal surround information related to the human machine dialogue; analyzing the multimodal surround information to track multimodal context of the human machine dialogue; and
personalizing spoken language understanding of the utterance, in a context aware manner based on the tracked multimodal context, to determine semantics of the utterance.
2. The method of claim 1, wherein the multimodal surround information includes acoustic and visual information.
3. The method of claim 1, wherein the multimodal context comprises at least one of objects in the dialogue scene and spatial relationships thereof;
at least one event related to the user occurred in the past and/or observed during the dialogue;
one or more acoustic/visual activities observed in the dialogue scene;
information about the user on known characteristics previously recorded and/or characteristics of the user observed in the dialogue; and
common sense knowledge.
4. The method of claim 3, wherein the characteristics of the user observed in the dialogue includes at least one of:
observation of the user on at least one of behavior, expression, and action; and inference of user's emotion and/or intent derived based on the observation.
5. The method of claim 1, wherein the step of personalizing spoken language understanding of the utterance comprises:
recognizing individual words spoken via the utterance by automatic speech recognition (ASR), wherein the ASR disambiguates a word uttered by the user based on characteristics of the user represented in the multimodal context; and
determining the semantics of the utterance through natural language understanding (NLU), wherein the NLU ascertains the semantics based on acoustic/visual activities observed in the dialogue scene and represented in the multimodal context.
6. The method of claim 1, further comprising
determining a response to the utterance based on a dialogue policy governing the human machine dialogue on the topic;
generating a personalized textual response based on the response in accordance with the multimodal context; and
generating a personalized acoustic response corresponding to the personalized textual response in a context aware manner based on the multimodal context.
7. The method of claim 6, wherein
the personalized textual response is selected from a parameterized content set associated with the response; and
the personalized textual response is selected
based on characteristics of the user, previously known and/or presently observed in the dialogue scene, represented in the multimodal context, and
in a context aware manner based on contextual information represented in the multimodal context.
8. The method of claim 6, wherein the personalized acoustic response is rendered in a manner that is personalized with respect to the user and context aware in accordance with contextual information represented in the multimodal context.
9. The method of claim 1, further comprising, when the dialogue corresponds to a tutoring session on the topic in accordance with a current tutoring plan,
evaluating, based on the semantics of the utterance, performance of the user with respect to one or more aspects of the current tutoring plan;
adjusting the current tutoring plan, based on the performance in a context aware manner in accordance with the multimodal context, to generate a personalized tutoring plan; and
applying the personalized tutoring plan in the dialogue to continue the tutoring session on the topic.
10. The method of claim 7, further comprising updating the dialogue policy based on the tracked multimodal context so that the dialogue policy is personalized and context aware.
11. A system for conducting a human machine dialogue, comprising:
a surround knowledge tracker configured for
obtaining multimodal surround information related to the human machine dialogue, and
analyzing the multimodal surround information to track multimodal context of the human machine dialogue; and
a spoken language understanding engine configured for
receiving an utterance from a user engaged in the human machine dialogue on a topic in a dialogue scene, and
personalizing spoken language understanding (SLU) of the utterance, in a context aware manner based on the tracked multimodal context, to determine semantics of the utterance.
12. The system of claim 11, wherein the multimodal surround information includes acoustic and visual information.
13. The system of claim 11, wherein the multimodal context comprises at least one of objects in the dialogue scene and spatial relationships thereof;
at least one event related to the user occurred in the past and/or observed during the dialogue;
one or more acoustic/visual activities observed in the dialogue scene; information about the user on known characteristics previously recorded and/or characteristics of the user observed in the dialogue; and
common sense knowledge.
14. The system of claim 13, wherein the characteristics of the user observed in the dialogue includes at least one of:
observation of the user on at least one of behavior, expression, and action; and inference of user's emotion and/or intent derived based on the observation.
15. The system of claim 11, wherein the spoken language understanding engine comprises:
an automatic speech recognition (ASR) engine configured for recognizing individual words spoken via the utterance, wherein the ASR engine disambiguates a word uttered by the user based on characteristics of the user represented in the multimodal context; and
a natural language understanding (NLU) engine configured for determining the semantics of the utterance, wherein the NLU engine ascertains the semantics based on acoustic/visual activities observed in the dialogue scene and represented in the multimodal context.
16. The system of claim 11, further comprising:
a dialogue manager configured for determining a response to the utterance based on a dialogue policy governing the human machine dialogue on the topic;
a natural language generation (NLG) engine configured for generating a personalized textual response based on the response in accordance with the multimodal context; and a text to speech (TTS) engine configured for generating a personalized acoustic response corresponding to the personalized textual response in a context aware manner based on the multimodal context.
17. The system 1 of claim 16, wherein
the personalized textual response is selected from a parameterized content set associated with the response, wherein the selection is made
based on characteristics of the user, previously known and/or presently observed in the dialogue scene, represented in the multimodal context, and
in a context aware manner based on contextual information represented in the multimodal context.
18. The system of claim 16, wherein the personalized acoustic response is rendered in a manner that is personalized with respect to the user and context aware in accordance with contextual information represented in the multimodal context.
19. The system of claim 11, further comprising, when the dialogue corresponds to a tutoring session on the topic in accordance with a current tutoring plan,
a grader unit configured for evaluating, based on the semantics of the utterance, performance of the user with respect to one or more aspects of the current tutoring plan;
a teaching plan adjustment unit configured for adjusting the current tutoring plan, based on the performance in a context aware manner in accordance with the multimodal context, to generate a personalized tutoring plan; and a teaching plan execution unit configured for applying the personalized tutoring plan in the dialogue to continue the tutoring session on the topic.
20. The system of claim 17, further comprising an agent mind update engine configured for updating the dialogue policy based on the tracked multimodal context so that the dialogue policy is personalized and context aware.
</claims>
</document>
