<document>

<filing_date>
2019-06-04
</filing_date>

<publication_date>
2020-05-22
</publication_date>

<priority_date>
2018-11-14
</priority_date>

<ipc_classes>
G10L15/02,G10L15/06,G10L15/065,G10L15/08,G10L15/12,G10L15/14,G10L15/16,G10L15/18
</ipc_classes>

<assignee>
TENCENT AMERICA
</assignee>

<inventors>
YU, DONG
WANG JUN
WENG, CHAO
WANG, GUANGSEN
SU, DAN
YU, CHENGZHU
CUI, JIA
</inventors>

<docdb_family_id>
70550658
</docdb_family_id>

<title>
N-BEST SOFTMAX SMOOTHING FOR MINIMUM BAYES RISK TRAINING OF ATTENTION BASED SEQUENCE-TO-SEQUENCE MODELS
</title>

<abstract>
A method and apparatus are provided that analyzing sequence-to-sequence data, such as sequence-to-sequence speech data or sequence-to-sequence machine translation data for example, by minimum Bayes risk (MBR) training a sequence-to-sequence model and within introduction of applications of softmax smoothing to an N-best generation of the MBR training of the sequence-to-sequence model.
</abstract>

<claims>
1. An apparatus comprising:
at least one memory configured to store computer program code;
at least one hardware processor configured to access said computer program code and operate as instructed by said computer program code, said computer program code including:
minimum Bayes risk (MBR) training code configured to cause said at least one hardware processor to train a sequence-to-sequence model; and
smoothing code configured to cause said at least one hardware processor to apply softmax smoothing to an N-best generation of the MBR training.
2. The apparatus according to claim 1, wherein the computer program code further includes beam search code configured to cause said at least one hardware processor to perform a beam search during the MBR training.
3. The apparatus according to claim 2, wherein the beam search code is further configured to, during each iteration of the beam search, apply the softmax smoothing to a label prediction distribution.
4. The apparatus according to claim 3, wherein the computer program code further comprises obtaining code configured to cause said at least one processor to obtain, as a result of applying the softmax smoothing, a plurality of hypothesized outputs applied to a hypothesis space for the MBR training.
5. The apparatus according to claim 1, wherein the MBR training code is configured to cause said at least one processor to apply an MBR loss operation to a plurality of pairs of training data and corresponding reference label sequences.
6. The apparatus according to claim 5, wherein the training data comprises training speech utterance data.
7. The apparatus according to claim 5, wherein the training data comprises training machine translation data.
8. The apparatus according to claim 5, wherein the MBR loss operation comprises a risk operation between a hypothesized label sequence and ones of the reference label sequences.
9. The apparatus according to claim 5, wherein the MBR loss operation comprises a sequence probability given the training data.
10. The apparatus according to claim 8, wherein the MBR training further comprises deriving gradients of the MBR loss operation with respect to a probability, of the sequence-tosequence model emitting a particular label of the label prediction distribution, and the risk operation.
11. A method performed by at least one computer processor comprising:
minimum Bayes risk (MBR) training a sequence-to-sequence model; and applying softmax smoothing to an N-best generation of the MBR training obtaining medical records.
12. The method according to claim 11, further comprising:
performing a beam search during the MBR training.
13. The method according to claim 12, further comprising:
during each step of the beam search, applying the softmax smoothing to a label prediction distribution.
14. The method according to claim 13, further comprising:
obtaining, as a result of applying the softmax smoothing, a plurality of hypothesized outputs applied to a hypothesis space for the MBR training.
15. The method according to claim 11, wherein the MBR training comprises applying an MBR loss operation to a plurality of pairs of training data and corresponding reference label sequences.
16. The method according to claim 15, wherein the training data comprises training speech utterance data.
17. The method according to claim 15, wherein the training data comprises training machine translation data.
18. The method according to claim 15, wherein the MBR loss operation comprises a risk operation between a hypothesized label sequence and ones of the reference label sequences.
19. The method according to claim 15, wherein the MBR loss operation comprises a sequence probability given the training data.
20. A non-transitory computer readable medium storing a program causing a computer to execute a process, the process comprising:
minimum Bayes risk (MBR) training a sequence-to-sequence model; and
applying softmax smoothing to an N-best generation of the MBR training obtaining medical records.
</claims>
</document>
