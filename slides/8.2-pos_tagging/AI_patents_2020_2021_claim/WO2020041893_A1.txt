<document>

<filing_date>
2019-08-30
</filing_date>

<publication_date>
2020-03-05
</publication_date>

<priority_date>
2018-09-01
</priority_date>

<ipc_classes>
A43D1/00,G06T1/40,G06T7/00,G06T7/10,G06T7/20
</ipc_classes>

<assignee>
DIGITAL ANIMAL INTERACTIVE
</assignee>

<inventors>
SMITH, WILLIAM RYAN
HENSON, MICHAEL
SHERRAH, JAMIE ROY
</inventors>

<docdb_family_id>
69643111
</docdb_family_id>

<title>
IMAGE PROCESSING METHODS AND SYSTEMS
</title>

<abstract>
A computer-implemented method performable with an imaging device comprises selecting a frame from a video feed output with the imaging device during a movement of the imaging device relative to a body part and detecting the body part in the frame. If the body part is detected, a first process is performed comprising: calculating an azimuth angle of the imaging device relative to the body part, calculating a metering region for the body part, and measuring a motion characteristic of the movement. The method also involves qualifying the frame based on at least one of the azimuth angle and the motion characteristic. If the frame is qualified, a second process is performed comprising: adjusting a setting of the imaging device based on the metering region, capturing an image of the body part with the imaging device based on the setting, identifying a location of the image relative to the body part based on the azimuth angle, and associating the image with a reference to the location.
</abstract>

<claims>
1. A computer-implemented method performable with an imaging device, the method comprising: selecting a frame from a video feed output with the imaging device during a movement of the imaging device relative to a body part; detecting the body part in the frame; performing, if the body part is detected, a first process comprising: calculating an azimuth angle of the imaging device relative to the body part; calculating a metering region for the body part; and measuring a motion characteristic of the movement; qualifying the frame based on at least one of the azimuth angle and the motion characteristic; and performing, if the frame is qualified, a second process comprising: adjusting a setting of the imaging device based on the metering region; capturing an image of the body part with the imaging device based on the setting; identifying a location of the image relative to the body part based on the azimuth angle; and associating the image with a reference to the location.
2. The method of claim 1 , wherein the body part pixels are identified with a machine learning process.
3. The method of claim 2, wherein the machine learning process comprises: inputting each frame to a deep convolutional neural network; applying, with the deep convolutional neural network, transforming feature layers to each image; and outputting, with the deep convolutional neural network, predictions for the body part in each image.
4. The method of claim 3, comprising outputting, with deep convolutional neural network, a confidence score for each image.
5. The method of claim 1 , wherein detecting the body part in the frame comprises: identifying body part pixels in the frame by: calculating a body part probability for each pixel of the frame by applying a hierarchy of known body part pixel characteristics to each pixel; and thresholding the calculated body part probabilities based on a predetermined value to generate a binary image comprising clusters of the body part pixels; and identifying body part features based on the body part pixels by: calculating a body part probability for each cluster of the body part pixels by applying a hierarchy of known body part features to each cluster; and detecting of the body part based on the body part probabilities
6. The method of any one of claims 2 to 5, comprising outputting first positioning instructions for locating the body part in the frame by guiding first additional movements of imaging device during the movement. 7. The method of claim 1 , wherein the first process is performed continuously when the video feed is being output with the imaging device.
8. The method of claim 1 , wherein calculating the azimuth angle comprises mapping the azimuth angle on the frame.
9. The method of claim 8, wherein calculating the azimuth angle comprises predicting the azimuth angle with a machine learning process.
10. The method of claim 1 , wherein calculating the azimuth angle comprises: calculating first predictions of the azimuth angle with a first prediction process; calculating second predictions of the camera azimuth angle with a second prediction process; and calculating the azimuth angle based on the first predictions and the second predictions.
11. The method of claims 10, wherein at least the first prediction process is based on a machine learning process.
12. The method of claim 11 , wherein the second prediction process is based on at least one of: an output from a measurement unit of the imaging device; and a simultaneous localization and mapping algorithm.
13. The method of any one of claims 10 to 12, wherein: the first predictions are generated at a first rate; the second predictions are generated at a second rate; and the first rate is different from the second rate.
14. The method of claim 13, comprising determining a confidence level of the azimuth angle based on one or more of the first estimates, the second estimates, and the combination thereof.
15. The method of claim 14, wherein the determining the confidence level comprises continuously analyzing the first estimates, the second estimates, or the combination thereof during the movement.
16. The method of claim 1 , wherein the metering region is calculated based on a machine learning process.
17. The method of claim 1 , wherein calculating the metering region comprises: generating a per-pixel body part probability for each pixel of the frame; thresholding the per-pixel body part probabilities to define a segmentation mask; and calculating the metering region based on the segmentation mask. 18. The method of claim 17, comprising: determining if the body part is centered in the frame; and outputting second positioning instructions for centering the body part in the frame by guiding second additional movements of the imaging device. 19. The method of claim 1 , wherein the motion characteristic comprises a movement speed of the imaging device relative to the body part.
20. The method of claim 19, wherein the movement speed is determined based on an output from a measurement unit of the imaging device.
21. The method of claim 20, comprising outputting third positioning instructions for modifying the movement speed by guiding third additional movements of the imaging device.
22 The method of claim 1 , wherein qualifying the frame comprises: determining if the azimuth angle is reliable based on a range of reliable azimuth angles; and determining if the motion characteristic is acceptable based on a range of acceptable motion characteristics.
23. The method of claim 22, further comprising outputting fourth position instructions restarting the movement by guiding fourth additional movements of the imaging device
24. The method of claim 1 , wherein imaging device comprises an optical camera, and adjusting the at least one setting of the imaging device comprises iteratively adjusting one of a focus, an exposure, and a gain of the optical camera. 25. The method of claim 1 , wherein identifying the location of the image relative to the body part comprises: locating a plurality of pose segments relative to the body part; and locating the image relative to one pose segment of the plurality of pose segments.
26. The method of claim 25, wherein associating the image with the reference to the location comprises associating the image with the one pose segment of the plurality of pose segments.
27. The method of claim 26, comprising: storing the image and the reference to the one pose segment as fit determination data; and returning to the selecting step until the fit determination data comprises at least one image stored with reference to each pose segment of the plurality of pose segments
28. The method of claim 26, comprising calculating a quality metric of the image.
29. The method of claim 28, comprising: storing the image, the reference to the one pose segment, and the quality metric as fit determination data; determining whether a previous image has been stored with reference to the one of the plurality of pose segments; comparing the quality metric of the image with a quality metric of the previous image; updating the fit determination data at the reference to comprise one of the image and its quality metric or the previous image and its quality metric; and returning to the selecting step until the fit determination data comprises at least one image stored with reference to each pose segment of the plurality of pose segments
30 The method of claim 27 or 29, comprising outputting fifth positioning instructions for moving the imaging device toward a different pose segment of the plurality of pose segments by guiding fifth additional movements of the imaging device. 31. The method of claim 27 or 29, comprising: generating fit determinations based on the fit determination data; making one or more recommendations based on the fit determinations; and communicating the fit determinations and the one or more recommendations to a user.
32. The method of claim 31 , wherein generating the fit determinations comprises outputting the fit determination data to a remote image processor with fit determination instructions.
33. The method of any preceding claim, wherein the first, second, third, fourth, and fifth positioning instructions comprise one or more of a visual signal, an audible signal, and a haptic signal output to guide the respective first, second, third, fourth, or fifth additional movements. 34. The method of claim 33, wherein the visual signal comprises: a dynamic display element responsive to the first, second, third, fourth, or fifth additional movements of the imaging device relative to the body part; and a fixed display element operable with the dynamic display element to guide the respective first, second, third, fourth, or fifth additional movements.
35. The method of claim 34, wherein the dynamic display element comprises a marker and the fixed display element comprises a target such that: moving the imaging device causes a corresponding movement of the marker relative to the target; and moving the marker to the target guides the respective first, second, third, fourth, or fifth additional movements.
36. The method of claim 35, wherein the marker comprises a representation of a ball and the target comprises a representation of a hole or track for the ball.
37. The method of claim 36, wherein the marker comprises a compass.
38. The method of claim 1 , comprising: outputting initial positioning instructions for starting the movement; and outputting subsequent positioning instructions for maintaining or restarting movement.
39. The method of claim 38, wherein the movement comprises a motion path extending at least partially around the body part. 40. The method of claim 39, wherein the motion path is segmented.
41. A computer-implemented method performable with an imaging device, the method comprising: selecting a frame from a video feed output with the imaging device during a movement of the imaging device relative to a body part; detecting, with a neural network, the body part in the frame; performing, if the body part is detected, a first process comprising: calculating, with the neural network, an azimuth angle of the imaging device relative to the body part; calculating, with the neural network, a metering region for the body part; and measuring a motion characteristic of the movement; qualifying the frame based on at least one of the azimuth angle and the motion characteristic; and performing, if the frame is qualified, a second process comprising: adjusting a setting of the imaging device based on the metering region; capturing an image of the body part with the imaging device based on the setting; identifying a location of the image relative to the body part based on the azimuth angle; and associating the image with a reference to the location.
42. A computer-implemented method performable with an imaging device, the method comprising: outputting positioning instructions for guiding a movement of an imaging device relative to a body part; initiating a video feed with the imaging device during the movement; selecting a frame from the video feed during the movement; detecting the body part in the frame; performing, if the body part is detected, a first process comprising: calculating an azimuth angle of the imaging device relative to the body part; calculating a metering region for the body part; and measuring a motion characteristic of the movement; qualifying the frame based on at least one of the azimuth angle and the motion characteristic; and performing, if the frame is qualified, a second process comprising: adjusting a setting of the imaging device based on the metering region; capturing an image of the body part with the imaging device based on the setting; identifying a location of the image relative to the body part based on the azimuth angle; and associating the image with a reference to the location.
43. The method of claim 42, wherein the positioning instructions guide the movement between different viewpoints of the body part, each different viewpoint having a different azimuth angle.
44. The method of claim 42, wherein the movement comprises a continuous motion extending in a random path about the body part.
45. The method of claim 42, wherein the movement comprises a continuous sweeping motion extending in a circular path around the body part.
46. The method of claim 42, wherein the movement comprises discrete motions extending between each viewpoint.
47. The method of claim 42, wherein the positioning instructions are output continuously during the movement.
48. The method of claim 42, wherein the positioning instructions comprises at least one of: visual signals output with a display source of the imaging device; audio signals output with a sound generator of the imaging device; and haptic signals output with a haptic communicator of the image device.
49. The method of claim 42, wherein the positioning instructions comprise; a dynamic display element output with the display source responsive to the inertial measurement unit; and a fixed display element output with the display source and operable with the dynamic display element to guide compensatory movements of the imaging device relative to the body part. 50. The method of claim 49, wherein the dynamic display element comprises a marker and the fixed display element comprises a target such that: moving the imaging device relative to the body part causes corresponding movements of the marker relative to the target; and moving the marker to the target guides additional movements of the imaging device toward positions relative to the body part.
51. The method of claim 50, wherein the marker comprises a representation of a ball and the target comprises a representation of a hole or track for the ball.
52. The method of claim 50, wherein the marker comprises a rotating compass.
53. The method of claim 42, wherein the positioning instructions are responsive to the movement.
54. The method of claim 42, wherein identifying the location of the image relative to the body part comprises: locating a plurality of pose segments relative to the body part, the plurality of pose segments comprising occupied segments and unoccupied segments; locating the image at one of the unoccupied segments; and storing the image in the memory with a reference to the one of unoccupied segments.
55. The method of claim 54, wherein the positioning instructions comprise an augmented reality element overlaid onto the video feed to provide a graphical representation of the plurality of pose segments.
56. The method of claim 55, wherein the positioning instructions guide movements relative to occupied and unoccupied segments of the plurality of pose segments. 57. The method of claim 56, comprising repeating the method until at least one image has been stored in the memory with reference to each of the unoccupied segments.
58. The method of claim 42, wherein measuring the motion characteristic comprises measuring a movement speed of the imaging device and the positioning instructions guide additional movements for modifying the movement speed of the imaging device.
59. The method of claim 58, wherein the positioning instructions are responsive to the additional movements.
60. The method of any one of claims 42 to 60, wherein the positioning instructions consist of non-visual signals. AMENDED CLAIMS
received by the International Bureau on
11 January 2020 (11.01.2020)
CLAIMS
1. A computer-implemented method performable with an imaging device, the method comprising: selecting a frame from a video feed output with the imaging device during a movement of the imaging device relative to a body part; detecting the body part in the frame with a first neural network trained to apply transforming feature layers to the frame and output predictions for the body part based a layer of the transforming feature layers; performing, if the body part is detected, a first process comprising: calculating an azimuth angle of the imaging device relative to the body part with a second neural network trained to map pose segments onto the frame and output predictions for the azimuth angle based a pose segment of the pose segments; calculating a metering region for the body part with a third neural network trained to define a segmentation mask for the body part in the frame and output predictions for the metering region based on the segmentation mask; and measuring a motion characteristic of the movement; qualifying the frame based on at least one of the azimuth angle and the motion characteristic; and performing, if the frame is qualified, a second process comprising: adjusting a setting of the imaging device based on the metering region; capturing an image of the body part with the imaging device based on the setting; identifying a location of the image relative to the body part based on the azimuth angle; and associating the image with a reference to the location.
2. The method of claim 1 , wherein the first neural network comprises a deep convolutional neural network and detecting the body part comprises: inputting the frame to the deep convolutional neural network; applying, with the deep convolutional neural network, the transforming feature layers to the frame; and outputting, with the deep convolutional neural network, the predictions for the body part in the frame.
3. The method of claim 2, comprising outputting, with deep convolutional neural network, confidence scores for the predictions for the body part. 4. The method of claim 1 , wherein detecting the body part in the frame comprises: calculating body part probabilities for the frame by applying a hierarchy of known body part pixel characteristics to each pixel of the frame; thresholding the calculated body part probabilities based on a predetermined value to generate a binary image comprising clusters of pixels; calculating body part probabilities for the binary image by applying a hierarchy of known body part features to each cluster of pixels; and detecting the body part in the frame based on the body part probabilities for the binary image.
5. The method of any one of claims 1 to 4, comprising outputting first positioning instructions for locating the body part in a subsequent frame of the video feed by guiding first additional movements of imaging device during the movement.
6. The method of claim 1 , wherein the first process is performed continuously when the video feed is being output with the imaging device.
7. The method of claim 1 , wherein calculating the azimuth angle comprises: calculating first predictions of the azimuth angle with a first prediction process utilizing the second neural network; calculating second predictions of the azimuth angle with a second prediction process that is different from the first prediction process; and calculating the azimuth angle based on the first predictions and the second predictions.
8. The method of claim 7, wherein the second prediction process is based on at least one of: an output from a measurement unit of the imaging device; and a simultaneous localization and mapping algorithm.
9. The method of any one of claims 7 to 8, wherein: the first predictions are generated at a first rate; the second predictions are generated at a second rate; and the first rate is different from the second rate.
10. The method of claim 9, comprising determining a confidence level of the azimuth angle based on one or more of the first estimates, the second estimates, and a combination of the first and second estimates.
11. The method of claim 10, wherein the determining the confidence level comprises continuously analyzing one or more of the first estimates, the second estimates, and the combination of the first and second estimates during the movement.
12. The method of claim 1 , wherein calculating the metering region comprises: generating a per-pixel body part probability for each pixel of the frame; thresholding the per-pixel body part probabilities to define the segmentation mask; and calculating the metering region by iteratively fitting a shape the segmentation mask outputting measurements based on the shape.
13. The method of claim 12, comprising: determining if the body part is centered in the frame; and outputting second positioning instructions for centering the body part in a subsequent frame of the video feed by guiding second additional movements of the imaging device.
14. The method of claim 1 , wherein the motion characteristic comprises a movement speed of the imaging device relative to the body part.
15. The method of claim 14, wherein the movement speed is determined based on an output from a measurement unit of the imaging device.
16. The method of claim 15, comprising outputting third positioning instructions for modifying the movement speed by guiding third additional movements of the imaging device.
17. The method of claim 1 , wherein qualifying the frame comprises: determining if the azimuth angle is within a range of reliable azimuth angles; and determining if the motion characteristic is within a range of acceptable motion characteristics.
18. The method of claim 17, further comprising outputting fourth position instructions for restarting the movement by guiding fourth additional movements of the imaging device.
19. The method of claim 1 , wherein the imaging device comprises an optical camera, and adjusting the at least one setting of the imaging device comprises iteratively adjusting one of a focus, an exposure, and a gain of the optical camera.
20. The method of claim 1 , wherein identifying the location of the image relative to the body part comprises: locating the pose segments relative to the body part; and locating the image relative to one pose segment of the pose segments.
21. The method of claim 20, wherein associating the image with the reference to the location comprises associating the image with the one pose segment of the pose segments.
22. The method of claim 21 , comprising: storing the image and the reference to the one pose segment as fit determination data; and returning to the selecting step until the fit determination data comprises at least one image stored with reference to each pose segment of the pose segments.
23. The method of claim 22, comprising calculating a quality metric of the image.
24. The method of claim 23, comprising: storing the image, the reference to the one pose segment, and the quality metric as the fit determination data; determining whether a previous image has been stored with the reference to the one pose segment; if the previous image has been stored with the reference to the one pose segment, comparing the quality metric of the image with a quality metric of the previous image; updating the fit determination data stored with the reference to the one pose segment to comprise one of the image and its quality metric or the previous image and its quality metric; and returning to the selecting step until the fit determination data comprises at least one image stored with reference to each pose segment of the pose segments.
25. The method of claim 22 or 24, comprising outputting fifth positioning instructions for moving the imaging device toward a different pose segment of the pose segments by guiding fifth additional movements of the imaging device.
26. The method of claim 22 or 24, comprising: generating fit determinations based on the fit determination data; making one or more recommendations based on the fit determinations; and communicating the fit determinations and the one or more recommendations to a user.
27. The method of claim 26, wherein generating the fit determinations comprises outputting the fit determination data to a remote image processor with fit determination instructions.
28. The method of any one of claims 1 to 27, wherein the first, second, third, fourth, and fifth positioning instructions comprise one or more of a visual signal, an audible signal, and a haptic signal output to guide the respective first, second, third, fourth, or fifth additional movements.
29. The method of claim 28, wherein the visual signal comprises: a dynamic display element responsive to the first, second, third, fourth, or fifth additional movements of the imaging device relative to the body part; and a fixed display element operable with the dynamic display element to guide the respective first, second, third, fourth, or fifth additional movements. 30. The method of claim 29, wherein the dynamic display element comprises a marker and the fixed display element comprises a target such that: moving the imaging device causes a corresponding movement of the marker relative to the target; and moving the marker to the target guides the respective first, second, third, fourth, or fifth additional movements.
31. The method of claim 30, wherein the marker comprises a representation of a ball and the target comprises a representation of a hole or track for the ball.
32. The method of claim 31 , wherein the marker comprises a compass.
33. The method of claim 1 , comprising: outputting initial positioning instructions for starting the movement; and outputting subsequent positioning instructions for maintaining or restarting movement.
34. The method of claim 33, wherein the movement comprises a motion path extending at least partially around the body part.
35. The method of claim 34, wherein the motion path is segmented.
36. A computer-implemented method performable with an imaging device, the method comprising: selecting a frame from a video feed output with the imaging device during a movement of the imaging device relative to a body part; detecting, with a neural network, the body part in the frame by applying transforming feature layers to the frame and outputting predictions for the body part based on a layer of the transforming feature layers; performing, if the body part is detected, a first process comprising: calculating, with the neural network, an azimuth angle of the imaging device relative to the body part by mapping pose segments onto the frame and outputting predictions for the azimuth angle based on a pose segment of the pose segments; calculating, with the neural network, a metering region for the body part by defining a segmentation mask for the body part in the frame and outputting predictions for the metering region based on the segmentation mask; and measuring a motion characteristic of the movement; qualifying the frame based on at least one of the azimuth angle and the motion characteristic; and performing, if the frame is qualified, a second process comprising: adjusting a setting of the imaging device based on the metering region; capturing an image of the body part with the imaging device based on the setting; identifying a location of the image relative to the body part based on the azimuth angle; and associating the image with a reference to the location.
37. A computer-implemented method performable with an imaging device, the method comprising: outputting positioning instructions for guiding a movement of an imaging device relative to a body part; initiating a video feed with the imaging device during the movement; selecting a frame from the video feed during the movement; detecting the body part in the frame with a first neural network trained to apply transforming feature layers to the frame and output predictions for the body part based a layer of the transforming feature layers; performing, if the body part is detected, a first process comprising: calculating an azimuth angle of the imaging device relative to the body part with a second neural network trained to map pose segments onto the frame and output predictions for the azimuth angle based a pose segment of the pose segments; calculating a metering region for the body part with a third neural network trained to define a segmentation mask for the body part in the frame and output predictions for the metering region based on the segmentation mask; and measuring a motion characteristic of the movement; qualifying the frame based on at least one of the azimuth angle and the motion characteristic; and performing, if the frame is qualified, a second process comprising: adjusting a setting of the imaging device based on the metering region; capturing an image of the body part with the imaging device based on the setting; identifying a location of the image relative to the body part based on the azimuth angle; and associating the image with a reference to the location.
38. The method of claim 37, wherein the positioning instructions guide the movement between different viewpoints of the body part, each different viewpoint having a different azimuth angle.
39. The method of claim 37, wherein the movement comprises a continuous motion extending in a random path about the body part.
40. The method of claim 37, wherein the movement comprises a continuous sweeping motion extending in a circular path around the body part.
41. The method of claim 37, wherein the movement comprises discrete motions extending between each viewpoint.
42. The method of claim 37, wherein the positioning instructions are output continuously during the movement.
43. The method of claim 37, wherein the positioning instructions comprises at least one of: visual signals output with a display source of the imaging device; audio signals output with a sound generator of the imaging device; and haptic signals output with a haptic communicator of the imaging device. 44. The method of claim 37, wherein the positioning instructions comprise: a dynamic display element output with the display source responsive to a measurement unit of the imaging device; and a fixed display element output with the display source and operable with the dynamic display element to guide compensatory movements of the imaging device relative to the body part.
45. The method of claim 44, wherein the dynamic display element comprises a marker and the fixed display element comprises a target such that: moving the imaging device relative to the body part causes corresponding movements of the marker relative to the target; and moving the marker to the target guides additional movements of the imaging device toward positions relative to the body part.
46. The method of claim 45, wherein the marker comprises a representation of a ball and the target comprises a representation of a hole or track for the ball.
47. The method of claim 45, wherein the marker comprises a rotating compass.
48. The method of claim 37, wherein the positioning instructions are responsive to the movement.
49. The method of claim 37, wherein identifying the location of the image relative to the body part comprises: locating the pose segments relative to the body part, the pose segments comprising occupied segments and unoccupied segments; locating the image at one of the unoccupied segments; and storing the image in the memory with a reference to the one of unoccupied segments. 50. The method of claim 49, wherein the positioning instructions comprise an augmented reality element overlaid onto the video feed to provide a graphical representation of the pose segments.
51. The method of claim 50, wherein the positioning instructions guide movements of the imaging device relative to one of the occupied segments and the unoccupied segments.
52. The method of claim 51 , comprising repeating the method until at least one at least one image has been stored in the memory with reference to each unoccupied segment of the pose segments.
53. The method of claim 37, wherein measuring the motion characteristic comprises measuring a movement speed of the imaging device and the positioning instructions guide additional movements for modifying the movement speed of the imaging device.
54. The method of claim 53, wherein the positioning instructions are responsive to the additional movements.
55. The method of any one of claims 37 to 59, wherein the positioning instructions consist of non-visual signals.
</claims>
</document>
