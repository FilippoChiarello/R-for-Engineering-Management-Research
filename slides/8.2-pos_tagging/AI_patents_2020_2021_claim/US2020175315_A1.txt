<document>

<filing_date>
2019-11-27
</filing_date>

<publication_date>
2020-06-04
</publication_date>

<priority_date>
2018-11-30
</priority_date>

<ipc_classes>
G01S13/86,G01S13/931,G01S7/41,G05D1/00,G05D1/02,G06K9/00,G06K9/46,G06K9/62,G06T7/60
</ipc_classes>

<assignee>
QUALCOMM
</assignee>

<inventors>
ANSARI, AMIN
GOWAIKAR, RADHIKA DILIP
WU, XINZHOU
SUBRAMANIAN, SUNDAR
SUKHAVASI, RAVI TEJA
MAJOR, BENCE
FONTIJNE, DANIEL HENDRICUS FRANCISCUS
LIM, TECK YIAN
</inventors>

<docdb_family_id>
68966081
</docdb_family_id>

<title>
EARLY FUSION OF CAMERA AND RADAR FRAMES
</title>

<abstract>
Disclosed are techniques for fusing camera and radar frames to perform object detection in one or more spatial domains. In an aspect, an on-board computer of a host vehicle receives, from a camera sensor of the host vehicle, a plurality of camera frames, receives, from a radar sensor of the host vehicle, a plurality of radar frames, performs a camera feature extraction process on a first camera frame of the plurality of camera frames to generate a first camera feature map, performs a radar feature extraction process on a first radar frame of the plurality of radar frames to generate a first radar feature map, converts the first camera feature map and/or the first radar feature map to a common spatial domain, and concatenates the first radar feature map and the first camera feature map to generate a first concatenated feature map in the common spatial domain.
</abstract>

<claims>
1. A method of fusing camera and radar frames to perform object detection in one or more spatial domains performed by an on-board computer of a host vehicle, comprising: receiving, from a camera sensor of the host vehicle, a plurality of camera frames; receiving, from a radar sensor of the host vehicle, a plurality of radar frames; performing a camera feature extraction process on a first camera frame of the plurality of camera frames to generate a first camera feature map; performing a radar feature extraction process on a first radar frame of the plurality of radar frames to generate a first radar feature map; converting the first camera feature map and/or the first radar feature map to a common spatial domain; concatenating the first radar feature map and the first camera feature map to generate a first concatenated feature map in the common spatial domain; and detecting one or more objects in the first concatenated feature map.
2. The method of claim 1, wherein the common spatial domain is a spatial domain of the radar sensor.
3. The method of claim 1, wherein converting the first camera feature map to the converted first camera feature map comprises performing an explicit inverse perspective mapping transformation on the first camera feature map.
4. The method of claim 1, wherein converting the first camera feature map to the converted first camera feature map occurs before or during performing the camera feature extraction process.
5. The method of claim 1, further comprising: hashing a plurality of blocks of the first camera frame to identify one or more blocks that have not changed between a previous camera frame of the plurality of camera frames and the first camera frame; and copying feature map values of a second camera feature map of the previous camera frame to corresponding feature map values of the first feature map.
6. The method of claim 1, further comprising: estimating a width and length of the one or more objects based on a bounding box in the first camera frame encapsulating each of the one or more objects.
7. The method of claim 6, wherein the width and/or length of the one or more objects is estimated based at least in part on a make and/or model of the one or more objects.
8. The method of claim 1, further comprising: performing the camera feature extraction process on a second camera frame of the plurality of camera frames to generate a second camera feature map; performing the radar feature extraction process on a second radar frame of the plurality of radar frames to generate a second radar feature map; converting the second camera feature map and/or the second radar feature map to a common spatial domain; and concatenating the converted second radar feature map and the converted second camera feature map to generate a second concatenated feature map, wherein detecting the one or more objects is further based on the second concatenated feature map.
9. The method of claim 1, wherein the radar sensor and the camera sensor are collocated in the host vehicle.
10. The method of claim 1, further comprising: performing an autonomous driving operation based on detecting the one or more objects.
11. The method of claim 10, wherein the autonomous driving operation is one or more of braking, accelerating, steering, adjusting a cruise control setting, or signaling.
12. A method of fusing camera and radar frames to perform object detection in one or more spatial domains performed by an on-board computer of a host vehicle, comprising: receiving, from a camera sensor of the host vehicle, a plurality of camera frames; receiving, from a radar sensor of the host vehicle, a plurality of radar frames; applying an encoder-decoder network on the first camera frame to generate a first camera feature map in a spatial domain of the first radar frame; combining the first radar frame and the first camera feature map to generate a first combined feature map in the spatial domain of the first radar frame; and detecting one or more objects in the first combined feature map.
13. The method of claim 12, further comprising: providing the first combined feature map to a neural network.
14. The method of claim 12, further comprising: performing an autonomous driving operation based on detecting the one or more objects.
15. The method of claim 14, wherein the autonomous driving operation is one or more of braking, accelerating, steering, adjusting a cruise control setting, or signaling.
16. An on-board computer of a host vehicle, comprising: at least one processor configured to: receive, from a camera sensor of the host vehicle, a plurality of camera frames; receive, from a radar sensor of the host vehicle, a plurality of radar frames; perform a camera feature extraction process on a first camera frame of the plurality of camera frames to generate a first camera feature map; perform a radar feature extraction process on a first radar frame of the plurality of radar frames to generate a first radar feature map; convert the first camera feature map and/or the first radar feature map to a common spatial domain; concatenate the first radar feature map and the first camera feature map to generate a first concatenated feature map in the common spatial domain; and detect one or more objects in the first concatenated feature map.
17. The on-board computer of claim 16, wherein the common spatial domain is a spatial domain of the radar sensor.
18. The on-board computer of claim 16, wherein the at least one processor being configured to convert the first camera feature map to the converted first camera feature map comprises the at least one processor being configured to perform an explicit inverse perspective mapping transformation on the first camera feature map.
19. The on-board computer of claim 16, wherein the at least one processor is configured to convert the first camera feature map to the converted first camera feature map before or during performance of the camera feature extraction process.
20. The on-board computer of claim 16, wherein the at least one processor is further configured to: hash a plurality of blocks of the first camera frame to identify one or more blocks that have not changed between a previous camera frame of the plurality of camera frames and the first camera frame; and copy feature map values of a second camera feature map of the previous camera frame to corresponding feature map values of the first feature map.
21. The on-board computer of claim 16, wherein the at least one processor is further configured to: estimate a width and length of the one or more objects based on a bounding box in the first camera frame encapsulating each of the one or more objects.
22. The on-board computer of claim 21, wherein the width and/or length of the one or more objects is estimated based at least in part on a make and/or model of the one or more objects.
23. The on-board computer of claim 16, wherein the at least one processor is further configured to: perform the camera feature extraction process on a second camera frame of the plurality of camera frames to generate a second camera feature map; perform the radar feature extraction process on a second radar frame of the plurality of radar frames to generate a second radar feature map; convert the second camera feature map and/or the second radar feature map to a common spatial domain; and concatenate the converted second radar feature map and the converted second camera feature map to generate a second concatenated feature map, wherein detection of the one or more objects is further based on the second concatenated feature map.
24. The on-board computer of claim 16, wherein the radar sensor and the camera sensor are collocated in the host vehicle.
25. The on-board computer of claim 16, wherein the at least one processor is further configured to: perform an autonomous driving operation based on detecting the one or more objects.
26. The on-board computer of claim 25, wherein the autonomous driving operation is one or more of braking, accelerating, steering, adjusting a cruise control setting, or signaling.
27. An on-board computer of a host vehicle, comprising: at least one processor configured to: receive, from a camera sensor of the host vehicle, a plurality of camera frames; receive, from a radar sensor of the host vehicle, a plurality of radar frames; apply an encoder-decoder network on the first camera frame to generate a first camera feature map in a spatial domain of the first radar frame; combine the first radar frame and the first camera feature map to generate a first combined feature map in the spatial domain of the first radar frame; and detect one or more objects in the first combined feature map.
28. The on-board computer of claim 27, wherein the at least one processor is further configured to: provide the first combined feature map to a neural network.
29. The on-board computer of claim 27, further comprising: perform an autonomous driving operation based on detecting the one or more objects.
30. The on-board computer of claim 27, wherein the autonomous driving operation is one or more of braking, accelerating, steering, adjusting a cruise control setting, or signaling.
</claims>
</document>
