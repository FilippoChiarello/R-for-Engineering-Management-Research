<document>

<filing_date>
2019-06-14
</filing_date>

<publication_date>
2020-12-17
</publication_date>

<priority_date>
2019-06-14
</priority_date>

<ipc_classes>
G06F17/15,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
PETROV, ALEKSANDR E.
THOMAS, JOHN J.
ALLARD, MAXIME
WANG, WANTING
</inventors>

<docdb_family_id>
73735860
</docdb_family_id>

<title>
Classification Of Sparsely Labeled Text Documents While Preserving Semantics
</title>

<abstract>
A method of training a neural network includes receiving a text corpus containing a labeled portion and an unlabeled portion, extracting local n-gram features and a sequence of the local n-gram features from the text corpus, processing the text corpus, using convolutional layers, according to the local n-gram features to determine capsule parameters of capsules configured to preserve the sequence of the local n-gram features, performing a forward-oriented dynamic routing between the capsules using the capsule parameters to extract global characteristics of the text corpus, and processing the text corpus according to the global characteristics using a long short-term memory layer to extract global sequential text dependencies from the text corpus, wherein parameters of the neural network are updated according to the local n-gram features, the capsule parameters, global characteristics, and global sequential text dependencies.
</abstract>

<claims>
1. A method of training a neural network to classify sparsely labeled text documents while preserving semantics comprising: receiving a text corpus containing a labeled portion and an unlabeled portion exceeding the labeled portion; extracting a plurality of local n-gram features and a sequence of the local n-gram features from the text corpus; processing the text corpus, using a plurality of convolutional layers, according to the local n-gram features to determine capsule parameters of a plurality of capsules configured to preserve the sequence of the local n-gram features; performing a forward-oriented dynamic routing between the plurality of capsules using the capsule parameters to extract a plurality of global characteristics of the text corpus; and processing the text corpus according to the global characteristics using a long short-term memory layer to extract a plurality of global sequential text dependencies from the text corpus, wherein a plurality of parameters of the neural network are updated according to the local n-gram features, the capsule parameters, global characteristics, and global sequential text dependencies.
2. The method of claim 1, further comprising: freezing the parameters of the neural network; and applying the neural network to a newly input text to determine a label according to the parameters of the neural network.
3. The method of claim 1, wherein the forward-oriented dynamic routing has a first layer of the capsules and a second layer of the capsules, and an input of each nth capsule in the second layer is received from outputs of the (1 to nth) capsules in the first layer.
4. The method of claim 1, further comprising applying a virtual adversarial training to the neural network to generalize the capsules.
5. The method of claim 4, wherein the virtual adversarial training inputs a plurality of perturbed text data into the neural network, wherein the neural network adapts the capsules to the perturbed text data.
6. The method of claim 1, wherein the neural network comprises a plurality of layers composed of sets convolutional filters, a set of the capsules within the convolutional filters, and the long short-term memory layer.
7. A neural network system executing on a computer system comprising: a first plurality of processing elements configured to extract a plurality of local features and a sequence of the local features from a text corpus, wherein the text corpus is sparsely labeled; a second plurality of processing elements configured to extract a plurality of global characteristics of a text corpus, wherein the processing elements are structured as capsules configured to preserve the sequence of the local features; and a third plurality of processing elements configured to extract a plurality of global sequential text dependencies from the text corpus given an output of the first plurality of processing elements, wherein the third plurality of processing elements are structured as a long short-term memory layer, which is configured to output a probability distribution over all labels generated by the first and second plurality of processing elements.
8. The neural network system of claim 7, wherein the first processing elements and the second processing elements are different configurations using the same output of a set of neurons.
9. The neural network system of claim 7, disposed on a computer network to receive input data and output labeled data.
10. A non-transitory, computer-readable storage medium embodying computer program code, the computer program code comprising computer executable instructions configured for training a neural network to classify sparsely labeled text documents while preserving semantics comprising: receiving a text corpus containing a labeled portion and an unlabeled portion exceeding the labeled portion; extracting a plurality of local n-gram features and a sequence of the local n-gram features from the text corpus; processing the text corpus, using a plurality of convolutional layers, according to the local n-gram features to determine capsule parameters of a plurality of capsules configured to preserve the sequence of the local n-gram features; performing a forward-oriented dynamic routing between the plurality of capsules using the capsule parameters to extract a plurality of global characteristics of the text corpus; and processing the text corpus according to the global characteristics using a long short-term memory layer to extract a plurality of global sequential text dependencies from the text corpus, wherein a plurality of parameters of the neural network are updated according to the local n-gram features, the capsule parameters, global characteristics, and global sequential text dependencies.
11. The non-transitory, computer-readable storage medium of claim 10, further comprising: freezing the parameters of the neural network; and applying the neural network to a newly input text to determine a label according to the parameters of the neural network.
12. The non-transitory, computer-readable storage medium of claim 10, wherein the forward-oriented dynamic routing has a first layer of the capsules and a second layer of the capsules, and an input of each nth capsule in the second layer is received from outputs of the (1 to nth) capsules in the first layer.
13. The non-transitory, computer-readable storage medium of claim 10, further comprising applying a virtual adversarial training to the neural network to generalize the capsules.
14. The non-transitory, computer-readable storage medium of claim 13, wherein the virtual adversarial training inputs a plurality of perturbed text data into the neural network, wherein the neural network adapts the capsules to the perturbed text data.
15. The non-transitory, computer-readable storage medium of claim 10, wherein the neural network comprises a plurality of layers composed of sets convolutional filters, a set of the capsules within the convolutional filters, and the long short-term memory layer.
</claims>
</document>
