<document>

<filing_date>
2019-11-12
</filing_date>

<publication_date>
2020-03-26
</publication_date>

<priority_date>
2017-11-03
</priority_date>

<ipc_classes>
G06K9/62,G06N3/04,G06N3/08,G06T7/00
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
NAKANO HIROKI
SAKAMOTO MASAHARU
SEKIYAMA, TARO
ZHAO, KUN
</inventors>

<docdb_family_id>
66327407
</docdb_family_id>

<title>
WEIGHTED CASCADING CONVOLUTIONAL NEURAL NETWORKS
</title>

<abstract>
A cascading convolutional neural network (CCNN) comprising a plurality of convolutional neural networks (CNNs) that are trained by weighting training data based on loss values of each training datum between CNNs of the CCN. The CCNN can receiving an input image from plurality of images, classify the input image using the CCNN, and present a classification of the input image.
</abstract>

<claims>
1. A method for classifying an image comprising: training a cascading convolutional neural network (CCNN) comprising a plurality of convolutional neural networks (CCNs), wherein the training comprises: receiving a plurality of images, wherein a first portion of the plurality of images is associated with a first classification and a second portion of the plurality of images is associated with a second classification, wherein the first portion is greater than the second portion; training a first CNN of the CCNN using the plurality of images; weighting data of the plurality of images based on a loss function associated with respective data of the plurality of images; training at least one subsequent CNN of the CCNN based on the weighted data; receiving an input image; classifying the input image using the CCNN; and presenting a classification of the input image to a user interface.
2. The method of claim 1, wherein at least one subsequent CNN of the CCNN is trained based on a first subset of the first portion of the plurality of images and a second subset of the second portion of the plurality of images, wherein the first subset of the first portion is greater than the second subset of the second portion, wherein a sum of the first subset and the second subset is less than the entirety of the plurality of images.
3. The method of claim 2, wherein training a CCNN further comprises: removing images from the plurality of images to create a second plurality of images comprising the first subset of the first portion and the second subset of the second portion, wherein at least one subsequent CNN is based on the second plurality of images.
4. The method of claim 1, wherein training a first CNN using the plurality of images further comprises: convoluting a receptive field of at least one image; establishing an activation map; applying an activation function to the activation map; max-pooling the activation map; computing weights for a next training stage; and determining at least one loss value.
5. The method of claim 1, wherein the plurality of images comprises computed tomography (CT) images, wherein the first portion of the plurality of images comprises CT images indicating no nodules, and wherein the second portion of the plurality of images comprises CT images indicating at least one nodule.
6. The method of claim 1, wherein weighting data is based on an output for a datum from at least one previous CNN and a predicted output for the datum from a current CNN.
7. The method according to claim 6, wherein weighting respective data is further based on an absolute value of the difference between the output for the respective data from the at least one previous CNN and a predicted output for the respective datum from a current CNN.
8. A system for classifying an image comprising: a computer readable storage medium storing a corpus of data; and a processor communicatively coupled to the computer readable storage medium and the user interface and having a memory comprising instructions which, when executed by the processor, cause the processor to: train a cascading convolutional neural network (CCNN) comprising a plurality of convolutional neural networks (CCNs), wherein the training comprises: receiving a plurality of images, wherein a first portion on the plurality of images are associated with a first classification, wherein a second portion of the plurality of images are associated with a second classification, wherein the first portion is greater than the second portion; training a first CNN of the CCNN using the plurality of images; weighting data of the plurality of images based on a loss function associated with respective data of the plurality of images; training at least one subsequent CNN of the CCNN based on the weighted data; receive an input image; classify the input image using the CCNN; and present a classification of the input image to a user interface.
9. The system of claim 8, wherein at least one subsequent CNN of the CCNN is trained based on a first subset of the first portion of the plurality of images and a second subset of the second portion of the plurality of images, wherein the first subset of the first portion is greater than the second subset of the second portion, wherein the sum of the first subset and second subset is less than the entirety of the plurality of images.
10. The system of claim 9, wherein training a CCNN further comprises: removing images from the plurality of images to create a second plurality of images comprising the first subset of the first portion and the second subset of the second portion, wherein at least one subsequent CNN is based on the second plurality of images.
11. The system of claim 8, wherein training a first CNN using the plurality of images further comprises: convoluting a receptive field of at least one image; establishing an activation map; applying an activation function to the activation map; max-pooling the activation map; computing weights for a next training stage; and determining at least one loss value.
12. The system of claim 8, wherein the plurality of images comprises computed tomography (CT) images, wherein the first portion of the plurality of images comprise CT images indicating no nodules, wherein the second portion of the plurality of images comprise CT images indicating at least one nodule.
13. The system of claim 8, wherein weighting data is based on an output for a datum from at least one previous CNN and a predicted output for the datum from a current CNN.
14. The system according to claim 12, wherein weighting data is further based on an absolute value of the difference between the output for the respective data from the at least one previous CNN and a predicted output for the respective datum from a current CNN.
15. A computer program product for classifying an image comprising a computer readable storage medium having program instructions embodied therewith, the program instructions executable by a processor to cause the processor to: train a cascading convolutional neural network (CCNN) comprising a plurality of convolutional neural networks (CCNs), wherein the training comprises: receiving a plurality of images, wherein a first portion on the plurality of images are associated with a first classification, wherein a second portion of the plurality of images are associated with a second classification, wherein the first portion is greater than the second portion; training a first CNN of the CCNN using the plurality of images; weighting data of the plurality of images based on a loss function associated with respective data of the plurality of images; training at least one subsequent CNN of the CCNN based on the weighted data; receive an input image; classify the input image using the CCNN; and present a classification of the input image to a user interface.
16. The computer program product of claim 15, wherein at least one subsequent CNN of the CCNN is trained based on a first subset of the first portion of the plurality of images and a second subset of the second portion of the plurality of images, wherein the first subset of the first portion is greater than the second subset of the second portion, wherein the sum of the first subset and second subset is less than the entirety of the plurality of images.
17. The computer program product of claim 16, wherein training a CCNN further comprises: removing images from the plurality of images to create a second plurality of images comprising the first subset of the first portion and the second subset of the second portion, wherein at least one subsequent CNN is based on the second plurality of images.
18. The computer program product of claim 15, wherein training a first CNN using the plurality of images further comprises: convoluting a receptive field of at least one image; establishing an activation map; applying an activation function to the activation map; max-pooling the activation map; computing weights for a next training stage; and determining at least one loss value.
19. The computer program product of claim 15, wherein the plurality of images comprises computed tomography (CT) images, wherein the first portion of the plurality of images comprise CT images indicating no nodules, wherein the second portion of the plurality of images comprise CT images indicating at least one nodule.
20. The computer program product of claim 15, wherein weighting data is based on an output for a datum from at least one previous CNN and a predicted output for the respective datum from a current CNN; wherein weighting data is further based on an absolute value of the difference between the output for the respective data from the at least one previous CNN and a predicted output for the respective datum from a current CNN.
</claims>
</document>
