<document>

<filing_date>
2018-11-19
</filing_date>

<publication_date>
2020-03-05
</publication_date>

<priority_date>
2018-09-04
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
NEC LABORATORIES EUROPE
</assignee>

<inventors>
JACOBS, TOBIAS
SCHMIDT, MISCHA
</inventors>

<docdb_family_id>
69641427
</docdb_family_id>

<title>
METHOD FOR TRAINING DEEP NEURAL NETWORK (DNN) USING AUXILIARY REGRESSION TARGETS
</title>

<abstract>
A method for training a machine learning model includes calculating auxiliary regression targets (ARTs) for a training data set, modifying an input neural network architecture to provide a modified neural network architecture that includes a parallel neural network layer stack for regressing the ARTs, and training the modified neural network architecture on the ARTs in addition to original machine learning problem targets for the training data set.
</abstract>

<claims>
1. A method for training a machine learning model, the method comprising: calculating auxiliary regression targets (ARTs) for a training data set; modifying an input neural network architecture to provide a modified neural network architecture that includes a parallel neural network layer stack for regressing the ARTs; and training the modified neural network architecture on the additionally regressed ARTs in addition to original machine learning problem targets for the training data set.
2. The method according to claim 1, further comprising providing the training dataset.
3. The method according to claim 2, wherein the training dataset includes a plurality of individual input data samples and targets associated with the individual input data samples.
4. The method according to claim 3, wherein calculating the ARTs for the training data set comprises identifying features of the individual input data samples, generating a feature vector for each of the identified features, constructing a feature space from the generated feature vectors, and generating, for each respective individual input data sample, a corresponding vector in the feature space that serves as an ART associated with the respective individual input data sample.
5. The method according to claim 4, wherein the input neural network architecture provides a first output corresponding to the targets associated with the individual input data samples.
6. The method according to claim 5, wherein the parallel neural network layer stack provides a second output corresponding to the ARTs such that the modified neural network layer architecture provides the first output and the second output.
7. The method according to claim 6, wherein training the modified neural network architecture on the additionally regressed ARTs in addition to original machine learning problem targets for the training data set comprises updating weights assigned to the modified neural network so as to minimize an error function.
8. The method according to claim 7, wherein the error function includes a first component that measures an error related to the first output and a second component that measures an error related to the second output.
9. The method according to claim 8, wherein the error function includes a first weight associated with the first component and a second weight associated with the second component.
10. The method according to claim 9, wherein training the modified neural network architecture on the additionally regressed ARTs in addition to original machine learning problem targets for the training data set further comprises adjusting the first weight and the second weight.
11. The method according to claim 1, further comprising specifying, for the input neural network, a number of hidden layers, a number of neurons per layer, and/or connections between neurons of different layers.
12. The method according to claim 1, further comprising specifying, for the input neural network, a plurality of hyperparameters that include one or more of a learning rate, a heuristic for initializing neural network connection weights, and a dropout fraction per layer.
13. The method according to claim 3, wherein the individual input data samples are individual images.
14. The method according to claim 13, wherein the targets associated with the individual input data samples are class labels.
15. A non-transitory computer readable medium having stored thereon instructions for performing a method for training a machine learning model, the method comprising: calculating auxiliary regression targets (ARTs) for a training data set; modifying an input neural network architecture to provide a modified neural network architecture that includes a parallel neural network layer stack for regressing the ARTs; and training the modified neural network architecture on the additionally regressed ARTs in addition to original machine learning problem targets for the training data set.
</claims>
</document>
