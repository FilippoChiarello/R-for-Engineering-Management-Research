<document>

<filing_date>
2020-05-26
</filing_date>

<publication_date>
2020-12-03
</publication_date>

<priority_date>
2019-05-31
</priority_date>

<ipc_classes>
G06K9/00,G06T7/246,G06T7/73
</ipc_classes>

<assignee>
APPLE
</assignee>

<inventors>
BAILEY, ROBERT
LIU, SHUJIE
SHARMA, VINAY
ZHANG, KE
ZHOU, XIAOSONG
GAO, Shuang
RYMKOWSKI, Bartlomiej
KIM, Emilie
ENGLERT, Ben
TIRA-THOMPSON, Ethan
</inventors>

<docdb_family_id>
73549718
</docdb_family_id>

<title>
AUTOMATED MEDIA EDITING OPERATIONS IN CONSUMER DEVICES
</title>

<abstract>
Techniques disclosed for managing video captured by an imaging device. Methods disclosed capture a video in response to a capture command received at the imaging device. Following a video capture, techniques for classifying the captured video based on feature(s) extracted therefrom, for marking the captured video based on the classification, and for generating a media item from the captured video according to the marking are disclosed. Accordingly, the captured video may be classified as representing a static event, and, as a result, a media item of a still image may be generated. Otherwise, the captured video may be classified as representing a dynamic event, and, as a result, a media item of a video may be generated.
</abstract>

<claims>
We claim:
1. A method for managing video captured by an imaging device, comprising: capturing a video in response to a capture command received at the imaging device; classifying the captured video based on feature(s) extracted from the captured video; marking the captured video based on the classification; and generating a media item from the captured video according to the marking.
2. The method of claim 1, wherein: when the classifying identifies the captured video as representing a static event, the generated media item is a still image; and when the classifying identifies the captured video as representing a dynamic event, the generated media item is a video.
3. The method of claim 1, wherein the feature(s) extracted are derived from object detection analysis and the classifying is based on detection of a predetermined object type from the captured video.
4. The method of claim 1, wherein the feature(s) extracted are derived from scene recognition analysis and the classifying is based on recognition of a predetermined scene type from the captured video.
5. The method of claim 1, wherein the feature(s) extracted are derived from motion recognition analysis and the classifying is based on recognition of a predetermined motion type from the captured video.
6. The method of claim 1, wherein the feature(s) extracted are derived from motion recognition and object detection analyses and wherein, when a detected object is recognized to have motion that is greater than a threshold amount, the classifying identifies the captured video as representing a dynamic event, and when the detected object is recognized to have motion that is lower than a threshold amount, the classifying identifies the captured video as representing a static event.
7. The method of claim 2, wherein: when the classifying identifies the captured video as representing a dynamic event, the marking the captured video based on the classification comprises marking at the captured video a beginning or an end of the video media item.
8. The method of claim 7, wherein the marking at the captured video of a beginning or an end of the video media item is based on appearance or disappearance of detected object(s) in the captured video.
9. The method of claim 7, wherein the marking at the captured video of a beginning or an end of the video media item is based on an act associated with a recognized predefined action type in the captured video.
10. The method of claim 7, wherein the marking at the captured video of a beginning or an end of the video media item is based on a location in the captured video temporally related to a receiving time of the capture command.
11. The method of claim 2, further comprising: when the classifying identifies the captured video as representing a static event, determining a quality level of the still image; determining quality levels of frame(s) from the captured video; if a determined quality level of a frame of the frame(s) is higher than the determined quality level of the still image, prompting a user of the imaging device; and if authorized by the user, replacing the still image with the higher quality frame.
12. The method of claim 11, wherein a quality level of a frame is determined based on recognizing an object's state in the frame, wherein a state of an object comprises a pose, an orientation, or an appearance.
13. A computer system, comprising: at least one processor associated with an imaging device; at least one memory comprising instructions configured to be executed by the at least one processor to perform a method comprising: capturing a video in response to a capture command received at the imaging device; classifying the captured video based on feature(s) extracted from the captured video; marking the captured video based on the classification; and generating a media item from the captured video according to the marking.
14. The system of claim 13, wherein: when the classifying identifies the captured video as representing a static event, the generated media item is a still image; and when the classifying identifies the captured video as representing a dynamic event, the generated media item is a video.
15. The system of claim 13, wherein the feature(s) extracted are derived from object detection analysis and the classifying is based on detection of a predetermined object type from the captured video.
16. The system of claim 13, wherein the feature(s) extracted are derived from scene recognition analysis and the classifying is based on recognition of a predetermined scene type from the captured video.
17. The system of claim 13, wherein the feature(s) extracted are derived from motion recognition analysis and the classifying is based on recognition of a predetermined motion type from the captured video.
18. The system of claim 13, wherein the feature(s) extracted are derived from motion recognition and object detection analyses and wherein, when a detected object is recognized to have motion that is greater than a threshold amount, the classifying identifies the captured video as representing a dynamic event, and when the detected object is recognized to have motion that is lower than a threshold amount, the classifying identifies the captured video as representing a static event.
19. The system of claim 13, wherein: when the classifying identifies the captured video as representing a dynamic event, the marking the captured video based on the classification comprises marking at the captured video a beginning or an end of the video media item.
20. The system of claim 19, wherein the marking at the captured video of a beginning or an end of the video media item is based on appearance or disappearance of detected object(s) in the captured video.
21. The system of claim 19, wherein the marking at the captured video of a beginning or an end of the video media item is based on an act associated with a recognized predefined action type in the captured video.
22. The system of claim 19, wherein the marking at the captured video of a beginning or an end of the video media item is based on a location in the captured video temporally related to a receiving time of the capture command.
23. The system of claim 13, further comprising: when the classifying identifies the captured video as representing a static event, determining a quality level of the still image; determining quality levels of frame(s) from the captured video; if a determined quality level of a frame of the frame(s) is higher than the determined quality level of the still image, prompting a user of the imaging device; and if authorized by the user, replacing the still image with the higher quality frame.
24. The system of claim 23, wherein a quality level of a frame is determined based on recognizing an object's state in the frame, wherein a state of an object comprises a pose, an orientation, or an appearance.
25. A non-transitory computer-readable medium comprising instructions executable by at least one processor associated with an imaging device to perform a method, the method comprising: capturing a video in response to a capture command received at the imaging device; classifying the captured video based on feature(s) extracted from the captured video; marking the captured video based on the classification; and generating a media item from the captured video according to the marking.
26. The medium of claim 25, wherein: when the classifying identifies the captured video as representing a static event, the generated media item is a still image; and when the classifying identifies the captured video as representing a dynamic event, the generated media item is a video.
27. The medium of claim 25, wherein the feature(s) extracted are derived from object detection analysis and the classifying is based on detection of a predetermined object type from the captured video.
28. The medium of claim 25, wherein the feature(s) extracted are derived from scene recognition analysis and the classifying is based on recognition of a predetermined scene type from the captured video.
29. The medium of claim 25, wherein the feature(s) extracted are derived from motion recognition analysis and the classifying is based on recognition of a predetermined motion type from the captured video.
30. The medium of claim 25, wherein the feature(s) extracted are derived from motion recognition and object detection analyses and wherein, when a detected object is recognized to have motion that is greater than a threshold amount, the classifying identifies the captured video as representing a dynamic event, and when the detected object is recognized to have motion that is lower than a threshold amount, the classifying identifies the captured video as representing a static event.
31. The medium of claim 26, wherein: when the classifying identifies the captured video as representing a dynamic event, the marking the captured video based on the classification comprises marking at the captured video a beginning or an end of the video media item.
32. The medium of claim 26, further comprising: when the classifying identifies the captured video as representing a static event, determining a quality level of the still image; determining quality levels of frame(s) from the captured video; if a determined quality level of a frame of the frame(s) is higher than the determined quality level of the still image, prompting a user of the imaging device; and if authorized by the user, replacing the still image with the higher quality frame.
</claims>
</document>
