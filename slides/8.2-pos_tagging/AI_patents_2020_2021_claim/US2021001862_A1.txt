<document>

<filing_date>
2020-07-14
</filing_date>

<publication_date>
2021-01-07
</publication_date>

<priority_date>
2010-06-07
</priority_date>

<ipc_classes>
B60W40/08,G06K9/00,G06N20/00,G06T3/00,G06T7/73
</ipc_classes>

<assignee>
AFFECTIVA
</assignee>

<inventors>
EL KALIOUBY, RANA
SENECHAL, THIBAUD
TURCOT, PANU JAMES
Mohamed, Mohamed Ezzeldin Abdelmonem Ahmed
</inventors>

<docdb_family_id>
74065987
</docdb_family_id>

<title>
VEHICULAR IN-CABIN FACIAL TRACKING USING MACHINE LEARNING
</title>

<abstract>
Vehicular in-cabin facial tracking is performed using machine learning. In-cabin sensor data of a vehicle interior is collected. The in-cabin sensor data includes images of the vehicle interior. A set of seating locations for the vehicle interior is determined. The set is based on the images. The set of seating locations is scanned for performing facial detection for each of the seating locations using a facial detection model. A view of a detected face is manipulated. The manipulation is based on a geometry of the vehicle interior. Cognitive state data of the detected face is analyzed. The cognitive state data analysis is based on additional images of the detected face. The cognitive state data analysis uses the view that was manipulated. The cognitive state data analysis is promoted to a using application. The using application provides vehicle manipulation information to the vehicle. The manipulation information is for an autonomous vehicle.
</abstract>

<claims>
1. A computer-implemented method for facial analysis comprising: collecting in-cabin sensor data of a vehicle interior, wherein the in-cabin sensor data includes images of the vehicle interior; determining a set of seating locations for the vehicle interior, based on the images; scanning the set of seating locations for performing facial detection for each of the seating locations using a facial detection model; manipulating a view of a detected face, based on a geometry of the vehicle interior; and analyzing cognitive state data of the detected face, based on additional images of the detected face, using the view that was manipulated.
2. The method of claim 1 further comprising promoting the cognitive state data to a using application.
3. The method of claim 2 further comprising providing vehicle manipulation information to the vehicle from the using application.
4. The method of claim 3 wherein the using application uses network connectivity remote from the vehicle to provide the manipulation information.
5. The method of claim 4 wherein the manipulation information is for an autonomous or semi-autonomous vehicle.
6. The method of claim 1 further comprising generating a human perception metric for the detected face.
7. The method of claim 6 wherein the human perception metric includes a distractedness, drowsiness, or impairment evaluation for an occupant.
8. The method of claim 7 wherein the human perception metric includes a mood for the occupant.
9. The method of claim 6 wherein the human perception metric includes a mood for the vehicle.
10. The method of claim 1 further comprising defining regions within the vehicle interior.
11. The method of claim 10 wherein the regions comprise a front-seat region and a back-seat region.
12. The method of claim 11 wherein the regions further comprise a third-row seat region.
13. The method of claim 10 further comprising tracking the detected face within a single region of the vehicle interior.
14. The method of claim 13 further comprising scaling the detected face based on a location of the single region.
15. The method of claim 1 wherein the performing facial detection is accomplished using a full-view facial detection model.
16. The method of claim 15 wherein the full-view facial detection model is processed remotely from the vehicle.
17. The method of claim 1 wherein the analyzing is performed using a landmark model.
18. The method of claim 17 wherein the landmark model is processed in an embedded processor.
19. The method of claim 1 wherein faces detected in a vehicle front seat are rotated toward a centerline of the vehicle.
20. The method of claim 1 wherein faces detected in a vehicle rear seat are stretched to a predetermined size.
21. The method of claim 1 wherein the facial detection and the analyzing cognitive state data are performed using machine learning.
22. The method of claim 21 wherein the machine learning is fragmented across a plurality of hardware platforms.
23. The method of claim 1 wherein the analyzing is performed using models provided through a software development kit (SDK).
24. A computer program product embodied in a non-transitory computer readable medium for facial analysis, the computer program product comprising code which causes one or more processors to perform operations of: collecting in-cabin sensor data of a vehicle interior, wherein the in-cabin sensor data includes images of the vehicle interior; determining a set of seating locations for the vehicle interior, based on the images; scanning the set of seating locations for performing facial detection for each of the seating locations using a facial detection model; manipulating a view of a detected face, based on a geometry of the vehicle interior; and analyzing cognitive state data of the detected face, based on additional images of the detected face, using the view that was manipulated.
25. A computer system for facial analysis comprising: a memory which stores instructions; one or more processors coupled to the memory wherein the one or more processors, when executing the instructions which are stored, are configured to: collect in-cabin sensor data of a vehicle interior, wherein the in-cabin sensor data includes images of the vehicle interior; determine a set of seating locations for the vehicle interior, based on the images; scan the set of seating locations for performing facial detection for each of the seating locations using a facial detection model; manipulate a view of a detected face, based on a geometry of the vehicle interior; and analyze cognitive state data of the detected face, based on additional images of the detected face, using the view that was manipulated.
</claims>
</document>
