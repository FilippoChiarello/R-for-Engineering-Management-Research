<document>

<filing_date>
2020-06-05
</filing_date>

<publication_date>
2020-09-24
</publication_date>

<priority_date>
2018-02-22
</priority_date>

<ipc_classes>
G06F3/01,G06F3/03,G06K9/00,G06K9/62,G06T7/73,G06T7/90
</ipc_classes>

<assignee>
INNODEM NEUROSCIENCES
</assignee>

<inventors>
DE VILLERS-SIDANI, ETIENNE
DROUIN-PICARO, PAUL ALEXANDRE
</inventors>

<docdb_family_id>
67617990
</docdb_family_id>

<title>
EYE TRACKING METHOD AND SYSTEM
</title>

<abstract>
A method for training a neural network for determining a gaze position of at least one eye in an initial image comprising the at least one eye. A plurality of training initial images are obtained, of which at least one training color component image is extracted, each of the training initial images respectively comprising at least one eye and a known gaze position. Those are fed into a neural network outputting a respective internal representation for each one of the at least one component image. The neural network is trained by readjusting weights in the neural network to have the respective internal representation for each one of the at least one training color component image more consistent with a respective one of the known gaze position. Once trained, the neural network is used to determine the estimated gaze position relative to a screen of an electronic device.
</abstract>

<claims>
1. A method for training a neural network for determining a gaze position of at least one eye in an initial image comprising the at least one eye, the method comprising: obtaining a plurality of training initial images of which at least one training color component image is extracted, each of the training initial images respectively comprising at least one eye and a known gaze position; feeding into a neural network the at least one training color component image of the training initial images, the neural network outputting a respective internal representation for each one of the at least one component image; and training the neural network by readjusting weights in the neural network as the at least one training color component image of the training initial images are fed into the neural network to have the respective internal representation for each one of the at least one training color component image more consistent with a respective one of the known gaze position.
2. The method of claim 1, wherein at least one portion of the neural network is a respective primary stream respective to each of the at least one training color component image, outputting the respective internal representation.
3. The method of claim 2, wherein the step of feeding comprises applying the respective primary stream to obtain the respective internal representation, the applying separately for each of the at least one training color component image, and comprises at least one convolutional layer and at least one fully-connected layer downstream of the at least one convolutional layer for each one of the at least one component image.
4. The method of claim 3, wherein the neural network comprises another portion, namely an internal stream, downstream the respective primary stream, wherein said determining an estimated gaze position is performed using the internal stream comprising at least one fusion layer, the internal stream having at least one fully-connected layer.
5. The method of claim 4, wherein the internal stream starts at a fusion layer receiving at least the respective internal representation and further comprises an output layer downstream the fusion layer and comprising at least one fully-connected layer.
6. The method of claim 1, wherein the at least one training color component image being extracted comprises each of three RGB components extracted from the initial image.
7. The method of claim 1, wherein each of the training initial images contains additional features other than the at least one eye, the method further comprising: identifying the at least one eye within the training initial image using a facial feature or landmark recognition method; and extracting a portion of the training initial image containing only the at least one eye, thereby obtaining a cropped image, wherein the at least one training color component image is extracted from the cropped image.
8. A method for determining a gaze position of at least one eye in an initial image comprising the at least one eye, the method comprising: feeding the initial image into a neural network trained according to the method of claim 1; and determining the gaze position in the initial image using an output from said neural network.
9. A method for determining a gaze position of at least one eye of a user in an initial image comprising the at least one eye, the method comprising: feeding into a neural network at least one component image, each of the at least one component image comprising a decomposition from the initial image in a single, distinct color, to obtain a respective internal representation for each one of the at least one component image; and determining the gaze position in the initial image using the respective internal representation for each of the at least one component image.
10. The method of claim 9, wherein at least one portion of the neural network is a respective primary stream respective to each of the at least one component image, outputting the respective internal representation.
11. The method of claim 10, wherein the step of feeding comprises applying the respective primary stream to obtain the respective internal representation, the applying separately for each of the at least one component image, and comprises at least one convolutional layer and at least one fully-connected layer downstream of the at least one convolutional layer for each one of the at least one component image.
12. The method of claim 11, wherein the neural network comprises another portion, namely an internal stream, downstream the respective primary stream, wherein said determining an estimated gaze position is performed using the internal stream comprising at least one fusion layer, the internal stream having at least one fully-connected layer.
13. The method of claim 12, wherein the internal stream starts at a fusion layer receiving at least the respective internal representation and further comprises an output layer downstream the fusion layer and comprising at least one fully-connected layer.
14. The method of claim 12, further comprising determining the estimated gaze position relative to a screen of an electronic device using a referential transformation, comprising querying screen properties of the electronic device for performing the referential transformation, and performing user interactions with the electronic device based on the determining the estimated gaze position relative to the screen of the electronic device.
15. The method of claim 9, wherein the at least one component image comprises each of three RGB component images from the initial image.
16. The method of claim 9, wherein the initial image contains additional features other than the at least one eye, the method further comprising: identifying the at least one eye within the initial image using a facial feature or landmark recognition method; extracting a portion of the initial image containing only the at least one eye, thereby obtaining a cropped image; and extracting at least one training color component image from the cropped image.
17. The method of claim 9, wherein feeding each of the at least one component image comprises feeding at least two distinct color components of the initial image to obtain at least two corresponding component images, the method further comprising: for each of the at least two corresponding component images, determining an illuminant value representative of a relative contribution of the each of the at least two corresponding component images to the initial image, wherein said determining an estimated gaze position further comprises combining the illuminant values with the respective internal representation of the at least one component image.
18. The method of claim 17, wherein the illuminant values are processed using an illuminant neural network comprising one or more fully-connected neural network layers.
19. The method of claim 18, wherein the initial image further contains at least one facial landmark, the method further comprising: extracting the at least one facial landmark to obtain a corresponding at least one landmark position; wherein said determining an estimated gaze position further comprises combining the at least one landmark position with the respective internal representation of the at least one component image, wherein said combining is performed using a landmark neural network comprising one or more fully-connected neural network layers.
20. The method of claim 9, further comprising: receiving at least one calibration image of the user associated with a calibration position; and determining a calibrated estimated gaze position based on said at least one calibration image.
</claims>
</document>
