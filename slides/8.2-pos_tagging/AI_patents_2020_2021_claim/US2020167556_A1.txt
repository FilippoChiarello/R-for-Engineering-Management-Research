<document>

<filing_date>
2018-11-26
</filing_date>

<publication_date>
2020-05-28
</publication_date>

<priority_date>
2018-11-26
</priority_date>

<ipc_classes>
G06F3/01,G06K9/00,G06K9/38,G06K9/46,G06N20/00
</ipc_classes>

<assignee>
ACCENTURE GLOBAL SOLUTIONS
</assignee>

<inventors>
SINGH, GAGANDEEP
SINGH CHANDHOK, TARANDEEP
AGARWAL, SHEFALI
SAINI, JITENDER
KAUR, RAMANDEEP
</inventors>

<docdb_family_id>
68531386
</docdb_family_id>

<title>
Real-time gesture detection and recognition
</title>

<abstract>
A device receives, as part of a gesture translation service, data that depicts gestures, wherein the data is image data or multimedia data. The device converts a set of frames that include the data to another set of frames that include modified data identifying a grayscale or black and white depiction of the gestures and generates graphical representations of the gestures identified by the modified data. The device selects, using a matching technique, a graphical representation of corresponding gestures that matches or satisfies a threshold level of similarity with the graphical representations of the gestures identified by the modified data. The device identifies response data that is representative of the corresponding gestures based on the response data being stored in association with an identifier of the graphical representation that has been selected. The device provides the response data to be displayed, via an interface, as text data or audio data.
</abstract>

<claims>
1. A method, comprising: receiving, by a device, data that depicts one or more gestures, wherein the data is image data or multimedia data; filtering, by the device, a first set of frames, wherein the first set of frames includes the data, and wherein the filtering is performed by using a technique to reduce image detail from the first set of frames; converting, by the device, the first set of frames to a second set of frames that include modified data, wherein the modified data identifies a grayscale or black and white depiction of the one or more gestures; generating, by the device, one or more graphical representations of the one or more gestures identified by the modified data; selecting, by the device and by using a matching technique, a graphical representation of one or more corresponding gestures, wherein the graphical representation of the one or more corresponding gestures matches or satisfies a threshold level of similarity with the one or more graphical representations of the one or more gestures identified by the modified data; identifying, by the device, response data that is representative of the one or more corresponding gestures, wherein the identifying is performed based on: the response data being stored in association with the graphical representation that has been selected, or an identifier for the graphical representation that has been selected; and causing, by the device, the response data to be presented in a text format or an audio format.
2. The method of claim 1, wherein the one or more corresponding gestures represent one or more signs that are part of a sign language.
3. The method of claim 1, wherein the data that depicts the one or more gestures is associated with a first written or spoken language or a first sign language, and the response data for the one or more corresponding gestures is associated with a second sign language or a second written or spoken language.
4. The method of claim 1, wherein filtering the first set of frames comprises: filtering a frame, of the first set of frames, by performing the filtering technique to reduce a resolution of the frame identifying the one or more gestures.
5. The method of claim 1, wherein converting the first set of frames to the second set of frames comprises: performing a color conversion technique to convert a frame, of the first set of frames, to a frame of the second set of frames that includes the modified data identifying the grayscale or the black and white depiction of the one or more gestures, and performing a contouring technique to identify boundaries of regions associated with the modified data included in the frame of the second set of frames; and wherein generating the one or more graphical representations of the modified data comprises: generating a histogram to serve as a particular graphical representation of the modified data that is included in the frame of the second set of frames.
6. The method of claim 1, wherein selecting the graphical representation of the one or more corresponding gestures comprises: providing the one or more graphical representations of the modified data as input to a data model to cause the data model to output one or more values that are used to select the graphical representation of the one or more corresponding gestures, wherein the data model has been trained on historical data and uses one or more machine learning techniques to select the graphical representation of the one or more corresponding gestures based on the graphical representation matching or satisfying the threshold level of similarity with the one or more graphical representations of the one or more gestures identified by the modified data.
7. The method of claim 6, wherein a first part of the processing performed by the data model involves identifying a type of sign language associated with the one or more gestures, and wherein identifying the type of sign language is based on location information of a user device that provided the data depicting the one or more gestures.
8. A device, comprising: one or more memories; and one or more processors, operatively coupled to the one or more memories, to: receive a request associated with a gesture translation service, wherein the request includes: text data that describes one or more gestures, or image data or multimedia data that depict the one or more gestures; determine whether the gesture translation service is a first type of gesture translation service or a second type of gesture translation service based on information included in the request, wherein: the first type of gesture translation service involves translating the text data to an image or multimedia format, and the second type of gesture translation service involves translating the image data or the multimedia data to a text format; determine whether to perform a set of filtering and conversion techniques based on whether the gesture translation service is the second type of gesture translation service, wherein the set of filtering and conversion techniques, when performed, cause a set of frames to be created that include modified image data identifying a grayscale or black and white depiction of the one or more gestures and cause one or more graphical representations of the one or more gestures identified by the modified image data to be generated; determine whether to perform a first matching technique or a second matching technique based on whether the gesture translation service is the first type of gesture translation service or the second type of gesture translation service; select, using the first matching technique or the second matching technique, a first identifier based on determining to perform the first matching technique or a second identifier based on determining to perform the second matching technique, wherein the first identifier is associated with particular text data that describes one or more corresponding gestures, and wherein the second identifier is associated with a graphical representation of the one or more corresponding gestures; identify response data using the first identifier or the second identifier, wherein the response data is stored in association with the first identifier and is particular image data or particular multimedia data that depicts the one or more corresponding gestures or the response data is stored in association with the second identifier and is the particular text data that describes the one or more corresponding gestures; and cause the response data to be displayed via an interface.
9. The device of claim 8, wherein the one or more gestures and the one or more corresponding gestures represent one or more signs that are part of a sign language.
10. The device of claim 8, wherein the text data that describes the one or more gestures or the image data or multimedia data that depict the one or more gestures is associated with a first written language or a first sign language and the response data is associated with a second sign language or a second written language.
11. The device of claim 8, wherein the device is a user device or a server device.
12. The device of claim 8, wherein the one or more processors, when determining whether to perform the set of filtering and conversion techniques, are to: determine to perform the set of filtering and conversion techniques based on determining that the gesture translation service is the second type of gesture translation service, filter an initial set of frames that include the image data to reduce a resolution of the initial set of frames; convert the initial set of frames that have been filtered to the set of frames that include the modified image data; and generate one or more histograms as the one or more graphical representations of the modified image data.
13. The device of claim 8, wherein the one or more processors, when selecting the second identifier, are to: provide the one or more graphical representations of the modified image data as input to a data model to cause the data model to output one or more values that are used to select the second identifier that is associated with the graphical representation of the one or more corresponding gestures, wherein the data model has been trained on historical data and uses one or more machine learning techniques.
14. The device of claim 8, wherein the device is a server device; and wherein the one or more processors, when receiving the request, are to: receive the request from a first user device that has engaged in a peer-to-peer session with a second user device; and wherein the one or more processors, when causing the response data to be displayed via the interface, are to: provide the response data to be displayed via the interface on the second user device.
15. A non-transitory computer-readable medium storing instructions, the instructions comprising: one or more instructions that, when executed by one or more processors of a device, cause the one or more processors to: receive, as part of a gesture translation service, data that depicts one or more gestures, wherein the data is image data or multimedia data; convert a first set of frames to a second set of frames that include modified data identifying a grayscale or black and white depiction of the one or more gestures; generate one or more graphical representations of the one or more gestures identified by the modified data; select, using a matching technique, a graphical representation of one or more corresponding gestures, wherein the graphical representation of the one or more corresponding gestures matches or satisfies a threshold level of similarity with the one or more graphical representations of the one or more gestures identified by the modified data; identify response data that is representative of the one or more corresponding gestures based on the response data being stored in association with the graphical representation that has been selected or an identifier for the graphical representation that has been selected; and provide the response data that is representative of the one or more corresponding gestures to be displayed via an interface, wherein the response data is displayed as text data or audio data.
16. The non-transitory computer-readable medium of claim 15, wherein the one or more gestures and the one or more corresponding gestures represent one or more signs in a sign language.
17. The non-transitory computer-readable medium of claim 15, wherein the data that depicts the one or more gestures is associated with a first spoken language or a first sign language, and the gesture data that is representative of the one or more corresponding gestures is associated with a second sign language or a second spoken language.
18. The non-transitory computer-readable medium of claim 15, wherein the device is a user device; and wherein the one or more instructions, that cause the one or more processors to receive the data, cause the one or more processors to: receive the data from an image capturing component of the user device, wherein the data represents a two-dimensional (2-D) depiction of the one or more gestures.
19. The non-transitory computer-readable medium of claim 15, wherein the one or more instructions, when executed by the one or more processors, further cause the one or more processors to: filter, after receiving the data that depicts the one or more gestures, the first set of frames that include the data by using a filtering technique to reduce image detail from the first set of frames.
20. The non-transitory computer-readable medium of claim 15, wherein the one or more instructions, that cause the one or more processors to select the graphical representation of the one or more corresponding gestures, cause the one or more processors to: provide the one or more graphical representations of one or more gestures identified by the modified data as input to a data model to cause the data model to output one or more values that are used to select the graphical representation of the one or more corresponding gestures, wherein the data model has been trained on historical data and uses one or more machine learning techniques to select the graphical representation of the one or more corresponding gestures based on the graphical representation matching or satisfying the threshold level of similarity with the one or more graphical representations of the one or more gestures identified by the modified data.
</claims>
</document>
