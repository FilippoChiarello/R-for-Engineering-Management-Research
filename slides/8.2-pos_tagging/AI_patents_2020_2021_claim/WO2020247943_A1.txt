<document>

<filing_date>
2020-06-08
</filing_date>

<publication_date>
2020-12-10
</publication_date>

<priority_date>
2019-06-06
</priority_date>

<assignee>
TIBCO SOFTWARE
</assignee>

<inventors>
O'CONNELL MICHAEL
HILL, THOMAS
ROPE, DANIEL
PAOLINI, GAIA VALERIA
HSU, Tun-Chieh
YELLAPRAGADA, Venkata Jagannath
</inventors>

<docdb_family_id>
73650696
</docdb_family_id>

<title>
A MULTI-METHOD SYSTEM FOR OPTIMAL PREDICTIVE MODEL SELECTION
</title>

<abstract>
A system for generating algorithmic models comprising a function module to generate a desirability function, an automated machine learning module, and a UI module. The desirability function defines a single desirability value based on an algorithmic model accuracy criteria, criteria for algorithmic model quality, criteria for model fidelity, and criteria for the benefits and cost of model deployment. Specific hard and soft constraints regarding these and other user-defined criteria can also be specified by the user. The automated machine learning module generates an algorithmic model by training the algorithmic model against a model data set, identifying the model with the greatest desirability with respect to all criteria as combined via the desirability function. The UI module generates a user interface to display the overall desirability as well as all model criteria configured by the user. The displayed criteria and desirability are selectable and definable.
</abstract>

<claims>
1. An apparatus for generating algorithmic models used for generating predictive analytics from a model data set for a process, the apparatus comprising:
a desirability function module configured to generate a desirability function, wherein the desirability function define:
at least one outcome variable and outcome variable type and at least one predictor variable and at least one predictor variable type; and at least one algorithmic model accuracy criterion, at least one model analytics type, at least one evaluation criterion for algorithmic model quality, and at least one evaluation criterion for model deployment cost; and an automated machine learning module configured to:
generate at least one algorithmic model having a variable set selected according to the desirability function; and
train the at least one algorithmic model against the model data set.
2. The apparatus of claim 1, wherein the analytics model type includes at least one selected from a group comprising: an analytic method, an analytic algorithm, and an analytic approach.
3. The apparatus of claim 1, wherein the evaluation criterion for algorithmic model quality includes at least one from a group comprising: measures for algorithmic model accuracy; measures for algorithmic model complexity; and measures for algorithmic model fidelity.
4. The apparatus of claim 1, wherein the evaluation criterion for model deployment cost includes at least one from a group comprising: cost of scoring the at least one algorithmic model; cost of false-positive prediction per categorical outcome; cost of false-negative prediction per categorical outcome; value of correct prediction per categorical outcome; cost for prediction error per continuous outcome; cost of acquiring data for each predictor variable; and cost of model building and
recalibration.
5. The apparatus of claim 4, wherein the cost of false-positive prediction per categorical outcome is stratified by each input value per class; wherein the cost of false-negative prediction per categorical outcome is stratified by each input value per class; wherein the value of correct prediction per categorical outcome is by each input value per class; and wherein cost for prediction error per continuous outcome is optionally stratified for each input value per class.
6. The apparatus of claim 1, wherein the desirability function further defines at least one from a group comprising: model quality criterion; model performance value; model performance cost; limits for model estimation cost; limits for model deployment cost; maximum number of inputs for interpretability of models; criteria for variables used to evaluate discriminatory impact of prediction models.
7. The apparatus of claim 1, wherein the desirability function further defines weights, hard limits, or a combination of weights and hard limits for one or more of the at least one algorithmic model accuracy criterion; the at least one evaluation criterion for algorithmic model quality; and the at least one evaluation criterion for model deployment cost.
8. A system for generating algorithmic models used for generating predictive analytics from a model data set for a process, the system comprising:
a desirability function module configured to generate a desirability function, wherein the desirability function defines:
at least one outcome variable and outcome variable type and at least one predictor variable and at least one predictor variable type; and
at least one algorithmic model accuracy criterion, at least one model analytics type, at least one evaluation criterion for algorithmic model quality, and at least one evaluation criterion for model deployment cost;
an automated machine learning module configured to:
generate at least one algorithmic model having a variable set selected according to the desirability function; and
train the at least one algorithmic model against the model data set; and a UI (User Interface) module configured to generate a user interface to display the at least one algorithmic model accuracy criterion, the at least one model analytics type, the at least one evaluation criterion for algorithmic model quality, and the at least one evaluation criterion for model deployment cost;
wherein the displayed criteria and cost are selectable and definable.
9. The system of claim 8, wherein the evaluation criterion for algorithmic model quality includes at least one from a group comprising: measures for algorithmic model accuracy; measures for algorithmic model complexity; and measures for algorithmic model fidelity.
10. The system of claim 8, wherein the evaluation criterion for model deployment cost includes at least one from a group comprising: cost of scoring the at least one algorithmic model; cost of false-positive prediction per categorical outcome; cost of false-negative prediction per categorical outcome; value of correct prediction per categorical outcome; cost for prediction error per continuous outcome; cost of acquiring data for each predictor variable; and cost of model building and
recalibration.
11. The system of claim 8, wherein the evaluation criterion for model deployment desirability includes at least one from a group comprising: desirability of scoring the at least one algorithmic model; desirability of false-positive prediction per categorical outcome; desirability of false-negative prediction per categorical outcome; value of correct prediction per categorical outcome; desirability for prediction error per continuous outcome; cost of acquiring data for each predictor variable; and cost of model building and recalibration.
12. The system of claim 8, wherein the desirability function further defines at least one from the group comprising: model quality criteria; model performance value; model performance cost; limits for model estimation cost; limits for model deployment cost; maximum number of inputs for interpretability of models; criteria for variables used to evaluate discriminatory impact of prediction models.
13. The system of claim 8, wherein the desirability function further defines weights, hard limits, or a combination of weights and hard limits for one or more of the at least one algorithmic model accuracy criterion; the at least one evaluation criterion for algorithmic model quality; the at least one evaluation criterion for model deployment cost.
14. A method for generating algorithmic models used for generating predictive analytics from a model data set for a process, the method comprising:
generating a desirability function;
defining at least one outcome variable and outcome variable type and at least one predictor variable and at least one predictor variable type;
defining at least one algorithmic model accuracy criterion, at least one model analytics type, at least one evaluation criterion for algorithmic model quality, and at least one evaluation criterion for model deployment cost;
generating at least one algorithmic model having a variable set selected according to the desirability function;
training the at least one algorithmic model against the model data set; and generating a user interface to display the at least one algorithmic model accuracy criterion, the at least one model analytics type, the at least one evaluation criterion for algorithmic model quality, and the at least one evaluation criterion for model deployment cost;
wherein the displayed criteria and cost are selectable and definable.
15. The method of claim 14, wherein the analytics model type includes at least one selected from a group comprising: an analytic method, an analytic algorithm, and an analytic approach.
16. The method of claim 14, wherein the evaluation criterion for algorithmic model quality includes at least one from a group comprising: measures for algorithmic model accuracy; measures for algorithmic model complexity; and measures for algorithmic model fidelity.
17. The method of claim 14, wherein the evaluation criterion for model deployment cost includes at least one from a group comprising: cost of scoring the at least one algorithmic model; cost of false-positive prediction per categorical outcome; cost of false-negative prediction per categorical outcome; value of correct prediction per categorical outcome; cost for prediction error per continuous outcome; cost of acquiring data for each predictor variable; and cost of model building and
recalibration.
18. The method of claim 17, wherein the cost of false-positive prediction per categorical outcome is stratified by each input value per class; wherein the cost of false-negative prediction per categorical outcome is stratified by each input value per class; wherein the value of correct prediction per categorical outcome is by each input value per class; and wherein cost for prediction error per continuous outcome is optionally stratified for each input value per class.
19. The method of claim 14, further includes defining at least one from a group comprising: model quality criterion; model performance value; model performance cost; limits for model estimation cost; limits for model deployment cost; maximum number of inputs for interpretability of models; criteria for variables used to evaluate discriminatory impact of prediction models.
20. The method of claim 14, further includes defining weights, hard limits, or a combination of weights and hard limits for one or more of the at least one algorithmic model accuracy criterion; the at least one evaluation criterion for algorithmic model quality; and the at least one evaluation criterion for model deployment cost.
</claims>
</document>
