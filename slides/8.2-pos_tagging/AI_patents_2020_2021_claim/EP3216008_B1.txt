<document>

<filing_date>
2014-11-05
</filing_date>

<publication_date>
2020-02-26
</publication_date>

<priority_date>
2014-11-05
</priority_date>

<ipc_classes>
G06K9/00,G06T13/20,G06T13/40
</ipc_classes>

<assignee>
INTEL CORPORATION
</assignee>

<inventors>
DU, YANGZHOU
TONG, XIAOFENG
LI, QIANG
LI, WENLONG
KIM, TAE-HOON
PARK, MINJE
</inventors>

<docdb_family_id>
55908373
</docdb_family_id>

<title>
AVATAR VIDEO APPARATUS AND METHOD
</title>

<abstract>
Apparatuses, methods and storage medium associated with creating an avatar video are disclosed herein. In embodiments, the apparatus may one or more facial expression engines, an animation-rendering engine, and a video generator. The one or more facial expression engines may be configured to receive video, voice and/or text inputs, and, in response, generate a plurality of animation messages having facial expression parameters that depict facial expressions for a plurality of avatars based at least in part on the video, voice and/or text inputs received. The animation-rendering engine may be configured to receive the one or more animation messages, and drive a plurality of avatar models, to animate and render the plurality of avatars with the facial expression depicted. The video generator may be configured to capture the animation and rendering of the plurality of avatars, to generate a video. Other embodiments may be described and/or claimed.
</abstract>

<claims>
1. An apparatus for animating-rendering an avatar, comprising: one or more processors; and a tongue-out detector to be operated by the one or more processors to detect a tongue-out condition in an image frame, including a mouth region detector (304) to identify locations of a plurality of facial landmarks associated with identifying a mouth in the image frame, a mouth region extractor (306) to extract a mouth region from the image frame,
based at least in part on the locations of the plurality of facial landmarks identified, and a tongue classifier (308) to analyze a plurality of sub-windows within the mouth region extracted to detect for tongue-out, the apparatus further comprising, one or more facial expression engines to receive video input and generate a plurality of animation messages having facial expression parameters that depict facial expressions for a plurality of avatars based at least in part on the video input received, wherein the one or more facial expression engines comprise a video driven facial expression engine that includes the tongue-out detector to detect a tongue-out condition in an image frame; an animation-rendering engine coupled with the one or more facial expression engines to receive the one or more animation messages, and drive a plurality of avatar models, in accordance with the plurality of animation messages, to animate and render the plurality of avatars with the facial expressions depicted; and a video generator coupled with the animation-rendering engine to capture the animation and rendering of the plurality of avatars, and generate a video based at least in part on the animation and rendering captured.
2. The apparatus of claim 1, wherein the mouth region detector is to identify locations of a chin point, a left corner of the mouth, and a right corner of the mouth in the image frame, and wherein the mouth region extractor is to extract the mouth region from the image frame, based at least in part the locations of the chin point, the left corner of the mouth, and the right corner of the mouth identified.
3. The apparatus of claim 2, wherein the mouth region extractor is to further size-wise normalize the mouth region extracted.
4. The apparatus of claim 1, wherein the tongue classifier is to analyze the plurality of sub-windows for a plurality of tongue-out features, including Haar-like features, histogram of gradient features, gradient features, or summed gradient features.
5. The apparatus of any one of claims 1 - 4, wherein the tongue-out detector further comprises a temporal filter to receive a plurality of results of the tongue classifier for a plurality of image frames, and output a notification of tongue-out detection on successive receipt of a plurality of results from the tongue classifier indicating tongue-out detection for a plurality of successive image frames.
6. The apparatus of claim 1, wherein the video driven facial expression engine is to analyze the image frames for landmarks of a face or head poses, and generate at least a subset of the plurality of animation messages having facial expression parameters that depict facial expressions for the plurality of avatars, including eye and mouth movements or head poses of the avatars, based at least in part on landmarks of a face or head poses in the image frames.
7. The apparatus of claim 1, wherein the one or more facial expression engines comprise a voice recognition facial expression engine to receive audio inputs, analyze the audio inputs for at least volume or syllable, and generate at least a subset of the plurality of animation messages having facial expression parameters that depict facial expressions for the plurality of avatars, including mouth movements of the plurality of avatars, based at least in part on volume or syllable of the audio inputs.
8. The apparatus of claim 1, wherein the one or more facial expression engines comprise a text based facial expression engine to receive text inputs, analyze the text inputs for semantics, and generate at least a subset of the plurality of animation messages having facial expression parameters that depict facial expressions for the plurality of avatars, including mouth movements of the plurality of avatars, based at least in part on semantics of the text inputs.
9. The apparatus of claim 1, wherein the video generator is to capture a plurality of image frames of the animation and rendering of the plurality of avatars, and generate a video based at least in part on the image frames of the animation and rendering captured.
10. A method for animating-rendering an avatar, comprising: receiving, by a computing device, a plurality of image frames ; and detecting for a tongue-out condition in one or more of the image frames, including: identifying locations of a plurality of facial landmarks associated with identifying a mouth in an image frame, extracting a mouth region from the image frame, based at least in part on the locations of the plurality of facial landmarks identified, and analyzing a plurality of sub-windows within the mouth region extracted to detect for tongue-out, the method further comprising, one or more facial expression engines to receive video input and generate a plurality of animation messages having facial expression parameters that depict facial expressions for a plurality of avatars based at least in part on the video input received, wherein the one or more facial expression engines comprise a video driven facial expression engine that includes the tongue-out detector to detect a tongue-out condition in an image frame; an animation-rendering engine coupled with the one or more facial expression engines to receive the one or more animation messages, and drive a plurality of avatar models, in accordance with the plurality of animation messages, to animate and render the plurality of avatars with the facial expressions depicted; and a video generator coupled with the animation-rendering engine to capture the animation and rendering of the plurality of avatars, and generate a video based at least in part on the animation and rendering captured.
11. The method of claim 10, wherein identifying comprises identifying locations of a chin point, a left corner of the mouth, and a right corner of the mouth in the image frame, and wherein extracting comprises extracting the mouth region from the image frame, based at least in part the locations of the chin point, the left corner of the mouth, and the right corner of the mouth identified; and size-wise normalizing the mouth region extracted.
12. The method of claim 10 or 11, further comprising temporally filtering a plurality of results of the analyses for a plurality of image frames, and output a notification of tongue-out detection on successive receipt of a plurality of results indicating tongue-out detection for a plurality of successive image frames.
13. The method of claim 10, further comprising: generating, by the computing device, a plurality of animation messages having facial expression parameters that depict facial expressions for the plurality of avatars based at least in part on the image frames received; driving, by the computing device, a plurality of avatar models, in accordance with the plurality of animation messages, to animate and render the plurality of avatars with the facial expressions depicted; and capturing, by the computing device, the animation and rendering of the plurality of avatars to generate a video based at least in part on the animation and rendering captured.
14. The method of claim 13, wherein receiving comprises receiving video inputs having the plurality of image frames; and generating comprises analyzing the image frames for landmarks of a face or head poses, and generating at least a subset of the plurality of animation messages having facial expression parameters that depict facial expressions for the plurality of avatars, including eye and mouth movements or head poses of the avatars, based at least in part on landmarks of a face or head poses in the image frames.
</claims>
</document>
