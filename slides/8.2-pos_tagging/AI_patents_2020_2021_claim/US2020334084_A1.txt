<document>

<filing_date>
2019-04-16
</filing_date>

<publication_date>
2020-10-22
</publication_date>

<priority_date>
2019-04-16
</priority_date>

<ipc_classes>
G06F11/30,G06F9/38,G06F9/455,G06F9/50
</ipc_classes>

<assignee>
KAZUHM
</assignee>

<inventors>
JACOBSON, Joshua Owen
</inventors>

<docdb_family_id>
72832429
</docdb_family_id>

<title>
DISTRIBUTED IN-PLATFORM DATA STORAGE UTILIZING GRAPHICS PROCESSING UNIT (GPU) MEMORY
</title>

<abstract>
Certain aspects of the present disclosure provide methods and systems for in-platform data storage. Embodiments include receiving data for storage in a distributed computing platform. Embodiments include determining that graphics processing unit (GPU) memory resources are available. Embodiments include storing the data in the GPU memory resources. Embodiments include monitoring demand for the GPU memory resources in the distributed computing platform. Embodiments include identifying a contention for the GPU memory resources. Embodiments include evacuating the data from the GPU memory resources based on the contention.
</abstract>

<claims>
1. A method for in-platform data storage, comprising: receiving data for storage in a distributed computing platform; determining that graphics processing unit (GPU) memory resources are available; storing the data in the GPU memory resources; monitoring demand for the GPU memory resources in the distributed computing platform; identifying a contention for the GPU memory resources; and evacuating the data from the GPU memory resources based on the contention.
2. The method of claim 1, wherein storing the data using the GPU memory resources comprises allocating the data as texture data using a library associated with the GPU.
3. The method of claim 1, wherein storing the data using the GPU memory resources comprises allocating the data as a direct memory buffer via a driver associated with the GPU.
4. The method of claim 1, wherein storing the data using the GPU memory resources comprises allocating the data as a high-level software abstraction via a library associated with the GPU.
5. The method of claim 1, wherein determining that the GPU memory resources are available comprises sending a request for utilization information to an interface application associated with the GPU.
6. The method of claim 1, wherein identifying the contention for the GPU memory resources comprises determining that another entity has requested to store second data in the GPU memory resources.
7. The method of claim 1, wherein evacuating the data from the GPU memory resources based on the contention comprises: removing the data from the GPU memory resources; and storing the data in a memory location that is separate from the GPU.
8. A method for in-platform data storage, comprising: identifying, by a management entity, data for storage in a distributed computing platform; determining, by the management entity, that graphics processing unit (GPU) memory resources of a node in the distributed computing platform are available; sending, by the management entity, the data to the node for storage in the GPU memory resources; identifying, by the management entity, a contention for the GPU memory resources based on information received from the node; and evacuating, by the management entity, the data from the GPU memory resources based on the contention.
9. The method of claim 8, wherein sending, by the management entity, the data to the node for storage in the GPU memory resources comprises instructing a component of the node to allocate the data as texture data using a library associated with the GPU.
10. The method of claim 8, wherein sending, by the management entity, the data to the node for storage in the GPU memory resources comprises instructing a component of the node to allocate the data as a direct memory buffer via a driver associated with the GPU.
11. The method of claim 8, wherein sending, by the management entity, the data to the node for storage in the GPU memory resources comprises instructing a component of the node to allocate the data as a high-level software abstraction via a library associated with the GPU.
12. The method of claim 8, wherein determining, by the management entity, that the GPU memory resources of the node are available comprises receiving utilization information from a component of the node.
13. The method of claim 8, wherein evacuating, by the management entity, the data from the GPU memory resources based on the contention comprises: instructing a component of the node to remove the data from the GPU memory resources; and storing the data in a memory location in the distributed computing platform that is separate from the GPU.
14. An apparatus, comprising: a memory comprising computer-executable instructions; and a processor in data communication with the memory and configured to execute the computer-executable instructions and cause the apparatus to perform a method for in-platform data storage, the method comprising: receiving data for storage in a distributed computing platform; determining that graphics processing unit (GPU) memory resources are available; storing the data in the GPU memory resources; monitoring demand for the GPU memory resources in the distributed computing platform; identifying a contention for the GPU memory resources; and evacuating the data from the GPU memory resources based on the contention.
15. The apparatus of claim 14, wherein storing the data using the GPU memory resources comprises allocating the data as texture data using a library associated with the GPU.
16. The apparatus of claim 14, wherein storing the data using the GPU memory resources comprises allocating the data as a direct memory buffer via a driver associated with the GPU.
17. The apparatus of claim 14, wherein storing the data using the GPU memory resources comprises allocating the data as a high-level software abstraction via a library associated with the GPU.
18. The apparatus of claim 14, wherein determining that the GPU memory resources are available comprises sending a request for utilization information to an interface application associated with the GPU.
19. The apparatus of claim 14, wherein identifying the contention for the GPU memory resources comprises determining that another entity has requested to store second data in the GPU memory resources.
20. The apparatus of claim 14, wherein evacuating the data from the GPU memory resources based on the contention comprises: removing the data from the GPU memory resources; and storing the data in a memory location that is separate from the GPU.
</claims>
</document>
