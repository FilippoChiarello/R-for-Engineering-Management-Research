<document>

<filing_date>
2019-05-15
</filing_date>

<publication_date>
2020-11-19
</publication_date>

<priority_date>
2019-05-15
</priority_date>

<ipc_classes>
G06F11/14,G06N3/08
</ipc_classes>

<assignee>
WESTERN DIGITAL TECHNOLOGIES
</assignee>

<inventors>
LI YAN
VUCINIC, DEJAN
SUN, CHAO
</inventors>

<docdb_family_id>
73231655
</docdb_family_id>

<title>
OPTIMIZED NEURAL NETWORK DATA ORGANIZATION
</title>

<abstract>
In some implementations, the present disclosure relates to a method. The method includes obtaining a set of weights for a neural network comprising a plurality of nodes and a plurality of connections between the plurality of nodes. The method also includes identifying a first subset of weights and a second subset of weights based on the set of weights. The first subset of weights comprises weights that used by the neural network. The second subset of weights comprises weights that are prunable. The method further includes storing the first subset of weights in a first portion of a memory. A first error correction code is used for the first portion of the memory. The method further includes storing the second subset of weights in a second portion of the memory. A second error correction code is used for the second portion of the memory. The second error correction code is weaker than the first error correction code.
</abstract>

<claims>
1. A method, comprising: selectively storing a first subset of weights for a neural network in a first portion of a memory, the neural network comprising a plurality of nodes and a plurality of connections between the plurality of nodes, wherein: the first subset of weights comprises weights used by the neural network; and a first error correction code is used for the first portion of the memory; and selectively storing a second subset of weights for the neural network in a second portion of the memory, wherein: the second subset of weights comprises weights that are prunable; a second error correction code is used for the second portion of the memory; and the second error correction code is weaker than the first error correction code.
2. The method of claim 1, more overprovisioning is used in the second portion of the memory than is used in the first portion of the memory.
3. The method of claim 1, wherein storing the first subset of weights in the first portion of the memory comprises: logically arranging the first subset of weights into a bit array, wherein: each row of the bit array comprises a weigh of the first subset of weights; and the first subset of weights are aligned by bit position.
4. The method of claim 3, wherein storing the first subset of weights in the first portion of the memory further comprises: encoding columns of the bit array using different error correction codes, wherein the different error correction codes comprises the first error correction code.
5. The method of claim 4, wherein: each column of the bit array is associated with a different bit significance; and columns associated with a lower bit significance are encoded using weaker error correction codes than columns associated with a higher bit significance.
6. The method of claim 5, wherein bits in columns associated with lower bit significances are updated more frequently than bits associated in columns associated with higher bit significances.
7. The method of claim 1, wherein: the memory comprises multiple types of the memory; and the first portion of the memory and the second portion of the memory are of a first type of memory.
8. The method of claim 7, wherein: file metadata associated with input files and inference results obtained based on the input files, are stored in a third portion of the memory; the input files are stored in a fourth portion of the memory; and the fourth portion of the memory is of a second type of memory.
9. The method of claim 8, wherein: the first portion of the memory, the second portion of the memory, the third portion of the memory, and the fourth portion of the memory use different error correction codes and different amounts of overprovisioning; and the first portion of the memory, the second portion of the memory, the third portion of the memory, and the fourth portion of the memory have different access latencies and different reliabilities.
10. The method of claim 1, wherein storing the first subset of weights in the first portion of the memory comprises: duplicating the first subset of weights across different portions of the memory.
11. An apparatus, comprising: a memory configured to store data; and a controller coupled to the memory, the controller configured to: selectively store a first subset of weights for a neural network in a first portion of the memory, the neural network comprising a plurality of nodes and a plurality of connections between the plurality of nodes, wherein: the first subset of weights comprises weights used by the neural network; and a first error correction code is used for the first portion of the memory; and selectively store a second subset of weights for the neural network in a second portion of the memory, wherein: the second subset of weights comprises weights that are prunable; a second error correction code is used for the second portion of the memory; and the second error correction code is weaker than the first error correction code.
12. The apparatus of claim 11, more overprovisioning is used in the second portion of the memory than is used in the first portion of the memory.
13. The apparatus of claim 11, wherein to selectively store the first subset of weights in the first portion of the memory the controller is further configured to: logically arrange the first subset of weights into a bit array, wherein: each row of the bit array comprises a weigh of the first subset of weights; and the first subset of weights are aligned by bit position.
14. The apparatus of claim 13, wherein to selectively store the first subset of weights in the first portion of the memory the controller is further configured to: encode columns of the bit array using different error correction codes, wherein the different error correction codes comprises the first error correction code.
15. The apparatus of claim 14, wherein: each column of the bit array is associated with a different bit significance; and columns associated with a lower bit significance are encoded using weaker error correction codes than columns associated with a higher bit significance.
16. The apparatus of claim 11, wherein: the memory comprises multiple types; and the first portion of the memory and the second portion of the memory are of a first type of memory.
17. The apparatus of claim 16, wherein: file metadata associated with input files and inference results obtained based on the input files, are stored in a third portion of the memory.
18. The apparatus of claim 17, wherein: the input files are stored in a fourth portion of the memory; and the fourth portion of the memory is of a second type of memory.
19. The apparatus of claim 11, wherein to store the first subset of weights in the first portion of the memory the controller is further configured to: duplicate the first subset of weights across different portions of the memory.
20. A non-transitory machine-readable medium having executable instructions to cause one or more processing devices to perform operations comprising: selectively storing a first subset of weights for a neural network in a first portion of a memory, the neural network comprising a plurality of nodes and a plurality of connections between the plurality of nodes, wherein: the first subset of weights comprises weights used by the neural network; and a first error correction code is used for the first portion of the memory; and selectively storing a second subset of weights for the neural network in a second portion of the memory, wherein: the second subset of weights comprises weights that are prunable; a second error correction code is used for the second portion of the memory; and the second error correction code is weaker than the first error correction code.
</claims>
</document>
