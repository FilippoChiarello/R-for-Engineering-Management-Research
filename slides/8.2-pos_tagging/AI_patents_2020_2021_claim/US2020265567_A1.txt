<document>

<filing_date>
2019-02-18
</filing_date>

<publication_date>
2020-08-20
</publication_date>

<priority_date>
2019-02-18
</priority_date>

<ipc_classes>
G06N3/08,G06T5/50
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
SHEIKH, HAMID R.
GLOTZBACH, JOHN, W.
PEKKUCUKSEN, IBRAHIM
ZHEN, RUIWEN
HU, YUTING
</inventors>

<docdb_family_id>
72040673
</docdb_family_id>

<title>
TECHNIQUES FOR CONVOLUTIONAL NEURAL NETWORK-BASED MULTI-EXPOSURE FUSION OF MULTIPLE IMAGE FRAMES AND FOR DEBLURRING MULTIPLE IMAGE FRAMES
</title>

<abstract>
A method includes obtaining multiple image frames of a scene using at least one camera of an electronic device. The method also includes using a convolutional neural network to generate blending maps associated with the image frames. The blending maps contain or are based on both a measure of motion in the image frames and a measure of how well exposed different portions of the image frames are. The method further includes generating a final image of the scene using at least some of the image frames and at least some of the blending maps. The final image of the scene may be generated by blending the at least some of the image frames using the at least some of the blending maps, and the final image of the scene may include image details that are lost in at least one of the image frames due to over-exposure or under-exposure.
</abstract>

<claims>
1. A method comprising: obtaining multiple image frames of a scene using at least one camera of an electronic device; using a convolutional neural network to generate blending maps associated with the image frames, wherein the blending maps contain or are based on both (i) a measure of motion in the image frames and (ii) a measure of how well exposed different portions of the image frames are; and generating a final image of the scene using at least some of the image frames and at least some of the blending maps.
2. The method of claim 1, wherein: generating the final image of the scene comprises blending the at least some of the image frames using the at least some of the blending maps; and the final image of the scene includes image details that are lost in at least one of the image frames due to over-exposure or under-exposure.
3. The method of claim 1, wherein the convolutional neural network performs multiple convolution and pooling operations, multiple upsampling and decoding operations, and a final convolution operation to generate the blending maps.
4. The method of claim 1, wherein: inputs to the convolutional neural network are image patches from the image frames, each image patch having multiple color channels; the image patches are concatenated along the color channels to increase a number of inputs; outputs from the convolutional neural network are blending map patches, each blending map patch having a single weight channel; and multiple blending map patches are combined along the weight channels to produce the blending maps.
5. The method of claim 1, wherein the convolutional neural network is configured to: extract scene contents from at least part of the image frames; spatially downsize feature maps associated with the scene contents; merge the downsized feature maps; upsample the merged feature maps; and translate the merged feature maps into the blending maps.
6. The method of claim 1, wherein the convolutional neural network is trained by: obtaining multiple initial images of different scenes; generating additional images of the scenes by transforming the initial images to simulate motion within the different scenes; generating ground truth blending maps using the initial images and the additional images; and using the initial images, the additional images, and the ground truth blending maps to train the convolutional neural network.
7. The method of claim 1, wherein the convolutional neural network is trained by: obtaining multiple initial images at different camera exposures and ground truth blending maps for the initial images; dividing the initial images and the associated ground truth blending maps into a training set, a validation set, and a testing set; identifying image patches in the images of the training and validation sets and corresponding ground truth training patches in the ground truth blending maps of the training and validation sets; repeatedly (i) training the convolutional neural network using the image patches and the ground truth training patches of the training set and (ii) validating the trained convolutional neural network using the image patches and the ground truth training patches of the validation set; identifying testing patches in the initial images and the associated ground truth blending maps of the testing set; and using the testing patches in the initial images and the associated ground truth blending maps of the testing set to test the trained and validated convolutional neural network.
8. An electronic device comprising: at least one camera; and at least one processing device configured to: obtain multiple image frames of a scene using the at least one camera; use a convolutional neural network to generate blending maps associated with the image frames, wherein the blending maps contain or are based on both (i) a measure of motion in the image frames and (ii) a measure of how well exposed different portions of the image frames are; and generate a final image of the scene using at least some of the image frames and at least some of the blending maps.
9. The electronic device of claim 8, wherein: to generate the final image of the scene, the at least one processing device is configured to blend the at least some of the image frames using the at least some of the blending maps; and the final image of the scene includes image details that are lost in at least one of the image frames due to over-exposure or under-exposure.
10. The electronic device of claim 8, wherein the convolutional neural network is configured to perform multiple convolution and pooling operations, multiple upsampling and decoding operations, and a final convolution operation to generate the blending maps.
11. The electronic device of claim 8, wherein: inputs to the convolutional neural network are image patches from the image frames, each image patch having multiple color channels; the image patches are concatenated along the color channels to increase a number of inputs; outputs from the convolutional neural network are blending map patches, each blending map patch having a single weight channel; and multiple blending map patches are combined along the weight channels to produce the blending maps.
12. The electronic device of claim 8, wherein the convolutional neural network is configured to: extract scene contents from at least part of the image frames; spatially downsize feature maps associated with the scene contents; merge the downsized feature maps; upsample the merged feature maps; and translate the merged feature maps into the blending maps.
13. The electronic device of claim 8, wherein the convolutional neural network is trained by: obtaining multiple initial images of different scenes; generating additional images of the scenes by transforming the initial images to simulate motion within the different scenes; generating ground truth blending maps using the initial images and the additional images; and using the initial images, the additional images, and the ground truth blending maps to train the convolutional neural network.
14. The electronic device of claim 8, wherein the convolutional neural network is trained by: obtaining multiple initial images at different camera exposures; generating ground truth blending maps for the initial images; dividing the initial images and the associated ground truth blending maps into a training set, a validation set, and a testing set; identifying image patches in the images of the training set and corresponding ground truth training patches in the ground truth blending maps of the training set; training the convolutional neural network using the image patches and the ground truth training patches; using the initial images and the associated ground truth blending maps in the validation set to validate the trained convolutional neural network; identifying testing patches in the initial images and the associated ground truth blending maps in the testing set; and using the testing patches in the initial images and the associated ground truth blending maps in the testing set to test the trained and validated convolutional neural network.
15. A non-transitory machine-readable medium containing instructions that when executed cause at least one processor of an electronic device to: obtain multiple image frames of a scene using at least one camera of the electronic device; use a convolutional neural network to generate blending maps associated with the image frames, wherein the blending maps contain or are based on both (i) a measure of motion in the image frames and (ii) a measure of how well exposed different portions of the image frames are; and generate a final image of the scene using at least some of the image frames and at least some of the blending maps.
16. The non-transitory machine-readable medium of claim 15, wherein: the instructions that when executed cause the at least one processor to generate the final image of the scene comprise instructions that when executed cause the at least one processor to blend the at least some of the image frames using the at least some of the blending maps; and the final image of the scene includes image details that are lost in at least one of the image frames due to over-exposure or under-exposure.
17. The non-transitory machine-readable medium of claim 15, wherein the convolutional neural network performs multiple convolution and pooling operations, multiple upsampling and decoding operations, and a final convolution operation to generate the blending maps.
18. The non-transitory machine-readable medium of claim 17, wherein: inputs to the convolutional neural network are image patches from the image frames, each image patch having multiple color channels; the image patches are concatenated along the color channels to increase a number of inputs; outputs from the convolutional neural network are blending map patches, each blending map patch having a single color channel; and multiple blending map patches are combined along the color channels to produce the blending maps.
19. The non-transitory machine-readable medium of claim 15, wherein the convolutional neural network is trained by: obtaining multiple initial images of different scenes; generating additional images of the scenes by transforming the initial images to simulate motion within the different scenes; generating ground truth blending maps using the initial images and the additional images; and using the initial images, the additional images, and the ground truth blending maps to train the convolutional neural network.
20. The non-transitory machine-readable medium of claim 15, wherein the convolutional neural network is trained by: obtaining multiple initial images at different camera exposures; generating ground truth blending maps for the initial images; dividing the initial images and the associated ground truth blending maps into a training set, a validation set, and a testing set; identifying image patches in the images of the training set and corresponding ground truth training patches in the ground truth blending maps of the training set; training the convolutional neural network using the image patches and the ground truth training patches; using the initial images and the associated ground truth blending maps in the validation set to validate the trained convolutional neural network; identifying testing patches in the initial images and the associated ground truth blending maps in the testing set; and using the testing patches in the initial images and the associated ground truth blending maps in the testing set to test the trained and validated convolutional neural network.
21. A method comprising: capturing multiple image frames of a scene at different camera exposures using at least one camera of an electronic device; determining whether to discard any of the captured image frames based on an amount of blur in the captured image frames; identifying portions of the captured image frames prone to blur; blending the image frames that have not been discarded to produce a blended image; performing deblurring of the blended image in only the identified portions; and performing filtering and motion compensation of the blended image to generate a final image of the scene.
22. The method of claim 21, further comprising: identifying a number of the image frames to be captured and exposure times associated with the image frames to be captured; wherein capturing the image frames comprises capturing the identified number of image frames at the identified exposure times.
23. The method of claim 22, wherein identifying the number of the image frames and the exposure times comprises identifying the number of the image frames and the exposure times based on an analysis of the scene.
24. The method of claim 21, wherein performing the filtering and the motion compensation comprises performing the filtering and the motion compensation in only the identified portions of the blended image.
25. The method of claim 21, further comprising: aligning the image frames; wherein identifying the portions of the captured image frames prone to blur comprises generating motion maps associated with the aligned image frames; wherein blending the image frames comprises blending the aligned image frames; and wherein the deblurring, the filtering, and the motion compensation are based on the motion maps.
26. The method of claim 21, further comprising: selecting one of the image frames as a reference image frame; wherein a sharpness metric is used to rank the image frames and select the reference image frame; and wherein the sharpness metric is used to determine whether to discard any of the captured image frames.
27. The method of claim 26, wherein the sharpness metric for each image frame is generated by: downsampling the image frame; filtering the image frame; multiplying the image frame by a weight mask; and adding and normalizing results of the multiplying to generate a sharpness score for the image frame.
</claims>
</document>
