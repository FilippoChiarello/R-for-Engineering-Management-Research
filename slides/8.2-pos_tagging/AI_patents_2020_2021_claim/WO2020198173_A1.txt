<document>

<filing_date>
2020-03-23
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2019-03-22
</priority_date>

<ipc_classes>
G06K9/00,G06K9/34,G06K9/62
</ipc_classes>

<assignee>
QUALCOMM
</assignee>

<inventors>
GAVVES, EFSTRATIOS
SMEULDERS, ARNOLD WILHELMUS MARIA
KILICKAYA, Mert
</inventors>

<docdb_family_id>
72514502
</docdb_family_id>

<title>
SUBJECT-OBJECT INTERACTION RECOGNITION MODEL
</title>

<abstract>
A method for processing an image is presented. The method locates a subject and an object of a subject-object interaction in the image. The method determines relative weights of the subject, the object, and a context region for classification. The method further classifies the subject-object interaction based on a classification of a weighted representation of the subject, a weighted representation of the object, and a weighted representation of the context region.
</abstract>

<claims>
WHAT IS CLAIMED IS:
1. A method for processing an image, comprising:
locating a subject and an object of a subject-object interaction in the image; determining relative weights of the subject, the object, and a context region for classification; and
classifying the subject-object interaction based on a classification of a weighted representation of the subject, a weighted representation of the object, and a weighted representation of the context region.
2. The method of claim 1, in which the image is out-of-context.
3. The method of claim 1, in which the subject is identified as a human.
4. The method of claim 1, further comprising:
learning to represent the subject and the object in a context-free manner; and localizing the subject and the object based on the learning.
5. The method of claim 1, further comprising:
receiving, at a convolutional neural network, a subject-only image region, an object-only image region, and a context-only image region; and
generating, by the convolutional neural network, image features corresponding to each subject, object, and context region.
6. The method of claim 5, further comprising:
masking the subject-only image region and the object-only image region to obtain the context-only image region;
masking the subject-only image region and the context-only image region to obtain the object-only image region; and
masking the context-only image region and the object-only image region to obtain the subject-only image region.
7. The method of claim 5, further comprising determining the relative weights of the subject, object, and the context region based on a relative importance to the subjectobject interaction classification, the relative importance determined based on the image features.
8. An apparatus for processing an image, comprising:
a memory; and
at least one processor coupled to the memory, the at least one processor configured:
to locate a subject and an object of a subject-object interaction in the image; to determine relative weights of the subject, the object, and a context region for classification; and
to classify the subject-object interaction based on a classification of a weighted representation of the subject, a weighted representation of the object, and a weighted representation of the context region.
9. The apparatus of claim 8, in which the image is out-of-context.
10. The apparatus of claim 8, in which the subject is identified as a human.
11. The apparatus of claim 8, in which the at least one processor is further configured:
to learn to represent the subject and the object in a context-free manner; and to localize the subject and the object based on the learning.
12. The apparatus of claim 8, in which the at least one processor is further configured:
to receive, at a convolutional neural network, a subject-only image region, an object-only image region, and a context-only image region; and
to generate, by the convolutional neural network, image features corresponding to each subject, object, and context region.
13. The apparatus of claim 12, in which the at least one processor is further configured: to mask the subject-only image region and the object-only image region to obtain the context-only image region;
to mask the subject-only image region and the context-only image region to obtain the object-only image region; and
to mask the context-only image region and the object-only image region to obtain the subject-only image region.
14. The apparatus of claim 12, in which the at least one processor is further configured to determine the relative weights of the subject, object, and the context region based on a relative importance to the subject-object interaction classification, the relative importance determined based on the image features.
15. An apparatus for processing an image, comprising:
means for locating a subject and an object of a subject-object interaction in the image;
means for determining relative weights of the subject, the object, and a context region for classification; and
means for classifying the subject-object interaction based on a classification of a weighted representation of the subject, a weighted representation of the object, and a weighted representation of the context region.
16. The apparatus of claim 15, in which the image is out-of-context.
17. The apparatus of claim 15, in which the subject is identified as a human.
18. The apparatus of claim 15, further comprising:
means for learning to represent the subject and the object in a context-free manner; and
means for localizing the subject and the object based on the learning.
19. The apparatus of claim 15, further comprising:
means for receiving, at a convolutional neural network, a subject-only image region, an object-only image region, and a context-only image region; and means for generating, by the convolutional neural network, image features corresponding to each subject, object, and context region.
20. The apparatus of claim 19, further comprising:
means for masking the subject-only image region and the object-only image region to obtain the context-only image region;
means for masking the subject-only image region and the context-only image region to obtain the object-only image region; and
means for masking the context-only image region and the object-only image region to obtain the subject-only image region.
21. The apparatus of claim 19, further comprising means for determining the relative weights of the subject, object, and the context region based on a relative importance to the subject-object interaction classification, the relative importance determined based on the image features.
22. A non-transitory computer-readable medium having program code recorded thereon for processing an image, the program code executed by a processor and comprising:
program code to locate a subject and an object of a subject-object interaction in the image;
program code to determine relative weights of the subject, the object, and a context region for classification; and
program code to classify the subject-object interaction based on a classification of a weighted representation of the subject, a weighted representation of the object, and a weighted representation of the context region.
23. The non-transitory computer-readable medium of claim 22, in which the image is out-of-context.
24. The non-transitory computer-readable medium of claim 22, in which the subject is identified as a human.
25. The non-transitory computer-readable medium of claim 22, in which the at least one processor is further configured:
to learn to represent the subject and the object in a context-free manner; and to localize the subject and the object based on the learning.
26. The non-transitory computer-readable medium of claim 22, in which the at least one processor is further configured:
to receive, at a convolutional neural network, a subject-only image region, an object-only image region, and a context-only image region; and
to generate, by the convolutional neural network, image features corresponding to each subject, object, and context region.
27. The non-transitory computer-readable medium of claim 26, in which the at least one processor is further configured:
to mask the subject-only image region and the object-only image region to obtain the context-only image region;
to mask the subject-only image region and the context-only image region to obtain the object-only image region; and
to mask the context-only image region and the object-only image region to obtain the subject-only image region.
28. The non-transitory computer-readable medium of claim 26, in which the at least one processor is further configured to determine the relative weights of the subject, object, and the context region based on a relative importance to the subject-object interaction classification, the relative importance determined based on the image features.
</claims>
</document>
