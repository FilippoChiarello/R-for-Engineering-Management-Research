<document>

<filing_date>
2019-10-24
</filing_date>

<publication_date>
2020-08-06
</publication_date>

<priority_date>
2019-02-01
</priority_date>

<ipc_classes>
G06F16/55
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
CHEN YITING
DUERIG, THOMAS J.
GAO, YAXI
JUAN, DA-CHENG
LI ZHEN
LU, CHUN-TA
PENG, FUTANG
RAVI, SUJITH
TIMOFEEV, ALEKSEI
TOMKINS, ANDREW
</inventors>

<docdb_family_id>
68610304
</docdb_family_id>

<title>
TRAINING IMAGE AND TEXT EMBEDDING MODELS
</title>

<abstract>
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training an image embedding model. In one aspect, a method comprises: obtaining training data comprising a plurality of training examples, wherein each training example comprises: an image pair comprising a first image and a second image; and selection data indicating one or more of: (i) a co-click rate of the image pair, and (ii) a similar-image click rate of the image pair; and using the training data to train an image embedding model having a plurality of image embedding model parameters.
</abstract>

<claims>
What is claimed is:
1. A method performed by one or more data processing apparatus, the method comprising:
obtaining training data comprising a plurality of training examples, wherein each training example comprises:
an image pair comprising a first image and a second image; and selection data indicating one or more of: (i) a co-click rate of the image pair, and (ii) a similar-image click rate of the image pair, wherein:
the co-click rate of the image pair characterizes how often users select both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query; and
the similar-image click rate of the image pair characterizes how often users select either the first image or the second image in response to the first image or the second image being identified by a search result for a search query respectively comprising either the second image or the first image; and
using the training data to train an image embedding model having a plurality of image embedding model parameters, wherein the training comprises, for each of the plurality of training examples:
processing the first image of the training example using the image embedding model to generate an embedding of the first image;
processing the second image of the training example using the image embedding model to generate an embedding of the second image;
determining a measure of similarity between the embedding of the first image and the embedding of the second image; and
adjusting the image embedding model parameters based at least in part on: (i) the measure of similarity between the embedding of the first image and the embedding of the second image, and (ii) the selection data of the training example.
2. The method of claim 1, wherein the training data is generated using a historical query log of a web search system.
3. The method of claim 1 or 2, wherein the co-click rate for each training example indicates a fraction of times users selected both the first image and the second image of the training example in response to both the first image and the second image of the training example being concurrently identified by search results for a search query.
4. The method of claim 1, 2 or 3, wherein the similar-image click rate for each training example indicates a fraction of times users selected either the first image or the second image in response to the first image or the second image being identified by a search result for a search query respectively comprising either the second image or the first image.
5. The method of claim 1, 2, 3 or 4, wherein the image embedding model comprises a convolutional neural network.
6. The method of claim 5, wherein adjusting the image embedding model parameters comprises:
determining a gradient of a loss function that depends on: (i) the measure of similarity between the embedding of the first image and the embedding of the second image, and (ii) the selection data of the training example; and
using the gradient to adjust the image embedding model parameters.
7. The method of claim 6, wherein the loss function comprises a multiplicative scaling factor that is determined based on the selection data of the training example.
8. The method of claim 7, wherein the multiplicative scaling factor is determined as a linear combination of the co-click rate and the similar-image click rate of the training example.
9. The method of any preceding claim, wherein determining a measure of similarity between the embedding of the first image and the embedding of the second image comprises: determining a Euclidean distance between the embedding of the first image and the embedding of the second image.
10. A system comprising:
one or more computers; and one or more storage devices communicatively coupled to the one or more computers, wherein the one or more storage devices store instructions that, when executed by the one or more computers, cause the one or more computers to perform operations comprising:
obtaining training data comprising a plurality of training examples, wherein each training example comprises:
an image pair comprising a first image and a second image; and selection data indicating one or more of: (i) a co-click rate of the image pair, and (ii) a similar-image click rate of the image pair, wherein:
the co-click rate of the image pair characterizes how often users select both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query; and
the similar-image click rate of the image pair characterizes how often users select either the first image or the second image in response to the first image or the second image being identified by a search result for a search query respectively comprising either the second image or the first image; and
using the training data to train an image embedding model having a plurality of image embedding model parameters, wherein the training comprises, for each of the plurality of training examples:
processing the first image of the training example using the image embedding model to generate an embedding of the first image;
processing the second image of the training example using the image embedding model to generate an embedding of the second image;
determining a measure of similarity between the embedding of the first image and the embedding of the second image; and
adjusting the image embedding model parameters based at least in part on: (i) the measure of similarity between the embedding of the first image and the embedding of the second image, and (ii) the selection data of the training example.
11. The system of claim 10, wherein the training data is generated using a historical query log of a web search system.
12. The system of claim 10 or 11, wherein the co-click rate for each training example indicates a fraction of times users selected both the first image and the second image of the training example in response to both the first image and the second image of the training example being concurrently identified by search results for a search query.
13. The system of claim 10, 11 or 12, wherein the similar-image click rate for each training example indicates a fraction of times users selected either the first image or the second image in response to the first image or the second image being identified by a search result for a search query respectively comprising either the second image or the first image.
14. The system of any one of claims 10 to 14, wherein the image embedding model comprises a convolutional neural network.
15. The system of claim 14, wherein adjusting the image embedding model parameters comprises:
determining a gradient of a loss function that depends on: (i) the measure of similarity between the embedding of the first image and the embedding of the second image, and (ii) the selection data of the training example; and
using the gradient to adjust the image embedding model parameters.
16. One or more non-transitory computer storage media storing instructions that when executed by one or more computers cause the one or more computers to perform operations comprising:
obtaining training data comprising a plurality of training examples, wherein each training example comprises:
an image pair comprising a first image and a second image; and selection data indicating one or more of: (i) a co-click rate of the image pair, and (ii) a similar-image click rate of the image pair, wherein:
the co-click rate of the image pair characterizes how often users select both the first image and the second image in response to both the first image and the second image being concurrently identified by search results for a search query; and
the similar-image click rate of the image pair characterizes how often users select either the first image or the second image in response to the first image or the second image being identified by a search result for a search query respectively comprising either the second image or the first image; and
using the training data to train an image embedding model having a plurality of image embedding model parameters, wherein the training comprises, for each of the plurality of training examples:
processing the first image of the training example using the image embedding model to generate an embedding of the first image;
processing the second image of the training example using the image embedding model to generate an embedding of the second image;
determining a measure of similarity between the embedding of the first image and the embedding of the second image; and
adjusting the image embedding model parameters based at least in part on: (i) the measure of similarity between the embedding of the first image and the embedding of the second image, and (ii) the selection data of the training example.
17. The non-transitory computer storage media of claim 16, wherein the training data is generated using a historical query log of a web search system.
18. The non-transitory computer storage media of claim 16 or 17, wherein the co-click rate for each training example indicates a fraction of times users selected both the first image and the second image of the training example in response to both the first image and the second image of the training example being concurrently identified by search results for a search query.
19. The non-transitory computer storage media of claim 16, 17 or 18, wherein the similarimage click rate for each training example indicates a fraction of times users selected either the first image or the second image in response to the first image or the second image being identified by a search result for a search query respectively comprising either the second image or the first image.
20. The non-transitory computer storage media of any one of claims 16 to 19, wherein the image embedding model comprises a convolutional neural network.
</claims>
</document>
