<document>

<filing_date>
2019-12-23
</filing_date>

<publication_date>
2020-07-16
</publication_date>

<priority_date>
2019-01-11
</priority_date>

<ipc_classes>
G01S17/894,G06K9/00,G06K9/20,G06K9/34,G06K9/62,G06T7/10,H04N5/222
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
BLEYER, MICHAEL
EDMONDS, CHRISTOPHER DOUGLAS
FENTON, MICHAEL SCOTT
FINOCCHIO, MARK JAMES
JUDNICH, JOHN ALBERT
WOOD, ERROLL WILLIAM
</inventors>

<docdb_family_id>
71516761
</docdb_family_id>

<title>
SUBJECT TRACKING WITH ALIASED TIME-OF-FLIGHT DATA
</title>

<abstract>
A method to identify one or more depth-image segments that correspond to a predetermined object type is enacted in a depth-imaging controller operatively coupled to an optical time-of-flight (ToF) camera; it comprises: receiving depth-image data from the optical ToF camera, the depth-image data exhibiting an aliasing uncertainty, such that a coordinate (X, Y) of the depth-image data maps to a periodic series of depth values {Zk}; and labeling, as corresponding to the object type, one or more coordinates of the depth- image data exhibiting the aliasing uncertainty.
</abstract>

<claims>
1. Enacted in a depth-imaging controller operatively coupled to an optical time-of-flight (ToF) camera, a method to identify one or more depth-image segments that correspond to a predetermined object type, the method comprising:
receiving depth-image data from the optical ToF camera, the depth-image data exhibiting an aliasing uncertainty, such that a coordinate (X, Y) of the depth-image data maps to a periodic series of depth values {Zk}; and
labeling, as corresponding to the object type, one or more coordinates of the depth-image data exhibiting the aliasing uncertainty.
2. The method of claim 1 wherein the depth-image data includes a series of raw shutters acquired by the optical ToF camera at an invariant modulation frequency.
3. The method of claim 1 wherein the optical ToF camera is a phase-based optical ToF camera operated at a modulation frequency f, and wherein adjacent elements of the periodic series of depth values differ by a phase wrapping of c / (2 f), where c is the speed of light in air.
4. The method of claim 1 wherein the object type is a foreground object type, and wherein the labeling includes differentiating coordinates corresponding to the foreground object type from coordinates corresponding to one or more background object types.
5. The method of claim 1 wherein the object type corresponds to a human hand.
6. The method of claim 1 wherein the one or more labeled coordinates define one or more depth-image segments, and wherein the depth-image data comprises one of a sequence of depth-video frames, the method further comprising tracking the one or more depth-image segments through the sequence of depth-video frames.
7. The method of claim 1 wherein the labeling is enacted in a segmentation engine trained by machine learning.
8. The method of claim 7 wherein the segmentation engine includes a convolutional neural network.
9. The method of claim 7 wherein the segmentation engine is trained to replicate, onto an aliased depth image of a training subject, a ground-truth segmentation label derived from a corresponding, externally labeled, de-aliased depth image of the training subject at corresponding coordinates, and wherein the aliased depth image and the ground-truth segmentation label are provided as training data to the segmentation engine.
10. The method of claim 7 wherein the segmentation engine is trained to attach, onto coordinates of a simulated aliased depth image of a training subject, a segmentation label consistent with a ground-truth segmentation label provided for those coordinates, and wherein the simulated aliased depth image and the ground-truth segmentation label are provided as training data to the segmentation engine.
11. The method of claim 1 wherein the depth-image data is acquired subsequent to a keyframe of a depth-video acquisition, the method further comprising:
operating the optical ToF camera at a series of modulation frequencies during the keyframe; receiving depth-image data acquired by the optical ToF camera during the keyframe;
processing cooperatively the depth-image data acquired by the optical ToF camera during the keyframe to return a de-aliased depth-image in which the aliasing uncertainty is resolved;
labeling, as corresponding to the object type, one or more coordinates of the de-aliased depth-image; and
operating the optical ToF camera at an invariant modulation frequency subsequent to the keyframe,
wherein labeling the one or more coordinates of the depth-image data acquired subsequent to the keyframe is further based on the labeled coordinates of the de-aliased depthimage.
12. The method of claim 1 wherein the depth-image data comprises an aliased depth image derived from a series of raw shutters acquired by the optical ToF camera at an invariant modulation frequency.
13. The method of claim 12 wherein the aliased depth image is one of a sequence of aliased depth images each derived from a corresponding series of raw shutters acquired by the optical ToF camera operating at a modulation frequency, the method further comprising varying the modulation frequency across the series of aliased depth images.
14. A depth-imaging controller operatively coupled to an optical time-of-flight (ToF) camera, the depth-imaging controller comprising:
a shutter-acquisition engine configured to cause the optical ToF camera to acquire a series of raw shutters at an acquisition rate;
a segmentation engine configured to output a series of segmented depth images at a segmentation rate, wherein one or more coordinates of each segmented depth image are labeled as corresponding to an object type,
and wherein the acquisition rate is less than six times the segmentation rate.
15. The depth-imaging controller of claim 14 wherein the acquisition rate is one third the segmentation rate.
</claims>
</document>
