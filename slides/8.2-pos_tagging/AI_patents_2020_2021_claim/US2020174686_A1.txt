<document>

<filing_date>
2019-09-05
</filing_date>

<publication_date>
2020-06-04
</publication_date>

<priority_date>
2018-12-04
</priority_date>

<ipc_classes>
G06F3/06,G06N3/04
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
SONG, JOONHO
</inventors>

<docdb_family_id>
70850124
</docdb_family_id>

<title>
METHOD AND APPARATUS FOR ALLOCATING MEMORY SPACE FOR DRIVING NEURAL NETWORK
</title>

<abstract>
A method of allocating a memory for driving a neural network including obtaining first capacity information of a space to store an input feature map of a first layer from among the layers of the neural network, and second capacity information of a space to store an output feature map of the first layer, and allocating a first storage space to store the input feature map in the memory based on an initial address value of the memory and the first capacity information and a second storage space to store the output feature map in the memory based on a last address value of the memory and the second capacity information.
</abstract>

<claims>
1. A method of allocating a memory to layers of a neural network, the method comprising: obtaining first capacity information of a space to store an input feature map of a first layer from among the layers of the neural network, and second capacity information of a space to store an output feature map of the first layer; and allocating a first storage space to store the input feature map in the memory based on an initial address value of the memory and the first capacity information and a second storage space to store the output feature map in the memory based on a last address value of the memory and the second capacity information.
2. The method of claim 1, further comprising: allocating a space to store an input feature map of a second layer subsequent to the first layer, to the second storage space; obtaining third capacity information of a space to store an output feature map of the second layer; and allocating a third storage space to store the output feature map of the second layer in the memory based on the initial address value of the memory and the third capacity information.
3. The method of claim 1, wherein the first storage space corresponds to a space from the initial address value to a first address value of the memory, and the second storage space corresponds to a space from a second address value to the last address value of the memory.
4. The method of claim 1, wherein: the obtaining further comprises obtaining fourth capacity information of a space to store a weight map for an operation with the input feature map, and the allocating further comprises allocating a space to store the weight map between the first storage space and the second storage space based on the fourth capacity information.
5. The method of claim 1, further comprising: dividing a weight map of the first layer into sub-weight maps; dividing the first layer into sublayers and respectively allocating the sub-weight maps to the sublayers; obtaining sub-capacity information of spaces to respectively store the sub-weight maps; and allocating, to each of the sublayers, a space to store a sub-weight map of the sub-weight maps based on the respective sub-capacity information of the sub-weight map, between the first storage space and the second storage space.
6. The method of claim 5, wherein channels of the output feature map are respectively generated from an operation between each of the sub-weight maps and the input feature map, and sequentially storing the channels of the output feature map in the second storage space.
7. The method of claim 1, further comprising: selecting an input tile in the input feature map of the first layer; obtaining capacity information about a capacity to respectively store the input tile, an output tile corresponding to the input tile, and a weight map of the first layer; and allocating spaces to respectively store the input tile, the output tile, and the weight map in the memory based on the capacity information.
8. The method of claim 1, wherein the memory is located in a processor of a device driving the neural network.
9. A non-transitory computer-readable storage medium storing instructions that, when executed by a processor, cause the processor to perform the method of claim 1.
10. A neural network device comprising: a memory; and a processor configured to drive to execute instructions to drive a neural network to: obtain first capacity information of a space to store an input feature map of a first layer from among layers of the neural network, and second capacity information of a space to store an output feature map of the first layer, and allocate a first storage space to store the input feature map in the memory based on an initial address value of the memory and the first capacity information and a second storage space to store the output feature map in the memory based on a last address value of the memory and the second capacity information.
11. The neural network device of claim 10, wherein the processor is further configured to: allocate a space to store an input feature map of a second layer subsequent to the first layer, to the second storage space; obtain third capacity information of a space to store an output feature map of the second layer; and allocate a third storage space to store the output feature map of the second layer in the memory based on the initial address value of the memory and the third capacity information.
12. The neural network device of claim 10, wherein the first storage space corresponds to a space from the initial address value to a first address value of the memory, and the second storage space corresponds to a space from a second address value to the last address value of the memory.
13. The neural network device of claim 10, wherein the processor is further configured to: obtain fourth capacity information of a space to store a weight map for an operation with the input feature map; and allocate a space to store the weight map between the first storage space and the second storage space based on the fourth capacity information.
14. The neural network device of claim 10, wherein the processor is further configured to: divide a weight map of the first layer into sub-weight maps, divide the first layer into sublayers, and respectively allocate the sub-weight maps to the sublayers; obtain sub-capacity information of spaces to respectively store the sub-weight maps; and allocate, to each of the sublayers, a space to store a sub-weight map of the sub-weight maps based on the respective sub-capacity information of the sub-weight map, between the first storage space and the second storage space.
15. The neural network device of claim 14, wherein the processor is further configured to: generate channels of the output feature map from an operation between each of the sub-weight maps and the input feature map; and sequentially store each of the channels of the output feature map.
16. The neural network device of claim 10, wherein the processor is further configured to: select an input tile in the input feature map of the first layer; obtain capacity information about a capacity to respectively store the input tile, an output tile corresponding to the input tile, and the weight map of the first layer; and allocate spaces to respectively store the input tile, the output tile, and the weight map in the memory based on the capacity information.
17. The neural network device of claim 10, wherein the memory is located in the processor.
</claims>
</document>
