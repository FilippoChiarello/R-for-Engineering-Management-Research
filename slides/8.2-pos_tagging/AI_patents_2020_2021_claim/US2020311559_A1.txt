<document>

<filing_date>
2020-06-15
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2017-06-20
</priority_date>

<ipc_classes>
G06N20/00,G06N5/00,G06N5/04
</ipc_classes>

<assignee>
BANSAL, RAJESH
CHATTOPADHYAY, RITA
Ma, Yuming
Ganguli, Mrittika
</assignee>

<inventors>
BANSAL, RAJESH
CHATTOPADHYAY, RITA
Ma, Yuming
Ganguli, Mrittika
</inventors>

<docdb_family_id>
72606438
</docdb_family_id>

<title>
OPTIMIZED DECISION TREE MACHINE LEARNING FOR RESOURCE-CONSTRAINED DEVICES
</title>

<abstract>
In one embodiment, an edge computing device for performing decision tree training and inference includes interface circuitry and processing circuitry. The interface circuitry receives training data and inference data that is captured, at least partially, by sensor(s). The training data corresponds to a plurality of labeled instances of a feature set, and the inference data corresponds to an unlabeled instance of the feature set. The processing circuitry: computes a set of feature value checkpoints that indicate, for each feature of the feature set, a subset of potential feature values to be evaluated for splitting tree nodes of a decision tree model; trains the decision tree model based on the training data and the set of feature value checkpoints; and performs inference using the decision tree model to predict a target variable for the unlabeled instance of the feature set.
</abstract>

<claims>
1. An edge computing device for performing decision tree training and inference, comprising: interface circuitry to: receive training data corresponding to a plurality of labeled instances of a feature set, wherein the training data is captured at least partially by one or more sensors; and receive inference data corresponding to an unlabeled instance of the feature set, wherein the inference data is captured at least partially by the one or more sensors; and processing circuitry to: compute, based on the training data, a set of feature value checkpoints for training a decision tree model, wherein the set of feature value checkpoints is to indicate, for each feature of the feature set, a subset of potential feature values to be evaluated for splitting tree nodes of the decision tree model; train the decision tree model based on the training data and the set of feature value checkpoints, wherein the decision tree model is to be trained to predict a target variable corresponding to the feature set; and perform inference using the decision tree model to predict the target variable for the unlabeled instance of the feature set.
2. The edge computing device of claim 1, wherein: the decision tree model is to be trained to predict failures associated with the edge computing device; and the target variable is to indicate whether a failure is predicted for the edge computing device.
3. The edge computing device of claim 1, wherein the processing circuitry to compute, based on the training data, the set of feature value checkpoints for training the decision tree model is further to: for each feature of the feature set: determine an optimal bin size for binning a set of feature values contained in the training data for a corresponding feature of the feature set; bin the set of feature values into a plurality of bins based on the optimal bin size; and identify feature value checkpoints for the corresponding feature based on the plurality of bins.
4. The edge computing device of claim 3, wherein the processing circuitry to determine the optimal bin size for binning the set of feature values contained in the training data for the corresponding feature of the feature set is further to: identify a plurality of possible bin sizes for binning the set of feature values; compute a plurality of performance costs for the plurality of possible bin sizes; and select the optimal bin size from the plurality of possible bin sizes, wherein the optimal bin size corresponds to a lowest performance cost of the plurality of performance costs.
5. The edge computing device of claim 1, wherein: the decision tree model comprises a random forest model, wherein the random forest model comprises a plurality of decision trees; and the processing circuitry to train the decision tree model based on the training data and the set of feature value checkpoints is further to: generate the plurality of decision trees for the random forest model based on the training data and the set of feature value checkpoints.
6. The edge computing device of claim 5, wherein the processing circuitry to generate the plurality of decision trees for the random forest model based on the training data and the set of feature value checkpoints is further to: extract, from the training data, a random training sample for generating a first decision tree of the plurality of decision trees; generate a root node for the first decision tree based on the random training sample; select, from the feature set, a random subset of features to be evaluated for splitting the root node; obtain, from the set of feature value checkpoints, a subset of feature value checkpoints for the random subset of features; compute a plurality of impurity values for the subset of feature value checkpoints; select, from the subset of feature value checkpoints, a corresponding feature value for splitting the root node, wherein the corresponding feature value is selected based on the plurality of impurity values; and split the root node into a set of child nodes based on the corresponding feature value.
7. The edge computing device of claim 6, wherein the plurality of impurity values comprises a plurality of Gini indexes.
8. The edge computing device of claim 1, wherein the processing circuitry comprises: a host processor to: compute, based on the training data, the set of feature value checkpoints for training the decision tree model; and an artificial intelligence accelerator to: train the decision tree model based on the training data and the set of feature value checkpoints; and perform inference using the decision tree model to predict the target variable for the unlabeled instance of the feature set.
9. The edge computing device of claim 8, wherein the artificial intelligence accelerator is implemented on a field-programmable gate array of the edge computing device.
10. An artificial intelligence accelerator to perform decision tree training and inference for a host processor, comprising: a host interface to communicate with the host processor; and processing circuitry to: receive, from the host processor via the host interface, training data corresponding to a plurality of labeled instances of a feature set, wherein the training data is captured at least partially by one or more sensors; receive, from the host processor via the host interface, a set of feature value checkpoints corresponding to the training data, wherein the set of feature value checkpoints is for training a decision tree model, and wherein the set of feature value checkpoints is to indicate, for each feature of the feature set, a subset of potential feature values to be evaluated for splitting tree nodes of the decision tree model; train the decision tree model based on the training data and the set of feature value checkpoints, wherein the decision tree model is to be trained to predict a target variable corresponding to the feature set; receive, from the host processor via the host interface, inference data corresponding to an unlabeled instance of the feature set, wherein the inference data is captured at least partially by the one or more sensors; perform inference using the decision tree model to generate a predicted value of the target variable for the unlabeled instance of the feature set; and send, to the host processor via the host interface, the predicted value of the target variable for the unlabeled instance of the feature set.
11. The artificial intelligence accelerator of claim 10, wherein: the decision tree model is to be trained to predict failures associated with an edge computing device; and the target variable is to indicate whether a failure is predicted for the edge computing device.
12. The artificial intelligence accelerator of claim 10, wherein the set of feature value checkpoints is computed by the host processor based on binning a set of feature values for each feature of the feature set using an optimal bin size.
13. The artificial intelligence accelerator of claim 10, wherein: the decision tree model comprises a random forest model, wherein the random forest model comprises a plurality of decision trees; and the processing circuitry to train the decision tree model based on the training data and the set of feature value checkpoints is further to: generate the plurality of decision trees for the random forest model based on the training data and the set of feature value checkpoints.
14. The artificial intelligence accelerator of claim 13, wherein the processing circuitry to generate the plurality of decision trees for the random forest model based on the training data and the set of feature value checkpoints is further to: extract, from the training data, a random training sample for generating a first decision tree of the plurality of decision trees; generate a root node for the first decision tree based on the random training sample; select, from the feature set, a random subset of features to be evaluated for splitting the root node; obtain, from the set of feature value checkpoints, a subset of feature value checkpoints for the random subset of features; compute a plurality of impurity values for the subset of feature value checkpoints; select, from the subset of feature value checkpoints, a corresponding feature value for splitting the root node, wherein the corresponding feature value is selected based on the plurality of impurity values; and split the root node into a set of child nodes based on the corresponding feature value.
15. The artificial intelligence accelerator of claim 14, wherein the plurality of impurity values comprises a plurality of Gini indexes.
16. At least one non-transitory machine-readable storage medium having instructions stored thereon, wherein the instructions, when executed on processing circuitry, cause the processing circuitry to: receive, via interface circuitry, training data corresponding to a plurality of labeled instances of a feature set, wherein the training data is captured at least partially by one or more sensors; compute, based on the training data, a set of feature value checkpoints for training a decision tree model, wherein the set of feature value checkpoints is to indicate, for each feature of the feature set, a subset of potential feature values to be evaluated for splitting tree nodes of the decision tree model; train the decision tree model based on the training data and the set of feature value checkpoints, wherein the decision tree model is to be trained to predict a target variable corresponding to the feature set; receive, via the interface circuitry, inference data corresponding to an unlabeled instance of the feature set, wherein the inference data is captured at least partially by the one or more sensors; and perform inference using the decision tree model to predict the target variable for the unlabeled instance of the feature set.
17. The storage medium of claim 16, wherein: the decision tree model is to be trained to predict failures associated with an edge computing device; and the target variable is to indicate whether a failure is predicted for the edge computing device.
18. The storage medium of claim 16, wherein the instructions that cause the processing circuitry to compute, based on the training data, the set of feature value checkpoints for training the decision tree model further cause the processing circuitry to: for each feature of the feature set: determine an optimal bin size for binning a set of feature values contained in the training data for a corresponding feature of the feature set; bin the set of feature values into a plurality of bins based on the optimal bin size; and identify feature value checkpoints for the corresponding feature based on the plurality of bins.
19. The storage medium of claim 18, wherein the instructions that cause the processing circuitry to determine the optimal bin size for binning the set of feature values contained in the training data for the corresponding feature of the feature set further cause the processing circuitry to: identify a plurality of possible bin sizes for binning the set of feature values; compute a plurality of performance costs for the plurality of possible bin sizes; and select the optimal bin size from the plurality of possible bin sizes, wherein the optimal bin size corresponds to a lowest performance cost of the plurality of performance costs.
20. The storage medium of claim 16, wherein: the decision tree model comprises a random forest model, wherein the random forest model comprises a plurality of decision trees; and the instructions that cause the processing circuitry to train the decision tree model based on the training data and the set of feature value checkpoints further cause the processing circuitry to: generate the plurality of decision trees for the random forest model based on the training data and the set of feature value checkpoints.
21. The storage medium of claim 20, wherein the instructions that cause the processing circuitry to generate the plurality of decision trees for the random forest model based on the training data and the set of feature value checkpoints further cause the processing circuitry to: extract, from the training data, a random training sample for generating a first decision tree of the plurality of decision trees; generate a root node for the first decision tree based on the random training sample; select, from the feature set, a random subset of features to be evaluated for splitting the root node; obtain, from the set of feature value checkpoints, a subset of feature value checkpoints for the random subset of features; compute a plurality of impurity values for the subset of feature value checkpoints; select, from the subset of feature value checkpoints, a corresponding feature value for splitting the root node, wherein the corresponding feature value is selected based on the plurality of impurity values; and split the root node into a set of child nodes based on the corresponding feature value.
22. The storage medium of claim 21, wherein the plurality of impurity values comprises a plurality of Gini indexes.
23. A method of performing decision tree training and inference on an edge computing device, comprising: receiving, via interface circuitry, training data corresponding to a plurality of labeled instances of a feature set, wherein the training data is captured at least partially by one or more sensors; computing, based on the training data, a set of feature value checkpoints for training a decision tree model, wherein the set of feature value checkpoints is to indicate, for each feature of the feature set, a subset of potential feature values to be evaluated for splitting tree nodes of the decision tree model; training the decision tree model based on the training data and the set of feature value checkpoints, wherein the decision tree model is to be trained to predict a target variable corresponding to the feature set; receiving, via the interface circuitry, inference data corresponding to an unlabeled instance of the feature set, wherein the inference data is captured at least partially by the one or more sensors; and performing inference using the decision tree model to predict the target variable for the unlabeled instance of the feature set.
24. The method of claim 23, wherein computing, based on the training data, the set of feature value checkpoints for training the decision tree model comprises: for each feature of the feature set: determining an optimal bin size for binning a set of feature values contained in the training data for a corresponding feature of the feature set; binning the set of feature values into a plurality of bins based on the optimal bin size; and identifying feature value checkpoints for the corresponding feature based on the plurality of bins.
25. The method of claim 24, wherein determining the optimal bin size for binning the set of feature values contained in the training data for the corresponding feature of the feature set comprises: identifying a plurality of possible bin sizes for binning the set of feature values; computing a plurality of performance costs for the plurality of possible bin sizes; and selecting the optimal bin size from the plurality of possible bin sizes, wherein the optimal bin size corresponds to a lowest performance cost of the plurality of performance costs.
</claims>
</document>
