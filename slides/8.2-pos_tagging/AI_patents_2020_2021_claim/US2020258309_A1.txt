<document>

<filing_date>
2020-04-28
</filing_date>

<publication_date>
2020-08-13
</publication_date>

<priority_date>
2019-01-22
</priority_date>

<ipc_classes>
G06T19/00,G06T7/70
</ipc_classes>

<assignee>
FYUSION COMPANY
</assignee>

<inventors>
RUSU, RADU BOGDAN
HOLZER, STEFAN JOHANNES JOSEF
TREVOR, ALEXANDER JAY BRUEN
MUNARO, MATTEO
LIAUDANSKAS, AIDAS
</inventors>

<docdb_family_id>
71946111
</docdb_family_id>

<title>
LIVE IN-CAMERA OVERLAYS
</title>

<abstract>
A live camera feed may be analyzed to determine the identify of an object, and augmented reality overlay data may be determined based on that identity. The overlay data may include one or more tags that are each associated with a respective location on the object. The live camera feed may be presented on a display screen with the tags being positioned as the respective location.
</abstract>

<claims>
1. A method comprising: determining via a processor at a computing device an object identity for an object represented in a live camera feed captured at a camera at the computing device; determining via the processor augmented reality overlay data based on the object identity, the augmented reality overlay data include one or more tags, each of the tags characterizing a feature of the object, each of the tags being associated with a respective location on the object, each of the respective locations being represented in a reference view of the object; determining, for each of a plurality of frames in the live camera feed, a respective frame location for one or more of the tags, each of the respective frame locations determined based on a correspondence between the reference view of the object and the respective frame; and presenting the live camera feed on a display screen, the live camera feed including the plurality of frames, each of the plurality of frames including a respective one of the tags, each of the tags being positioned at the respective frame location.
2. The method recited in claim 1, the method further comprising: for each of the frames, determining the correspondence between the reference view of the object and the respective frame.
3. The method recited in claim 1, wherein the live camera feed is divided into an initialization phase and a presentation phase, the initialization phase preceding the presentation phase.
4. The method recited in claim 3, wherein the initialization phase involves projecting one or more triangulated points into a designated frame.
5. The method recited in claim 4, wherein the projection is performed based on camera pose information determined based on data collected from an inertial measurement unit at the computing device.
6. The method recited in claim 3, wherein the presentation phase involves triangulating a three-dimensional representation of the object for each of the frames.
7. The method recited in claim 6, wherein the three-dimensional representation is triangulated based on the correspondence between the reference view and the respective frame.
8. The method recited in claim 1, wherein the reference view of the object is a multi-view interactive digital media representation, the multi-view interactive digital media representation including a plurality of images of the object, each of the images of the object being captured from a different perspective view.
9. The method recited in claim 8, wherein the multi-view interactive digital media representation is navigable in one or more dimensions.
10. The method recited in claim 9, the method further comprising: generating the multi-view interactive digital media representation via the processor.
11. The method recited in claim 10, determining a three-dimensional model of the object based on the multi-view interactive digital media representation.
12. The method recited in claim 1, wherein the object is a vehicle, and wherein the reference view of the object includes each of a left vehicle door, a right vehicle door, and a windshield.
13. A computing device comprising: a camera configured to capture a live camera feed of an object; a processor configured to: determine an object identity for an object represented the live camera feed, determine augmented reality overlay data based on the object identity, the augmented reality overlay data include one or more tags, each of the tags characterizing a feature of the object, each of the tags being associated with a respective location on the object, each of the respective locations being represented in a reference view of the object, and determine, for each of a plurality of frames in the live camera feed, a respective frame location for one or more of the tags, each of the respective frame locations determined based on a correspondence between the reference view of the object and the respective frame; and a display screen configured to present the live camera feed including the plurality of frames, each of the plurality of frames including a respective one of the tags, each of the tags being positioned at the respective frame location.
14. The computing device recited in claim 13, the method further comprising: for each of the frames, determining the correspondence between the reference view of the object and the respective frame.
15. The computing device recited in claim 13, wherein the live camera feed is divided into an initialization phase and a presentation phase, the initialization phase preceding the presentation phase.
16. The computing device recited in claim 15, wherein the initialization phase involves projecting one or more triangulated points into a designated frame, and wherein the projection is performed based on camera pose information determined based on data collected from an inertial measurement unit at the computing device.
17. The computing device recited in claim 15, wherein the presentation phase involves triangulating a three-dimensional representation of the object for each of the frames, wherein the three-dimensional representation is triangulated based on the correspondence between the reference view and the respective frame.
18. The computing device recited in claim 13, wherein the reference view of the object is a multi-view interactive digital media representation, the multi-view interactive digital media representation including a plurality of images of the object, each of the images of the object being captured from a different perspective view.
19. The computing device recited in claim 18, wherein the multi-view interactive digital media representation is navigable in one or more dimensions, the method further comprising: generating the multi-view interactive digital media representation via the processor.
20. One or more non-transitory computer readable media having instructions stored thereon for performing a method, the method further comprising: determining via a processor at a computing device an object identity for an object represented in a live camera feed captured at a camera at the computing device; determining via the processor augmented reality overlay data based on the object identity, the augmented reality overlay data include one or more tags, each of the tags characterizing a feature of the object, each of the tags being associated with a respective location on the object, each of the respective locations being represented in a reference view of the object; determining, for each of a plurality of frames in the live camera feed, a respective frame location for one or more of the tags, each of the respective frame locations determined based on a correspondence between the reference view of the object and the respective frame; and presenting the live camera feed on a display screen, the live camera feed including the plurality of frames, each of the plurality of frames including a respective one of the tags, each of the tags being positioned at the respective frame location.
</claims>
</document>
