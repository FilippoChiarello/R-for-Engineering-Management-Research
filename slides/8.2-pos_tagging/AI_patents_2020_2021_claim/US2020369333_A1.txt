<document>

<filing_date>
2019-05-24
</filing_date>

<publication_date>
2020-11-26
</publication_date>

<priority_date>
2019-05-24
</priority_date>

<ipc_classes>
B25J11/00,B62D57/032
</ipc_classes>

<assignee>
DISNEY ENTERPRISES
</assignee>

<inventors>
DICKINSON, DEXTER J.
BISHOP, JARED EDWARD
CESARE, KYLE MICHAEL
REES, JERRY W.
THOMPSON, KYLE ROBERT
HOPKINS, MICHAEL ANTHONY
LAVALLEY, SCOTT CHRISTOPHER
</inventors>

<docdb_family_id>
73457563
</docdb_family_id>

<title>
LEGGED HIGH-DEXTERITY SELF-BALANCING CAPABLE ROBOT ACTOR
</title>

<abstract>
A robot actor, or character mobility hardware platform, adapted to unleash or provide a wide variety of characters in the physical world. The robot actor enables the often screen-constrained characters to become life-like, interactive participants with nearby people in ways not presently achievable. The robot actor is an untethered, free-roaming robot that is has two (or more) legs, is adapted for high dexterity, is controlled and designed to be self-balancing, and, due to this combination of characteristics, the robot can provide characters with an illusion of life and, in many cases, in correct proportion and scale. The hardware and software of the robot actor will become a new generation of animatronic figures by providing a hardware platform capable of continuously evolving to become more capable through advances in controls and artificial intelligence (AI).
</abstract>

<claims>
We claim:
1. A robot for bringing characters to life in the physical world, comprising: movable components; memory storing a set of animations, motion parameters, or scripts defining a set of gestures and a set of movements for each of a plurality of moods or emotions, wherein the set of animations or scripts are generated based on one of the characters; and a controller located onboard the robot, wherein the robot is untethered and free-ranging in a physical space, wherein the controller determines, in real time, a present mood or emotion associated with the robot, wherein the controller selects one of the sets of gestures or one of the sets of movements based on the present mood or emotion, and wherein the controller operates a set of actuators to operate the movable components using the selected one of the sets of gestures or the sets of movements, whereby the robot is operated to provide an illusion of life.
2. The robot of claim 1, further comprising sensors for sensing input from the physical space and wherein the controller processes the input from the physical space to determine the present mood or emotion.
3. The robot of claim 1, wherein the controller operates the sets of actuators by sequentially selecting, in real time, differing ones of the sets of gestures or the sets of movements to provide motion synthesis for the robot while maintaining balance in the physical space.
4. The robot of claim 3, wherein the controller generates, in real time and without external input, transitional movements and control signals for the set of actuators between each sequential pair of the sets of gestures or the sets of movements.
5. The robot of claim 1, wherein the movable components comprise two or more legs each comprising an upper leg housing and a lower leg housing coupled via a knee joint, wherein the upper leg housing includes sidewalls defining a hollow interior space, wherein channels are provided in the upper leg housing for receiving two or more of the actuators, wherein the sidewalls include air inlets for drawing air into the hollow interior space and air outlets for exhausting air out of the hollow interior space, and wherein a stack of spaced apart metal fins are positioned within the hollow interior space that are coupled to sidewalls defining the channels and that define air plenums for air between the air inlets and the air outlets.
6. The robot of claim 5, wherein the sidewalls defining the channels each provides a housing for one of the actuators received in the upper leg housing.
7. The robot of claim 1, wherein the movable components comprise two or more legs each comprising an upper leg housing and a lower leg housing coupled via a knee joint, wherein the upper leg housing includes sidewalls defining a hollow interior space, wherein channels are provided in the upper leg housing for receiving two or more of the actuators including an ankle pitch actuator, and wherein two belts extend from the upper leg housing and the ankle pitch actuator over a pulley in the lower leg housing with a configuration that transmits torque of the ankle pitch actuator across the knee joint to provide assistive torque during stance using reversed gastrocnemius.
8. The robot of claim 1, wherein at least a subset of the actuators each includes a clutch acting as a self-resetting mechanical fuse when transient torques exceed actuator component limits to handle impact events.
9. A method of generating control animations, motion parameters, or scripts for a self-balancing, legged robot, comprising: animating a performance for a legged robot with a predefined hardware configuration; verifying whether the animated performance complies with physical constraints, wherein when the animated performance is noncompliant with the physical constraints the animating is repeated; and running a simulation of the animated performance based on the predefined hardware configuration to determine whether the legged robot balances, wherein the animating is repeated when the legged robot loses balance during the running of the simulation.
10. The method of claim 9, wherein the physical constraints are verified by checking at least one of joint velocity overspeed, foot slip, ROM violations, angular momentum, and COP within a support polygon.
11. The method of claim 9, wherein the animating the performance comprises processing an input path for the legged robot to automatically generate a set of footsteps for the legged robot and further comprises receiving user input modifying the set of footsteps for the legged robot to define the animated performance.
12. The method of claim 9, further including, prior to the running the simulation, automatically correcting a pelvis trajectory of the legged robot, and wherein the method further comprises displaying results of the simulation with the corrected pelvis trajectory, receiving user input indicating whether performance by the legged robot is acceptable, and exporting, in response to the user input indicating an unacceptable performance, the animated performance with the corrected pelvis trajectory.
13. The method of claim 9, further comprising performing a hardware check by operating the legged robot to perform the animated performance, wherein the hardware check includes determining whether the legged robot retains balance and whether generated torques in actuators in the legged robot are acceptable.
14. A robot for bringing characters to life in the physical world, comprising: a pair of movable legs; memory storing authored animation content generated to correspond with a screen-based character; sensors perceiving inputs from a physical environment in which the robot is positioned; and a controller including a behavior engine processing the authored animation content along with the inputs from the sensors to generate motion commands to control motion of the pair of movable legs, wherein the controller performs motion synthesis prior to generating the motion commands, wherein the behavior engine selects and places in sequential order a set of movements in the authored animation content based on the inputs from the sensors, and wherein a set of actuators in the pair of legs operate in response to the motion commands.
15. The robot of claim 14, wherein the motion synthesis includes generating transitional movements between one or more of sequential pairs of the selected and ordered set of movements.
16. The robot of claim 14, wherein the legs each comprise an upper leg housing and a lower leg housing coupled via a knee joint, wherein the upper leg housing includes sidewalls defining a hollow interior space, wherein channels are provided in the upper leg housing for receiving two or more of the actuators, wherein the sidewalls include air inlets for drawing air into the hollow interior space and air outlets for exhausting air out of the hollow interior space, and wherein a stack of spaced apart metal fins are positioned within the hollow interior space that are coupled to sidewalls defining the channels and that define air plenums for air between the air inlets and the air outlets.
17. The robot of claim 16, wherein the sidewalls defining the channels each provides a housing for one of the actuators received in the upper leg housing.
18. The robot of claim 14, wherein the legs each comprise an upper leg housing and a lower leg housing coupled via a knee joint, wherein the upper leg housing includes sidewalls defining a hollow interior space, wherein channels are provided in the upper leg housing for receiving two or more of the actuators including an ankle pitch actuator, and wherein two belts extend from the upper leg housing and the ankle pitch actuator over a pulley in the lower leg housing with a configuration that transmits torque of the ankle pitch actuator across the knee joint to provide assistive torque during stance using reversed gastrocnemius.
19. The robot of claim 14, wherein at least a subset of the actuators each includes a clutch acting as a self-resetting mechanical fuse when transient torques exceed actuator component limits to handle impact events.
</claims>
</document>
