<document>

<filing_date>
2017-04-11
</filing_date>

<publication_date>
2020-03-17
</publication_date>

<priority_date>
2017-04-11
</priority_date>

<ipc_classes>
G01S17/89,G01S17/93,G06K9/46,G06K9/62,G06T19/20,G06T7/11
</ipc_classes>

<assignee>
ZOOX
</assignee>

<inventors>
DOUILLARD, BERTRAND ROBERT
WANG, ZENG
ANGUELOV, DRAGOMIR DIMITROV
DAS, SUBHASIS
</inventors>

<docdb_family_id>
69779077
</docdb_family_id>

<title>
Perspective conversion for multi-dimensional data analysis
</title>

<abstract>
Multi-dimensional data can be mapped to a projection shape and converted for image analysis. In some examples, the multi-dimensional data may include data captured by a LIDAR system for use in conjunction with a perception system for an autonomous vehicle. Converting operations can include converting three-dimensional LIDAR data to multi-channel two-dimensional data. Data points of the multi-dimensional data can be mapped to a projection shape, such as a sphere. Characteristics of the projection shape may include a shape, a field of view, a resolution, and a projection type. After data is mapped to the projection shape, the projection shape can be converted to a multi-channel, two-dimensional image. Image segmentation and classification may be performed on the two-dimensional data. Further, segmentation information may be used to segment the three-dimensional LIDAR data, while a rendering plane may be positioned relative to the segmented data to perform classification on a per-object basis.
</abstract>

<claims>
1. A system comprising: one or more processors; and one or more non-transitory computer readable storage media communicatively coupled to the one or more processors and storing instructions that are executable by the one or more processors to: receive a three-dimensional dataset captured by at least one LIDAR sensor installed on an autonomous vehicle, the at least one LIDAR sensor associated with a first perspective; receive segmentation information associated with the three-dimensional dataset, the segmentation information including, for an individual data point of the three-dimensional dataset, a segmentation identifier associated with a particular object; select a portion of the three-dimensional dataset based at least in part on the segmentation identifier; extract, as extracted data, the portion of the three-dimensional dataset, wherein the extracted data represents a first width and a first height relative to the first perspective; perform a principal component analysis on the extracted data to determine at least a first principal component, a second principal component, and a third principal component of the extracted data; position a rendering plane relative to the extracted data based at least in part on the principal component analysis, wherein an orientation of the rendering plane is associated with a second perspective different than the first perspective, wherein the orientation of the rendering plane is selected such that the extracted data represents a second width and a second height relative to the second perspective, and wherein the second width is greater than the first width; project, as projected data, the extracted data onto the rendering plane, the projected data representing a two-dimensional representation of the extracted data; and perform classification on the projected data to determine an object classification associated with the projected data.
2. The system of claim 1, wherein a classification function performing the classification includes at least one convolutional neural network to determine the object classification associated with the projected data.
3. The system of claim 1, wherein the orientation of the rendering plane is selected to substantially maximize the second width associated with the extracted data relative to the rendering plane.
4. The system of claim 1, wherein the second height is greater than the first height, and wherein the orientation of the rendering plane is selected to substantially maximize the second height associated with the extracted data relative to the rendering plane.
5. The system of claim 1, wherein the segmentation information is associated with a plurality of objects represented in the three-dimensional dataset, and wherein a first rendering plane is positioned relative to a first object of the plurality of objects and a second rendering plane is positioned relative to a second object of the plurality of objects.
6. The system of claim 1, wherein the at least one LIDAR sensor includes a plurality of LIDAR sensors, and wherein the three-dimensional dataset includes LIDAR data fused from data captured by the plurality of LIDAR sensors.
7. The system of claim 1, wherein a size of the rendering plane is based at least in part on an input image size associated with a classification function performing the classification.
8. A method comprising: receiving segmentation information associated with a dataset, the dataset including three-dimensional data captured with respect to a first perspective; extracting, as extracted data, a portion of the dataset, wherein the extracting is based at least in part on the segmentation information; positioning a rendering plane relative to the extracted data, wherein an orientation of the rendering plane is associated with a second perspective; projecting, as projected data, the extracted data onto the rendering plane; and performing classification on the projected data to determine an object classification of an object represented in the projected data.
9. The method of claim 8, wherein the segmentation information includes at least a three-dimensional bounding box, the method further comprising extracting the portion of the dataset bound within the three-dimensional bounding box.
10. The method of claim 8, wherein the segmentation information includes at least a segmentation identifier associated with individual data points of the dataset, the method further comprising: selecting the segmentation identifier from a plurality of segmentation identifiers; and extracting, as the extracted data, the portion of the dataset based at least in part on the segmentation identifier.
11. The method of claim 8, wherein: the extracted data represents a first width and a first height relative to the first perspective; and wherein the orientation of the rendering plane is selected such that the extracted data represents a second width and a second height relative to the second perspective, and wherein the second width is greater than the first width.
12. The method of claim 8, wherein the performing the classification on the projected data includes providing the projected data to a convolutional neural network trained to classify objects represented in two-dimensional data.
13. The method of claim 8, wherein the orientation is a first orientation, the method further comprising: determining a first variance of the dataset projected onto a plane associated with the first perspective, the first variance associated with a second orientation; determining a second variance of the dataset projected onto the rendering plane, the second variance associated with the first orientation; and selecting the first orientation of the rendering plane based at least in part on the second variance being higher than the first variance.
14. The method of claim 8, wherein: the positioning the rendering plane relative to the extracted data includes at least performing a principal component analysis on the extracted data; and the orientation of the rendering plane is based at least in part on the principal component analysis.
15. The method of claim 8, wherein the projecting the extracted data onto the rendering plane converts the portion of the dataset into two-dimensional data.
16. The method of claim 8, wherein the dataset represents at least one of: LIDAR data captured by one or more LIDAR sensors; or simulated LIDAR data generated by a simulator.
17. A system comprising: one or more processors; and one or more non-transitory computer readable storage media communicatively coupled to the one or more processors and storing instructions that are executable by the one or more processors to: receive segmentation information associated with a dataset, the dataset representing three-dimensional data captured with respect to a first perspective; extract, as extracted data, a portion of the dataset, wherein the extracted data is based at least in part on the segmentation information; position a rendering plane relative to the extracted data, wherein an orientation of the rendering plane is associated with a second perspective; project, as projected data, the extracted data onto the rendering plane; and perform classification on the projected data to determine an object classification of an object represented in the projected data.
18. The system of claim 17, wherein the second perspective is different than the first perspective.
19. The system of claim 17, wherein the instructions are further executable by the one or more processors to generate a trajectory for an autonomous vehicle based at least in part on the object represented in the projected data.
20. The system of claim 17, wherein: the extracted data represents a first width and a first height relative to the first perspective; and wherein the orientation of the rendering plane is selected such that the extracted data represents a second width and a second height relative to the second perspective, and wherein the second width is greater than the first width.
</claims>
</document>
