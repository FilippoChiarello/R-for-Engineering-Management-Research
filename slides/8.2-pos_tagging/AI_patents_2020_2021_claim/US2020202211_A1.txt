<document>

<filing_date>
2018-12-29
</filing_date>

<publication_date>
2020-06-25
</publication_date>

<priority_date>
2018-12-25
</priority_date>

<ipc_classes>
G06N3/08
</ipc_classes>

<assignee>
ABBYY PRODUCTION
</assignee>

<inventors>
INDENBOM, EUGENE
ANASTASIEV, DANIIL
</inventors>

<docdb_family_id>
70735124
</docdb_family_id>

<title>
NEURAL NETWORK TRAINING UTILIZING LOSS FUNCTIONS REFLECTING NEIGHBOR TOKEN DEPENDENCIES
</title>

<abstract>
Systems and methods for neural network training utilizing loss functions reflecting neighbor token dependencies. An example method comprises: receiving a training dataset comprising a plurality of labeled tokens; determining, by a neural network, a first tag associated with a current token processed by the neural network, a second tag associated with a previous token which has been processed by the neural network before processing the current token, and a third tag associated with a next token to be processed by the neural network after processing the current token; computing, for the training dataset, a value of a loss function reflecting a first loss value, a second loss value, and a third loss value, wherein the first loss value is represented by a first difference of the first tag and a first label associated with the current token by the training dataset, wherein the second loss value is represented by a second difference of the second tag and a second label associated with the previous token by the training dataset, and wherein the third loss value is represented by a third difference of the third tag and a third label associated with the next token by the training dataset; and adjusting a parameter of the neural network based on the value of the loss function.
</abstract>

<claims>
1. A method, comprising: receiving a training dataset comprising a plurality of labeled tokens; determining, by a neural network, a first tag associated with a current token processed by the neural network, a second tag associated with a previous token which has been processed by the neural network before processing the current token, and a third tag associated with a next token to be processed by the neural network after processing the current token; computing, for the training dataset, a value of a loss function reflecting a first loss value, a second loss value, and a third loss value, wherein the first loss value is represented by a first difference of the first tag and a first label associated with the current token by the training dataset, wherein the second loss value is represented by a second difference of the second tag and a second label associated with the previous token by the training dataset, and wherein the third loss value is represented by a third difference of the third tag and a third label associated with the next token by the training dataset; and adjusting a parameter of the neural network based on the value of the loss function.
2. The method of claim 1, further comprising: performing, using the neural network, a natural language processing task.
3. The method of claim 1, further comprising: repeating the determining, computing, and adjusted operations until the value of the loss function falls below a predetermined threshold.
4. The method of claim 1, wherein the current token is represented by a natural language word, and wherein the first tag identifies a part of speech associated with the current token.
5. The method of claim 1, wherein the current token is represented by a natural language word, and wherein the first tag identifies one or more grammatical attributes associated with the current token.
6. The method of claim 1, wherein determining the first tag further comprises: producing a feature vector representing the first token; producing a vector encoding information about the feature vector by processing the feature vector by a bi-directional long-short term memory (LSTM) layer; and producing the first tag by processing the information encoding vector by a prediction layer.
7. The method of claim 6, wherein the feature vector is represented by a combination of a character-level embedding representing the first token and a grammeme embedding representing the first token.
8. The method of claim 1, wherein the value of the loss function is represented by a linear combination of the first loss value, the second loss value, and the third loss value.
9. The method of claim 1, wherein adjusting the parameter of the neural network further comprises: back-propagating an error reflected by the value of the loss function to one or more previous layers of the neural network; and adjusting an edge weight in order to minimize the loss function.
10. The method of claim 1, wherein the neural network comprises a feature extraction layer, a bi-directional long-short term memory (BiLSTM) layer, and a prediction layer, and wherein the BiLSTM layer further comprises a BiLSTM, a backward LSTM and a forward LSTM, such that a first output of the backward LSTM and a second output of the forward LSTM is fed to the BiLSTM.
11. A method, comprising: receiving a training dataset comprising a plurality of labeled natural language words, wherein each label identifies a part of speech (POS) associated with a respective word; determining, by a neural network, a first tag identifying a first POS associated with a current word processed by the neural network, a second tag identifying a second POS associated with a previous word which has been processed by the neural network before processing the current word, and a third tag identifying a third POS associated with a next word to be processed by the neural network after processing the current word; computing, for the training dataset, a value of a loss function reflecting a first loss value, a second loss value, and a third loss value, wherein the first loss value is represented by a first difference of the first tag and a first label associated with the current word by the training dataset, wherein the second loss value is represented by a second difference of the second tag and a second label associated with the previous word by the training dataset, and wherein the third loss value is represented by a third difference of the third tag and a third label associated with the next word by the training dataset; and adjusting a parameter of the neural network based on the value of the loss function.
12. The method of claim 11, further comprising: performing, using the neural network, a natural language processing task.
13. The method of claim 11, further comprising: repeating the determining, computing, and adjusted operations until the value of the loss function falls below a predetermined threshold.
14. The method of claim 11, wherein determining the first tag further comprises: producing a feature vector representing the first word; producing a vector encoding information about the feature vector by processing the feature vector by a bi-directional long-short term memory (LSTM) layer; and producing the first tag by processing the information encoding vector by a prediction layer.
15. The method of claim 11, wherein the neural network comprises a feature extraction layer, a bi-directional long-short term memory (BiLSTM) layer, and a prediction layer, and wherein the BiLSTM layer further comprises a BiLSTM, a backward LSTM and a forward LSTM, such that a first output of the backward LSTM and a second output of the forward LSTM is fed to the BiLSTM.
16. A computer-readable non-transitory storage medium comprising executable instructions that, when executed by a computer system, cause the computer system to: receive a training dataset comprising a plurality of labeled natural language words, wherein each label identifies a part of speech (POS) associated with a respective word; determine, by a neural network, a first tag identifying a first POS associated with a current word processed by the neural network, a second tag identifying a second POS associated with a previous word which has been processed by the neural network before processing the current word, and a third tag identifying a third POS associated with a next word to be processed by the neural network after processing the current word; compute, for the training dataset, a value of a loss function reflecting a first loss value, a second loss value, and a third loss value, wherein the first loss value is represented by a first difference of the first tag and a first label associated with the current word by the training dataset, wherein the second loss value is represented by a second difference of the second tag and a second label associated with the previous word by the training dataset, and wherein the third loss value is represented by a third difference of the third tag and a third label associated with the next word by the training dataset; and adjust a parameter of the neural network based on the value of the loss function.
17. The computer-readable non-transitory storage medium of claim 16, further comprising executable instructions to cause the computer system to: perform, using the neural network, a natural language processing task.
18. The computer-readable non-transitory storage medium of claim 16, further comprising executable instructions to cause the computer system to: repeating the determining, computing, and adjusted operations until the value of the loss function falls below a predetermined threshold.
19. The computer-readable non-transitory storage medium of claim 16, wherein determining the first tag further comprises: producing a feature vector representing the first word; producing a vector encoding information about the feature vector by processing the feature vector by a bi-directional long-short term memory (LSTM) layer; and producing the first tag by processing the information encoding vector by a prediction layer.
20. The computer-readable non-transitory storage medium of claim 16, wherein the neural network comprises a feature extraction layer, a bi-directional long-short term memory (BiLSTM) layer, and a prediction layer, and wherein the BiLSTM layer further comprises a BiLSTM, a backward LSTM and a forward LSTM, such that a first output of the backward LSTM and a second output of the forward LSTM is fed to the BiLSTM.
</claims>
</document>
