<document>

<filing_date>
2020-08-04
</filing_date>

<publication_date>
2020-11-19
</publication_date>

<priority_date>
2017-11-24
</priority_date>

<ipc_classes>
G06K9/00,G10L25/63
</ipc_classes>

<assignee>
GENESIS LAB COMPANY
</assignee>

<inventors>
LEE, YOUNG-BOK
YOO, DAE-HUN
</inventors>

<docdb_family_id>
66630616
</docdb_family_id>

<title>
MULTI-MODAL EMOTION RECOGNITION DEVICE, METHOD, AND STORAGE MEDIUM USING ARTIFICIAL INTELLIGENCE
</title>

<abstract>
A multi-modal emotion recognition system is disclosed. The system includes a data input unit for receiving video data and voice data of a user, a data pre-processing unit including a voice pre-processing unit for generating voice feature data from the voice data and a video pre-processing unit for generating one or more face feature data from the video data, a preliminary inference unit for generating situation determination data as to whether or not the user's situation changes according to a temporal sequence based on the video data. The system further comprises a main inference unit for generating at least one sub feature map based on the voice feature data or the face feature data, and inferring the user's emotion state based on the sub feature map and the situation determination data.
</abstract>

<claims>
1. (canceled)
2. (canceled)
3. (canceled)
4. (canceled)
5. (canceled)
6. (canceled)
7. A multi-modal emotion recognition device using artificial intelligence, comprising: a data input unit for receiving video data and voice data of a user; a data pre-processing unit comprising a voice pre-processing unit for generating voice feature data from the voice data and a video pre-processing unit for generating one or more face feature data from the video data; a preliminary inference unit for generating situation determination data as to whether or not the user's situation changes according to a temporal sequence based on the video data; and a main inference unit for generating at least one sub feature map based on the voice feature data or the face feature data, and inferring the user's emotion state based on the sub feature map and the situation determination data.
8. The multi-modal emotion recognition device using artificial intelligence of claim 7, wherein the situation determination data comprises conversation determination data as to whether or not the user is in a conversation state or overlapping determination data as to whether or not a tracking target area that is a part of the entire video area of the video data and a recognition target area that is different from the tracking target area overlap with each other, wherein the preliminary inference unit generates the conversation determination data for determining whether or not the user is in a conversation state based on the face feature data, or wherein the preliminary inference unit generates location inference data for inferring the location of the tracking target area based on the video data, and generates the overlapping determination data as to whether or not the tracking target area and the recognition target area overlap with each other based on the face feature data and the location inference data.
9. The multi-modal emotion recognition device using artificial intelligence of claim 8, wherein the preliminary inference unit comprises a conversation state inference module for using a first learning model, and generating the conversation determination data based on the face feature data.
10. The multi-modal emotion recognition device using artificial intelligence of claim 9, wherein the face feature data comprises mouth video data that is video data for the portion corresponding to the user's mouth in the recognition target area, and wherein the conversation state inference module generates the conversation determination data as to whether or not the user is in a conversation state from the mouth video data using the first learning model.
11. The multi-modal emotion recognition device using artificial intelligence of claim 8, wherein the preliminary inference unit comprises a hand detection inference module for detecting hand video data for the tracking target area from the video data, and generating the location inference data based on the hand video data using a second learning model; and a face overlapping checking module for determining whether or not the recognition target area and the tracking target area overlap with each other based on the face feature data and the location inference data, and generating the overlapping determination data according to an overlapping determination result.
12. The multi-modal emotion recognition device using artificial intelligence of claim 11, wherein the hand detection inference module generates a location inference feature map for the location inference data, and infers the user's emotion state based on the sub feature map, the situation determination data, and the location inference feature map.
13. The multi-modal emotion recognition device using artificial intelligence of claim 8, wherein the situation determination data further comprises recognition target area change data for a change in the recognition target area, and wherein a weight of the recognition target area change data increases as the change amount of the recognition target area becomes larger.
14. The multi-modal emotion recognition device using artificial intelligence of claim 8, wherein the tracking target area and the recognition target area are formed for a plurality of users, respectively, and wherein the preliminary inference unit generates the situation determination data for each user based on the tracking target area and the recognition target area, and determines a conversation state for the plurality of users, respectively, to generate unique voice information of each user, and applies it to the emotion recognition of each user.
15. The multi-modal emotion recognition device using artificial intelligence of claim 7, wherein the main inference unit comprises a plurality of sub feature map generation unit for generating the plurality of sub feature maps for the voice feature data and the face feature data based on the voice feature data and the face feature data using a third learning model; a multi-modal feature map generation unit for generating a multi-modal feature map from the plurality of sub feature map with reference to the situation determination data; and an emotion recognition inference unit for inferring the emotion state based on the multi-modal feature map using a fourth learning model.
16. The multi-modal emotion recognition device using artificial intelligence of claim 15, wherein the situation determination data has a predetermined situation determination value according to the user's situation, and wherein the multi-modal feature map generation unit generates the multi-modal feature map by applying the situation determination value to at least one of the plurality of sub feature maps.
17. The multi-modal emotion recognition device using artificial intelligence of claim 7, wherein the voice pre-processing unit comprises a voice correction module for correcting the voice data; and a voice feature data extraction module for extracting the feature of the voice data that has passed the voice correction module to generate the voice feature data.
18. The multi-modal emotion recognition device using artificial intelligence of claim 7, wherein the video data comprises a plurality of frames, wherein when a feature map cannot be formed based on any specific frames of the plurality of frames, the application of a temporal learning model to all frames that cannot form the feature map is excluded, the last frame in which the feature map immediately before the frame that cannot form the feature map is formed is set as a replacement frame, and the user's emotion state is recognized by applying the temporal learning model to the replacement frame, and wherein the time corresponding to the all frames that cannot form the feature map is a feature map non-detection time, and the temporal learning model is applied to the replacement frame during the feature map non-detection time.
19. A multi-modal emotion recognition method using artificial intelligence, comprising: inputting data that receives video data and voice data of a user; pre-processing data comprising pre-processing voice that generates voice feature data from the voice data and pre-processing video that generates one or more face feature data from the video data; performing preliminary inference that generates situation determination data as to whether or not the user's situation changes according to a temporal sequence based on the video data; and performing main inference that generates at least one sub feature map based on the voice feature data or the face feature data, and infers the user's emotion state based on the sub feature map and the situation determination data.
20. The method of claim 19, wherein the situation determination data comprises conversation determination data as to whether or not the user is in a conversation state or overlapping determination data as to whether or not a tracking target area that is a part of the entire video area of the video data and a recognition target area that is different from the tracking target area overlap with each other, wherein the preliminary inference unit generates the conversation determination data for determining whether or not the user is in a conversation state based on the face feature data, or wherein the preliminary inference unit generates location inference data for inferring the location of the tracking target area based on the video data, and generates the overlapping determination data as to whether or not the tracking target area and the recognition target area overlap with each other based on the face feature data and the location inference data.
21. The method of claim 20, wherein the performing the preliminary inference comprises inferring a conversation state that uses a first learning model, and generates the conversation determination data based on the face feature data, wherein the face feature data comprises mouth video data that is video data for the portion corresponding to the user's mouth in the recognition target area, and wherein the inferring the conversation state generates the conversation determination data as to whether or not the user is in a conversation state from the mount video data using the first learning model.
22. The method of claim 20, wherein the performing the preliminary inference comprises inferring hand detection that detects hand video data for the tracking target area from the video data, and generates the location inference data based on the hand video data using a second learning model; and checking face overlapping that determines whether or not the recognition target area and the tracking target area overlap with each other based on the face feature data and the location inference data, and generates the overlapping determination data according to an overlapping determination result.
23. The method of claim 22, wherein the inferring the hand detection generates a location inference feature map for the location inference data, and infers the user's emotion state based on the sub feature map, the situation determination data, and the location inference feature map.
24. The method of claim 23, wherein the performing the main inference comprises generating a plurality of sub feature maps that generates the plurality of sub feature maps for the voice feature data and the face feature data based on the voice feature data and the face feature data using a third learning model; generating a multi-modal feature map that generates a multi-modal feature map from the plurality of sub feature maps with reference to the situation determination data; and inferring emotion recognition that infers the emotion state based on the multi-modal feature map using a fourth learning model.
25. The method of claim 20, wherein the situation determination data has a predetermined situation determination value according to the user's situation, and wherein the generating the multi-modal feature map generates the multi-modal feature map by applying the situation determination value to at least one of the plurality of sub feature maps.
26. A computer readable storage medium for storing computer program codes for performing a multi-modal emotion recognition method using artificial intelligence, the multi-modal emotion recognition method using artificial intelligence comprises inputting data that receives video data and voice data of a user; pre-processing data comprising pre-processing voice that generates voice feature data from the voice data and pre-processing video that generates one or more face feature data from the video data; performing preliminary inference that generates situation determination data as to whether or not the user's situation changes according to a temporal sequence based on the video data; and performing main inference that generates at least one sub feature map based on the voice feature data or the face feature data, and infers the user's emotion state based on the sub feature map and the situation determination data.
</claims>
</document>
