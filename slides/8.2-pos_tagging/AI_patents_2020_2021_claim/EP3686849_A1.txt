<document>

<filing_date>
2020-01-14
</filing_date>

<publication_date>
2020-07-29
</publication_date>

<priority_date>
2019-01-28
</priority_date>

<ipc_classes>
G06K9/00,G06T11/00
</ipc_classes>

<assignee>
STRADVISION
</assignee>

<inventors>
BOO, SUKHOON
CHO, HOJIN
JANG, TAEWOONG
JE, HONGMO
JEONG, KYUNGJOONG
KIM, HAK-KYOUNG
KIM, INSU
KIM, KYE-HYEON
KIM, YONGJOONG
NAM, WOONHYUN
RYU, WOOJU
SUNG, MYUNGCHUL
YEO, DONGHUN
</inventors>

<docdb_family_id>
67477545
</docdb_family_id>

<title>
LEARNING METHOD AND LEARNING DEVICE FOR RUNTIME INPUT TRANSFORMATION OF REAL IMAGE ON REAL WORLD INTO VIRTUAL IMAGE ON VIRTUAL WORLD, TO BE USED FOR OBJECT DETECTION ON REAL IMAGES, BY USING CYCLE GAN CAPABLE OF BEING APPLIED TO DOMAIN ADAPTATION
</title>

<abstract>
A method for learning a runtime input transformation of real images into virtual images by using a cycle GAN capable of being applied to domain adaptation is provided. The method can be also performed in virtual driving environments. The method includes steps of: (a) (i) instructing first transformer to transform a first image to second image, (ii-1) instructing first discriminator to generate a 1_1-st result, and (ii-2) instructing second transformer to transform the second image to third image, whose characteristics are same as or similar to those of the real images; (b) (i) instructing the second transformer to transform a fourth image to fifth image, (ii-1) instructing second discriminator to generate a 2_1-st result, and (ii-2) instructing the first transformer to transform the fifth image to sixth image; (c) calculating losses. By the method, a gap between virtuality and reality can be reduced, and annotation costs can be reduced.
</abstract>

<claims>
1. A learning method for learning transformation of one or more real images on a real world into one or more virtual images on a virtual world by using a cycle GAN (Generative Adversarial Network), comprising steps of: (a) a learning device, if at least one first image, which is one of the real images, is acquired, (i) instructing a first transformer to transform the first image to at least one second image, whose one or more characteristics are same as or similar to those of the virtual images, (ii-1) instructing a first discriminator to determine whether the second image is one of primary virtual images or one of secondary virtual images, wherein the primary virtual images are at least part of the virtual images without a transformation from at least part of the real images, and the secondary virtual images are at least part of the virtual images with the transformation from at least part of the real images, to thereby generate a 1_1-st result, and (ii-2) instructing a second transformer to transform the second image to at least one third image, whose one or more characteristics are same as or similar to those of the real images; (b) the learning device, if at least one fourth image, which is one of the virtual images, is acquired, (i) instructing the second transformer to transform the fourth image to at least one fifth image, whose one or more characteristics are same as or similar to those of the real images, (ii-1) instructing a second discriminator to determine whether the fifth image is one of primary real images or one of secondary real images, wherein the primary real images are at least part of the real images without a transformation from at least part of the virtual images, and the secondary real images are at least part of the real images with the transformation from at least part of the virtual images, to thereby generate a 2_1-st result, and (ii-2) instructing the first transformer to transform the fifth image to at least one sixth image, whose one or more characteristics are same as or similar to those of the virtual images; and (c) the learning device calculating one or more losses by referring to at least part of the first image, the second image, the third image, the fourth image, the fifth image, the sixth image, the 1_1-st result, and the 2_1-st result, to thereby learn at least part of parameters of the first transformer, the second transformer, the first discriminator, and the second discriminator.
2. The learning method of Claim 1, wherein, at the step of (c), a transformer loss included in said one or more losses is defined by a formula above, I is the first image, G(I) is the second image, DG(G(I))) is the 1_1-st result, F(G(I)) is the third image, X is the fourth image, F(X) is the fifth image, DF(F(X)) is the 2_1-st result, G(F(X)) is the sixth image, γ and β are constants for adjusting each of weights of each of |I - F(G(I))| and |X - G(F(X))|.
3. The learning method of Claim 1, wherein, at the step of (c), the learning device, if (i) a virtual object detection result on at least part of the second image and the sixth image generated by a virtual object detector, which detects at least one virtual object included in its inputted image, and (ii) its corresponding GT are acquired, instructs a loss unit to generate at least part of said one or more losses by further referring to the virtual object detection result.
4. The learning method of Claim 3, wherein, at the step of (c), a transformer loss included in said one or more losses is defined by a formula above, I is the first image, G(I) is the second image, DG(G(I))) is the 1_1-st result, F(G(I)) is the third image, X is the fourth image, F(X) is the fifth image, DF(F(X)) is the 2_1-st result, G(F(X)) is the sixth image, γ and β are constants for adjusting each of weights of each of |I - F(G(I))| and |X.- G(F(X))|, and argmax(λ1 × OD(G(I)) + λ2 × OD (G(F(X)))) is a partial loss of the transformer loss, corresponding to the virtual object detection result, λ1 and λ2 are constants for adjusting each of weights of each of OD(G(I)) and OD(G(F(X))).
5. The learning method of Claim 1, wherein, at the step of (c), an FD loss for the first discriminator included in the losses is defined by a formula above, VI is any arbitrary virtual image among the virtual images, DG(VI) is a 1_2-nd result, from the first discriminator, of determining the arbitrary virtual image, G(I) is the second image, and DG(G(I)) is the 1_1-st result.
6. The learning method of Claim 1, wherein, at the step of (c), an SD loss for the second discriminator included in the losses is defined by a formula above, RI is any arbitrary real image among the real images, DF(RI) is a 2_2-nd result, from the second discriminator, of determining the arbitrary real image, F(X) is the fifth image, and DF(F(X)) is the 2_1-st result.
7. The learning method of Claim 1, wherein each of the first transformer and the second transformer includes at least part of one or more encoding layers and one or more decoding layers.
8. The learning method of Claim 7, wherein (i) at least part of first parameters included in (i-1) one or more first specific encoding layers, which are at least part of first encoding layers included in the first transformer and (i-2) one or more first specific decoding layers, which are at least part of first decoding layers included in the first transformer, and (ii) at least part of second parameters included in (ii-1) one or more second specific encoding layers, which are at least part of second encoding layers included in the second transformer and (ii-2) one or more second specific decoding layers, which are at least part of second decoding layers included in the second transformer, are learned relatedly, to thereby allow a degree of relationship between the first parameters and the second parameters to be larger than a threshold value.
9. The learning method of Claim 8, wherein the first specific encoding layers and the first specific decoding layers are selected among the first encoding layers and the first decoding layers respectively such that one or more first distances from a first latent space, located between the first encoding layer and the first decoding layers, are smaller than a first threshold distance, and the second specific encoding layers and the second specific decoding layers are selected among the second encoding layers and the second decoding layers respectively such that one or more second distances from a second latent space, located between the second encoding layer and the second decoding layers, are smaller than a second threshold distance.
10. A testing method for testing transformation of one or more real images for testing on a real world into one or more virtual images for testing on a virtual world by using a cycle GAN (Generative Adversarial Network), comprising a step of:
on condition that (1) a learning device (i) has instructed a first transformer to transform at least one first training image, which is one of real images for training, to at least one second training image, whose one or more characteristics are same as or similar to those of one or more virtual images for training, (ii-1) has instructed a first discriminator to determine whether the second training image is one of primary virtual images or one of secondary virtual images, wherein the primary virtual images are at least part of the virtual images for training without a transformation from at least part of the real images for training, and the secondary virtual images are at least part of the virtual images for training with the transformation from at least part of the real images for training, to thereby generate a 1_1-st result, and (ii-2) has instructed a second transformer to transform the second training image to at least one third training image, whose one or more characteristics are same as or similar to those of the real images for training, (2) the learning device (i) has instructed the second transformer to transform at least one fourth training image, which is one of the virtual images for training, to at least one fifth training image, whose one or more characteristics are same as or similar to those of the real images for training, (ii-1) has instructed a second discriminator to determine whether the fifth training image is one of primary real images or one of secondary real images, wherein the primary real images are at least part of the real images for training without a transformation from at least part of the virtual images for training, and the secondary real images are at least part of the real images for training with the transformation from at least part of the virtual images for training, to thereby generate a 2_1-st result, and (ii-2) has instructed the first transformer to transform the fifth training image to at least one sixth training image, whose one or more characteristics are same as or similar to those of the virtual images for training, and (3) the learning device has calculated one or more losses by referring to at least part of the first training image, the second training image, the third training image, the fourth training image, the fifth training image, the sixth training image, the 1_1-st result, and the 2_1-st result, to thereby learn at least part of parameters of the first transformer, the second transformer, the first discriminator, and the second discriminator; a testing device instructing the first transformer to acquire at least one test image, which is one of the real images for testing, and to transform the test image into at least one transformed test image, whose one or more characteristics are same as or similar to those of the virtual images for testing.
11. The testing method of Claim 10, wherein the transformed test image is used for fine-tuning of parameters included in a virtual object detector.
12. The testing method of Claim 10, wherein the test image is one of the real images for testing acquired by a camera included in an autonomous vehicle, and a virtual object detector included in the autonomous vehicle detects at least one object included in the transformed test image to thereby support the autonomous vehicle to drive in the real world.
13. A learning device for learning transformation of one or more real images on a real world into one or more virtual images on a virtual world by using a cycle GAN (Generative Adversarial Network), comprising: at least one memory that stores instructions; and at least one processor configured to execute the instructions to: perform processes of (I), if at least one first image, which is one of the real images, is acquired, (i) instructing a first transformer to transform the first image to at least one second image, whose one or more characteristics are same as or similar to those of the virtual images, (ii-1) instructing a first discriminator to determine whether the second image is one of primary virtual images or one of secondary virtual images, wherein the primary virtual images are at least part of the virtual images without a transformation from at least part of the real images, and the secondary virtual images are at least part of the virtual images with the transformation from at least part of the real images, to thereby generate a 1_1-st result, and (ii-2) instructing a second transformer to transform the second image to at least one third image, whose one or more characteristics are same as or similar to those of the real images, (II) if at least one fourth image, which is one of the virtual images, is acquired, (i) instructing the second transformer to transform the fourth image to at least one fifth image, whose one or more characteristics are same as or similar to those of the real images, (ii-1) instructing a second discriminator to determine whether the fifth image is one of primary real images or one of secondary real images, wherein the primary real images are at least part of the real images without a transformation from at least part of the virtual images, and the secondary real images are at least part of the real images with the transformation from at least part of the virtual images, to thereby generate a 2_1-st result, and (ii-2) instructing the first transformer to transform the fifth image to at least one sixth image, whose one or more characteristics are same as or similar to those of the virtual images, and (III) calculating one or more losses by referring to at least part of the first image, the second image, the third image, the fourth image, the fifth image, the sixth image, the 1_1-st result, and the 2_1-st result, to thereby learn at least part of parameters of the first transformer, the second transformer, the first discriminator, and the second discriminator.
14. The learning device of Claim 13, wherein, at the process of (III), a transformer loss included in said one or more losses is defined by a formula above, I is the first image, G(I) is the second image, DG(G(I))) is the 1_1-st result, F(G(I)) is the third image, X is the fourth image, F(X) is the fifth image, DF(F(X)) is the 2_1-st result, G(F(X)) is the sixth image, γ and β are constants for adjusting each of weights of each of |I - F(G(I))| and |X - G(F(X))|.
15. The learning device of Claim 13, wherein, at the process of (III), the processor, if (i) a virtual object detection result on at least part of the second image and the sixth image generated by a virtual object detector, which detects at least one virtual object included in its inputted image, and (ii) its corresponding GT are acquired, instructs a loss unit to generate at least part of said one or more losses by further referring to the virtual object detection result.
</claims>
</document>
