<document>

<filing_date>
2019-10-29
</filing_date>

<publication_date>
2020-11-12
</publication_date>

<priority_date>
2019-05-09
</priority_date>

<ipc_classes>
G01S17/89,G06F16/583,G06T7/00
</ipc_classes>

<assignee>
SRI INTERNATIONAL
</assignee>

<inventors>
KUMAR, RAKESH
CHIU, HAN-PANG
SAMARASEKERA, SUPUN
SIKKA, KARAN
Mithun, Niluthpol
Seymour, Zachary
</inventors>

<docdb_family_id>
73046500
</docdb_family_id>

<title>
SEMANTICALLY-AWARE IMAGE-BASED VISUAL LOCALIZATION
</title>

<abstract>
A method, apparatus and system for visual localization includes extracting appearance features of an image, extracting semantic features of the image, fusing the extracted appearance features and semantic features, pooling and projecting the fused features into a semantic embedding space having been trained using fused appearance and semantic features of images having known locations, computing a similarity measure between the projected fused features and embedded, fused appearance and semantic features of images, and predicting a location of the image associated with the projected, fused features. An image can include at least one image from a plurality of modalities such as a Light Detection and Ranging image, a Radio Detection and Ranging image, or a 3D Computer Aided Design modeling image, and an image from a different sensor, such as an RGB image sensor, captured from a same geo-location, which is used to determine the semantic features of the multi-modal image.
</abstract>

<claims>
1. A method for improved visual localization of an image, the method comprising: extracting appearance features of the image using a first neural network and spatial attention; extracting semantic features of the image using a second neural network and spatial attention; fusing the extracted appearance features and semantic features; pooling the fused features and projecting the pooled, fused features into a semantic embedding space having been trained using fused appearance and semantic features of images having known locations; computing a similarity measure between the projected, fused features and embedded, fused appearance and semantic features of images in the semantic embedding space having known locations; and predicting a location of the image associated with the projected, fused features by determining nearest embedded, fused appearance and semantic features to the projected, fused features of the image in the semantic embedding space based on the similarity measures computed for the projected, fused features of the image.
2. The method of claim 1, wherein the image comprises one of a Light Detection and Ranging image, a Radio Detection and Ranging image, or a 3D Computer Aided Design modeling image, and an image from a different sensor captured from a same geo-location as the one of a Light Detection and Ranging image, a Radio Detection and Ranging image, or a 3D Computer Aided Design modeling image is used to determine the semantic features of the image.
3. The method of claim 2, further comprising generating a common embedding space for determining the semantic features of the one of a Light Detection and Ranging image, a Radio Detection and Ranging image, or a 3D Computer Aided Design modeling image, the generating comprising; capturing image pairs from a same geo-location using at least two different sensors; and embedding the captured image pairs in the common embedding space such that embedded image pairs that are related are closer together in the common embedding space than unrelated image pairs.
4. The method of claim 1, wherein the embedding space is trained by using a max-margin based triplet ranking loss function.
5. The method of claim 1, wherein at least one of a Specific Places Dataset, a Nordland dataset, a St. Lucia dataset, and an Oxford RobotCar dataset is used to train the semantic embedding space.
6. The method of claim 1, comprising: computing the spatial attention by summarizing the appearance and the semantic features across spatial dimensions using average and max pooling and passing the appearance and semantic features through a multi-layer perception followed by an addition and a non-linearity.
7. The method of claim 6, wherein the non-linearity comprises a sigmoidal non-linearity.
8. The method of claim 1, comprising: pooling the fused features using spatial pyramid pooling.
9. The method of claim 1, comprising: pooling the fused features using NetVLAD pooling.
10. The method of claim 1, comprising: normalizing and scaling the embeddings and projections into the semantic embedding space.
11. The method of claim 10, wherein the embeddings and projections into the semantic embedding space are L2 normalized and scaled by a factor of 10.
12. The method of claim 1, wherein at least one of the first neural network and the second neural network comprises a ResNet50 network.
13. The method of claim 1, wherein at least one of the first neural network and the second neural network comprises a Pyramid Scene Parsing Network.
14. A method of creating a semantic embedding space for improved visual localization of an image, the method comprising: for each of a plurality of images having known locations; extracting appearance features of an image using a first neural network and spatial attention; extracting semantic features of the image using a second neural network and spatial attention; fusing the extracted appearance features and semantic features; creating a feature vector representation of the fused features; and semantically embedding the feature vector in a semantic embedding space such that embedded feature vectors that are related are closer together in the semantic embedding space than unrelated feature vectors.
15. The method of claim 14, wherein the semantic embedding space is created using a max-margin based triplet ranking loss function.
16. The method of claim 14, wherein the plurality of images having known locations include images from at least one of a Specific Places Dataset, a Nordland dataset, a St. Lucia dataset, and an Oxford RobotCar dataset.
17. An apparatus for visual localization of an image, comprising: an appearance detection module to extract appearance features of the image using a first neural network including spatial attention; a semantic feature module to extract semantic features of the image using a second neural network including spatial attention; a modality fusion module to fuse the appearance features and the semantic features using a shared channel attention; an attention module to determine the spatial attention for the appearance features and the semantic features and to determine the shared channel attention for the fused features; and a pooling module to: pool the fused features and project the fused features into a semantic embedding space having been trained using fused appearance and semantic features of images having known locations; compute a similarity measure between the projected, fused features and embedded, fused appearance and semantic features of images in the semantic embedding space having known locations; and predict a location of the image associated with the projected, fused features by determining nearest embedded, fused appearance and semantic features to the projected, fused features of the image in the semantic embedding space based on the similarity measures computed for the projected, fused features of the image.
18. The apparatus of claim 17 where the image comprises one of a Light Detection and Ranging image, a Radio Detection and Ranging image, or a 3D Computer Aided Design modeling image, and the semantic feature module is further configured to use an image from a different sensor captured from a same geo-location as the one of a Light Detection and Ranging image, a Radio Detection and Ranging image, or a 3D Computer Aided Design modeling image to determine the semantic features of the image.
19. The apparatus of claim 18, wherein the apparatus is further configured to generate a common embedding space for determining the semantic features of the one of a Light Detection and Ranging image, a Radio Detection and Ranging image, or a 3D Computer Aided Design modeling image, the generating comprising; capturing image pairs from a same geo-location using at least two different sensors; and embedding the captured image pairs in the common embedding space such that embedded image pairs that are related are closer together in the common embedding space than unrelated image pairs.
20. The apparatus of claim 19, wherein the at least two different sensors comprise an RGB image sensor and a Light Detection and Ranging image sensor.
</claims>
</document>
