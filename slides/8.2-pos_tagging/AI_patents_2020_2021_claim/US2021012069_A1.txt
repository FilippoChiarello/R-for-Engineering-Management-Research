<document>

<filing_date>
2019-02-14
</filing_date>

<publication_date>
2021-01-14
</publication_date>

<priority_date>
2018-03-02
</priority_date>

<ipc_classes>
G06F40/20,G06F40/47,G06N3/04
</ipc_classes>

<assignee>
NTT (NIPPON TELEGRAPH AND TELEPHONE CORPORATION)
</assignee>

<inventors>
HIRAO, TSUTOMU
KAMIGAITO, Hidetaka
NAGATA, Masaaki
</inventors>

<docdb_family_id>
67806107
</docdb_family_id>

<title>
SYMBOL SEQUENCE GENERATION APPARATUS, TEXT COMPRESSION APPARATUS, SYMBOL SEQUENCE GENERATION METHOD AND PROGRAM
</title>

<abstract>
A symbol sequence generation device which, when a first symbol sequence x representing a sentence is input, generates a second symbol sequence y corresponding to the sentence and in accordance with a predetermined purpose by a pre-trained neural network, wherein the neural network includes an encoding unit that converts each element xi of an input first symbol sequence x into a first hidden state, an attention mechanism part that weights the first hidden state and outputs the weighted first hidden state as a second hidden state, a decoding unit that outputs a third hidden state based on a t−th element xt of the first symbol sequence x, a (t−1)th element yt−1 of the second symbol sequence y, and the second hidden state, and an output unit that generates a t−th element yt of the second symbol sequence y based on the second hidden state and the third hidden state and outputs the generated element yt. The attention mechanism part computes a first probability Pparent (Xj|xt, x) that a parent of an element xt contained in the first symbol sequence x is an element xj other than that element xt for each of elements in a dependency structure tree corresponding to the sentence, a second probability αd, t, j that a d-ary parent of the element xt is an element xj other than that element xt using the computed first probability Pparent (xj|xt, x) for each of elements, and outputs γd,t that is obtained by weighting the first hidden state using the computed second probability αd, t, j as the second hidden state.
</abstract>

<claims>
1. 1.-8. (canceled)
9. A computer-implemented method for processing a set of symbols, the method comprising: receiving a first sequence of symbols, the first sequence of symbols representing an input sentence; generating, based on encoding one or more elements of the received first sequence of symbols, a first hidden state of a neural network; generating, based on weighting the generated first hidden state, a second hidden state of the neural network, wherein the weighting relates at least to a probability of a symbol in the first sequence of symbols being distinct from another symbol as a parent in a dependency structure tree of the first sequence of symbols; generating a third hidden state of the neural network based at least on a combination of: a first element in the first sequence of symbols, a second element in a second sequence of symbols, the second element preceding the first element in a sequence of elements, and the generated second hidden state; generating, based on the second hidden state and the third hidden state, a third element in the second sequence of symbols, wherein the third element is subsequent to the second element; generating the second sequence of symbols, wherein the second sequence of symbols represents a sequence of labels for removing one or more elements in the first sequence of symbols; and generating an output sequence of symbols based on the removal of the one or more elements from the first sequence of symbols according to the generated second sequence of symbols.
10. The computer-implemented method of claim 9, wherein the second sequence of symbols represents a compressed sentence based on the first sequence of symbols.
11. The computer-implemented method of claim 9, the method further comprising: generating a first probability of the first element in the first sequence of symbols being distinct from a fourth element in the first sequence of symbols at a parent of the first element in the dependency structure tree of the first sequence of symbols; generating, based on the first probability, a second probability of the first sequence of symbols being distinct from a fifth element in the first sequence of symbols at two or more levels toward the root of the dependency structure tree of the first sequence of symbols; and weighting the first hidden state based at least on the second probability.
12. The computer-implemented method of claim 9, the method further comprising: generating one or more weights based at least on the first hidden state and the third hidden state, wherein the third hidden is based on one or more elements in the second sequence of symbols from the beginning to the second element.
13. The computer-implemented method of claim 9, wherein the first hidden state includes a multi-dimensional vector representing the received first sequence of symbols.
14. The computer-implemented method of claim 9, the method further comprising: generate a label for at least a word in the input sentence in a language based on the second sequence of symbols, wherein the label relates one or more words of another language for a language translation of the input sentence.
15. The computer-implemented method of claim 9, the method further comprising: generate a label for at least a word in the input sentence based on the second sequence of symbols, wherein the label comprises one of deletion or retention of the at least one word; retaining the one or more words in the input sentence according to the label in the generated second sequence of symbols; generating an output sentence based on the retained one or more words in the input sentence; and providing the output sentence.
16. A system for processing a set of symbols, the system comprises: a processor; and a memory storing computer-executable instructions that when executed by the processor cause the system to: receive a first sequence of symbols, the first sequence of symbols representing an input sentence; generate, based on encoding one or more elements of the received first sequence of symbols, a first hidden state of a neural network; generate, based on weighting the generated first hidden state, a second hidden state of the neural network, wherein the weighting relates at least to a probability of a symbol in the first sequence of symbols being distinct from another symbol as a parent in a dependency structure tree of the first sequence of symbols; generate a third hidden state of the neural network based at least on a combination of: a first element in the first sequence of symbols, a second element in a second sequence of symbols, the second element preceding the first element in a sequence of elements, and the generated second hidden state; generate, based on the second hidden state and the third hidden state, a third element in the second sequence of symbols, wherein the third element is subsequent to the second element; generate the second sequence of symbols, wherein the second sequence of symbols represent a sequence of labels for removing one or more elements in the first sequence of symbols; and generate an output sequence of symbols based on the removal of the one or more elements from the first sequence of symbols according to the generated second sequence of symbols.
17. The system of claim 16, wherein the second sequence of symbols represents a compressed sentence based on the first sequence of symbols.
18. The system of claim 16, the computer-executable instructions when executed further causing the system to: generate a first probability of the first element in the first sequence of symbols being distinct from a fourth element in the first sequence of symbols at a parent of the first element in the dependency structure tree of the first sequence of symbols; generate, based on the first probability, a second probability of the first sequence of symbols being distinct from a fifth element in the first sequence of symbols at two or more levels toward the root of the dependency structure tree of the first sequence of symbols; and weight the first hidden state based at least on the second probability.
19. The system of claim 16, the computer-executable instructions when executed further causing the system to: generate one or more weights based at least on the first hidden state and the third hidden state, wherein the third hidden is based on one or more elements in the second sequence of symbols from the beginning to the second element.
20. The system of claim 16, wherein the first hidden state includes a multi-dimensional vector representing the received first sequence of symbols.
21. The system of claim 16, the computer-executable instructions when executed further causing the system to: generate a label for at least a word in the input sentence in a language based on the second sequence of symbols, wherein the label relates one or more words of another language for a language translation of the input sentence.
22. The system of claim 16, the computer-executable instructions when executed further causing the system to: generate a label for at least a word in the input sentence based on the second sequence of symbols, wherein the label comprises one of deletion or retention of the at least one word; retain the one or more words in the input sentence according to the label in the generated second sequence of symbols; generate an output sentence based on the retained one or more words in the input sentence; and provide the output sentence.
23. A computer-readable non-transitory recording medium storing computer-executable instructions that when executed by a processor cause a computer system to: receive a first sequence of symbols, the first sequence of symbols representing an input sentence; generate, based on encoding one or more elements of the received first sequence of symbols, a first hidden state of a neural network; generate, based on weighting the generated first hidden state, a second hidden state of the neural network, wherein the weighting relates at least to a probability of a symbol in the first sequence of symbols being distinct from another symbol as a parent in a dependency structure tree of the first sequence of symbols; generate a third hidden state of the neural network based at least on a combination of: a first element in the first sequence of symbols, a second element in a second sequence of symbols, the second element preceding the first element in a sequence of elements, and the generated second hidden state; generate, based on the second hidden state and the third hidden state, a third element in the second sequence of symbols, wherein the third element is subsequent to the second element; generate the second sequence of symbols, wherein the second sequence of symbols represent a sequence of labels for removing one or more elements in the first sequence of symbols; and generate an output sequence of symbols based on the removal of the one or more elements from the first sequence of symbols according to the generated second sequence of symbols.
24. The computer-readable non-transitory recording medium of claim 23, wherein the second sequence of symbols represents a compressed sentence based on the first sequence of symbols.
25. The computer-readable non-transitory recording medium of claim 23, the computer-executable instructions when executed further causing the system to: generate a first probability of the first element in the first sequence of symbols being distinct from a fourth element in the first sequence of symbols at a parent of the first element in the dependency structure tree of the first sequence of symbols; generate, based on the first probability, a second probability of the first sequence of symbols being distinct from a fifth element in the first sequence of symbols at two or more levels toward the root of the dependency structure tree of the first sequence of symbols; and weight the first hidden state based at least on the second probability.
26. The computer-readable non-transitory recording medium of claim 23, the computer-executable instructions when executed further causing the system to: generate one or more weights based at least on the first hidden state and the third hidden state, wherein the third hidden is based on one or more elements in the second sequence of symbols from the beginning to the second element.
27. The computer-readable non-transitory recording medium of claim 23, wherein the first hidden state includes a multi-dimensional vector representing the received first sequence of symbols.
28. The computer-readable non-transitory recording medium of claim 23, the computer-executable instructions when executed further causing the system to: generate a label for at least a word in the input sentence based on the second sequence of symbols, wherein the label comprises one of deletion or retention of the at least one word; retain the one or more words in the input sentence according to the label in the generated second sequence of symbols; generate an output sentence based on the retained one or more words in the input sentence; and provide the output sentence.
</claims>
</document>
