<document>

<filing_date>
2019-11-01
</filing_date>

<publication_date>
2020-05-14
</publication_date>

<priority_date>
2018-11-08
</priority_date>

<ipc_classes>
G06F16/783
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
ZHOU MING
JI LEI
DUAN, NAN
</inventors>

<docdb_family_id>
68655693
</docdb_family_id>

<title>
MULTIMODAL CHAT TECHNOLOGY
</title>

<abstract>
The present disclosure provides a technical solution of multi-modal chatting, which may provide response to user query by using multi-modal response in the interaction between chatbot and human beings, so that the expressing ways and the expressed content by the chatbot could be richer by using such response in a multi-modal way.
</abstract>

<claims>
1. A device, comprising:
a plurality of single-modal data fusing modules, configured to perform a singlemodal data fusing processing containing a similarity matching calculation on an acquired user query and each single-modal data in each multi-modal tuple of a plurality of multimodal tuples, and generate a single-modal fused data, wherein the single-modal fused data contains a first similarity information on the similarity between the user query and the single-modal data;
a multi-modal data fusing module, configured to perform a multi-modal data fusing processing on a plurality of single-modal fused data corresponding to each multi-modal tuple, and generate a multi-modal fused data, wherein the multi-modal fused data contains a similarity information on the similarity between the user query and the multi-modal tuples; a similarity ranking module, configured to perform ranking on the similarity information on the similarity between the user query and the multi-modal tuples, and determine one or more multi-modal tuples for generating a multi-modal response according to the result of the ranking on the similarity information; and
an outputting module, configured to generate a multi-modal response according to the determined one or more multi-modal tuples.
2. The device according to claim 1, wherein the plurality of multi-modal tuples comprise a context sequential relationship therebetween, and the device further comprises a context fusing module,
wherein the multi-modal data fusing module is further configured to perform a first multi-modal data fusing processing on a plurality of single-modal fused data corresponding to each multi-modal tuple, and generate a first multi-modal fused data, wherein the first multi-modal fused data contains a first similarity information on similarity between the user query and the multi-modal tuples,
the context fusing module is configured to perform a second multi-modal data fusing processing on a plurality of first multi-modal fused data according to a context sequential relationship and generate a plurality of second multi-modal fused data, the second multi-modal fused data contains a second similarity information on similarity between the user query and the multi-modal tuples after the context sequential relationship is introduced, and
the similarity ranking module is further configured to perform ranking on the second similarity information on similarity between the user query and the multi-modal tuples, and determine one or more multi-modal tuples for generating a multi-modal response according to the result of the ranking on the second similarity information.
3. The device according to claim 1, wherein the device further comprises:
a multi-modal acquiring module, configured to perform searching in a database of the multi-modal tuples according to the user query and acquire a plurality of multi-modal tuples related to the user query.
4. The device according to claim 1, wherein the multi-modal tuple comprises a triple data consisted of a dynamic image clip, an audio clip, and a subtitle, which are correlated with each other.
5. The device according to claim 4, wherein the device further comprises:
a video searching module configured to perform searching in a video searching engine according to a user query and acquire a plurality of videos related to the user query; and
a multi-modal tuple generating module configured to perform clipping on the video to obtain a plurality of video clips, and generate a plurality of multi-modal tuples based on the plurality of video clips.
6. The device according to claim 5, wherein the multi-modal response comprises one or more video clips and a summary information corresponding to the one or more video clips.
7. A method, comprising:
acquiring a user query;
inputting the user query and a plurality of multi-modal tuples into a multi-modal response generating model;
performing a single-modal data fusing processing containing a similarity matching calculation on the user query and each single-modal data in each of the plurality of the multimodal tuples, and generating a single-modal fused data, wherein the single-modal fused data contains a first similarity information on the similarity between the user query and the single-modal data;
performing a multi-modal data fusing processing on a plurality of single-modal fused data corresponding to each multi-modal tuple, and generating a multi-modal fused data, wherein the multi-modal fused data contains a similarity information on the similarity between the user query and the multi-modal tuples; and
determining one or more multi-modal tuples for generating a multi-modal response according to the similarity information on the similarity between the user query and the multi-modal tuples and generating a multi-modal response according to the determined multi-modal tuples.
8. The method according to claim 7, wherein the plurality of multi-modal tuples input into the multi-modal response generating model have a context sequential relationship therebetween,
wherein the performing a multi-modal data fusing processing on a plurality of single-modal fused data corresponding to each multi-modal tuple and generating a multimodal fused data comprises:
performing a first multi-modal data fusing processing on a plurality of singlemodal fused data corresponding to each multi-modal tuple and generating a first multimodal fused data, wherein the first multi-modal fused data contains a first similarity information on similarity between the user query and the multi-modal tuples; and
performing a second multi-modal data fusing processing on a plurality of first multi-modal fused data according to a context sequential relationship and generating a plurality of second multi-modal fused data, the second multi-modal fused data contains a second similarity information on similarity between the user query and the multi-modal tuples after the context sequential relationship is introduced,
the determining one or more multi-modal tuples for generating a multi-modal response according to the similarity information on the similarity between the user query and the multi-modal tuples comprises:
determining one or more multi-modal tuples for generating a multi-modal response according to the second similarity information.
9. The method according to claim 7, wherein before the inputting the user query and a plurality of multi-modal tuples into a multi-modal response generating model, the method further comprises:
performing searching in a database of the multi-modal tuples according to a user query and acquiring a plurality of multi-modal tuples related to the user query as an input to the multi-modal response generating model.
10. The method according to claim 7, wherein the multi-modal tuple comprises a triple data consisted of a dynamic image clip, an audio clip, and a subtitle, which are correlated with each other.
11. The method according to claim 10, wherein before the inputting the user query and a plurality of multi-modal tuples into a multi-modal response generating model, the method further comprises:
performing searching in a video searching engine according to a user query and acquiring a plurality of videos related to the user query; and
performing clipping on a video of the plurality of videos to obtain a plurality of video clips, and generating a plurality of multi-modal tuples based on the plurality of video clips.
12. An electronic apparatus, comprising:
a processing unit; and
a memory, coupled to the processing unit and containing instructions stored thereon, the instructions cause the electronic apparatus to perform operations upon being executed by the processing unit, the operations comprise:
acquiring a user query;
inputting the user query and a plurality of multi-modal tuples into a multi-modal response generating model;
performing a single-modal data fusing processing containing a similarity matching calculation on the user query and each single-modal data in each of the plurality of the multimodal tuples, and generating a single-modal fused data, wherein the single-modal fused data contains a first similarity information on the similarity between the user query and the single-modal data; performing a multi-modal data fusing processing on a plurality of single-modal fused data corresponding to each multi-modal tuple, and generating a multi-modal fused data, wherein the multi-modal fused data contains a similarity information on the similarity between the user query and the multi-modal tuples; and
determining one or more multi-modal tuples for generating a multi-modal response according to the similarity information on the similarity between the user query and the multi-modal tuples and generating a multi-modal response according to the determined multi-modal tuples.
13. The electronic apparatus according to claim 12, wherein the plurality of multimodal tuples input into the multi-modal response generating model have a context sequential relationship therebetween,
wherein the performing a multi-modal data fusing processing on a plurality of single-modal fused data corresponding to each multi-modal tuple and generating a multimodal fused data comprises:
performing a first multi-modal data fusing processing on a plurality of singlemodal fused data corresponding to each multi-modal tuple and generating a first multimodal fused data, wherein the first multi-modal fused data contains a first similarity information on similarity between the user query and the multi-modal tuples; and
performing a second multi-modal data fusing processing on a plurality of first multi-modal fused data according to a context sequential relationship and generating a plurality of second multi-modal fused data, the second multi-modal fused data contains a second similarity information on similarity between the user query and the multi-modal tuples after the context sequential relationship is introduced,
the determining one or more multi-modal tuples for generating a multi-modal response according to the similarity information on the similarity between the user query and the multi-modal tuples comprises:
determining one or more multi-modal tuples for generating a multi-modal response according to the second similarity information.
14. The electronic apparatus according to claim 12, wherein the multi-modal tuple comprises a triple data consisted of a dynamic image clip, an audio clip, and a subtitle, which are correlated with each other.
15. The electronic apparatus according to claim 14, wherein before the inputting the user query and a plurality of multi-modal tuples into a multi-modal response generating model, the operations further comprise:
performing searching in a video searching engine according to a user query and acquiring a plurality of videos related to the user query; and
performing clipping on a video of the plurality of videos to obtain a plurality of video clips, and generating a plurality of multi-modal tuples based on the plurality of video clips.
</claims>
</document>
