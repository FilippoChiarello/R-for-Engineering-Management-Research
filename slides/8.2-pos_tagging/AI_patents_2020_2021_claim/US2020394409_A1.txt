<document>

<filing_date>
2020-08-21
</filing_date>

<publication_date>
2020-12-17
</publication_date>

<priority_date>
2019-01-03
</priority_date>

<ipc_classes>
G06K9/00,G06T7/11
</ipc_classes>

<assignee>
LUCOMM TECHNOLOGIES
</assignee>

<inventors>
CRISTACHE, LUCIAN
</inventors>

<docdb_family_id>
73744800
</docdb_family_id>

<title>
System for physical-virtual environment fusion
</title>

<abstract>
A system for physical-virtual environment fusion includes a sensor with a computing system and a memory in communication with the computing system, the memory storing a plurality of endpoints. The computing system is configured to select a semantic identity of an object from among a group of objects indicated by a user pointer indicator by determining, based on inputs from the sensor, the approximate orientation of the user pointer indicator towards a first endpoint from among the plurality of endpoints, the first endpoint having the group of objects located within the first endpoint. It further renders, on a display surface, the group of objects, receives from the user an indication regarding the semantic identity of the object and presents, on the display surface, an indication of the selection of the object from among the group of objects based on the further indications from the user.
</abstract>

<claims>
I claim:
1. A semantic augmentation system, comprising: a sensor; a computing system and a memory in communication with the computing system, the memory storing a plurality of endpoints; the computing system further being configured to select a semantic identity of an object from among a group of objects indicated by a user pointer indicator by: determining, based on inputs from the sensor, the approximate orientation of the user pointer indicator towards a first endpoint from among the plurality of endpoints, the first endpoint having the group of objects located within the first endpoint; render, on a display surface, the group of objects; receive from the user an indication regarding the semantic identity of the object; and present, on the display surface, an indication of the selection of the object from among the group of objects based on the further indications from the user.
2. The semantic augmentation system of claim 1, wherein: the object is a user interface object, and the computer system is further configured to map the plurality of endpoints to the display surface and to map the user interface object to the first endpoint; the processor further being configured to select the semantic identity of the user interface object based on an indication by the user pointer indicator and a collimation projected from a determined observing field of view of the sensor to the first endpoint; wherein the system is further configured to select the semantic identity of the user interface object based on an obturation of the first endpoint by the user pointer indicator during a pointing movement of the user, as detected at the sensor within the observing field of view.
3. The semantic augmentation system of claim 2, wherein the system is configured to select the user interface object based on an orientation of the user pointer indicator towards the first endpoint mapped to the indicated user interface object and further projected within a second endpoint contained within the endpoint.
4. The system of claim 2, wherein the processor is configured to select the user interface object mapped to the display surface based on the determined orientation of the at least one user pointer indicator, and further by an input from a camera, the camera being oriented to encompass the user pointer indicator and the display surface.
5. The semantic augmentation system of claim 1, wherein the sensor is an optical mesh embedded in a lens.
6. The semantic augmentation system of claim 1, wherein the sensor is at least one wearable sensor.
7. The semantic augmentation system of claim 1, wherein the object is localized into a store environment.
8. The semantic augmentation system of claim 1, wherein the object is localized into a home environment.
9. The semantic augmentation system of claim 2, wherein the user interface object is localized into a physical-virtual fusion healthcare environment.
10. The semantic augmentation system of claim 2, wherein the user interface object is localized into a physical-virtual fusion sporting environment.
11. The semantic augmentation system of claim 1, wherein the sensor is configured to adjust the observing field of view based on an inference of the user iris orientation.
12. The semantic augmentation system of claim 1, further comprising: the plurality of endpoints mapped to the display surface; the system further being configured to map the user interface on the first endpoint on the display surface and to select the user interface object based on the collimation of an area delimited by one or more user pointer indicators as projected from the sensor to the endpoint; and wherein the system is configured to select the user interface object based on an inferred tapping between at least two user pointer indicators at a location within the delimited area and further projected within a second endpoint contained within the endpoint.
13. The semantic augmentation system of claim 12, wherein the user interface object is localized into a physical-virtual fusion store environment.
14. The semantic augmentation system of claim 12, wherein the user interface object is localized into a physical-virtual fusion home environment.
15. The semantic augmentation system of claim 1, wherein the user interface object is localized into a healthcare environment.
16. The semantic augmentation system of claim 1, wherein the user interface object is localized into a sporting environment.
17. The semantic augmentation system of claim 1, wherein the sensor or display surface is an optical mesh.
18. The semantic augmentation system of claim 1, wherein the sensor is a camera.
19. The semantic augmentation system of claim 1, wherein the sensor is embedded in glasses.
20. The semantic augmentation system of claim 1, wherein the processor is configured to display objects associated with a pointed object associated with the first endpoint based on the determined orientation of the at least one user pointer indicator, and further based on an input from a camera, the camera being oriented to encompass the user pointer indicator and the first endpoint.
21. The semantic augmentation system of claim 20, wherein the system is configured to adjust the camera field of view based on a determined direction of the user pointer indicator and further to adjust the camera encompass the pointed object.
22. The semantic augmentation system of claim 21, wherein the system is configured to infer an intermediate endpoint within the inferred direction of the user pointer indicator and to adjust the camera field of view based on the intermediate endpoint.
23. The semantic augmentation system of claim 20, wherein the pointed object supports a plurality of objects in the environment.
24. The semantic augmentation system of claim 23, wherein the plurality of objects is associated with the plurality of display objects and further associated with endpoints comprised in the first endpoint.
</claims>
</document>
