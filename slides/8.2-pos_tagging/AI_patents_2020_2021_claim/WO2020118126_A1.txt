<document>

<filing_date>
2019-12-06
</filing_date>

<publication_date>
2020-06-11
</publication_date>

<priority_date>
2018-12-07
</priority_date>

<ipc_classes>
G06T5/20,G06T7/00,G06T7/11
</ipc_classes>

<assignee>
KLA CORPORATION
</assignee>

<inventors>
BHATTACHARYYA, SANTOSH
GEORGE, JACOB
PARAMASIVAM, SARAVANAN
PLIHAL, MARTIN
</inventors>

<docdb_family_id>
70972606
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR DIFFERENCE FILTER AND APERTURE SELECTION USING SHALLOW DEEP LEARNING
</title>

<abstract>
A system for defect review and classification is disclosed. The system may include a controller, wherein the controller may be configured to receive one or more training images of a specimen. The one or more training images including a plurality of training defects. The controller may be further configured to apply a plurality of difference filters to the one or more training images, and receive a signal indicative of a classification of a difference filter effectiveness metric for at least a portion of the plurality of difference filters. The controller may be further configured to generate a deep learning network classifier based on the received classification and the attributes of the plurality of training defects. The controller may be further configured to extract convolution layer filters of the deep learning network classifier, and generate one or more difference filter recipes based on the extracted convolution layer filters.
</abstract>

<claims>
What is claimed:
1. A system, comprising:
a controller configured to:
receive one or more training images of a specimen, the one or more training images including a plurality of training areas of interest;
apply a plurality of difference filters to the one or more training images;
receive a signal indicative of a classification of a difference filter effectiveness metric for at least a portion of the plurality of difference filters; generate a deep learning network classifier based on the received classification and the attributes of the plurality of training areas of interest; extract convolution layer filters of the deep learning network classifier; and
generate one or more difference filter recipes based on the extracted convolution layer filters.
2. The system of Claim 1 , further comprising an inspection sub-system, wherein the controller is configured to receive at least one of the one or more training images or the one or more product images from the inspection sub-system.
3. The system of Claim 1 , further comprising an inspection sub-system, wherein the inspection sub-system is configured to acquire one or more product images of at least a portion of a specimen using the one or more difference filter recipes.
4. The system of Claim 3, wherein the inspection sub-system comprises a scanning electron microscope (SEM) inspection system.
5. The system of Claim 3, wherein the inspection sub-system is configured to acquire one or more product images of at least a portion of a specimen using a first difference filter recipe for a first region of the portion of the specimen and an additional difference filter recipe for an additional region of the portion of the specimen.
6. The system of Claim 1, wherein receiving a signal indicative of a classification of a difference filter effectiveness metric for at least a portion of the plurality of difference filters comprises:
receiving a signal from a user interface device indicative of a manual classification of a difference filter effectiveness metric for at least a portion of the plurality of difference filters.
7. The system of Claim 1 , wherein the classification of the difference filter effectiveness metric for at least a portion of the plurality of difference filters comprises:
a first difference filter effectiveness metric for a first difference filter applied to a first group of areas of interest; and
a second difference filter effectiveness metric for a second difference filter applied to the first group of areas of interest.
8. The system of Claim 1 , wherein the classification of the difference filter effectiveness metric for at least a portion of the plurality of difference filters comprises:
a first difference filter effectiveness metric for a first difference filter applied to a group of areas of interest; and
a second difference filter effectiveness metric for the first difference filter applied to the group of areas of interest.
9. The system of Claim 1 , wherein the controller is further configured to record the one or more training images to a virtual inspector-virtual analyzer (VIVA) network. 10. The system of Claim 1 , wherein the plurality of areas of interest include at least one of one or more defects or one or more nuisance regions.
11. A system, comprising:
a controller configured to:
receive one or more training images of a specimen using a plurality of aperture settings, the one or more training images including a plurality of training areas of interest;
receive a signal indicative of a classification of an aperture setting effectiveness metric for at least a portion of the plurality of aperture settings; generate a deep learning network classifier based on the received classification and the attributes of the plurality of training areas of interest; extract convolution layer filters of the deep learning network classifier;
perform one or more mathematical analyses on the extracted convolution layer filters to generate one or more aperture setting recipes based on the extracted convolution layer filters; and
generate one or more aperture setting recipes based on the extracted convolution layer filters.
12. The system of Claim 1 1 , further comprising an inspection sub-system, wherein the controller is configured to receive at least one of the one or more training images or the one or more product images from the inspection sub-system.
13. The system of Claim 1 1 , further comprising an inspection sub-system, wherein the inspection sub-system is configured to acquire one or more product images of at least a portion of a specimen using the one or more aperture setting recipes.
14. The system of Claim 13, wherein the inspection sub-system comprises a scanning electron microscope (SEM) inspection system. 15. The system of Claim 13, wherein the inspection sub-system is configured to acquire one or more product images of at least a portion of a specimen using a first aperture seting recipe for a first region of the portion of the specimen and an additional aperture setting recipe for an additional region of the portion of the specimen.
16. The system of Claim 1 1 , wherein receiving a signal indicative of a classification of an aperture setting effectiveness metric for at least a portion of the plurality of aperture settings comprises:
receiving a signal from a user interface device indicative of a manual classification of an aperture setting effectiveness metric for at least a portion of the plurality of aperture setings.
17. The system of Claim 1 1 , wherein the classification of the aperture setting effectiveness metric for at least a portion of the plurality of aperture setings comprises:
a first aperture setting effectiveness metric for a first aperture setting applied to a first group of areas of interest; and
a second aperture setting effectiveness metric for a second aperture setting applied to the first group of areas of interest.
18. The system of Claim 1 1 , wherein the classification of the aperture setting effectiveness metric for at least a portion of the plurality of aperture setings comprises:
a first aperture setting effectiveness metric for a first aperture setting applied to a group of areas of interest; and
a second aperture setting effectiveness metric for the aperture setting filter applied to the group of areas of interest. 19. The system of Claim 11 , wherein the controi!er is further configured to record the one or more training images to a virtual inspector-virtual analyzer (VIVA) network.
20. The system of Claim 1 1, wherein the one or more mathematical analyses comprises a Fourier analysis.
21. The system of Claim 11, wherein the plurality of areas of interest include at least one of one or more defects or one or more nuisance regions.
22. A method, comprising:
acquiring one or more training images of a specimen, the one or more training images including a plurality of training areas of interest;
applying a plurality of difference filters to the one or more training images to generate one or more filtered training images;
receiving a signal indicative of a classification of a difference filter effectiveness metric for at least a portion of the plurality of difference filters;
generating a deep learning network classifier based on the received classification and the attributes of the plurality of training areas of interest;
extracting convolution layer filters of the deep learning network classifier; generating one or more difference filter recipes based on the extracted convolution layer filters; and
acquiring one or more product images of a specimen using at least one of the one or more difference filter recipes.
23. The method of Claim 22, wherein receiving a signal indicative of a classification of a difference filter effectiveness metric for at least a portion of the plurality of difference filters comprises:
receiving a signal from a user interface device indicative of a manual classification of a difference filter effectiveness metric for at least a portion of the plurality of difference filters. 24. The method of Claim 22, wherein the classification of the difference filter effectiveness metric for at least a portion of the plurality of difference filters comprises:
a first difference filter effectiveness metric for a first difference filter applied to a first group of areas of interest; and
a second difference filter effectiveness metric for a second difference filter applied to the first group of areas of interest.
25. The method of Claim 22, wherein the classification of the difference filter effectiveness metric for at least a portion of the plurality of difference filters comprises:
a first difference filter effectiveness metric for a first difference filter applied to a group of areas of interest;
a second difference filter effectiveness metric for the first difference filter applied to the group of areas of interest.
26. The method of Claim 22, wherein the one or more training images comprise one or more scanning electron microscope (SEM) images.
27. The method of Claim 22, wherein the method further comprises recording the one or more training images to a virtual inspector-virtual analyzer (VIVA) network.
28. The method of Claim 22, wherein acquiring one or more product images of a specimen using at least one of the one or more difference filter recipes comprises: acquiring one or more product images of a specimen using a first difference filter recipe for a first region of the specimen, and an additional difference filter recipe for an additional region of the specimen.
29. The method of Claim 22, wherein the plurality of areas of interest include at least one of one or more defects or one or more nuisance regions. 30. A method, comprising:
acquiring one or more training images of a first specimen using a plurality of aperture settings, the one or more training images including a plurality of training areas of interest;
receiving a signal indicative of a classification of an aperture setting effectiveness metric for at least a portion of the plurality of aperture settings;
generating a deep learning network classifier based on the received manual classification and the attributes of the plurality of training areas of interest;
extracting convolution layer filters of the deep learning network classifier; performing one or more frequency domain analyses on the extracted convolution layer filters to generate one or more aperture setting recipes based on the extracted convolution layer filters; and
acquiring one or more product images of a second specimen using at least one of the one or more aperture setting recipes.
31. The method of Claim 30, wherein receiving a signal indicative of a classification of an aperture setting effectiveness metric for at least a portion of the plurality of aperture settings comprises:
receiving a signal from a user interface device indicative of a manual classification of an aperture setting effectiveness metric for at least a portion of the plurality of aperture settings.
32. The method of Claim 30, wherein the classification of the aperture setting effectiveness metric for at least a portion of the plurality of aperture settings comprises:
a first aperture setting effectiveness metric for a first aperture setting applied to a first group of areas of interest; and
a second aperture setting effectiveness metric for a second aperture setting applied to the first group of areas of interest. 33. The method of Claim 30, wherein the classification of the aperture setting effectiveness metric for at least a portion of the plurality of aperture settings comprises:
a first aperture setting effectiveness metric for a first aperture setting applied to a group of areas of interest;
a second aperture setting effectiveness metric for the first aperture setting applied to the group of areas of interest.
34. The method of Claim 30, wherein the one or more training images comprise one or more scanning electron microscope (SEM) images.
35. The method of Claim 30, wherein the method further comprises recording the one or more training images to a virtual inspector-virtual analyzer (VIVA) network.
36. The method of Claim 30, wherein acquiring one or more product images of a specimen using at least one of the one or more aperture setting recipes comprises: acquiring one or more product images of a specimen using a first aperture setting recipe for a first region of the specimen, and an additional aperture setting recipe for an additional region of the specimen.
37. The method of Claim 30, wherein the plurality of areas of interest include at least one of one or more defects or one or more nuisance regions.
38. The method of Claim 30, wherein the one or more frequency domain analyses include a Fourier analysis.
</claims>
</document>
