<document>

<filing_date>
2018-11-15
</filing_date>

<publication_date>
2020-05-21
</publication_date>

<priority_date>
2018-11-15
</priority_date>

<ipc_classes>
G06K9/00,G06K9/34,G06T3/00,G06T7/11,G06T7/20,G06T7/33
</ipc_classes>

<assignee>
TOYOTA RESEARCH INSTITUTE
</assignee>

<inventors>
KANZAWA, YUSUKE
DELP, MICHAEL JAMES
</inventors>

<docdb_family_id>
70728179
</docdb_family_id>

<title>
Systems and methods for registering 3D data with 2D image data
</title>

<abstract>
Systems and methods described herein relate to registering three-dimensional (3D) data with two-dimensional (2D) image data. One embodiment receives 3D data from one or more sensors and 2D image data from one or more cameras; identifies a 3D segment in the 3D data and associates it with an object; identifies 2D boundary information for the object; determines a speed and a heading for the object; and registers the 3D segment with the 2D boundary information by adjusting the relative positions of the 3D segment and the 2D boundary information based on the speed and heading of the object and matching, in 3D space, the 3D segment with projected 2D boundary information.
</abstract>

<claims>
1. A system for registering three-dimensional (3D) data with two-dimensional (2D) image data, the system comprising: one or more sensors to produce 3D data; one or more cameras to produce 2D image data; one or more processors; a memory communicably coupled to the one or more processors and storing: a 3D-data segmentation module including instructions that when executed by the one or more processors cause the one or more processors to identify, in the 3D data, a 3D segment; a data association module including instructions that when executed by the one or more processors cause the one or more processors to associate the 3D segment with an object; an image segmentation module including instructions that when executed by the one or more processors cause the one or more processors to identify 2D boundary information for the object in the 2D image data; a velocity estimation module including instructions that when executed by the one or more processors cause the one or more processors to determine a speed and a heading for the object; and an integration module including instructions that when executed by the one or more processors cause the one or more processors to register the 2D boundary information with the 3D segment by performing one of: shifting the 2D boundary information in 2D image space to a position that, based on the object's speed and heading, corresponds to a time at which the 3D data was captured, projecting the time-shifted 2D boundary information into 3D space, and matching the projected time-shifted 2D boundary information with the 3D segment; projecting the 2D boundary information into 3D space, shifting the projected 2D boundary information in 3D space to a position that, based on the object's speed and heading, corresponds to a time at which the 3D data was captured, and matching the time-shifted projected 2D boundary information with the 3D segment; and projecting the 2D boundary information into 3D space, shifting the 3D segment in 3D space to a position that, based on the object's speed and heading, corresponds to a time at which the 2D image data was captured, and matching the projected 2D boundary information with the time-shifted 3D segment.
2. The system of claim 1, wherein the 2D boundary information is one of a bounding box, a perspective bounding box, a shape estimate, and a shape estimate with an accompanying pose estimate.
3. The system of claim 1, wherein the 3D segment is 3D boundary information that includes one of a convex hull, a voxelization, a non-convex hull, a bounding box, and mesh data.
4. The system of claim 1, wherein the 3D segment is a point-cloud cluster.
5. The system of claim 1, wherein: the 2D boundary information is one of a perspective bounding box, a shape estimate, and a shape estimate with an accompanying pose estimate; the 3D segment is 3D boundary information that includes one of a convex hull, a voxelization, a non-convex hull, a bounding box, and mesh data; and the integration module includes instructions to perform one of: matching the projected time-shifted 2D boundary information with the 3D segment by performing shape matching between the projected time-shifted 2D boundary information and the 3D boundary information; matching the time-shifted projected 2D boundary information with the 3D segment by performing shape matching between the time-shifted projected 2D boundary information and the 3D boundary information; and matching the projected 2D boundary information with the time-shifted 3D segment by performing shape matching between the projected 2D boundary information and the time-shifted 3D boundary information.
6. The system of claim 1, wherein the one or more sensors include at least one of a Light Detection and Ranging (LIDAR) sensor, a set of stereo cameras, a Red Green Blue Depth (RGB-D) sensor, and a radar sensor.
7. The system of claim 1, wherein the velocity estimation module includes instructions to determine the speed and the heading for the object based on an analysis of the 2D image data.
8. The system of claim 1, wherein the velocity estimation module includes instructions to determine the speed and the heading for the object based on an analysis of the 3D data.
9. The system of claim 1, wherein the velocity estimation module includes instructions to determine the speed and the heading for the object based, at least in part, on information from a radar sensor.
10. A method of registering three-dimensional (3D) data with two-dimensional (2D) image data, the method comprising: receiving 3D data from one or more sensors; receiving 2D image data from one or more cameras; identifying a 3D segment in the 3D data; associating the 3D segment with an object; identifying 2D boundary information for the object in the 2D image data; determining a speed and a heading for the object; and registering the 2D boundary information with the 3D segment by performing one of: shifting the 2D boundary information in 2D image space to a position that, based on the object's speed and heading, corresponds to a time at which the 3D data was captured, projecting the time-shifted 2D boundary information into 3D space, and matching the projected time-shifted 2D boundary information with the 3D segment; projecting the 2D boundary information into 3D space, shifting the projected 2D boundary information in 3D space to a position that, based on the object's speed and heading, corresponds to a time at which the 3D data was captured, and matching the time-shifted projected 2D boundary information with the 3D segment; and projecting the 2D boundary information into 3D space, shifting the 3D segment in 3D space to a position that, based on the object's speed and heading, corresponds to a time at which the 2D image data was captured, and matching the projected 2D boundary information with the time-shifted 3D segment.
11. The method of claim 10, wherein the 2D boundary information is one of a bounding box, a perspective bounding box, a shape estimate, and a shape estimate with an accompanying pose estimate.
12. The method of claim 10, wherein the 3D segment is 3D boundary information that includes one of a convex hull, a voxelization, a non-convex hull, a bounding box, and mesh data.
13. The method of claim 10, wherein the 3D segment is a point-cloud cluster.
14. The method of claim 10, wherein: the 2D boundary information is one of a perspective bounding box, a shape estimate, and a shape estimate with an accompanying pose estimate; the 3D segment is 3D boundary information that includes one of a convex hull, a voxelization, a non-convex hull, a bounding box, and mesh data; and the method includes performing one of: matching the projected time-shifted 2D boundary information with the 3D segment by performing shape matching between the projected time-shifted 2D boundary information and the 3D boundary information; matching the time-shifted projected 2D boundary information with the 3D segment by performing shape matching between the time-shifted projected 2D boundary information and the 3D boundary information; and matching the projected 2D boundary information with the time-shifted 3D segment by performing shape matching between the projected 2D boundary information and the time-shifted 3D boundary information.
15. The method of claim 10, wherein the one or more sensors include at least one of a Light Detection and Ranging (LIDAR) sensor, a set of stereo cameras, a Red Green Blue Depth (RGB-D) sensor, and a radar sensor.
16. The method of claim 10, wherein determining a speed and a heading for the object is based on an analysis of the 2D image data.
17. The method of claim 10, wherein determining a speed and a heading for the object is based on an analysis of the 3D data.
18. The method of claim 10, wherein determining a speed and a heading for the object is based, at least in part, on information from a radar sensor.
19. A system for registering three-dimensional (3D) data with two-dimensional (2D) image data, the system comprising: one or more sensors to produce 3D data; one or more cameras to produce 2D image data; one or more processors; a memory communicably coupled to the one or more processors and storing: a 3D-data segmentation module including instructions that when executed by the one or more processors cause the one or more processors to identify, in the 3D data, a 3D segment, wherein the 3D segment is 3D boundary information that includes one of a convex hull, a voxelization, a non-convex hull, a bounding box, and mesh data; an image segmentation module including instructions that when executed by the one or more processors cause the one or more processors to identify 2D boundary information for an object in the 2D image data, the 2D boundary information including one of a perspective bounding box, a shape estimate, and a shape estimate with an accompanying pose estimate; and an integration module including instructions that when executed by the one or more processors cause the one or more processors to: project the 2D boundary information into 3D space to produce projected 2D boundary information; and perform shape matching between the projected 2D boundary information and the 3D boundary information to register the projected 2D boundary information with the 3D boundary information.
20. A method of registering three-dimensional (3D) data with two-dimensional (2D) image data, the method comprising: receiving 3D data from one or more sensors; receiving 2D image data from one or more cameras; identifying a 3D segment in the 3D data, wherein the 3D segment is 3D boundary information that includes one of a convex hull, a voxelization, a non-convex hull, a bounding box, and mesh data; identifying 2D boundary information for an object in the 2D image data, the 2D boundary information including one of a perspective bounding box, a shape estimate, and a shape estimate with an accompanying pose estimate; projecting the 2D boundary information into 3D space to produce projected 2D boundary information; and performing shape matching between the projected 2D boundary information and the 3D boundary information to register the projected 2D boundary information with the 3D boundary information.
</claims>
</document>
