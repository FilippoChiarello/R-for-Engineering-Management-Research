<document>

<filing_date>
2018-06-25
</filing_date>

<publication_date>
2020-05-28
</publication_date>

<priority_date>
2017-07-11
</priority_date>

<ipc_classes>
G06K9/46,G06K9/62,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
SIEMENS HEALTHCARE DIAGNOSTICS
</assignee>

<inventors>
CHEN, TERRENCE
CHANG, YAO-JEN
KLUCKNER, STEFAN
POLLACK, BENJAMIN S.
</inventors>

<docdb_family_id>
65001767
</docdb_family_id>

<title>
METHODS AND SYSTEMS FOR LEARNING-BASED IMAGE EDGE ENHANCEMENT OF SAMPLE TUBE TOP CIRCLES
</title>

<abstract>
Methods for image-based detection of the tops of sample tubes used in an automated diagnostic analysis system may be based on a convolutional neural network to pre-process images of the sample tube tops to intensify the tube top circle edges while suppressing the edge response from other objects that may appear in the image. Edge maps generated by the methods may be used for various image-based sample tube analyses, categorizations, and/or characterizations of the sample tubes to control a robot in relationship to the sample tubes. Image processing and control apparatus configured to carry out the methods are also described, as are other aspects.
</abstract>

<claims>
1. An image processing and control apparatus, comprising: image capture apparatus configured to capture an image of one or more tops of one or more respective sample tubes; a robot configured to move the one or more respective sample tubes; and a system controller comprising a processor and a memory, the system controller configured via programming instructions stored in the memory to process the image of the one or more tops of the one or more respective sample tubes by applying the image to a convolutional neural network to: intensify sample tube top edges appearing in the image; suppress edge responses from other objects appearing in the image; generate an edge map of the image of the one or more tops of the one or more respective sample tubes; and control the robot to move one or more sample tubes based on the generated edge map.
2. The image processing and control apparatus of claim 1, wherein the robot comprises an end effector configured to grasp and move the one or more sample tubes based on the edge map.
3. The image processing and control apparatus of claim 1, further comprising a tube tray configured to receive the one or more sample tubes therein.
4. The image processing and control apparatus of claim 1, wherein the convolutional neural network is a fully convolutional network comprising a plurality of convolution layers.
5. The image processing and control apparatus of claim 4, wherein the fully convolutional network comprises one or more convolution layers and one or more max-pooling layers followed by one or more fully-connected convolutional layers.
6. The image processing and control apparatus of claim 5, wherein a first convolution layer includes a kernel size of 5, a stride of 1, and a pad of 0, a second convolution layer includes a kernel size of 5, a stride of 1, and a pad of 0, a first fully-connected convolution layer includes a kernel size of 5, a stride of 1, and a pad of 0, and a second fully-connected convolution layer includes a kernel size of 1, a stride of 1, and a pad of 0.
7. The image processing and control apparatus of claim 1, wherein the convolutional neural network is a patch-based convolutional network comprising a plurality of convolution layers followed by a fusion module that fuses edge responses of individual patches into one edge map representing an input image.
8. The image processing and control apparatus of claim 7, wherein the patch-based convolutional network comprises one or more convolution layers and one or more max-pooling layers followed by one or more fully-connected layers.
9. A non-transitory computer-readable medium comprising computer instructions of a fully convolutional network and parameters thereof capable of being executed in a processor and of applying the fully convolutional network and the parameters to an image of sample tube tops to generate an edge map to be stored in the non-transitory computer-readable medium and accessible to a controller to control a robot based on the edge map, the fully convolutional network comprising one or more convolution layers and one or more second max-pooling layers followed by first and second fully-connected convolutional layers.
10. The non-transitory computer-readable medium of claim 9 further comprising computer instructions of a patch-based convolutional network comprising a plurality of convolution layers followed by a fusion module that fuses edge responses of individual patches into one edge map representing an input image.
11. The non-transitory computer-readable medium of claim 10, wherein the patch-based convolutional network comprises one or more convolution layers and one or more max-pooling layers followed by one or more fully-connected layers.
12. The non-transitory computer-readable medium of claim 9, wherein the fully convolutional network further comprises nonlinear layer between the first and second fully-connected convolutional layers.
13. The non-transitory computer-readable medium of claim 9, wherein a first convolution layer includes a kernel size of 5, a stride of 1, and a pad of 0, and a first max-pooling layer includes a kernel size of 2, a stride of 2, and a pad of 0.
14. The non-transitory computer-readable medium of claim 9, wherein a second convolution layer includes a kernel size of 5, a stride of 1, and a pad of 0, and a second max-pooling layer includes a kernel size of 2, a stride of 2, and a pad of 0.
15. A method of processing an image of sample tube tops and controlling a robot based thereon, comprising: receiving an input image of one or more tops of one or more respective sample tubes; applying to the input image a fully convolutional network having one or more convolution layers and one or more second max-pooling layers followed by first and second fully-connected convolutional layers; generating an edge map in response to the applying; determining sample tube categories or characteristics based on the generated edge map; and controlling a robot based on the determined sample tube categories or characteristics.
16. The method of claim 15, wherein the receiving comprises receiving the input image as an array of pixel values.
17. The method of claim 15, wherein a first convolution layer includes a kernel size of 5, a stride of 1, and a pad of 0.
18. The method of claim 15, wherein a first max-pooling layer includes a kernel size of 2, a stride of 2, and a pad of 0.
19. The method of claim 15, wherein the first fully-connected convolution layer includes a kernel size of 5, a stride of 1, and a pad of 0.
20. The method of claim 15, wherein the second fully-connected convolution layer includes a kernel size of 1, a stride of 1, and a pad of 0.
</claims>
</document>
