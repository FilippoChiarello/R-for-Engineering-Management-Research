<document>

<filing_date>
2019-07-24
</filing_date>

<publication_date>
2021-01-28
</publication_date>

<priority_date>
2019-07-24
</priority_date>

<ipc_classes>
G06F3/0482,G06F3/0488,G06K9/62,G06T15/50,G06T19/00,G06T7/11,G06T7/187
</ipc_classes>

<assignee>
HUANG, XIAOYI
WU, YI
Wang, Jingwen
Ai, Xin
</assignee>

<inventors>
HUANG, XIAOYI
WU, YI
Wang, Jingwen
Ai, Xin
</inventors>

<docdb_family_id>
74189981
</docdb_family_id>

<title>
MOBILE DEVICE IMAGE ITEM REPLACEMENTS
</title>

<abstract>
A system for replacing physical items in images is discussed. A depicted item can be selected and removed from an image via image mask data and pixel merging techniques. Virtual light source positions can be generated based on real-world light source data from the image. A rendered simulation of a virtual item can then be integrated into the image to create a modified image for display.
</abstract>

<claims>
1. A method comprising: generating, using one or more processors of a mobile device, an image of a physical environment; receiving a selection an object to lie replaced in the image; removing, from the image, the object using regions that are proximate to the object in the image; generating a render of a virtual model illuminated by one or more virtual light sources based on a lighting scheme in the image; and generating a modified image that depicts the render replacing the object in the physical environment.
2. The method of claim 1, further comprising: determining a lighting scheme of the image.
3. The method of claim 2, wherein determining the lighting scheme comprises determining one or more bright regions of the image.
4. The method of claim 3, further comprising: positioning, in a virtual environment, the one or more virtual light sources based on locations of the one or more bright regions of the image.
5. The method of claim 3, wherein the determining of the one or more bright regions of the image comprises determining an area of pixels in the image having higher brightness values.
6. The method of claim 1, wherein, in the image, the object is depicted in an object image region, and the regions that are proximate to the object in the image are proximate regions that are external to the object image region.
7. The method of claim 6, wherein the object is removed by merging the proximate regions and the object image region.
8. The method of claim 7, wherein the proximate regions and the object image region are merged using a neural network that implements partial convolutional layers.
9. The method of claim 6, wherein the object is removed by interpolating the proximate regions and the object image region.
10. The method of claim 1, further comprising: displaying the image on a display device of the mobile device; and receiving selection of the object through the display device of the mobile device.
11. The method of claim 10, wherein receiving selection of the object comprises receiving selection of a selected region of the image that depicts the object.
12. The method of claim 11, further comprising: generating an image mask using the selected region.
13. The method of claim 11, further comprising: segmenting the image into segment regions using an image segmentation convolutional neural network (CNN), wherein the selected region is identified from a user input on the image as displayed by the mobile device.
14. The method of claim 13, wherein the user input is one of the following: a tap gesture or a click.
15. The method of claim 11, wherein receiving selection of the object through the display device comprises: receiving, through the display device, a swipe gesture over at east a portion of the object as depicted in the image.
16. The method of claim 1, further comprising: classifying the object into an object category using an object classification neural network.
17. The method of claim 16, further comprising: determining a pose of the object using a convolutional neural network trained for the object category; and arranging, in a virtual environment on the mobile device, the virtual model in the pose, wherein the modified image depicts the render in the pose.
18. The method of claim 16, further comprising: determining a pose of the object using a convolutional neural network trained for the object category; determining a plurality of items having virtual models based on the object category of the object; and receiving, on the mobile device, a selection of an item from the plurality of items, the selected item corresponding to the virtual model.
19. A system comprising: one or more processors; a memory storing instructions that, when executed by the one or more processors, cause the system to perform operations comprising: generating an image of a physical environment; receiving a selection an object to be replaced in the image; removing, from the image, the object using regions that are proximate to the object in the image; generating a render of a virtual model illuminated by one or more virtual light sources based on a lighting scheme in the image; and generating a modified image that depicts the render replacing the object in the physical environment.
20. A machine-readable storage device embodying instructions that, when executed by a device, cause the device to perform operations comprising: generating an image of a physical environment; receiving a selection an object to be replaced in the image; removing, from the image, the object using regions that are proximate to the object in the image; generating a render of a virtual model illuminated by one or more virtual light sources based on a lighting scheme in the image; and generating a modified image that depicts the render replacing the object in the physical environment.
</claims>
</document>
