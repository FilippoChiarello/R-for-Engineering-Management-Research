<document>

<filing_date>
2015-12-14
</filing_date>

<publication_date>
2020-08-05
</publication_date>

<priority_date>
2014-12-15
</priority_date>

<ipc_classes>
G10L15/00
</ipc_classes>

<assignee>
BAIDU USA
</assignee>

<inventors>
CASE, CARL
CASPER, JARED
CATANZARO, BRYAN
COATES, ADAM
DIAMOS, GREGORY
ELSEN, ERICH
HANNUN, AWNI
NG, ANDREW
PRENGER, RYAN
SATHEESH, SANJEEV
SHUBHABRATA, SENGUPTA
</inventors>

<docdb_family_id>
56111776
</docdb_family_id>

<title>
SYSTEMS AND METHODS FOR SPEECH TRANSCRIPTION
</title>

<abstract>
Presented herein are embodiments of state-of-the-art speech recognition systems developed using end-to-end deep learning. In embodiments, the model architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, embodiments of the system do not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learn a function that is robust to such effects. A phoneme dictionary, nor even the concept of a 'phoneme,' is needed. Embodiments include a well-optimized recurrent neural network (RNN) training system that can use multiple GPUs, as well as a set of novel data synthesis techniques that allows for a large amount of varied data for training to be efficiently obtained. Embodiments of the system can also handle challenging noisy environments better than widely used, state-of-the-art commercial speech systems.
</abstract>

<claims>
1. A computer-implemented method for training a transcription model, the method comprising: for each of a set of utterances: inputting (105) an utterance that comprises a set of spectrogram frames into a first layer of the transcription model that evaluates each of the spectrogram frames from the set of spectrogram frames with a context of one or more spectrogram frames; outputting (115) from the transcription model a predicted character or character probabilities for the utterance; and computing (120) a loss to measure error in prediction for the utterance; evaluating (125) a gradient of predicted outputs of the transcription model given the ground-truth characters; and updating (130) the transcription model using back-propagation; characterized in that the method further comprises:
jittering at least some of the set of utterances prior to inputting into the first layer of the transcription model, comprising: generating (305) a jitter set of utterances for an utterance by translating an audio file of the utterance by one or more time values, the one or more time values are half a filter bank step size; converting (310) the jitter set of utterances and the utterance into a set of spectrograms; obtaining (315) output results from the transcription model or from a set of transcription models for the set of spectrograms; and blending (320) the output results for the set of spectrograms to obtain an output for the audio file.
2. The computer-implemented method of Claim 1 wherein the blending (320) the output results for the set of spectrograms to obtain an output for the audio file comprises:
averaging the output results for the set of spectrograms to obtain an output for the audio file.
3. The computer-implemented method of Claim 2 further comprising:
generating one or more utterances for a set of training data for use in training the transcription model.
4. The computer-implemented method of Claim 3 wherein generating one or more utterances for the set of training data for using in training the transcription model comprises: having a person wear headphones as the person records an utterance; intentionally inducing a Lombard effect during data collection of the utterance by playing background noise through the headphones worn by the person; and capturing the Lombard-effected utterance of the person via a microphone without capturing the background noise.
5. The computer-implemented method of Claim 3 wherein generating one or more utterances for the set of training data for using in training the transcription model comprises: adding one or more noise clips selected from a set of approved noise clips to form a noise track; adjusting a signal-to-noise ratio of the noise track relative to an audio file; adding the adjusted noise track to the audio file to form a synthesized noise audio file; and adding the synthesized noise audio file to the set of training data, preferably, the set of approved noise clips are generated by performing the steps comprising: collecting a set of candidate noise clips; and repeating the following steps until each noise clip from the set of candidate noise clips has been evaluated: selecting a candidate noise clip from the set of candidate noise clips for evaluation; responsive to the candidate noise clip's average powers in frequency bands not differing significantly from average powers in frequency bands observed in real noise recordings, adding the candidate noise clip to the set of approved noise clips; and responsive to the candidate noise clip's average powers in frequency bands differing significantly from average powers observed in real noise recordings, discarding the candidate noise clip.
6. The computer-implemented method of Claim 1 further comprising incorporating one or more optimizations in the training of the transcription model, comprising: incorporating one or more data parallelisms; incorporating a model parallelism; and striding the input into the first layer of the transcription model.
7. The computer-implemented method of Claim 6 wherein the step of incorporating one or more data parallelism comprises: using several copies of the transcription model across multiple processing units with each processing unit processing a separate minibatch of utterances; and combining a computed gradient from a processing unit with its peers during each iteration, or wherein the step of incorporating one or more data parallelism comprises: having each processing unit process many utterances in parallel by concatenating many utterances into a single matrix; and sorting utterances by length and combining similarly-sized utterances into minibatches and padding utterances with silence so that all utterances in a given minibatch have the same length.
8. A computer-implemented method for transcribing speech comprising: receiving (1305) an input audio from a user; normalizing (1310) the input audio to make a total power of the input audio consistent with a total power of a set of training samples used to train a trained neural network model; generating (1315) a jitter set of audio files from the normalized input audio by translating the normalized input audio by one or more time values; for each audio file from the jitter set of audio files, which includes the normalized input audio: generating (1320) a set of spectrogram frames for each audio file; inputting (1325) the audio file along with a context of spectrogram frames into a trained neural network; obtaining predicted character probabilities outputs from the trained neural network; and decoding (1335) a transcription of the input audio using the predicted character probabilities outputs from the trained neural network constrained by a language model that interprets a string of characters from the predicted character probabilities outputs as a word or words; characterized in that the one or more time values are half a filter bank step size; and the obtaining predicted character probabilities outputs from the trained neural network comprises: obtaining (315) output results from the trained neural network for the set of spectrograms; and blending (320) the output results for the set of spectrograms to obtain an output for the audio file.
9. The computer-implemented method of Claim 8 wherein the step of generating a set of spectrogram frames for each audio file comprises:
generating spectrogram frames wherein each spectrogram frame comprises a set of linearly spaced log filter banks computed over windows of a first value of milliseconds strided by a second value of milliseconds.
10. The computer-implemented method of Claim 8 wherein: the step of inputting the audio file along with a context of spectrogram frames into a trained neural network comprises:
inputting the audio file along with a context of spectrogram frames into a plurality of trained neural network; and the step of obtaining predicted character probabilities outputs from the trained neural network comprises:
ensembling predicted character probabilities outputs from the set of trained neural networks to obtain the predicted character probabilities, preferably, the step of ensembling predicted character probabilities outputs from the set of trained neural networks to obtain the predicted character probabilities comprises:
addressing time shifts between trained neural network models by using one or more of the following comprising: using neural network models that exhibit the same temporal shift; checking alignment between outputs of neural network models and shifting one or more of the outputs to align the outputs; and shifting the inputs into one or more of the neural network models to have aligned outputs.
11. The computer-implemented method of Claim 8 wherein the step of decoding a transcription of the input audio using the predicted character probabilities outputs from the trained neural network constrained by a language model that interprets the string of characters as a word or words comprises:
given the predicted character probabilities outputs from the trained neural network, performing a search to find a sequence of characters that is most probable according to both the predicted character probabilities outputs and a trained N-gram language model that interprets a string of characters from the predicted character probabilities outputs as a word or words, or
wherein the neural network model comprises a five-layer model comprising: a first set of three layers that are non-recurrent; a fourth layer that is a bi-directional recurrent network, which includes two sets of hidden units comprising a set with forward recurrence and a set with backward recurrence; and a fifth layer that is a non-recurrent layer, which takes outputs of forward and backward units from the fourth layer as inputs and outputs the predicted character probabilities.
12. A non-transitory computer-readable medium or media comprising one or more sequences of instructions which, when executed by one or more processors, causes steps to be performed, the steps comprising: receiving an input audio from a user; generating a set of spectrogram frames from the audio file; inputting the audio file along with a context of spectrogram frames into a set of trained neural networks; obtaining predicted character probabilities outputs from the set of trained neural network; and decoding a transcription of the input audio using the predicted character probabilities outputs from the set of trained neural networks constrained by a language model that interprets a string of characters from the predicted character probabilities outputs as a word or words; characterized in that the steps further comprise: generating (1315) a jitter set of audio files from the normalized input audio by translating the normalized input audio by one or more time values, the one or more time values are half a filter bank step size; and the obtaining predicted character probabilities outputs from the set of trained neural network comprises: obtaining (315) output results from the trained neural network for the set of spectrograms; and blending (320) the output results for the set of spectrograms to obtain an output for the audio file.
13. The non-transitory computer-readable medium or media of Claim 12 wherein the step of generating a set of spectrogram frames comprises:
generating spectrogram frames wherein each spectrogram frame comprises a set of linearly spaced log filter banks computed over windows of a first value of milliseconds strided by a second value of milliseconds.
14. The non-transitory computer-readable medium or media of Claim 12 wherein the step of obtaining predicted character probabilities outputs from the set of trained neural network comprises:
ensembling predicted character probabilities outputs from the set of trained neural networks to obtain the predicted character probabilities,
preferably, the step of ensembling predicted character probabilities outputs from the set of trained neural networks to obtain the predicted character probabilities comprises:
addressing time shifts between trained neural network models by using one or more of the following comprising: using neural network models that exhibit the same temporal shift; checking alignment between outputs of neural network models and shifting one or more of the outputs to align the outputs; and shifting the inputs into one or more of the neural network models to have aligned outputs.
</claims>
</document>
