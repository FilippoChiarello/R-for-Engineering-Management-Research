<document>

<filing_date>
2019-05-02
</filing_date>

<publication_date>
2020-11-05
</publication_date>

<priority_date>
2019-05-02
</priority_date>

<ipc_classes>
G06N20/00
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
BUI, TRUNG HUU
COHEN, SCOTT
LIN ZHE
Ling, Mingyang
Wu, Chenyun
</inventors>

<docdb_family_id>
73016556
</docdb_family_id>

<title>
MULTI-MODULE AND MULTI-TASK MACHINE LEARNING SYSTEM BASED ON AN ENSEMBLE OF DATASETS
</title>

<abstract>
Techniques and systems are provided for training a machine learning model using different datasets to perform one or more tasks. The machine learning model can include a first sub-module configured to perform a first task and a second sub-module configured to perform a second task. The first sub-module can be selected for training using a first training dataset based on a format of the first training dataset. The first sub-module can then be trained using the first training dataset to perform the first task. The second sub-module can be selected for training using a second training dataset based on a format of the second training dataset. The second sub-module can then be trained using the second training dataset to perform the second task.
</abstract>

<claims>
1. A method of training a machine learning model, comprising: generating a machine learning model configured to perform one or more tasks based on different training datasets, wherein the machine learning model includes a first sub-module configured to perform a first task and a second sub-module configured to perform a second task; obtaining a first training dataset having a first format; obtaining a second training dataset having a second format, the second format being different than the first format; training, using the first training dataset, the first sub-module to perform the first task, wherein the first sub-module is selected for training using the first training dataset based on the first format; and training, using the second training dataset, the second sub-module to perform the second task, wherein the second sub-module is selected for training using the second training dataset based on the second format.
2. The method of claim 1, further comprising: selecting the first sub-module for being trained by the first training dataset based on the first format of the first training dataset; and selecting the second sub-module for being trained by the second training dataset based on the second format of the second training dataset.
3. The method of claim 1, further comprising: obtaining an output from the first sub-module based on the training of the first sub-module using the first training dataset; and selecting an additional dataset for training the first sub-module based on the obtained output.
4. The method of claim 1, wherein a combination of a portion of the first training dataset and a portion of the second training dataset is processed by the machine learning model.
5. The method of claim 4, wherein a percentage of data from the first training dataset and a percentage of data from the second training dataset included in the combination are configurable using one or more parameters input to the machine learning model.
6. The method of claim 1, wherein the machine learning model includes at least one shared layer included in the first sub-module and the second sub-module, and includes at least one non-shared layer included in the first sub-module and not included in the second sub-module.
7. The method of claim 6, wherein the at least one shared layer is trained using the first training dataset and the second training dataset, and wherein the at least one non-shared layer is trained using the first training dataset.
8. The method of claim 1, further comprising: obtaining a third training dataset; and training, using the third training dataset, the first sub-module and the second sub-module to perform at least a third task.
9. The method of claim 1, further comprising: obtaining a third training dataset; and training, using the first training dataset and the third training dataset, the first sub-module to perform at least the first task.
10. The method of claim 1, wherein the first format of the first training dataset includes at least one of a name of the first training dataset or a task for which the first training dataset is applicable, and wherein the second format of the second training dataset includes at least one of a name of the second training dataset or a task for which the second training dataset is applicable.
11. The method of claim 1, wherein the machine learning model is a neural network model.
12. A machine learning system, comprising: an input device configured to receive a first training dataset and a second training dataset, the first training dataset having a first format and the second training dataset having a second format, wherein the first format is different than the second format; a machine learning model including a first sub-module with a first plurality of neural network layers for performing a first task and a second sub-module with a second plurality of neural network layers for performing a second task; a sub-module determination engine configured to: select, based on the first format, the first sub-module for being trained by the first training dataset, wherein the first plurality of neural network layers of the first sub-module are trained using the first training dataset to perform the first task; and select, based on the second format, the second sub-module for being trained by the second training dataset, wherein the second plurality of neural network layers of the second sub-module are trained using the second training dataset to perform the second task; and an output device configured to output a machine learning output of the machine learning model.
13. The machine learning system of claim 12, wherein the output device is configured to obtain an output from the first sub-module based on the training of the first sub-module using the first training dataset, and wherein the input device is configured to select an additional dataset for training the first sub-module based on the obtained output.
14. The machine learning system of claim 12, wherein the input device is configured to receive one or more parameters, and wherein the machine learning model is configured to use the one or more parameters to determine a percentage of data from the first training dataset and a percentage of data from the second training dataset to use for training the first sub-module and the second sub-module.
15. The machine learning system of claim 12, wherein the first sub-module and the second sub-module include at least one shared neural network layer included in both the first sub-module and the second sub-module, and wherein the first sub-module includes at least one non-shared neural network layer included in the first sub-module and not included in the second sub-module.
16. The machine learning system of claim 15, wherein the at least one shared neural network layer is trained using the first training dataset and the second training dataset, and wherein the at least one non-shared neural network layer is trained using the first training dataset and not the second training dataset.
17. The machine learning system of claim 12, wherein the input device is configured to obtain a third training dataset, and wherein the first plurality of neural network layers of the first sub-module and the second plurality of neural network layers of the second sub-module are trained using the third training dataset to perform at least a third task.
18. The machine learning system of claim 12, wherein the input device is configured to obtain a third training dataset, and wherein the first plurality of neural network layers of the first sub-module are trained using the first training dataset and the third training dataset to perform at least the first task.
19. A non-transitory computer-readable medium having stored thereon instructions that, when executed by one or more processors, cause the one or more processors to: generate a machine learning model configured to perform one or more tasks based on different training datasets, wherein the machine learning model includes a first sub-module configured to perform a first task and a second sub-module configured to perform a second task; obtain a first training dataset having a first format; obtain a second training dataset having a second format, the second format being different than the first format; train, using the first training dataset, the first sub-module to perform the first task, wherein the first sub-module is selected for training using the first training dataset based on the first format; and train, using the second training dataset, the second sub-module to perform the second task, wherein the second sub-module is selected for training using the second training dataset based on the second format.
20. The non-transitory computer-readable medium of claim 19, wherein the first sub-module and the second sub-module include at least one shared layer included in both the first sub-module and the second sub-module, and wherein the first sub-module includes at least one non-shared layer included in the first sub-module and not included in the second sub-module.
</claims>
</document>
