<document>

<filing_date>
2019-11-22
</filing_date>

<publication_date>
2020-05-28
</publication_date>

<priority_date>
2018-11-28
</priority_date>

<ipc_classes>
B22C9/04,B33Y50/02,G05B13/02,G05B19/4099,G06K9/62,G06N3/08
</ipc_classes>

<assignee>
ELEMENT AI
</assignee>

<inventors>
HOOPER, CHARLES
STEEVES, PATRICK
TANNINEN, PETRI JUHANI
DLUBAK, ANNA
SHAL ZOGHI, HAMED
</inventors>

<docdb_family_id>
70770739
</docdb_family_id>

<title>
SYSTEMS AND METHODS FOR ERROR REDUCTION IN MATERIALS CASTING
</title>

<abstract>
Deep learning approaches and systems are described to control the process of casting physical objects. A neural network, operating on one or more processors of a server or distributed computing resources and maintained in one or more data storage devices, is trained to recognize relationships between the target digital representation and the resulting metal parts that are cast, and a number of specific approaches are described herein to overcome technical issues in relation to misalignments between reference points, among others. These deep learning approaches are then used for generation of command or control signals which modify how the casting process is conducted. Command or control signals can be used to modify how a cast mold is made, to modify environmental variables, to modify manufacturing parameters, and combinations thereof.
</abstract>

<claims>
1. A system for casting physical parts from a 3D CAD model, the system comprising: a design model data receiver configured to receive data sets representative of 3D points relating to a 3D CAD model for an ideal part; a parts measurement data receiver configured to receive data sets representative of 3D points relating to a cast mold or cast physical parts; a receiver configured to receive data sets representative of 3D points relating to a cast mold and the cast physical parts; and a neural network training engine configured for tracking relationships between features of between the 3D points relating to the cast mold and the cast physical parts and the 3D points relating to a 3D CAD model for the ideal part.
2. The system of claim 1, wherein the casting is sand casting.
3. The system of claim 2, wherein the 3D digital representations are established through point cloud sets such that the reference 3D digital representation is a reference point cloud set and the 3D digital representation of the physical parts is a point cloud set of the physical parts.
4. The system of claim 3, wherein the reference point cloud set is generated by: 3D scanning a prototype part representative of the ideal part to create a point cloud set representation of the prototype part; extending projections from each point in the point cloud set representation of the prototype part that are perpendicular to the surface of the 3D CAD model, wherein the reference point cloud set includes points where the extending projections intersect with the surface of the object represented in the 3D CAD model.
5. The system of claim 4, wherein the neural network training set is generated by: 3D scanning the surface of the physical parts to create a point cloud set representation; converting the point cloud set representation into a projected 3D surface of each physical part; and extending projections from each point in the reference point cloud set that are perpendicular with the surface of the CAD model, wherein the neural network training set comprises of the points where the projection intersects with the projected 3D surface of each physical part.
6. The system of claim 5, wherein the converting of the point cloud set representation of each physical part into the corresponding projected 3D surface of the physical part utilizes a non-uniform rational b-spline.
7. The system of claim 6, wherein the training of the neural network includes aspects in a feature set other than the 3D spatial data, the training further comprising: appending at least one metadata quantity representing the ideal conditions the casting process is to be run at onto the reference point cloud representation; recording metadata corresponding with the quantities appended to the reference point cloud representation and appending the metadata to point cloud representations in the training data set.
8. The system of claim 7, wherein the characteristics other than the 3D spatial data include least one of humidity or temperature.
9. The system of claim 1, wherein the neural network is a deep neural net inference process of instructions stored on computer readable media.
10. The system of claim 1, wherein the 3D digital representation form is triangulated meshes, voxels, or NURBS surfaces.
11. A method for casting physical parts from a 3D CAD model, the method comprising: manufacturing a prototype part from the 3D CAD model; creating a cast mold from the prototype part; creating a reference 3D digital representation from the 3D CAD model; casting a plurality of physical parts from the cast mold; generating a neural network training data set by creating 3D digital representations of the physical parts; training a neural network by comparing the reference 3D digital representation with the neural network training data set; running the trained neural network in reverse to create a 3D digital representation of a new prototype part to create a new cast from; creating a new cast mold from the new prototype part; and casting physical parts from the new cast mold.
12. The method of claim 11, wherein the casting is sand casting.
13. The method of claim 12, wherein the 3D digital representations are established using point cloud sets such that the reference 3D digital representation is the reference point cloud set and the 3D digital representation of the physical parts is the point cloud set of the physical parts.
14. The method of claim 13, wherein the reference point cloud set generated by: 3D scanning the prototype part to create a point cloud set representation of the prototype part; and extending projections from each point in the point cloud set representation of the prototype part that are perpendicular to the surface of the 3D CAD model, wherein the reference point cloud set comprises of the points where the extending projections intersect with the surface of the object represented in the 3D CAD model.
15. The method of claim 14, wherein the generating of the neural network training set comprises: 3D scanning the surface of the physical part to create a point cloud set representation; converting the point cloud set representation into a projected 3D surface of the physical part; and extending projections from each point in the reference point cloud set that are perpendicular with the surface of the CAD model, wherein the neural network training set comprises of the points where the projection intersects with the projected 3D surface of the physical part.
16. The method of claim 15, wherein the converting of the point cloud set representation of the physical part into the projected 3D surface of the physical part is achieved through a non-uniform rational b-spline.
17. The method of claim 16, wherein the training of the neural network takes into consideration characteristics other than the 3D spatial data, the training further comprising: appending at least one metadata quantity representing the ideal conditions the casting process is to be run at onto the reference point cloud representation; recording metadata corresponding with the quantities appended to the reference point cloud representation and appending the metadata to point cloud representations in the training data set.
18. The method of claim 17, wherein the characteristics other than the 3D spatial data include least one of humidity or temperature.
19. The method of claim 11, wherein the trained neural network used is a deep neural net inference process.
20. The method of claim 11, wherein the 3D digital representation form includes at least one of triangulated meshes, voxels, or NURBS surfaces.
</claims>
</document>
