<document>

<filing_date>
2016-12-05
</filing_date>

<publication_date>
2020-09-01
</publication_date>

<priority_date>
2016-12-05
</priority_date>

<ipc_classes>
G06F17/16,G06N3/04,G06N3/08,G06N7/00
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
FUSI, NICOLO
</inventors>

<docdb_family_id>
62243241
</docdb_family_id>

<title>
Probabilistic matrix factorization for automated machine learning
</title>

<abstract>
In embodiments of probabilistic matrix factorization for automated machine learning, a computing system memory maintains different workflows that each include preprocessing steps for a machine learning model, the machine learning model, and one or more parameters for the machine learning model. The computing system memory additionally maintains different data sets, upon which the different workflows can be trained and tested. A matrix is generated from the different workflows and different data sets, where cells of the matrix are populated with performance metrics that each indicate a measure of performance for a workflow applied to a data set. A low-rank decomposition of the matrix with populated performance metrics is then determined. Based on the low-rank decomposition, an optimum workflow for a new data set can be determined. The optimum workflow can be one of the different workflows or a hybrid of at least two of the different workflows.
</abstract>

<claims>
1. A computing system implemented for probabilistic matrix factorization, the system comprising: memory configured to maintain different data sets and different workflows for use in probabilistic matrix factorization for automated machine learning; a processor system to implement a matrix generation module and a matrix factorization module, the matrix generation module configured to: generate a populated matrix using at least two of the different workflows and at least two of the different data sets; select a subset of the different data sets and for each data set in the selected subset: apply one or more of the different workflows to the data set, the one or more different workflows selected to be trained and tested on the data set; and record a workflow performance metric for each of the selected one or more different workflows applied to the data set, the workflow performance metric recorded in a cell of the populated matrix; the matrix factorization module configured to: calculate a low-rank decomposition of the populated matrix with the workflow performance metrics; and determine, for a new data set, a hybrid workflow using the calculated low-rank decomposition of the populated matrix, the hybrid workflow including a combination of portions of at least two of the different workflows, corresponding predicted performances of the at least two different workflows determined based on the low-rank decomposition, a predicted performance of the hybrid workflow determined by interpolating between the corresponding predicted performances, and the hybrid workflow not included in either of the populated matrix of the at least two different workflows.
2. The computing system as recited in claim 1, wherein each of the different workflows comprises one or more preprocessing steps for a machine learning model, the machine learning model, and at least one parameter for the machine learning model or the one or more preprocessing steps.
3. The computing system as recited in claim 2, wherein a combination of the preprocessing steps, the machine learning model, and the at least one parameter of a workflow differs from a combination of the preprocessing steps, the machine learning model, and the at least one parameter of the other different workflows.
4. The computing system as recited in claim 2, wherein the at least one parameter for the machine learning model comprises one or more of a number of leaves of a tree structure, a depth of the tree structure, a number of latent factors in a matrix factorization, a learning rate, or a number of hidden layers in a deep neural network (DNN).
5. The computing system as recited in claim 2, wherein the machine learning model comprises one of a deep neural network (DNN), a convolutional neural network (CNN), or a random forest.
6. The computing system as recited in claim 1, wherein individual ones of the different data sets include data that is not comparable to or congruent with data of other ones of the different data sets.
7. The computing system as recited in claim 1, wherein the workflow performance metric is an accuracy of workflow of the workflows applied to the data set.
8. The computing system as recited in claim 1, wherein the workflow performance metric is a runtime of the different workflows applied to the data set.
9. The computing system as recited in claim 1, wherein each row of the populated matrix corresponds to one of the different workflows and each column of the populated matrix corresponds to one of the different data sets.
10. The computing system as recited in claim 1, wherein the low-rank decomposition of the populated matrix is calculated using a probabilistic non-linear low-rank decomposition.
11. The computing system as recited in claim 1, wherein the low-rank decomposition of the populated matrix is calculated using dimensionality reduction techniques based on Gaussian processes.
12. The computing system as recited in claim 1, wherein the hybrid workflow comprises a workflow that provides a maximum performance metric, among the selected workflows, for the new data set.
13. The computing system as recited in claim 1, wherein the hybrid workflow is determined based on an expected improvement of the hybrid workflow relative to an expected improvement threshold.
14. The computing system as recited in claim 1, wherein the hybrid workflow is determined based on a probability of improvement of the hybrid workflow relative to a probability of improvement threshold.
15. The computing system as recited in claim 1, wherein the hybrid workflow is determined based on an expected improvement per unit of time of hybrid workflow relative to an expected improvement per unit of time threshold.
16. The computing system as recited in claim 1, wherein the matrix factorization module is further configured to apply the hybrid workflow to the new data set, record a performance metric resulting from the application of the hybrid workflow to the new data set, and update the populated matrix to include data corresponding to the performance metric resulting from the application of the hybrid workflow to the new data set.
17. A method, comprising: generating a matrix using at least two different data sets to define columns of the matrix and at least two different workflows to define rows of the matrix, each of the different workflows comprising one or more preprocessing steps for a machine learning model, the machine learning model, and at least one parameter for the machine learning model or the one or more preprocessing steps; selecting a subset of the different data sets and for each data set in the subset of data sets: selecting one or more of the different workflows to be trained and tested on the data set; applying the selected one or more workflows to the data set; and generating a populated matrix by recording a workflow performance metric for each workflow applied to the data set in a cell of the generated matrix; calculating a low-rank decomposition of the populated matrix; and determining, for a new data set, a hybrid workflow using the calculated low-rank decomposition of the populated matrix, the hybrid workflow comprising one of the different workflows that provides a maximum performance metric for the new data set the hybrid workflow including a combination of portions of at least two of the different workflows, corresponding predicted performances of the at least two different workflows determined based on the low-rank decomposition, a predicted performance of the hybrid workflow determined by interpolating between the corresponding predicted performances, and the hybrid workflow not included in either of the populated matrix of the at least two different workflows.
18. A non-transitory computer-readable storage memory comprising stored instructions that are executable and, responsive to execution of the stored instructions by a computing system, the computing system performs operations comprising: generating a matrix using at least two different data sets to define columns of the matrix and at least two different workflows to define rows of the matrix, each of the different workflows comprising one or more preprocessing steps for a machine learning model, the machine learning model, and at least one parameter for the machine learning model or the one or more preprocessing steps; selecting a subset of the different data sets and for each data set in the subset of data sets: selecting one or more of the different workflows to be trained and tested on the data set; applying the selected one or more workflows to the data set; and generating a populated matrix by recording a workflow performance metric for each workflow applied to the data set in a cell of the generated matrix; calculating a low-rank decomposition of the populated matrix; and determining, for a new data set, a hybrid workflow using the calculated low-rank decomposition of the populated matrix, the hybrid workflow comprising one of the different workflows that provides a maximum performance metric for the new data set, the hybrid workflow including a combination of portions of at least two of the different workflows, corresponding predicted performances of the at least two different workflows determined based on the low-rank decomposition, a predicted performance of the hybrid workflow determined by interpolating between the corresponding predicted performances, and the hybrid workflow not included in either of the populated matrix of the at least two different workflows.
</claims>
</document>
