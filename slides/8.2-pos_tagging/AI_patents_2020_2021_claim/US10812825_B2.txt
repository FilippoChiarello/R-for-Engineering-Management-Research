<document>

<filing_date>
2017-09-29
</filing_date>

<publication_date>
2020-10-20
</publication_date>

<priority_date>
2016-11-14
</priority_date>

<ipc_classes>
G06N3/08,G06T3/40,H04N19/587,H04N19/89,H04N7/01,H04N7/12
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
AGARWALA, ASEEM
LIU, YIMING
Liu, Ziwei
</inventors>

<docdb_family_id>
60117784
</docdb_family_id>

<title>
Video frame synthesis with deep learning
</title>

<abstract>
The present disclosure provides systems and methods that leverage machine-learned models (e.g., neural networks) to provide video frame synthesis. In particular, the systems and methods of the present disclosure can include or otherwise leverage a machine-learned video frame synthesis model to allow for video frames to be synthesized from videos. In one particular example, the video frame synthesis model can include a convolutional neural network having a voxel flow layer and provides one or more synthesized video frames as part of slow-motion video.
</abstract>

<claims>
1. A computer-implemented method for video frame synthesis, the method comprising: receiving, by one or more computing devices, a video; inputting, by the one or more computing devices, a first set of sequential frame data descriptive of the video into a machine-learned video frame synthesis model, wherein the machine-learned video frame synthesis model comprises at least one convolutional neural network having a voxel flow layer, the at least one convolutional neural network is at least partially trained using unsupervised learning to predict a voxel flow field comprising a per-pixel, 3D optical flow vector across space and time; receiving, by the one or more computing devices, one or more synthesized frames from the video, the one or more synthesized frames output by the machine-learned video frame synthesis model; and providing, by the one or more computing devices, information regarding the one or more synthesized frames.
2. The computer-implemented method of claim 1, wherein: the at least one convolutional neural network comprises a convolutional encoder-decoder network.
3. The computer-implemented method of claim 2, wherein the convolutional encoder-decoder network includes at least one skip connection between at least one convolution layer and at least one deconvolution layer.
4. The computer-implemented method of claim 1, further comprising: providing, by the one or more computing devices, information regarding the one or more synthesized frames as part of a slow-motion video.
5. The computer-implemented method of claim 1, wherein at least one of the one or more synthesized frames output by the machine-learned video frame synthesis model comprises an interpolated frame in-between the first set of sequential frame data.
6. The computer-implemented method of claim 1, wherein at least one of the one or more synthesized frames output by the machine-learned video frame synthesis model comprises a predicted frame after the first set of sequential frame data.
7. The computer-implemented method of claim 1, further comprising providing, by the one or more computing devices, a modified video that includes an interpolated frame.
8. A computing system for video frame synthesis, the computing system comprising: at least one processor; a machine-learned video frame synthesis model that comprises at least one convolutional neural network having a voxel flow layer, wherein the video frame synthesis model is trained to receive a first set of sequential frame data descriptive of a video and, in response to receipt of the first set of sequential frame data, determine one or more synthesized frames from the video, the at least one convolutional neural network is at least partially trained using unsupervised learning to predict a voxel flow field comprising a per-pixel, 3D optical flow vector across space and time; and at least one tangible, non-transitory computer-readable medium that stores instructions that, when executed by the at least one processor, cause the at least one processor to: obtain the first set of sequential frame data descriptive of the video; input the first set of sequential frames into the video frame synthesis model; and receive, as an output of the video frame synthesis model, the one or more synthesized frames.
9. The computing system of claim 8, wherein the video comprises a high definition video.
10. The computing system of claim 8, wherein at least one of the one or more synthesized frames output by the machine-learned video frame synthesis model comprises an interpolated frame in-between the first set of sequential frame data.
11. The computing system of claim 8, wherein at least one of the one or more synthesized frames output by the machine-learned video frame synthesis model comprises a predicted frame after the first set of sequential frame data.
12. The computing system of claim 8, wherein the at least one convolutional neural network comprises an intermediate layer in the machine-learned video frame synthesis model.
13. A user computing device, the user computing device comprising: at least one processor; and at least one non-transitory computer-readable medium that stores instructions that, when executed by the at least one processor, cause the user computing device to: receive a video; input a first set of sequential frame data descriptive of the video into a machine-learned video frame synthesis model, wherein the machine-learned video frame synthesis model comprises at least one convolutional neural network having a voxel flow layer, the at least one convolutional neural network is at least partially trained using unsupervised learning to predict a voxel flow field comprising a per-pixel, 3D optical flow vector across space and time; receive one or more synthesized frames from the video, the one or more synthesized frames output by the machine-learned video frame synthesis model; and display information regarding the one or more synthesized frames.
14. The computer-readable medium of a claim 13, further configured to store the machine-learned video frame synthesis model.
15. The computer-readable medium of a claim 13, wherein the execution of the instructions further causes the at least one processor to: provide the synthesized frames as part of a slow-motion video.
16. The computer-implemented method of claim 2, wherein: the convolution encoder-decoder network is trained to predict the voxel flow field comprising the per-pixel, 3D optical flow vector across space and time; the one or more synthesized frames are output by the machine-learned video frame synthesis model based on the first set of sequential frames and the predicted voxel flow field by sampling the voxel flow field; the convolutional neural network is trained in an unsupervised fashion by: removing one or more frames from the received video; providing a plurality of the remaining frames as the first set of sequential frames; generating the one or more synthesized frames to correspond to the one or more removed frames; calculating a loss function to represent a similarity between the one or more synthesized frames and the one or more removed frames; and adjusting one or more parameters of the convolutional neural network in order to minimize the calculated loss function.
17. The computer-implemented method of claim 16, wherein: the one or more synthesized frames are generated by the machine-learned video frame synthesis model based on the first set of sequential frames and the predicted voxel flow field by warping one or more frames of the received video according to the predicted voxel flow field.
18. The computer-implemented method of claim 17, wherein: the machine-learned video frame synthesis model warps the one or more frames of the received video using a volume sampling function operating on spatio-temporal coordinates.
19. The computer-implemented method of claim 18, wherein the machine-learned video frame synthesis model includes: one or more convolution layers and one or more deconvolution layers; and one or more network hyperparameters specifying a size of one or more feature maps and a number of channels.
20. The computing system of claim 8, wherein: the at least one convolutional neural network comprises a convolutional encoder-decoder network; the convolution encoder-decoder network is trained to predict the voxel flow field comprising the per-pixel, 3D optical flow vector across space and time; the one or more synthesized frames are output by the machine-learned video frame synthesis model based on the first set of sequential frames and the predicted voxel flow field by sampling the voxel flow field; the convolutional neural network is trained in an unsupervised fashion by: providing a plurality of the remaining frames as the first set of sequential frames; generating the one or more synthesized frames to correspond to the one or more removed frames; calculating a loss function to represent a similarity between the one or more synthesized frames and the one or more removed frames; and adjusting one or more parameters of the convolutional neural network in order to minimize the calculated loss function.
</claims>
</document>
