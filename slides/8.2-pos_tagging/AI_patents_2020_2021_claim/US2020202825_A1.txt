<document>

<filing_date>
2018-12-21
</filing_date>

<publication_date>
2020-06-25
</publication_date>

<priority_date>
2018-12-21
</priority_date>

<ipc_classes>
G06K9/00,G06N20/00,G10H1/00
</ipc_classes>

<assignee>
ELECTRONIC ARTS
</assignee>

<inventors>
AGHDAIE, NAVID
CHAPUT, HAROLD HENRY
KOLEN, JOHN
MOSS, KENNETH ALAN
ZAMAN, KAZI ATIF-UZ
</inventors>

<docdb_family_id>
71098694
</docdb_family_id>

<title>
Enhanced real-time audio generation via cloud-based virtualized orchestra
</title>

<abstract>
Systems and methods are provided for enhanced real-time audio generation via a virtualized orchestra. An example method includes receiving, from a user device, a request to generate output associated with a musical score. Actions associated with virtual musicians with respect to respective instruments are simulated based on one or more machine learning models, with the simulated actions being associated with a virtual musician and indicative of an expected playing style during performance of the musical score. Output audio to be provided to the user device is generated, with the output audio being generated based on the simulated actions.
</abstract>

<claims>
1. A computer-implemented method comprising: by a system of one or more computers, receiving, from a user device, a request to generate output audio associated with a musical score; simulating, based on one or more machine learning models, actions associated with virtual musicians with respect to respective instruments during performance of the musical score; receiving user input comprising conductor cues during the performance of the musical score, the user input being received from an interactive user interface presented via the user device and the interactive user interface presenting a graphical representation of the virtual musicians or the instruments, wherein a particular conductor cue of the conductor cues is directed to a subset of the virtual musicians or to a particular instrument of the subset of the virtual musicians, and wherein the simulated actions associated with the subset of the virtual musicians are generated based, at least in part, on the particular conductor cue, the simulated actions being indicative of the subset of the virtual musicians performing the musical score based, at least in part, on the particular conductor cue; and generating output audio to be provided to the user device, the output audio being generated based on the simulated actions by the virtual musicians.
2. The computer-implemented method of claim 1, wherein the generated output audio is provided to the user device in substantially real-time, such that the user may provide the user input to adjust the simulated actions during performance of the musical score.
3. The computer-implemented method of claim 2, wherein the conductor cues are based on movement of a user's hands or an input device during performance of the musical score.
4. The computer-implemented method of claim 3, wherein a conductor cue indicates a particular tempo, a particular beat, an adjustment of articulation, or an adjustment of dynamics.
5. The computer-implemented method of claim 1, wherein the simulated actions indicate key-features associated with a playing style.
6. The computer-implemented method of claim 1, wherein the simulated actions represent continual actions generated via the one or more machine learning models, the continual actions representing expected actions a real-world musician would perform on an instrument.
7. The computer-implemented method of claim 1, wherein generating output comprises generating audio associated with each of the instruments and aggregating the generated audio.
8. The computer-implemented method of claim 7, wherein generating audio associated with a particular instrument comprises: accessing information describing a physical model of the particular instrument, the physical model representing geometric and/or structural characteristics of the particular instrument; applying the simulated actions associated with the particular instrument; and generating audio based on the application.
9. The computer-implemented method of claim 7, wherein generating audio associated with a particular instrument comprises: accessing a plurality of musical instrument digital interface (MIDI) samples associated with the particular instrument, the MIDI samples being associated with labels representing aspects of a playing style; selecting, based on the simulated actions associated with the particular instrument, MIDI samples; and generating audio based on the selected MIDI samples.
10. A system comprising one or more computers and non-transitory computer storage media storing instructions that when executed by the one or more computers, cause the one or more computers to perform operations comprising: receiving, from a user device, a request to generate output audio associated with a musical score; simulating, based on one or more machine learning models, actions associated with virtual musicians with respect to respective instruments during performance of the musical score; receiving user input comprising conductor cues during the performance of the musical score, the user input being received from an interactive user interface presented via the user device and the interactive user interface presenting a graphical representation of the virtual musicians or the instruments, wherein a particular conductor cue of the conductor cues is directed to a subset of the virtual musicians or to a particular instrument of the subset of the virtual musicians, and wherein the simulated actions associated with the subset of the virtual musicians are generated based, at least in part, on the particular conductor cue, the simulated actions being indicative of the subset of the virtual musicians performing the musical score based, at least in part, on the particular conductor cue; and generating output audio to be provided to the user device, the output audio being generated based on the simulated actions by the virtual musicians.
11. The system of claim 10, wherein the conductor cues are based on movement of a user's hands or an input device.
12. The system of claim 11, wherein a conductor cue indicates a particular tempo, a particular beat, an adjustment of articulation, or an adjustment of dynamics.
13. The system of claim 10, wherein the simulated actions indicate key-features associated with a playing style.
14. The system of claim 10, wherein the simulated actions represent continual actions generated via the one or more machine learning models, the continual actions representing expected actions a real-world musician would perform on an instrument.
15. The system of claim 10, wherein generating output comprises generating audio associated with each of the instruments and aggregating the generated audio.
16. The system of claim 15, wherein generating audio associated with a particular instrument comprises: accessing a plurality of musical instrument digital interface (MIDI) samples associated with the particular instrument, the MIDI samples being associated with labels representing aspects of a playing style; selecting, based on the simulated actions associated with the particular instrument, MIDI samples; and generating audio based on the selected MIDI samples.
17. Non-transitory computer storage media storing instructions that when executed by a system of one or more computers, cause the computers to perform operations comprising: receiving, from a user device, a request to generate output audio associated with a musical score; simulating, based on one or more machine learning models, actions associated with virtual musicians with respect to respective instruments during performance of the musical score; receiving user input comprising conductor cues during the performance of the musical score, the user input being received from an interactive user interface presented via the user device and the interactive user interface presenting a graphical representation of the virtual musicians or the instruments, wherein a particular conductor cue of the conductor cues is directed to a subset of the virtual musicians or to a particular instrument of the subset of the virtual musicians, and wherein the simulated actions associated with the subset of the virtual musicians are generated based, at least in part, on the particular conductor cue, the simulated actions being indicative of the subset of the virtual musicians performing the musical score based, at least in part, on the particular conductor cue; and generating output audio to be provided to the user device, the output audio being generated based on the simulated actions by the virtual musicians.
18. The computer storage media of claim 17, wherein the conductor cues are based on movement of a user's hands or an input device.
19. The computer storage media of claim 17, wherein generating output comprises generating audio associated with each of the instruments and aggregating the generated audio.
20. The computer storage media of claim 19, wherein generating audio associated with a particular instrument comprises: accessing a plurality of musical instrument digital interface (MIDI) samples associated with the particular instrument, the MIDI samples being associated with labels representing aspects of a playing style; selecting, based on the simulated actions associated with the particular instrument, MIDI samples; and generating audio based on the selected MIDI samples.
</claims>
</document>
