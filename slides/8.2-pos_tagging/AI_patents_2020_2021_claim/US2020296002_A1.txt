<document>

<filing_date>
2020-06-01
</filing_date>

<publication_date>
2020-09-17
</publication_date>

<priority_date>
2017-11-27
</priority_date>

<ipc_classes>
G06N20/00,H04L12/24
</ipc_classes>

<assignee>
MIT (MASSACHUSETTS INSTITUTE OF TECHNOLOGY)
</assignee>

<inventors>
CALACCI, DANIEL
ADJODAH, DHAVAL
LENG, YAN
MORO, ESTEBAN
DUBEY, ABHIMANYU
PENTLAND, ALEX PAUL
Krafft, Peter
</inventors>

<docdb_family_id>
66632845
</docdb_family_id>

<title>
Methods and Apparatus for Communication Network
</title>

<abstract>
In some implementations of this invention, the performance of a network of reinforcement learning agents is maximized by optimizing the communication topology between the agents for the communication of gradients, weights or rewards. For instance, a sparse Erdos-Renyi network may be employed, and network density may be selected in such a way as to maximize reachability and to minimize homogeneity. In some cases, a sparse network topology is employed for massively distributed learning, such as across entire fleets of autonomous vehicles or mobile phones that learn from each other instead of requiring a master to coordinate learning.
</abstract>

<claims>
1. A system comprising a network that includes multiple nodes and that is configured to perform a reinforcement learning algorithm, wherein: (a) each node in the network is a processor or computational core and is programmed to perform calculations of a reinforcement learning agent in the reinforcement learning algorithm; (b) in the reinforcement learning algorithm, each agent has its own parameter set; and (c) in each specific iteration in a set of iterations of the reinforcement learning algorithm, each specific agent in the network performs an update of the specific agent's parameter set in such a way that (i) the update is based on information regarding only a subset of the nodes in the network, which subset consists of less than 15% of the nodes in the network, and (ii) the update is performed separately from that for any other node in the network.
2. The system of claim 1, wherein the subset consists of less than 10% of the nodes in the network.
3. The system of claim 1, wherein the subset consists of less than 5% of the nodes in the network.
4. The system of claim 1, wherein the system is configured to perform with a fixed probability, in each specific iteration in a set of iterations of the reinforcement learning algorithm, a broadcast of a parameter set ("broadcast parameter set") in such a way that each specific agent in the network updates the specific agent's parameter set with the broadcast parameter set.
5. The system of claim 1, wherein: (a) the system is configured to perform with a fixed probability, in each specific iteration in a set of iterations of the reinforcement learning algorithm, a broadcast of a parameter set ("broadcast parameter set") in such a way that each specific agent in the network updates the specific agent's parameter set with the broadcast parameter set; and (b) the probability is greater than or equal to 0.5.
6. The system of claim 1, wherein the network is a random Erdos-Renyi graph.
7. The system of claim 1, wherein: (a) the network is a random Erdos-Renyi graph; (b) each specific pair of nodes in the graph has a fixed probability of being connected by an edge of the graph, which edge connects a node in the specific pair with another node in the specific pair; and (c) the probability is less than 0.6.
8. The system of claim 1, wherein the network is not fully connected.
9. The system of claim 1, wherein: (a) the network comprises a multiple clusters of nodes; and (b) some but not all nodes in each specific cluster are connected by an edge to a node in another cluster in the set of clusters.
10. The system of claim 1, wherein each node is housed in a device that is separate from that for any other node.
11. The system of claim 1, wherein: (a) each node is housed in a device that is separate from that for any other node; and (b) the device is a vehicle or a cellular phone.
</claims>
</document>
