<document>

<filing_date>
2020-01-10
</filing_date>

<publication_date>
2020-07-23
</publication_date>

<priority_date>
2019-01-18
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,G06T7/80
</ipc_classes>

<assignee>
NEC LABORATORIES AMERICA
</assignee>

<inventors>
CHANDRAKER, MANMOHAN
TRAN, QUOC-HUY
JI, PAN
ZHUANG, BINGBING
</inventors>

<docdb_family_id>
71609002
</docdb_family_id>

<title>
CAMERA SELF-CALIBRATION NETWORK
</title>

<abstract>
Systems and methods for camera self-calibration are provided. The method includes receiving real uncalibrated images, and estimating, using a camera self-calibration network, multiple predicted camera parameters corresponding to the real uncalibrated images. Deep supervision is implemented based on a dependence order between the plurality of predicted camera parameters to place supervision signals across multiple layers according to the dependence order. The method also includes determining calibrated images using the real uncalibrated images and the predicted camera parameters.
</abstract>

<claims>
1. A method for camera self-calibration, comprising:
receiving at least one real uncalibrated image;
estimating, using a camera self-calibration network, a plurality of predicted camera parameters corresponding to the at least one real uncalibrated image;
implementing deep supervision based on a dependence order between the plurality of predicted camera parameters to place supervision signals across multiple layers according to the dependence order; and
determining at least one calibrated image using the at least one real uncalibrated image and at least one of the plurality of predicted camera parameters.
2. The method as recited in claim 1, further comprising:
receiving, during a training phase, at least one training calibrated image and at least one training camera parameter corresponding to the at least one training calibrated image; and
generating, using the at least one training calibrated image and the at least one training camera parameter, at least one synthesized camera parameter and at least one synthesized uncalibrated image corresponding to the at least one synthesized camera parameter.
3. The method as recited in claim 2, further comprising: training the camera self-calibration network using the at least one synthesized uncalibrated image as input data and the at least one synthesized camera parameter as a supervision signal.
4. The method as recited in claim 1, wherein estimating the at least one predicted camera parameter further comprises:
performing at least one of principal point estimation, focal length estimation, and radial distortion estimation.
5. The method as recited in claim 1, wherein implementing deep supervision further comprises:
implementing deep supervision based on principal point estimation as an intermediate task for radial distortion estimation and focal length estimation, wherein learned features for estimating principal point are used for estimating radial distortion, and image appearance is determined based on a composite effect of radial distortion and focal length.
6. The method as recited in claim 1, further comprising:
determining a calibrated video based on the at least one calibrated image; and estimating a camera trajectory and scene structure observed in the calibrated video based on simultaneous localization and mapping (SLAM).
7. The method as recited in claim 1, further comprising: estimating at least one camera pose and scene structure using structure from motion (SFM) based on the at least one calibrated image.
8. The method as recited in claim 1, wherein determining the at least one calibrated image using the at least one real uncalibrated image and the at least one predicted camera parameter further comprises:
processing the at least one real uncalibrated image and the at least one predicted camera parameter via a rectification process to determine the at least one calibrated image.
9. The method as recited in claim 1, further comprising:
implementing the camera self-calibration network using a residual network as a base and adding at least one convolutional layer, and at least one batch normalization layer.
10. A computer system for camera self-calibration, comprising:
a processor device operatively coupled to a memory device, the processor device being configured to:
receive at least one real uncalibrated image;
estimate, using a camera self-calibration network, a plurality of predicted camera parameters corresponding to the at least one real uncalibrated image; implement deep supervision based on a dependence order between the plurality of predicted camera parameters to place supervision signals across multiple layers according to the dependence order; and
determine at least one calibrated image using the at least one real uncalibrated image and the at least one predicted camera parameter.
11. The system as recited in claim 10, wherein the processor device is further configured to:
receive, during a training phase, at least one training calibrated image and at least one training camera parameter corresponding to the at least one training calibrated image; and
generate, using the at least one training calibrated image and the at least one training camera parameter, at least one synthesized camera parameter and at least one synthesized uncalibrated image corresponding to the at least one synthesized camera parameter.
12. The system as recited in claim 11, the processor device is further configured to: train the camera self-calibration network using the at least one synthesized uncalibrated image as input data and the at least one synthesized camera parameter as a supervision signal.
13. The system as recited in claim 10, wherein, when estimating the at least one predicted camera parameter, the processor device is further configured to: perform at least one of principal point estimation, focal length estimation, and radial distortion estimation.
14. The system as recited in claim 10, wherein, when implementing deep supervision, the processor device is further configured to:
implement deep supervision based on principal point estimation as an
intermediate task for radial distortion estimation and focal length estimation, wherein learned features for estimating principal point are used for estimating radial distortion, and image appearance is determined based on a composite effect of radial distortion and focal length.
15. The system as recited in claim 10, wherein the processor device is further configured to:
determine a calibrated video based on the at least one calibrated image; and estimate a camera trajectory and scene structure observed in the calibrated video based on simultaneous localization and mapping (SLAM).
16. The system as recited in claim 10, wherein the processor device is further configured to:
estimate at least one camera pose and scene structure using structure from motion (SFM) based on the at least one calibrated image.
17. The system as recited in claim 10, wherein, when determining the at least one calibrated image using the at least one real uncalibrated image and the at least one predicted camera parameter, wherein the processor device is further configured to: process the at least one real uncalibrated image and the at least one predicted camera parameter via a rectification process to determine the at least one calibrated image.
18. The system as recited in claim 10, wherein the processor device is further configured to:
implement the camera self-calibration network using a residual network as a base and adding at least one convolutional layer, and at least one batch normalization layer.
19. A computer program product for camera self-calibration, the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computing device to cause the computing device to perform the method comprising:
receiving at least one real uncalibrated image;
estimating, using a camera self-calibration network, at least one predicted camera parameter corresponding to the at least one real uncalibrated image; and
determining at least one calibrated image using the at least one real uncalibrated image and the at least one predicted camera parameter.
20. The computer program product for camera self-calibration of claim 19, wherein the program instructions executable by a computing device further comprise:
receiving, during a training phase, at least one training calibrated image and at least one training camera parameter corresponding to the at least one training calibrated image; and
generating, using the at least one training calibrated image and the at least one training camera parameter, at least one synthesized camera parameter and at least one synthesized uncalibrated image corresponding to the at least one synthesized camera parameter.
</claims>
</document>
