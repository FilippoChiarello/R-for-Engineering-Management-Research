<document>

<filing_date>
2018-11-15
</filing_date>

<publication_date>
2020-05-21
</publication_date>

<priority_date>
2018-11-15
</priority_date>

<ipc_classes>
G06K9/00,G06K9/34,G06K9/62,G06T3/00,G06T7/11,G06T7/20
</ipc_classes>

<assignee>
TOYOTA RESEARCH INSTITUTE
</assignee>

<inventors>
KANZAWA, YUSUKE
DELP, MICHAEL JAMES
</inventors>

<docdb_family_id>
70727758
</docdb_family_id>

<title>
Systems and methods for registering 3D data with 2D image data
</title>

<abstract>
Systems and methods described herein relate to registering three-dimensional (3D) data with two-dimensional (2D) image data. One embodiment receives 3D data from one or more sensors and 2D image data from one or more cameras; identifies a 3D segment in the 3D data and associates it with an object; classifies pixels in the 2D image data; determines a speed and a heading for the object; and registers the 3D segment with a portion of the classified pixels by either (1) shifting the 3D segment to a position that, based on the associated object's speed and heading, corresponds to a 2D image data capture time and projecting the time-shifted 3D segment onto 2D image space; or (2) projecting the 3D segment onto 2D image space and shifting the projected 3D segment to a position that, based on the associated object's speed and heading, corresponds to a 2D image data capture time.
</abstract>

<claims>
1. A system for registering three-dimensional (3D) data with two-dimensional (2D) image data, the system comprising: one or more sensors to produce 3D data; one or more cameras to produce 2D image data; one or more processors; a memory communicably coupled to the one or more processors and storing: a 3D-data segmentation module including instructions that when executed by the one or more processors cause the one or more processors to identify, in the 3D data, a 3D segment; a data association module including instructions that when executed by the one or more processors cause the one or more processors to associate the 3D segment with an object; an image segmentation module including instructions that when executed by the one or more processors cause the one or more processors to classify pixels in the 2D image data; a velocity estimation module including instructions that when executed by the one or more processors cause the one or more processors to determine a speed and a heading for the object; and an integration module including instructions that when executed by the one or more processors cause the one or more processors to register the 3D segment with a portion of the classified pixels in the 2D image data by performing one of: shifting the 3D segment to a position that, based on the associated object's speed and heading, corresponds to a time at which the 2D image data was captured and projecting the time-shifted 3D segment onto 2D image space; and projecting the 3D segment onto 2D image space and shifting the projected 3D segment to a position that, based on the associated object's speed and heading, corresponds to a time at which the 2D image data was captured.
2. The system of claim 1, wherein the one or more sensors include at least one of a Light Detection and Ranging (LIDAR) sensor, a set of stereo cameras, a Red Green Blue Depth (RGB-D) sensor, and a radar sensor, and the 3D segment is a point-cloud cluster.
3. The system of claim 2, wherein the image segmentation module further includes instructions to perform semantic segmentation of the 2D image data, the semantic segmentation assigning a class label to each pixel in the 2D image data to classify that pixel, and the integration module further includes instructions to count the number of pixels in at least one class that overlap with points in the time-shifted and projected point-cloud cluster.
4. The system of claim 3, wherein the integration module further includes instructions to: count the number of pixels in each of a plurality of classes that overlap with points in the time-shifted and projected point-cloud cluster; and associate the time-shifted and projected point-cloud cluster with a class in the plurality of classes for which the greatest number of pixels overlap with points in the time-shifted and projected point-cloud cluster.
5. The system of claim 1, wherein the 3D segment is 3D boundary information that includes one of a convex hull, a voxelization, a non-convex hull, a bounding box, and mesh data.
6. The system of claim 5, wherein the image segmentation module further includes instructions to produce an instance segmentation from the classified pixels in the 2D image data, the instance segmentation identifying the portion of the classified pixels in the 2D image data as a specific instance of a particular class of object, and the integration module further includes instructions to match the time-shifted and projected 3D boundary information with the instance segmentation.
7. The system of claim 5, wherein the image segmentation module further includes instructions to perform semantic segmentation of the 2D image data, the semantic segmentation assigning a class label to each pixel in the 2D image data to classify that pixel, and the integration module further includes instructions to count the number of pixels in at least one class that lie within the time-shifted and projected 3D boundary information.
8. The system of claim 7, wherein the integration module further includes instructions to: count the number of pixels in each of a plurality of classes that lie within the time-shifted and projected 3D boundary information; and associate the time-shifted and projected 3D boundary information with a class in the plurality of classes for which the greatest number of pixels lie within the time-shifted and projected 3D boundary information.
9. A method of registering three-dimensional (3D) data with two-dimensional (2D) image data, the method comprising: receiving 3D data from one or more sensors; receiving 2D image data from one or more cameras; identifying a 3D segment in the 3D data; associating the 3D segment with an object; classifying pixels in the 2D image data; determining a speed and a heading for the object; and registering the 3D segment with a portion of the classified pixels in the 2D image data by performing one of: shifting the 3D segment to a position that, based on the associated object's speed and heading, corresponds to a time at which the 2D image data was captured and projecting the time-shifted 3D segment onto 2D image space; and projecting the 3D segment onto 2D image space and shifting the projected 3D segment to a position that, based on the associated object's speed and heading, corresponds to a time at which the 2D image data was captured.
10. The method of claim 9, wherein the one or more sensors include at least one of a Light Detection and Ranging (LIDAR) sensor, a set of stereo cameras, a Red Green Blue Depth (RGB-D) sensor, and a radar sensor, and the 3D segment is a point-cloud cluster.
11. The method of claim 10, wherein classifying pixels in the 2D image data includes performing semantic segmentation of the 2D image data, the semantic segmentation assigning a class label to each pixel in the 2D image data, and the registering includes counting the number of pixels in at least one class that overlap with points in the time-shifted and projected point-cloud cluster.
12. The method of claim 11, wherein the registering includes: counting the number of pixels in each of a plurality of classes that overlap with points in the time-shifted and projected point-cloud cluster; and associating the time-shifted and projected point-cloud cluster with a class in the plurality of classes for which the greatest number of pixels overlap with points in the time-shifted and projected point-cloud cluster.
13. The method of claim 9, wherein the 3D segment is 3D boundary information that includes one of a convex hull, a voxelization, a non-convex hull, a bounding box, and mesh data.
14. The method of claim 13, wherein classifying pixels in the 2D image data includes producing an instance segmentation, the instance segmentation identifying the portion of the classified pixels in the 2D image data as a specific instance of a particular class of object, and the registering includes matching the time-shifted and projected 3D boundary information with the instance segmentation.
15. The method of claim 13, wherein classifying pixels in the 2D image data includes performing semantic segmentation of the 2D image data, the semantic segmentation assigning a class label to each pixel in the 2D image data to classify that pixel, and the registering includes counting the number of pixels in at least one class that lie within the time-shifted and projected 3D boundary information.
16. The method of claim 15, wherein the registering includes: counting the number of pixels in each of a plurality of classes that lie within the time-shifted and projected 3D boundary information; and associating the time-shifted and projected 3D boundary information with a class in the plurality of classes for which the greatest number of pixels lie within the time-shifted and projected 3D boundary information.
17. A system for registering three-dimensional (3D) data with two-dimensional (2D) image data, the system comprising: one or more sensors to produce 3D data; one or more cameras to produce 2D image data; one or more processors; a memory communicably coupled to the one or more processors and storing: a 3D-data segmentation module including instructions that when executed by the one or more processors cause the one or more processors to identify, in the 3D data, 3D boundary information that includes one of a convex hull, a voxelization, a non-convex hull, a bounding box, and mesh data; an image segmentation module including instructions that when executed by the one or more processors cause the one or more processors to produce an instance segmentation from pixels in the 2D image data, the instance segmentation identifying a portion of the pixels in the 2D image data as a specific instance of a particular class of object; and an integration module including instructions that when executed by the one or more processors cause the one or more processors to: project the 3D boundary information onto 2D image space to produce projected 3D boundary information; and match the projected 3D boundary information with the instance segmentation to register the projected 3D boundary information with the instance segmentation.
18. The system of claim 17, further comprising: a data association module including instructions that when executed by the one or more processors cause the one or more processors to associate the 3D boundary information with an object; and a velocity estimation module including instructions that when executed by the one or more processors cause the one or more processors to determine a speed and a heading for the object; wherein the integration module further includes instructions to perform one of: before the 3D boundary information is projected onto 2D image space, shifting the 3D boundary information to a position that, based on the associated object's speed and heading, corresponds to a time at which the 2D image data was captured; and after the 3D boundary information has been projected onto 2D image space, shifting the projected 3D boundary information to a position that, based on the associated object's speed and heading, corresponds to a time at which the 2D image data was captured.
19. A method of registering three-dimensional (3D) data with two-dimensional (2D) image data, the method comprising: receiving 3D data from one or more sensors; receiving 2D image data from one or more cameras; identifying 3D boundary information in the 3D data; producing an instance segmentation from pixels in the 2D image data, the instance segmentation identifying a portion of the pixels in the 2D image data as a specific instance of a particular class of object; projecting the 3D boundary information onto 2D image space to produce projected 3D boundary information; and matching the projected 3D boundary information with the instance segmentation to register the projected 3D boundary information with the instance segmentation.
20. The method of claim 19, further comprising: associating the 3D boundary information with an object; determining a speed and a heading for the object; and performing one of: before the 3D boundary information is projected onto 2D image space, shifting the 3D boundary information to a position that, based on the associated object's speed and heading, corresponds to a time at which the 2D image data was captured; and after the 3D boundary information has been projected onto 2D image space, shifting the projected 3D boundary information to a position that, based on the associated object's speed and heading, corresponds to a time at which the 2D image data was captured.
</claims>
</document>
