<document>

<filing_date>
2019-02-06
</filing_date>

<publication_date>
2020-11-24
</publication_date>

<priority_date>
2019-02-06
</priority_date>

<ipc_classes>
G06K9/00,G06K9/32,G06K9/36,H04N19/117,H04N19/167,H04N19/80
</ipc_classes>

<assignee>
APICAL
</assignee>

<inventors>
CROXFORD, DAREN
</inventors>

<docdb_family_id>
71837993
</docdb_family_id>

<title>
Video data processing
</title>

<abstract>
A method for processing video data, which includes generating, with an image sensor of an imaging device, input data representative of input frames, and generating, at the imaging device, corresponding output video data by encoding regions of interest within the input frames in accordance with a different encoding scheme to the remaining regions within the input frames. The regions of interest within the input frames are designated by region of interest data that is received at the imaging device. Also disclosed is a video processing system, including an imaging device having an image sensor and a sensor data processor, and a computing system having at least one processor. The sensor data processor generates output data based on input data from the image sensor, by encoding regions of interest, as identified by data received from the computing system, with a different encoding scheme, and outputs such data to the computing system.
</abstract>

<claims>
1. Method for processing video data, comprising: generating, with an image sensor of an imaging device, input video data representative of a plurality of successive input frames; receiving, at the imaging device, first region of interest data, defining one or more regions of interest; generating, at the imaging device, output video data, which is based on the input video data, including by: in response to receiving said first region of interest data, selecting one or more first input frames from the plurality of input frames represented in the input video data; and for a given one of the selected one or more first input frames: encoding, in accordance with a first encoding scheme, a portion of the input video data that is representative of one or more regions of interest within the given first frame, thus generating first encoded data, the one or more regions of interest within the given first frame corresponding to said regions of interest defined by the first region of interest data, and encoding, in accordance with a second, different encoding scheme, a portion of the input video data that is representative of one or more remaining regions within the given first frame, thus generating second encoded data; and outputting, from the imaging device, said output video data, wherein the output video data comprises the first and second encoded data for each of the first one or more input frames.
2. The method according to claim 1, wherein a compression loss rate for said first encoding scheme is lower than a compression loss rate for said second encoding scheme.
3. The method according to claim 2, wherein the compression loss rate for the first encoding scheme is zero.
4. The method according to claim 1, wherein a spatial resolution for said first encoding scheme is higher than a spatial resolution for said second encoding scheme.
5. The method according to claim 1, wherein a computational complexity of said first encoding process is greater than a computational complexity of said second encoding process.
6. The method according to claim 1, wherein said first encoding process comprises a first spatial filtering process; wherein said second encoding process either does not include any spatial filtering processes, or comprises a second spatial filtering process, which has a computational complexity that is lower than a computational complexity for said first spatial filtering process.
7. The method according to claim 1, wherein said first encoding process comprises a first temporal filtering process; wherein said second encoding process either does not include any temporal filtering processes, or comprises a second temporal filtering process, which has a computational complexity that is lower than a computational complexity for said first temporal filtering process.
8. The method according to claim 1, wherein the selected one or more input frames comprise a group of input frames and wherein said second encoding scheme is such that, for each of a sub-group of the selected group of input frames, the corresponding second encoded data is null data or is zero bits in size.
9. The method according to claim 1, wherein selecting one or more first input frames comprises selecting a plurality of first input frames at a frequency that is dependent upon a total magnitude of the one or more regions of interest defined in said first region of interest data.
10. The method according to claim 1, wherein said generating of the output video data further comprises, in response to receiving no region of interest data within a predetermined time interval, generating reduced frame-rate output data, which is representative of only a fraction of a plurality of input frames represented by a portion of the input video data that is received subsequent to the predetermined time interval; and wherein the output video data comprises said reduced frame-rate output data.
11. The method according to claim 1, wherein said output video data comprises data indicating the regions of interest within each of the first one or more frames.
12. A method according to claim 1, further comprising receiving second region of interest data, defining one or more regions of interest, which results from a process that is downstream in the imaging pipeline relative to said generating of the output video data; wherein said generating and outputting of the output video data additionally comprises: in response to receiving said second region of interest data, selecting one or more second input frames, and for a given one of the selected one or more second input frames: encoding, in accordance with a first encoding scheme, a portion of the input video data that is representative of one or more regions of interest within the given selected frame, thus generating first encoded data, the one or more regions of interest within the given second frame corresponding to said regions of interest defined by the second region of interest data, and encoding, in accordance with a second, different encoding scheme, a portion of the input video data that is representative of one or more remaining regions of interest within the given second frame, thus generating second encoded data; and wherein the output video data further comprises the first and second encoded data for each of the second one or more input frames.
13. The method according to claim 1, wherein said first region of interest data results from a process that is downstream in an imaging pipeline relative to said generating of the output video data, said image sensor defining an upstream end of said imaging pipeline.
14. The method according to claim 13, wherein said imaging pipeline comprises: an encoding module, which carries out said generating and outputting of output video data, and is implemented on the imaging device; and an image processing module, which is downstream from said encoding module and which generates said first region of interest data; wherein said image processing module implements at least one of: an object recognition algorithm, a feature detection algorithm, a computer vision algorithm, and a simultaneous localization and mapping algorithm.
15. The method according to claim 14, wherein said image processing module implements a computer vision algorithm that has been trained using image data, representative of one or more images, and gaze direction data for one or more users viewing said one or more images.
16. A video processing system comprising: an imaging device, which comprises: an image sensor, which is configured to generate input video data representative of a plurality of successive input frames, at least one sensor data processor, and storage accessible by the at least one sensor data processor; a computing system, which comprises: at least one processor, and storage accessible by the at least one sensor data processor; and a data exchange interface, which connects the computing system and the image sensor, wherein instructions are stored on said storage accessible by the at least one sensor data processor that, when executed by the at least one sensor data processor, cause the at least one sensor data processor to: receive said input video data from the image sensor, receive, from the at least one processor of the computing system, via said data exchange interface, first region of interest data that defines one or more regions of interest, and generate output video data, which is based on the input video data, including by: in response to receiving said first region of interest data, selecting one or more input frames, and, for a given one of the selected one or more of input frames: encoding, in accordance with a first encoding scheme, a portion of the input video data that is representative of one or more regions of interest within the given selected frame, thus generating first encoded data, the one or more regions of interest within the given selected frame corresponding to said regions of interest defined by the first region of interest data, and encoding, in accordance with a second, different encoding scheme, a portion of the input video data that is representative of one or more remaining regions of interest within the selected frame in question, thus generating second encoded data, output said output video data to the computing system, via said data exchange interface.
17. A video processing system according to claim 16, wherein said data exchange interface is a camera serial interface.
18. A video processing system according to claim 16, wherein the at least one processor of the computing system comprises an image signal processor and a central processor unit; and wherein the central processor unit is connected to said data exchange interface via an image signal processor.
19. A video processing system according to claim 16, wherein the at least one processor of the computing system further comprises a convolutional neural network accelerator, which is connected to said image signal processor via said central processing unit.
20. An imaging device, which comprises: an image sensor, which is configured to generate input video data representative of a plurality of successive input frames; at least one sensor data processor; storage accessible by the at least one sensor data processor; and at least one connector connectable to a computing system, so as to provide a data exchange interface therebetween; wherein instructions are stored on said storage accessible by the at least one sensor data processor that, when executed by the at least one sensor data processor, cause the at least one sensor data processor to: receive said input video data from the image sensor, receive, from the at least one processor of the computing system, via said at least one connector, first region of interest data that defines one or more regions of interest, and generate output video data, which is based on the input video data, including by: in response to receiving said first region of interest data, selecting one or more input frames, and for a given one of the selected one or more of input frames: encoding, in accordance with a first encoding scheme, a portion of the input video data that is representative of one or more regions of interest within the given selected frame, thus generating first encoded data, the one or more regions of interest within the given selected frame corresponding to said regions of interest defined by the first region of interest data, and encoding, in accordance with a second, different encoding scheme, a portion of the input video data that is representative of one or more remaining regions of interest within the selected frame in question, thus generating second encoded data, output said output video data to the computing system, via said at least one connector.
</claims>
</document>
