<document>

<filing_date>
2018-12-11
</filing_date>

<publication_date>
2020-09-30
</publication_date>

<priority_date>
2017-12-19
</priority_date>

<ipc_classes>
G06K9/00,G06N3/02,G06T9/00,H04N19/54
</ipc_classes>

<assignee>
HUAWEI TECHNOLOGIES COMPANY
</assignee>

<inventors>
LIU, Yunhui
YUAN, Kebin
WANG, Jiangliu
</inventors>

<docdb_family_id>
66984194
</docdb_family_id>

<title>
IMAGE CODING METHOD, ACTION RECOGNITION METHOD, AND COMPUTER DEVICE
</title>

<abstract>
This application discloses an image coding method. The method includes: obtaining a plurality of groups of human skeleton data of performing a target action, where each group of human skeleton data includes joint point data of performing the target action; extracting, based on joint point data in the plurality of groups of human skeleton data, a motion feature matrix corresponding to the plurality of groups of human skeleton data; and encoding the motion feature matrix to obtain a motion feature image. In this application, the motion feature matrix corresponding to the plurality of groups of human skeleton data is extracted, and then the extracted motion feature matrix corresponding to the plurality of groups of human skeleton data is encoded as the motion feature image. Because a data amount of the motion feature image is smaller than a data amount of a plurality of action feature vector sequences, consumption of storage resources and calculation resources is reduced.
</abstract>

<claims>
1. An image coding method, wherein the method comprises: obtaining a plurality of groups of human skeleton data of performing a target action, wherein each group of human skeleton data comprises joint point data of performing the target action; extracting, based on joint point data in the plurality of groups of human skeleton data, a motion feature matrix corresponding to the plurality of groups of human skeleton data; and encoding the motion feature matrix to obtain a motion feature image.
2. The method according to claim 1, wherein the motion feature matrix comprises a linear velocity matrix, and the joint point data comprises coordinates of a corresponding joint point in a three-dimensional coordinate system; and
the extracting, based on joint point data in the plurality of groups of human skeleton data, a motion feature matrix corresponding to the plurality of groups of human skeleton data comprises: subtracting coordinates of a joint point in a first group of human skeleton data in the three-dimensional coordinate system from coordinates of the corresponding joint point in a second group of human skeleton data in the three-dimensional coordinate system to obtain linear velocity units corresponding to the first group of human skeleton data, wherein the first group of human skeleton data and the second group of human skeleton data are any two adjacent groups of human skeleton data in the plurality of groups of human skeleton data, and the first group of human skeleton data is a previous group of human skeleton data of the second group of human skeleton data; and forming, by using all the obtained linear velocity units, a linear velocity matrix corresponding to the plurality of groups of human skeleton data.
3. The method according to claim 2, wherein the encoding the motion feature matrix to obtain a motion feature image comprises: preprocessing the linear velocity matrix; encoding a plurality of linear velocity units in the preprocessed linear velocity matrix to obtain a plurality of linear velocity pixel frames; and forming a linear velocity image by using the plurality of linear velocity pixel frames.
4. The method according to claim 2, wherein the encoding the motion feature matrix to obtain a motion feature image comprises: preprocessing the linear velocity matrix; encoding a plurality of linear velocity units in the preprocessed linear velocity matrix to obtain a plurality of linear velocity pixel frames; extracting a plurality of key linear velocity pixel frames from the plurality of linear velocity pixel frames; and forming a linear velocity image by using the plurality of key linear velocity pixel frames.
5. The method according to claim 3 or 4, wherein the preprocessing the linear velocity matrix comprises: obtaining a maximum linear velocity element value and a minimum linear velocity element value in the linear velocity matrix; and performing normalization processing on each linear velocity element value in the linear velocity matrix based on the maximum linear velocity element value and the minimum linear velocity element value, to obtain a normalized linear velocity matrix, wherein each linear velocity element value in the normalized linear velocity matrix is between a first value and a second value, and the first value is less than the second value.
6. The method according to claim 3 or 4, wherein the encoding a plurality of linear velocity units in the preprocessed linear velocity matrix to obtain a plurality of linear velocity pixel frames comprises:
using coordinates of a joint point in each preprocessed linear velocity unit in the three-dimensional coordinate system as image channels, and encoding a plurality of preprocessed linear velocity units to obtain a plurality of linear velocity pixel frames.
7. The method according to claim 4, wherein the extracting a plurality of key linear velocity pixel frames from the plurality of pixel frames comprises: calculating linear velocity energy change values of the plurality of linear velocity pixel frames based on the preprocessed linear velocity matrix; and extracting the plurality of key linear velocity pixel frames from the plurality of linear velocity pixel frames in descending order of the linear velocity energy change values.
8. The method according to claim 7, wherein the calculating linear velocity energy change values of the plurality of linear velocity pixel frames based on the preprocessed linear velocity matrix comprises: calculating a quadratic sum of coordinates of each joint point in a first linear velocity pixel frame in the three-dimensional coordinate system, and adding up the quadratic sums of the coordinates of all the joint points in the three-dimensional coordinate system to obtain a linear velocity energy value of the first linear velocity pixel frame; calculating a quadratic sum of coordinates of each joint point in a second linear velocity pixel frame in the three-dimensional coordinate system, and adding up the quadratic sums of the coordinates of all the joint points in the three-dimensional coordinate system to obtain a linear velocity energy value of the second linear velocity pixel frame, wherein the first linear velocity pixel frame and the second linear velocity pixel frame are any two adjacent linear velocity pixel frames, and the first linear velocity pixel frame is a previous linear velocity pixel frame of the second linear velocity pixel frame; and subtracting the linear velocity energy value of the first linear velocity pixel frame from the linear velocity energy value of the second linear velocity pixel frame to obtain a linear velocity energy change value of the first linear velocity pixel frame.
9. The method according to claim 1, wherein the motion feature matrix comprises an angular velocity matrix, the joint point data comprises coordinates of a corresponding joint point in a three-dimensional coordinate system, and the extracting, based on joint point data in the plurality of groups of human skeleton data, a motion feature matrix corresponding to the plurality of groups of human skeleton data comprises: calculating direction angles of joint points in the plurality of groups of human skeleton data in the three-dimensional coordinate system based on coordinates of the joint points in the plurality of groups of human skeleton data in the three-dimensional coordinate system; subtracting direction angles of a joint point in a first group of human skeleton data in the three-dimensional coordinate system from direction angles of the corresponding joint point in a second group of human skeleton data in the three-dimensional coordinate system to obtain angular velocity units, wherein the first group of human skeleton data and the second group of human skeleton data are any two adjacent groups of human skeleton data, and the first group of human skeleton data is a previous group of human skeleton data of the second group of human skeleton data; and forming, by using all the obtained angular velocity units, an angular velocity matrix corresponding to the plurality of groups of human skeleton data.
10. The method according to claim 9, wherein the encoding the motion feature matrix to obtain a motion feature image comprises: preprocessing the angular velocity matrix; encoding a plurality of angular velocity units in the preprocessed linear velocity matrix to obtain a plurality of angular velocity pixel frames; and forming an angular velocity image by using the plurality of angular velocity pixel frames.
11. The method according to claim 9, wherein the encoding the motion feature matrix to obtain a motion feature image comprises: preprocessing the angular velocity matrix; encoding a plurality of angular velocity units in the preprocessed angular velocity matrix to obtain a plurality of angular velocity pixel frames; extracting a plurality of key angular velocity pixel frames from the plurality of angular velocity pixel frames; and forming an angular velocity image by using the plurality of key angular velocity pixel frames.
12. The method according to claim 10 or 11, wherein the preprocessing the angular velocity matrix comprises: obtaining a maximum angular velocity element value and a minimum angular velocity element value in the angular velocity matrix; and performing normalization processing on each angular velocity element value in the angular velocity matrix based on the maximum angular velocity element value and the minimum angular velocity element value, to obtain a normalized angular velocity matrix, wherein each angular velocity element value in the normalized angular velocity matrix is between a first value and a second value, and the first value is less than the second value.
13. The method according to claim 10 or 11, wherein the encoding a plurality of angular velocity units in the preprocessed angular velocity matrix to obtain a plurality of angular velocity pixel frames comprises:
using direction angles of a joint point in each preprocessed angular velocity unit in the three-dimensional coordinate system as image channels, and encoding a plurality of preprocessed angular velocity units to obtain a plurality of angular velocity pixel frames.
14. The method according to claim 11, wherein the extracting a plurality of key angular velocity pixel frames from the plurality of angular velocity pixel frames comprises: calculating angular velocity energy change values of the plurality of angular velocity pixel frames based on the preprocessed angular velocity matrix; and extracting the plurality of key angular velocity pixel frames from the plurality of angular velocity pixel frames in descending order of the angular velocity energy change values.
15. The method according to claim 14, wherein the calculating angular velocity energy change values of the plurality of angular velocity pixel frames based on the preprocessed angular velocity matrix comprises: calculating a quadratic sum of direction angles of each joint point in a first angular velocity pixel frame in the three-dimensional coordinate system, and adding up the quadratic sums of the direction angles of all the joint points in the three-dimensional system to obtain an angular velocity energy value of the first angular velocity pixel frame; calculating a quadratic sum of direction angles of each joint point in a second angular velocity pixel frame in the three-dimensional coordinate system, and adding up the quadratic sums of the direction angles of all the joint points in the three-dimensional coordinate system to obtain an angular velocity energy value of the second angular velocity pixel frame, wherein the first angular velocity pixel frame and the second angular velocity pixel frame are any two adjacent angular velocity pixel frames, and the first angular velocity pixel frame is a previous angular velocity pixel frame of the second angular velocity pixel frame; and subtracting the linear velocity energy value of the first angular velocity pixel frame from the linear velocity energy value of the second angular velocity pixel frame to obtain a linear velocity energy change value of the first angular velocity pixel frame.
16. The method according to any one of claims 1 to 15, wherein the method further comprises:
inputting at least one motion feature image of the target action and an identifier of the target action into a convolutional neural network CNN model, and performing training to obtain an action recognition model.
17. An action recognition model training method, wherein the method comprises: obtaining a plurality of reference motion feature images respectively corresponding to a plurality of types of actions, wherein each reference motion feature image is obtained by using the method according to any one of claims 1 to 15; and inputting the plurality of reference motion feature images and identifiers of the plurality of actions into a convolutional neural network CNN model, and performing training to obtain an action recognition model.
18. An action recognition method, wherein the method comprises: obtaining a to-be-recognized motion feature image, wherein the to-be-recognized motion feature image is an image obtained by encoding a plurality of groups of to-be-recognized human skeleton data of a to-be-recognized action; and recognizing the to-be-recognized motion feature image based on an action recognition model, to obtain a recognition result, wherein the action recognition model is obtained through training based on a plurality of reference motion feature images respectively corresponding to a plurality of types of actions and identifiers of the plurality of types of actions, and the recognition result is used to indicate an action type of the to-be-recognized action.
19. The method according to claim 18, wherein the obtaining a to-be-recognized motion feature image comprises: collecting the plurality of groups of to-be-recognized human skeleton data of performing the to-be-recognized action, wherein each group of to-be-recognized human skeleton data comprises joint point data of performing the to-be-recognized action; extracting, based on joint point data in the plurality of groups of to-be-recognized human skeleton data, a to-be-recognized motion feature matrix corresponding to the plurality of groups of to-be-recognized human skeleton data; and encoding the to-be-recognized motion feature matrix to obtain the to-be-recognized motion feature image.
20. The method according to claim 19, wherein the to-be-recognized motion feature matrix comprises a to-be-recognized linear velocity matrix, the joint point data comprises coordinates of a corresponding joint point in a three-dimensional coordinate system, and the extracting, based on joint point data in the plurality of groups of to-be-recognized human skeleton data, a to-be-recognized motion feature matrix corresponding to the plurality of groups of to-be-recognized human skeleton data comprises: subtracting coordinates of a joint point in a first group of to-be-recognized human skeleton data in the three-dimensional coordinate system from coordinates of the corresponding joint point in a second group of to-be-recognized human skeleton data in the three-dimensional coordinate system to obtain to-be-recognized linear velocity units of the first group of to-be-recognized human skeleton data, wherein the first group of to-be-recognized human skeleton data and the second group of to-be-recognized human skeleton data are any two adjacent groups of to-be-recognized human skeleton data in the plurality of groups of to-be-recognized human skeleton data, and the first group of to-be-recognized human skeleton data is a previous group of to-be-recognized human skeleton data of the second group of to-be-recognized human skeleton data; and forming, by using all the obtained to-be-recognized linear velocity units, a to-be-recognized linear velocity matrix corresponding to the plurality of groups of to-be-recognized human skeleton data.
21. The method according to claim 20, wherein the encoding the to-be-recognized motion feature matrix to obtain the to-be-recognized motion feature image comprises: preprocessing the to-be-recognized linear velocity matrix; encoding a plurality of linear velocity units in the preprocessed to-be-recognized linear velocity matrix to obtain a plurality of to-be-recognized linear velocity pixel frames; and forming a to-be-recognized linear velocity image by using the plurality of to-be-recognized linear velocity pixel frames.
22. The method according to claim 20, wherein the encoding the to-be-recognized motion feature matrix to obtain the to-be-recognized motion feature image comprises: preprocessing the to-be-recognized linear velocity matrix; encoding a plurality of to-be-recognized linear velocity units in the preprocessed to-be-recognized linear velocity matrix to obtain a plurality of to-be-recognized linear velocity pixel frames; extracting a plurality of to-be-recognized key linear velocity pixel frames from the plurality of to-be-recognized linear velocity pixel frames; and forming a to-be-recognized linear velocity image by using the plurality of to-be-recognized key linear velocity pixel frames.
23. The method according to claim 21 or 22, wherein the preprocessing the to-be-recognized linear velocity matrix comprises: obtaining a maximum to-be-recognized linear velocity element value and a minimum to-be-recognized linear velocity element value in the to-be-recognized linear velocity matrix; and performing normalization processing on each to-be-recognized linear velocity element value in the to-be-recognized linear velocity matrix based on the maximum to-be-recognized linear velocity element value and the minimum to-be-recognized linear velocity element value, to obtain a normalized to-be-recognized linear velocity matrix, wherein each to-be-recognized linear velocity element value in the normalized to-be-recognized linear velocity matrix is between a first value and a second value, and the first value is less than the second value.
24. The method according to claim 21 or 22, wherein the encoding a plurality of to-be-recognized linear velocity units in the preprocessed to-be-recognized linear velocity matrix to obtain a plurality of to-be-recognized linear velocity pixel frames comprises:
using coordinates of a joint point in each preprocessed to-be-recognized linear velocity unit in the three-dimensional coordinate system as image channels, and encoding a plurality of preprocessed to-be-recognized linear velocity units to obtain a plurality of to-be-recognized linear velocity pixel frames.
25. The method according to claim 22, wherein the extracting a plurality of to-be-recognized key linear velocity pixel frames from the plurality of to-be-recognized pixel frames comprises: calculating linear velocity energy change values of the plurality of to-be-recognized linear velocity pixel frames based on the preprocessed to-be-recognized linear velocity matrix; and extracting the plurality of to-be-recognized key linear velocity pixel frames from the plurality of to-be-recognized linear velocity pixel frames in descending order of the linear velocity energy change values.
26. The method according to claim 25, wherein the calculating linear velocity energy change values of the plurality of to-be-recognized linear velocity pixel frames based on the preprocessed to-be-recognized linear velocity matrix comprises: calculating a quadratic sum of coordinates of each joint point in a first to-be-recognized linear velocity pixel frame in the three-dimensional coordinate system, and adding up the quadratic sums of the coordinates of all the joint points in the three-dimensional coordinate system to obtain a linear velocity energy value of the first to-be-recognized linear velocity pixel frame; calculating a quadratic sum of coordinates of each joint point in a second to-be-recognized linear velocity pixel frame in the three-dimensional coordinate system, and adding up the quadratic sums of the coordinates of all the joint points in the three-dimensional coordinate system to obtain a linear velocity energy value of the second to-be-recognized linear velocity pixel frame, wherein the first to-be-recognized linear velocity pixel frame and the second to-be-recognized linear velocity pixel frame are any two adjacent to-be-recognized linear velocity pixel frames, and the first to-be-recognized linear velocity pixel frame is a previous to-be-recognized linear velocity pixel frame of the second to-be-recognized linear velocity pixel frame; and subtracting the linear velocity energy value of the first to-be-recognized linear velocity pixel frame from the linear velocity energy value of the second to-be-recognized linear velocity pixel frame to obtain a linear velocity energy change value of the first to-be-recognized linear velocity pixel frame.
27. The method according to claim 19, wherein the to-be-recognized motion feature matrix comprises a to-be-recognized angular velocity matrix, the joint point data comprises coordinates of a corresponding joint point in a three-dimensional coordinate system, and the extracting, based on joint point data in the plurality of groups of to-be-recognized human skeleton data, a to-be-recognized motion feature matrix corresponding to the plurality of groups of to-be-recognized human skeleton data comprises: calculating direction angles of a joint point in the plurality of groups of to-be-recognized human skeleton data in the three-dimensional coordinate system based on a coordinate matrix corresponding to the plurality of groups of to-be-recognized human skeleton data; subtracting direction angles of a joint point in a first group of to-be-recognized human skeleton data in the three-dimensional coordinate system from direction angles of the corresponding joint point in a second group of to-be-recognized human skeleton data in the three-dimensional coordinate system to obtain to-be-recognized angular velocity units, wherein the first group of to-be-recognized human skeleton data and the second group of to-be-recognized human skeleton data are any two adjacent groups of to-be-recognized human skeleton data, and the first group of to-be-recognized human skeleton data is a previous group of to-be-recognized human skeleton data of the second group of to-be-recognized human skeleton data; and forming, by using all the obtained to-be-recognized angular velocity units, a to-be-recognized angular velocity matrix corresponding to the plurality of groups of to-be-recognized human skeleton data.
28. The method according to claim 27, wherein the encoding the to-be-recognized motion feature matrix to obtain the to-be-recognized motion feature image comprises: preprocessing the to-be-recognized angular velocity matrix; encoding a plurality of to-be-recognized angular velocity units in the preprocessed to-be-recognized linear velocity matrix to obtain a plurality of to-be-recognized angular velocity pixel frames; and forming a to-be-recognized angular velocity image by using the plurality of to-be-recognized angular velocity pixel frames.
29. The method according to claim 27, wherein the encoding the to-be-recognized motion feature matrix to obtain the to-be-recognized motion feature image comprises: preprocessing the to-be-recognized angular velocity matrix; encoding a plurality of to-be-recognized angular velocity units in the preprocessed to-be-recognized angular velocity matrix to obtain a plurality of to-be-recognized angular velocity pixel frames; extracting a plurality of to-be-recognized key angular velocity pixel frames from the plurality of to-be-recognized angular velocity pixel frames; and forming a to-be-recognized angular velocity image by using the plurality of to-be-recognized key angular velocity pixel frames.
30. The method according to claim 28 or 29, wherein the preprocessing the to-be-recognized angular velocity matrix comprises: obtaining a maximum to-be-recognized angular velocity element value and a minimum to-be-recognized angular velocity element value in the to-be-recognized angular velocity matrix; and performing normalization processing on each to-be-recognized angular velocity element value in the to-be-recognized angular velocity matrix based on the maximum to-be-recognized angular velocity element value and the minimum to-be-recognized angular velocity element value, to obtain a normalized to-be-recognized angular velocity matrix, wherein each to-be-recognized angular velocity element value in the normalized to-be-recognized angular velocity matrix is between a first value and a second value, and the first value is less than the second value.
31. The method according to claim 28 or 29, wherein the encoding a plurality of to-be-recognized angular velocity units in the preprocessed to-be-recognized angular velocity matrix to obtain a plurality of to-be-recognized angular velocity pixel frames comprises:
using direction angles of a joint point in each preprocessed to-be-recognized angular velocity unit in the three-dimensional coordinate system as image channels, and encoding a plurality of preprocessed to-be-recognized angular velocity units to obtain a plurality of to-be-recognized angular velocity pixel frames.
32. The method according to claim 29, wherein the extracting a plurality of to-be-recognized key angular velocity pixel frames from the plurality of to-be-recognized angular velocity pixel frames comprises: calculating angular velocity energy change values of the plurality of to-be-recognized angular velocity pixel frames based on the preprocessed to-be-recognized angular velocity matrix; and extracting the plurality of to-be-recognized key angular velocity pixel frames from the plurality of to-be-recognized angular velocity pixel frames in descending order of the angular velocity energy change values.
33. The method according to claim 32, wherein the calculating angular velocity energy change values of the plurality of to-be-recognized angular velocity pixel frames based on the preprocessed to-be-recognized angular velocity matrix comprises: calculating a quadratic sum of direction angles of each joint point in a first to-be-recognized angular velocity pixel frame in the three-dimensional coordinate system, and adding up the quadratic sums of the direction angles of all the joint points in the three-dimensional system to obtain an angular velocity energy value of the first to-be-recognized angular velocity pixel frame; calculating a quadratic sum of direction angles of each joint point in a second to-be-recognized angular velocity pixel frame in the three-dimensional coordinate system, and adding up the quadratic sums of the direction angles of all the joint points in the three-dimensional coordinate system to obtain an angular velocity energy value of the second to-be-recognized angular velocity pixel frame, wherein the first to-be-recognized angular velocity pixel frame and the second to-be-recognized angular velocity pixel frame are any two adjacent to-be-recognized angular velocity pixel frames, and the first to-be-recognized angular velocity pixel frame is a previous to-be-recognized angular velocity pixel frame of the second to-be-recognized angular velocity pixel frame; and subtracting the linear velocity energy value of the first to-be-recognized angular velocity pixel frame from the linear velocity energy value of the second to-be-recognized angular velocity pixel frame to obtain a linear velocity energy change value of the first to-be-recognized angular velocity pixel frame.
34. The method according to claim 18, wherein before the recognizing the to-be-recognized motion feature image based on the action recognition model, to obtain a recognition result, the method further comprises: performing a zero padding operation on the to-be-recognized motion feature image; and the recognizing the to-be-recognized motion feature image based on the action recognition model, to obtain a recognition result comprises:
recognizing, based on the action recognition model, the to-be-recognized motion feature image obtained through the zero padding operation, to obtain the recognition result.
35. The method according to claim 18, wherein the action recognition model is obtained by using the method according to claim 17.
36. A computer device, comprising a processor, a memory, a communications interface, and a bus, wherein
the memory, the processor, and the communications interface are connected to each other by using the bus, the memory is configured to store a computer instruction, and when the computer device runs, the processor runs the computer instruction, so that the computer device performs the image coding method according to any one of claims 1 to 16.
37. A computer device, comprising a processor, a memory, a communications interface, and a bus, wherein
the memory, the processor, and the communications interface are connected to each other by using the bus, the memory is configured to store a computer instruction, and when the computer device runs, the processor runs the computer instruction, so that the computer device performs the action recognition model training method according to claim 17.
38. A computer device, comprising a processor, a memory, a communications interface, and a bus, wherein
the memory, the processor, and the communications interface are connected to each other by using the bus, the memory is configured to store a computer instruction, and when the computer device runs, the processor runs the computer instruction, so that the computer device performs the action recognition method according to claims 18 to 35.
39. A computer-readable storage medium, wherein the storage medium comprises at least one instruction, and when the instruction is run on a computer device, the computer device is enabled to perform the image coding method according to any one of claims 1 to 16.
40. A computer-readable storage medium, wherein the storage medium comprises at least one instruction, and when the instruction is run on a computer device, the computer device is enabled to perform the action recognition model training method according to claim 17.
41. A computer-readable storage medium, wherein the storage medium comprises at least one instruction, and when the instruction is run on a computer device, the computer device is enabled to perform the action recognition method according to any one of claims 18 to 35.
</claims>
</document>
