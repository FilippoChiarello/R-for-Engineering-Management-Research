<document>

<filing_date>
2019-07-11
</filing_date>

<publication_date>
2021-01-14
</publication_date>

<priority_date>
2019-07-11
</priority_date>

<ipc_classes>
G06N3/04,G06N3/063
</ipc_classes>

<assignee>
FACEBOOK TECHNOLOGY COMPANY
</assignee>

<inventors>
VENKATESH, GANESH
LAI, Liangzhen
</inventors>

<docdb_family_id>
71948741
</docdb_family_id>

<title>
SYSTEMS AND METHODS FOR PIPELINED PARALLELISM TO ACCELERATE DISTRIBUTED PROCESSING
</title>

<abstract>
Disclosed herein includes a system, a method, and a device for pipelined parallelism to accelerate distributed learning network graph. First data for a first layer of a neural network may be stored in memory. First circuitry including a first plurality of processing element (PE) circuits may read the first data from the memory and perform computation for the first layer of the neural network using the first data to generate second data. The first circuitry includes a plurality of buffers for outputting the generated second data as input to second circuitry to perform computation for a second layer of the neural network. The second circuitry includes a second plurality of PE circuits configured to perform computation for the second layer of the neural network using the second data.
</abstract>

<claims>
1. A device comprising: memory configured to store first data for a first layer of a neural network; first circuitry comprising a first plurality of processing element (PE) circuits configured to read the first data from the memory and to perform computation for the first layer of the neural network using the first data to generate second data, the first circuitry further comprising a plurality of buffers configured to output the generated second data as input to second circuitry to perform computation for a second layer of the neural network; and the second circuitry comprising a second plurality of PE circuits configured to perform computation for the second layer of the neural network using the second data.
2. The device of claim 1, wherein the first plurality of PE circuits is configured to perform computation for at least one node of the neural network while the second plurality of PE circuits is performing computation for the second layer of the neural network.
3. The device of claim 2, wherein the at least one node is from a third layer of the neural network or from the first layer of the neural network.
4. The device of claim 1, wherein the plurality of buffers is configured to output the generated second data as input to the second circuitry by bypassing any transfer of the second data into or out of the memory.
5. The device of claim 1, wherein the second plurality of PE circuits is further configured to use the second data to generate third data.
6. The device of claim 5, wherein the second plurality of PE circuits is further configured to store the generated third data to the memory.
7. The device of claim 5, wherein the second circuitry further comprises a plurality of buffers configured to output the generated third data as input to third circuitry.
8. The device of claim 1, wherein the first data comprises at least one of weight or activation information for the first layer of the neural network, and the second data comprises at least one of weight or activation information for the second layer of the neural network.
9. The device of claim 1, wherein the first plurality of PE circuits is configured to perform a convolution operation using the first data, and the second plurality of PE circuits is configured to perform dot-product operations using the second data.
10. The device of claim 1, wherein the first circuitry and the second circuitry are formed on a same semiconductor device.
11. The device of claim 1, wherein the plurality of buffers is configured with sufficient capacity to buffer the generated second data and output the generated second data to the second circuitry.
12. A method comprising: storing first data for a first layer of a neural network in a memory; reading, by a first plurality of processing element (PE) circuits, the first data from the memory; using, by the first plurality of PE circuits, the first data to perform computation for the first layer of the neural network to generate second data; providing, by a plurality of buffers of the first plurality of PE circuits, the generated second data as input to a second plurality of PE circuits to perform computation for a second layer of the neural network; and using, by the second plurality of PE circuits, the second data to perform computation for the second layer of the neural network.
13. The method of claim 12, further comprising performing, by the first plurality of PE circuits, computation for at least one node of the neural network while the second plurality of PE circuits is performing computation for the second layer of the neural network.
14. The method of claim 13, wherein the at least one node is from a third layer of the neural network or from the first layer of the neural network.
15. The method of claim 12, comprising providing, by the plurality of buffers, the generated second data as input to the second plurality of PE circuits by bypassing any transfer of the second data into or out of the memory.
16. The method of claim 12, further comprising generating, by the second plurality of PE circuits, third data using the second data.
17. The method of claim 16, further comprising storing, by the second plurality of PE circuits, the generated third data to the memory.
18. The method of claim 16, further comprising providing, by a plurality of buffers corresponding to the second plurality of PE circuits, the generated third data as input to third circuitry.
19. The method of claim 12, wherein the first data comprises at least one of weight or activation information for the first layer of the neural network, and the second data comprises at least one of weight or activation information for the second layer of the neural network.
20. The method of claim 12, comprising performing, by the first plurality of PE circuits, a convolution operation using the first data, and performing, by the second plurality of PE circuits, dot-product operations using the second data.
</claims>
</document>
