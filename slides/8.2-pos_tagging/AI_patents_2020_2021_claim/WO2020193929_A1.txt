<document>

<filing_date>
2019-09-17
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2018-03-26
</priority_date>

<ipc_classes>
G06K9/00,G06N3/00,G06T13/20,G06T13/40,G10L21/10,G16H50/20,H04L12/58
</ipc_classes>

<assignee>
ORBITAL MEDIA AND ADVERTISING
</assignee>

<inventors>
ALLEN-VERCOE, Hayden
DICKSON, Ethan
BRADY, Peter Alistair
SANKARPANDI, Santhish
</inventors>

<docdb_family_id>
62068149
</docdb_family_id>

<title>
INTERACTIVE SYSTEMS AND METHODS
</title>

<abstract>
A method of producing an avatar video, the method comprising the steps of: providing a reference image of a person's face; providing a plurality of characteristic features representative of a facial model X0 of the person's face, the characteristic features defining a facial pose dependent on the person speaking; providing a target phrase to be rendered over a predetermined time period during the avatar video and providing a plurality of time intervals t within the predetermined time period; generating, for each of said times intervals t, speech features from the target phrase, to provide a sequence of speech features; and generating, using the plurality of characteristic features and sequence of speech features, a sequence of facial models Xt for each of said time intervals t.
</abstract>

<claims>
1. A method of producing an avatar video, the method comprising the steps of:
providing a reference image of a person's face;
providing a plurality of characteristic features representative of a facial model X0 of the person's face, the characteristic features defining a facial pose dependent on the person speaking;
providing a target phrase to be rendered over a predetermined time period during the avatar video and providing a plurality of time intervals t within the predetermined time period;
generating, for each of said times intervals t, speech features from the target phrase, to provide a sequence of speech features; and
generating, using the plurality of characteristic features and sequence of speech features, a sequence of facial models Xt for each of said time intervals t.
2. A method according to claim 1, further comprising the step of generating, from the sequence of facial models Xt, a sequence of face images to produce the avatar video.
3. A method according to claim 1 or claim 2, wherein the target phrase is provided as text data and/or audio data.
4. A method according to any preceding claim, wherein at least one of said speech features comprises a phonetic label.
5. A method according to any preceding claims, wherein the speech features are extracted with a phonetic classifier module using a Deep Convolutional Network (DCN).
6. A method according to any preceding claim, wherein the plurality of characteristic features comprises at least one Active Shape Model landmark, and at least one latent descriptors representing abstract appearance features.
7. A method according to claim 6, wherein the at least one latent descriptor is extracted using a Deep Convolutional Network (DCN).
8. A method according to any preceding claims, wherein the sequence of facial models Xt is generated using a recursive model.
9. A method according to claim 8, wherein the recursive model is comprises a
sequence-to-sequence encoder decoder method.
10. A method according to claim 9, wherein the recursive model is generated with a Long Short-Term Memory network.
1 1. A method according to any of claims 2 to 10, wherein generating the sequence of face images comprises using a frame generator to combine the reference image with the sequence of facial models Xt.
12. A method according to claim 11, wherein the frame generator comprises a
discriminator module using at least one loss function for reducing differences between the reference image and each of the facial models Xt in said sequence of facial models Xt.
13. A method for providing an answer to a user, the method comprising the steps of: providing a database comprising an indexed question library and a plurality of responses; providing a correlation between the indexed question library and the plurality of responses; receiving a question from the user as user input;
searching keyword information in the indexed question library based on the user input; and providing at least one response to the user based on said correlation.
14. A method according to claim 13, wherein, the method further comprises the steps of:
receiving feedback input from the user in response to the at least one response provided to the user; and based on the feedback input, searching further keyword information in the indexed symptoms library; and providing at least one further response to the user based on said correlation.
15. A method according to claim 13 or claim 14, wherein the correlation is provided using Al algorithms comprising a Long Short-Term Memory (LSTM) algorithm implemented by a Bi-directional Recurrent Neural network.
16. A method according to claim 15, wherein the Al algorithms form a high-level classifier and a low-level classifier.
17. A method according to any of claims 13 to 16, wherein, before receiving the user input, the user input is pre-processed, said pre-processing comprising the steps of tokenizing the user input and vectorising the tokenised user input.
18. A method according to any of claims 13 to 17, wherein providing at least one response comprises providing an avatar video produced according to any of claims 1 to 12.
19. A system for producing an avatar video, the method comprising the steps of:
an image processing model for receiving a reference image of a person's face and for
extracting a plurality of characteristic features representative of a facial model X0 of the person's face, the characteristic features defining a facial pose dependent on the person speaking;
a speech processing module for extracting a target phrase to be rendered over a
predetermined time period during the avatar video and for providing a plurality of time intervals t within the predetermined time period;
the speech processing module configured to generate, for each of said times intervals t, speech features from the target phrase, to provide a sequence of speech features; and
an avatar rendering module for generating, using the plurality of characteristic features and sequence of speech features, a sequence of facial models Xt for each of said time intervals t.
20. A system for producing an avatar video according to claim 19, wherein the avatar rendering module is configured to represent physical dynamics of the speech features by solving a system of ordinary differential equations (ODEs).
21. A system according to claim 20, wherein the physical dynamics of speech are
represented with a neural network.
22. An interactive system for providing an answer to a user, the system comprising: a database comprising an indexed question library and a plurality of responses;
a processing module for providing a correlation between the indexed question library and the plurality of responses;
input means for receiving a question from the user as user input;
wherein the processing module is configured to search keyword information in the indexed question library based on the user input; and
providing at least one response to the user based on said correlation.
23. An interactive system according to claim 22, wherein the plurality of responses comprise at least one avatar video produced using a system according to any of claims 19 22.
24. A healthcare information system comprising an interactive system according to claim 23.
</claims>
</document>
