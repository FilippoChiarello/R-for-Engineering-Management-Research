<document>

<filing_date>
2018-09-07
</filing_date>

<publication_date>
2020-07-22
</publication_date>

<priority_date>
2017-09-12
</priority_date>

<ipc_classes>
G06K9/62
</ipc_classes>

<assignee>
TENCENT TECHNOLOGY (SHENZHEN) COMPANY
</assignee>

<inventors>
MA, LIN
LIU, WEI
JIANG, WENHAO
</inventors>

<docdb_family_id>
62831544
</docdb_family_id>

<title>
TRAINING METHOD FOR IMAGE-TEXT MATCHING MODEL, BIDIRECTIONAL SEARCH METHOD, AND RELATED APPARATUS
</title>

<abstract>
This application relates to the field of artificial intelligence technologies, and in particular, to a training method for an image-text matching model, a bidirectional search method, and a related apparatus. The training method includes: extracting global representations and local representations of an image sample and a text sample; and training a matching model, to determine model parameters of the matching model, the matching model being used to determine, according to a global representation and a local representation of an image and a global representation and a local representation of a text, a matching degree between the image and the text. The matching degree in this application is obtained based on all of the detailed features and the global feature of the image, and the obtained matching degree is more accurate and comprehensive.
</abstract>

<claims>
1. A training method for an image-text matching model, carried out by a computer and comprising: extracting a global feature and a local feature of an image sample; extracting a global feature and a local feature of a text sample; and training a matching model according to the extracted global feature and local feature of the image sample and the extracted global feature and local feature of the text sample, to determine model parameters of the matching model, the matching model being used to determine, according to a global feature and a local feature of an inputted image and a global feature and a local feature of an inputted text, a matching degree between the image and the text.
2. The method according to claim 1, further comprising:
mapping the respective global features of the image and the text to a specified semantic space through the matching model, and calculating a similarity between the global features of the image and the text; mapping the respective local features of the image and the text to the specified semantic space, and calculating a similarity between the local features of the image and the text; and determining the matching degree between the image and the text using a weighted summation manner according to a preset weight of the similarity between the global features and a preset weight of the similarity between the local features.
3. The method according to claim 1, wherein the matching model comprises at least two fully connected layers, and the model parameters of the matching model comprise parameters of the at least two fully connected layers; and
the training a matching model according to the extracted global feature and local feature of the image sample and the extracted global feature and local feature of the text sample, to determine model parameters of the matching model comprises: mapping the respective global features of the image sample and the text sample to the specified semantic space through the at least two fully connected layers; mapping the respective local features of the image sample and the text sample to the specified semantic space through the at least two fully connected layers; and
determining the parameters of the at least two fully connected layers according to mapping results and a preset target function, the preset target function being used to determine that a similarity of a semantically associated image-text pair is higher than a similarity of a non-semantically associated image-text pair, or
the preset target function being used to determine that a similarity of a semantically associated image-text pair is higher than a similarity of a non-semantically associated image-text pair, and that a similarity between text samples associated with a same image sample is higher than a similarity between text samples associated with different image samples.
4. The method according to claim 3, wherein when the preset target function is used to determine that the similarity of the semantically associated image-text pair is higher than the similarity of the non-semantically associated image-text pair,
the mapping results comprise: a global feature and a local feature of the image sample represented by the parameters of the at least two fully connected layers, and a global feature and a local feature of the text sample represented by the parameters of the at least two fully connected layers; and
the determining the parameters of the at least two fully connected layers according to mapping results and a preset target function comprises: determining a similarity of a semantically associated image-text pair represented by the parameters of the at least two fully connected layers and a similarity of a non-semantically associated image-text pair represented by the parameters of the at least two fully connected layers according to the global feature and the local feature of the image sample represented by the parameters of the at least two fully connected layers, and the global feature and the local feature of the text sample represented by the parameters of the at least two fully connected layers; and inputting the similarity of the semantically associated image-text pair represented by the parameters of the at least two fully connected layers and the similarity of the non-semantically associated image-text pair represented by the parameters of the at least two fully connected layers into the preset target function, to determine the parameters of the at least two fully connected layers.
5. The method according to claim 4, wherein
the preset target function is: wherein represents the preset target function; and represent the text sample; and represent the image sample; d()d() represents a similarity between global features of the text sample and the image sample when a similarity between global features is determined; d()d() represents a similarity between local features of the text sample and the image sample when a similarity between local features is determined; λ1λ1 and λ2λ2 both represent preset coefficients; represents the similarity of the semantically associated image-text pair; and both represent the similarity of the non-semantically associated image-text pair; and and µ2 both represent preset thresholds.
6. The method according to claim 3, wherein when the preset target function is used to determine that the similarity of the semantically associated image-text pair is higher than the similarity of the non-semantically associated image-text pair, and that the similarity between text samples associated with the same image sample is higher than the similarity between text samples associated with different image samples,
the mapping results comprise: a global feature and a local feature of the image sample represented by the parameters of the at least two fully connected layers, and a global feature and a local feature of the text sample represented by the parameters of the at least two fully connected layers; and
the determining the parameters of the at least two fully connected layers according to mapping results and a preset target function comprises: determining a similarity of a semantically associated image-text pair represented by the parameters of the at least two fully connected layers, a similarity of a non-semantically associated image-text pair represented by the parameters of the at least two fully connected layers, a similarity between text samples associated with a same image sample represented by the parameters of the at least two fully connected layers, and a similarity between text samples associated with different image samples represented by the parameters of the at least two fully connected layers according to the global feature and the local feature of the image sample represented by the parameters of the at least two fully connected layers, and the global feature and the local feature of the text sample represented by the parameters of the at least two fully connected layers; and inputting the similarity of the semantically associated image-text pair represented by the parameters of the at least two fully connected layers, the similarity of the non-semantically associated image-text pair represented by the parameters of the at least two fully connected layers, the similarity between text samples associated with the same image sample represented by the parameters of the at least two fully connected layers, and the similarity between text samples associated with different image samples represented by the parameters of the at least two fully connected layers into the preset target function, to determine the parameters of the at least two fully connected layers.
7. The method according to claim 6, wherein the preset target function is determined through one of following: and wherein wherein LY is the preset target function, represents a relationship between the similarity of the semantically associated image-text pair and the similarity of the non-semantically associated image-text pair, and L(Si, Sl, Sj) represents a relationship between the similarity between text samples associated with the same image sample and the similarity between text samples associated with different image samples; and represent the text sample; and represent the image sample; d()d() represents a similarity between global features of the text sample and the image sample when a similarity between global features is determined; d()d() represents a similarity between local features of the text sample and the image sample when a similarity between local features is determined; λ1λ1 λ2 and λ2 both represent preset coefficients; represents the similarity of the semantically associated image-text pair; and both represent the similarity of the non-semantically associated image-text pair; and u2 both represent preset thresholds; and wherein Si, Sl Si, Sl represent the text samples semantically associated with the same image sample; Si, Sj Si, Sj represent the text samples associated with different image samples; d()d() represents the similarity between global features of the text samples when the similarity between global features is determined; d()d() represents the similarity between local features of the text samples when the similarity between local features is determined; and u3u3 represents a preset threshold.
8. The method according to claim 1, wherein the extracting a local feature of an image sample comprises: dividing the image sample into a specified quantity of image blocks, and for each image block, calculating a probability that the image block comprises a specified category of image information; and selecting maximum probabilities of respective specified categories of image information in the specified quantity of image blocks, the maximum probabilities of the respective specified categories of image information constituting the local feature of the image sample.
9. The method according to claim 1, wherein the extracting a global feature of a text sample comprises: performing word segmentation on the text sample; for each word segment, determining a vector of the word segment, different word segments having the same vector length; and inputting vectors of word segments of a same text sample into a convolutional neural network to extract the global feature of the text sample, the neural network used to extract the global feature of the text sample comprising a plurality of convolutional layers and a pooling layer connected to the plurality of convolutional layers, and a field of view of a specified size of a previous convolutional layer being used as input of a current convolutional layer, the field of view of the specified size comprising features of vectors of at least two word segments extracted by the previous convolutional layer.
10. An image-text bidirectional search method, carried out by a computer and comprising: receiving a reference sample, the reference sample being a text or an image; extracting a global feature and a local feature of the reference sample; inputting the global feature and the local feature of the reference sample into a matching model, to enable the matching model to calculate a matching degree between the reference sample and a corresponding material; the corresponding material being an image when the reference sample is a text, and the corresponding material being a text when the reference sample is an image; and the matching model being capable of determining the matching degree between the reference sample and the corresponding material based on the global feature and the local feature of the reference sample and a global feature and a local feature of the corresponding material; and selecting a corresponding material whose matching degree is greater than a specified matching degree as a material matching the reference sample.
11. A training apparatus for an image-text matching model, comprising: an image feature extraction module, configured to extract a global feature and a local feature of an image sample; a text feature extraction module, configured to extract a global feature and a local feature of a text sample; and a training module, configured to train a matching model according to the extracted global feature and local feature of the image sample and the extracted global feature and local feature of the text sample, to determine model parameters of the matching model, the matching model being used to determine, according to a global feature and a local feature of an image and a global feature and a local feature of a text, a matching degree between the image and the text.
12. An image-text bidirectional search apparatus, comprising: a reference sample receiving module, configured to receive a reference sample, the reference sample being a text or an image; a reference sample feature extraction module, configured to extract a global feature and a local feature of the reference sample; a search module, configured to input the global feature and the local feature of the reference sample into a matching model, to enable the matching model to calculate a matching degree between the reference sample and a corresponding material; the corresponding material being an image when the reference sample is a text, and the corresponding material being a text when the reference sample is an image; and the matching model being capable of determining the matching degree between the reference sample and the corresponding material based on the global feature and the local feature of the reference sample and a global feature and a local feature of the corresponding material; and a selection module, configured to select a corresponding material whose matching degree is greater than a specified matching degree as a material matching the reference sample.
13. A computing device, comprising a memory and a processor, the memory being configured to store program instructions, and the processor being configured to invoke the program instructions stored in the memory, to perform the training method for an image-text matching model according to any one of claims 1 to 9 according to the program instructions.
14. A computer storage medium, storing computer-executable instructions, the computer-executable instructions being used to enable the computer to perform the training method for an image-text matching model according to any one of claims 1 to 9 according to the computer-executable instructions.
15. A computing device, comprising a memory and a processor, the memory being configured to store program instructions, and the processor being configured to invoke the program instructions stored in the memory to perform the image-text bidirectional search method according to claim 10 according to the program instructions.
16. A computer storage medium, storing computer-executable instructions, the computer-executable instructions being used to enable the computer to perform the image-text bidirectional search method according to claim 10 according to the computer-executable instructions.
</claims>
</document>
