<document>

<filing_date>
2020-04-15
</filing_date>

<publication_date>
2020-11-04
</publication_date>

<priority_date>
2019-05-02
</priority_date>

<ipc_classes>
G06K9/00,G06K9/32,G06K9/54
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
Baek, Jiwon
Ko, Minsu
Lee, Solae
Lee, Hana
Han, Seungju
</inventors>

<docdb_family_id>
70289717
</docdb_family_id>

<title>
METHOD AND APPARATUS WITH LIVENESS DETECTION
</title>

<abstract>
A liveness detection method and apparatus, and a facial verification method and apparatus are disclosed. The liveness detection method includes detecting a face region in an input image, measuring characteristic information of the face region, adjusting the measured characteristic information in response to the characteristic information not satisfying a condition, and performing a liveness detection on the face region with the adjusted characteristic information upon the measured characteristic information not satisfying the condition.
</abstract>

<claims>
1. A processor-implemented liveness detection method, comprising: detecting a face region in an input image; measuring characteristic information of the detected face region; in response to the measured characteristic information being determined as not satisfying a condition, generating an adjusted face region by adjusting the characteristic information of the face region; and performing a liveness detection on the adjusted face region.
2. The method of claim 1, wherein the measuring of the characteristic information of the face region comprises measuring a hue value of the face region, wherein the generating of the adjusted face region comprises: in response to a determination that the measured hue value is not included in a preset hue value range, performing the generating of the adjusted face region by adjusting the hue value of the face region to a hue value included in the preset hue value range, preferably by applying a hue adjustment model to the face region.
3. The method of any of the previous claims, wherein the measuring of the characteristic information of the face region comprises measuring a face tilt value indicating a degree of a tilt of a face included in the face region, preferably by: detecting feature points corresponding to a left eye, a right eye, and both mouth ends in the face region; and measuring the face tilt value based on the detected feature points, wherein the generating of the adjusted face region comprises:
in response to a determination that the measured face tilt value is not included in a preset face tilt value range, performing the generating of the adjusted face region by adjusting the face tilt value, preferably by rotating the face region based on the measured face tilt value.
4. The method of any of the previous claims, wherein the measuring of the characteristic information of the face region comprises measuring a white balance value of the face region, wherein the generating of the face adjusted region comprises: in response to a determination that the measured white balance value is not included in a preset white balance value range, performing the generating of the adjusted face region by adjusting the white balance value of the face region to a white balance value included in the preset white balance value range.
5. The method of any of the previous claims, further comprising:
in response to a determination that the measured characteristic information satisfies the condition, performing the liveness detection based on the detected face region without performing the generating of the adjusted face region.
6. The method of any of the previous claims, wherein the performing of the liveness detection comprises performing the liveness detection using a neural network-based liveness detection model; and/or a neural network is used to perform a facial verification on the adjusted face region.
7. The method of any of the previous claims, wherein the measuring of the characteristic information of the face region comprises: measuring any one or any combination of any two or more of a hue value, a face tilt value, and a white balance value of the face region; and in response to any one or any combination of any two or more of the measured hue value, the measured face tilt value, and the measured white balance value being respectively determined to be out of range of a preset hue value range, a preset face tilt value range, and a preset white balance value range, generating the adjusted face region by respectively adjusting the hue value, the face tilt value, and the white balance value to respectively fall within the preset hue value range, the preset face tilt value range, and the preset white balance value range.
8. A processor-implemented facial verification method using a neural network, the facial verification method comprising: extracting a current facial image of a face region from an input image; calculating a current image feature value of the current facial image; comparing the current image feature value to a range of respective image feature values of training facial images; in response to a determination that the current image feature value is out of the range of the respective image feature values, generating an adjusted current facial image by adjusting the current image feature value of the current facial image to be included in the range of the respective image feature values; and inputting of the adjusted current facial image to the neural network.
9. The method of claim 8, wherein the range of the respective image feature values is an absolute range determined based on a minimum image feature value and a maximum image feature value among the image feature values of the training facial images, or wherein the range of the respective image feature values is a statistical range determined based on a distribution characteristic of the image feature values of the training facial images, and
wherein the distribution characteristic includes a mean and a standard deviation of the image feature values, wherein the statistical range is preferably one of a range of 1 standard deviation, a range of 2 standard deviations, a range of 3 standard deviations, and a range of 4 standard deviations, on both sides from the mean as a center.
10. The method of claim 8 or 9, wherein the neural network is configured to perform a liveness detection on a face object in the input image, preferably using the method according to any of claims 1-7; and/or wherein the neural network is configured to verify a face object in the input image.
11. The method of any pf claims 8-10, wherein the calculating of the current image feature value comprises: calculating a hue value of the current facial image, wherein the adjusting of the current facial image comprises:
in response to a determination that the calculated hue value is out of the range of the respective image feature values, generating the adjusted current facial image by adjusting the hue value of the current facial image to be included in the range of the respective image feature values, and/or wherein the calculating of the current image feature value comprises: calculating a face tilt value indicating a degree of a tilt of a face in the current facial image, wherein the adjusting of the current facial image comprises:
in response to a determination that the calculated face tilt value is out of the range of the respective image feature values, generating the adjusted current facial image by rotating the face region of the current facial image so a tilt value of the adjusted current facial image to be included in the range of the respective image feature values, and/or wherein the calculating of the current image feature value comprises: calculating a white balance value of the face region in the current facial image, wherein the adjusting of the current facial image comprises:
in response to a determination that the calculated white balance value is out of the range of the respective image feature values, generating the adjusted current facial image by adjusting the white balance value of the current facial image so a while balance value of the adjusted current facial image to be included in the range of the respective image feature values.
12. A non-transitory computer-readable storage medium storing instructions that, when executed by one or more processors, configure the one or more processors to perform the method of any of the previous claims.
13. An apparatus, comprising:
one or more processors configured to perform the method of any of claims 1-7.
14. An apparatus using a neural network, the apparatus comprising:
one or more processors configured to perform the method of any of claims 8-10.
15. The apparatus of claim 13 or 14, wherein the apparatus is any of a smartphone, a wearable device, a tablet computer, a netbook, a laptop computer, a desktop computer, a personal digital assistant (PDA), a set-top box, a home appliance, a biometric door lock, a security device, and a vehicle start device.
</claims>
</document>
