<document>

<filing_date>
2018-12-13
</filing_date>

<publication_date>
2020-06-18
</publication_date>

<priority_date>
2018-12-13
</priority_date>

<ipc_classes>
G06N20/00,G06N5/04,G06Q30/02
</ipc_classes>

<assignee>
SAP
</assignee>

<inventors>
SHOSHAN, ITZHAK
</inventors>

<docdb_family_id>
71072695
</docdb_family_id>

<title>
DISTRIBUTED MACHINE LEARNING DECENTRALIZED APPLICATION PLATFORM
</title>

<abstract>
A request for an inference from a customer is received at a machine learning (ML) decentralized application (DAPP) platform, where the request includes a data record associated with a user that is associated with the customer. The data record is distributed by the ML DAPP platform among a number of service providers. An inference is received at the ML DAPP platform from each service provider. The received inferences are returned to the customer by the ML DAPP platform.
</abstract>

<claims>
1. A computer-implemented method, comprising: receiving, at a machine learning (ML) decentralized application (DAPP) platform, a request for an inference from a customer, wherein the request comprises a data record associated with a user that is associated with the customer; distributing, by the ML DAPP platform, the data record among a plurality of service providers; receiving, at the ML DAPP platform, an inference from each service provider; and returning, by the ML DAPP platform, the received inferences to the customer.
2. The computer-implemented method of claim 1, wherein each service provider is in an underutilized state.
3. The computer-implemented method of claim 1, further comprising: prior to receiving a request for an interference from the customer, triggering, by the user, a triggering event.
4. The computer-implemented method of claim 3, further comprising: presenting, by the customer and to the user, a personalized offering that is based on the returned inferences.
5. The computer-implemented method of claim 1, further comprising: prior to distributing the data record: sending, by the ML DAPP platform, an inference quote to the customer, wherein the inference quote is made by evaluating the received database; and receiving, at the ML DAPP platform, an acceptance of the inference quote from the customer if the customer accepts the inference quote.
6. The computer-implemented method of claim 1, wherein the plurality of service providers belong to a network of devices.
7. The computer-implemented method of claim 6, wherein each service provider is registered at a ML model repository, and wherein each registered service provider is loaded with a ML model received from the ML model repository.
8. The computer-implemented method of claim 7, further comprising: in response to receiving a result from service providers, sending, from the ML DAPP platform and to the network of devices, a request for assigning a token to each service provider.
9. The computer-implemented method of claim 7, wherein each inference is generated by the service provider executing the data record using the received ML model.
10. The computer-implemented method of claim 7, further comprising: sending, by the ML DAPP platform, the inferences to the ML model repository, wherein the inferences are used to train models stored in the ML model repository.
11. The computer-implemented method of claim 1, wherein the received request further comprises a customer-defined model, wherein the data record is distributed to the service providers in separate data batches, and wherein the customer-defined model is trained on each service provider using each data batch.
12. The computer-implemented method of claim 11, wherein each service provider is in an underutilized state and is ready to accept tasks from the ML DAPP platform.
13. A non-transitory, computer-readable medium storing one or more instructions executable by a computer system to perform operations comprising: receiving, at a machine learning (ML) decentralized application (DAPP) platform, a request for an inference from a customer, wherein the request comprises a data record associated with a user that is associated with the customer; distributing, by the ML DAPP platform, the data record among a plurality of service providers; receiving, at the ML DAPP platform, an inference from each service provider; and returning, by the ML DAPP platform, the received inferences to the customer.
14. The non-transitory, computer-readable medium of claim 13, wherein each service provider is in an underutilized state.
15. The non-transitory, computer-readable medium of claim 13, further comprising: presenting, by the customer and to the user, a personalized offering that is based on the returned inferences.
16. The non-transitory, computer-readable medium of claim 13, wherein the plurality of service providers belong to a network of devices.
17. A computer-implemented system, comprising: one or more computers; and one or more computer memory devices interoperably coupled with the one or more computers and having tangible, non-transitory, machine-readable media storing one or more instructions that, when executed by the one or more computers, perform one or more operations comprising: receiving, at a machine learning (ML) decentralized application (DAPP) platform, a request for an inference from a customer, wherein the request comprises a data record associated with a user that is associated with the customer; distributing, by the ML DAPP platform, the data record among a plurality of service providers; receiving, at the ML DAPP platform, an inference from each service provider; and returning, by the ML DAPP platform, the received inferences to the customer.
18. The computer-implemented system of claim 17, wherein each service provider is in an underutilized state.
19. The computer-implemented system of claim 17, further comprising: presenting, by the customer and to the user, a personalized offering that is based on the returned inferences.
20. The computer-implemented system of claim 17, wherein the plurality of service providers belong to a network of devices.
</claims>
</document>
