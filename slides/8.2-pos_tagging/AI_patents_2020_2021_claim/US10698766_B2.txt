<document>

<filing_date>
2018-04-18
</filing_date>

<publication_date>
2020-06-30
</publication_date>

<priority_date>
2018-04-18
</priority_date>

<ipc_classes>
G06F11/00,G06F11/14,G06F9/46,G06F9/48,G06K9/62,G06N20/00,G06T1/20,G06T1/60
</ipc_classes>

<assignee>
EMC IP HOLDING COMPANY
</assignee>

<inventors>
SAVIC, DRAGAN
ZHAO JUNPING
</inventors>

<docdb_family_id>
68237872
</docdb_family_id>

<title>
Optimization of checkpoint operations for deep learning computing
</title>

<abstract>
Systems and methods are provided to optimize checkpoint operations for deep learning (DL) model training tasks. For example, a distributed DL model training process is executed to train a DL model using multiple accelerator devices residing on one or more server nodes, and a checkpoint operation is performed to generate and store a checkpoint of an intermediate DL model. A checkpoint operation includes compressing a checkpoint of an intermediate DL model stored in memory of a given accelerator device to generate a compressed checkpoint, and scheduling a time to perform a memory copy operation to transfer a copy of the compressed checkpoint from the memory of the given accelerator device to a host system memory. The scheduling is performed based on information regarding bandwidth usage of a communication link to be utilized to transfer the compressed checkpoint to perform the memory copy operation, wherein the memory copy operation is performed at the scheduled time.
</abstract>

<claims>
1. A method, comprising: executing a distributed deep learning (DL) model training process to train a DL model using a plurality of accelerator devices residing on one or more server nodes of a computing system, wherein the distributed DL model training process comprises multiple iterations, wherein each iteration other than an initial iteration results in generation of an intermediate DL model which is an updated version of a previous intermediate DL model that is generated in conjunction with a previous iteration, and wherein a final iteration results in generation of a final DL model; and performing a checkpoint operation for the distributed DL model training process to generate and store a checkpoint image of a given intermediate DL model which is generated in conjunction with a given iteration of the distributed DL model training process prior to the final iteration, wherein performing the checkpoint operation comprises: accessing an in-memory image of the given intermediate DL model from a memory of a given accelerator device of the plurality of accelerator devices in which the given intermediate DL model is stored; compressing the in-memory image of the given intermediate DL model to generate a compressed checkpoint image and storing the compressed checkpoint image in the memory of the given accelerator device; scheduling a time to perform a memory copy operation to transfer a copy of the compressed checkpoint image from the memory of the given accelerator device to a host system memory, wherein the scheduling is performed based at least in part on a bandwidth usage of a communication link of the computing system, which is to be utilized to transfer the compressed checkpoint image from the memory of the given accelerator device to the host system memory to perform the memory copy operation; and performing the memory copy operation at the scheduled time.
2. The method of claim 1, further comprising initiating a checkpoint operation for the distributed DL model training process when a condition for performing the checkpoint operation has been met, wherein the condition for performing the checkpoint operation comprises a completion of a predefined number of iterations of the DL model training process.
3. The method of claim 1, wherein compressing the in-memory image of the given intermediate DL model to generate the compressed checkpoint image comprises: selecting an accelerator device among the plurality of accelerator devices to compress the in-memory image of the given intermediate DL model; and loading a data compression kernel for execution by the selected accelerator device to compress the in-memory image of the given intermediate DL model.
4. The method of claim 1, wherein compressing the in-memory image of the given intermediate DL model to generate the compressed checkpoint image comprises utilizing a dedicated data compression accelerator device to generate the compressed checkpoint image.
5. The method of claim 1, wherein performing the checkpoint operation further comprises: determining a classification accuracy of the given intermediate DL model; comparing the determined classification accuracy of the given intermediate DL model to a classification accuracy of a last stored compressed checkpoint image of a previous intermediate DL model; and compressing the in-memory image of the given intermediate DL model in response to determining that the classification accuracy of the given intermediate DL model exceeds the classification accuracy of the last stored compressed checkpoint image of the previous intermediate DL model.
6. The method of claim 1, wherein scheduling the time to perform the memory copy operation comprises: receiving notification that the compressed checkpoint image is stored in the memory of the given accelerator device; determining a current bandwidth usage of the communication link of the computing system, which is to be utilized to transfer the compressed checkpoint image to perform the memory copy operation; comparing the determined bandwidth usage of the communication link with a predefined bandwidth usage threshold to determine if there is sufficient bandwidth to transfer the compressed checkpoint image over the communication link to perform the memory copy operation; and initiating the memory copy operation responsive to a determination that there is sufficient bandwidth to transfer the compressed checkpoint image over the communication link.
7. The method of claim 6, wherein scheduling the time to perform the memory copy operation further comprises placing the memory copy operation in a pending state for execution at a subsequent time when it is determined that there is sufficient bandwidth to transfer the compressed checkpoint image over the communication link.
8. The method of claim 7, wherein scheduling the time to perform the memory copy operation further comprises initiating the pending memory copy operation upon the expiration of a predetermined period of time irrespective of whether there is sufficient bandwidth to transfer the compressed checkpoint image over the communication link.
9. The method of claim 1, wherein the plurality of accelerator devices comprises graphics processing unit (GPU) devices.
10. An article of manufacture comprising a processor-readable storage medium having stored program code of one or more software programs, wherein the program code is executable by one or more processors to implement method steps comprising: executing a distributed deep learning (DL) model training process to train a DL model using a plurality of accelerator devices residing on one or more server nodes of a computing system, wherein the distributed DL model training process comprises multiple iterations, wherein each iteration other than an initial iteration results in generation of an intermediate DL model which is an updated version of a previous intermediate DL model that is generated in conjunction with a previous iteration, and wherein a final iteration results in generation of a final DL model; and performing a checkpoint operation for the distributed DL model training process to generate and store a checkpoint image of a given intermediate DL model which is generated in conjunction with a given iteration of the distributed DL model training process prior to the final iteration, wherein performing the checkpoint operation comprises: accessing an in-memory image of the given intermediate DL model from a memory of a given accelerator device of the plurality of accelerator devices in which the given intermediate DL model is stored; compressing the in-memory image of the given intermediate DL model to generate a compressed checkpoint image and storing the compressed checkpoint image in the memory of the given accelerator device; scheduling a time to perform a memory copy operation to transfer a copy of the compressed checkpoint image from the memory of the given accelerator device to a host system memory, wherein the scheduling is performed based at least in part on a bandwidth usage of a communication link of the computing system, which is to be utilized to transfer the compressed checkpoint image from the memory of the given accelerator device to the host system memory to perform the memory copy operation; and performing the memory copy operation at the scheduled time.
11. The article of manufacture of claim 10, further comprising executable program code for initiating a checkpoint operation for the distributed DL model training process when a condition for performing the checkpoint operation has been met, wherein the condition for performing the checkpoint operation comprises a completion of a predefined number of iterations of the DL model training process.
12. The article of manufacture of claim 10, wherein compressing the in-memory image of the given intermediate DL model to generate the compressed checkpoint image comprises: selecting an accelerator device among the plurality of accelerator devices to compress the in-memory image of the given intermediate DL model; and loading a data compression kernel for execution by the selected accelerator device to compress the in-memory image of the given intermediate DL model.
13. The article of manufacture of claim 10, wherein compressing the in-memory image of the given intermediate DL model to generate the compressed checkpoint image comprises utilizing a dedicated data compression accelerator device to generate the compressed checkpoint image.
14. The article of manufacture of claim 10, wherein performing the checkpoint operation further comprises: determining a classification accuracy of the given intermediate DL model; comparing the determined classification accuracy of the given intermediate DL model to a classification accuracy of a last stored compressed checkpoint image of a previous intermediate DL model; and compressing the in-memory image of the given intermediate DL model in response to determining that the classification accuracy of the given intermediate DL model exceeds the classification accuracy of the last stored compressed checkpoint image of the previous intermediate DL model.
15. The article of manufacture of claim 10, wherein scheduling the time to perform the memory copy operation comprises: receiving notification that the compressed checkpoint image is stored in the memory of the given accelerator device; determining a current bandwidth usage of the communication link of the computing system, which is to be utilized to transfer the compressed checkpoint image to perform the memory copy operation; comparing the determined bandwidth usage of the communication link with a predefined bandwidth usage threshold to determine if there is sufficient bandwidth to transfer the compressed checkpoint image over the communication link to perform the memory copy operation; and initiating the memory copy operation responsive to a determination that there is sufficient bandwidth to transfer the compressed checkpoint image over the communication link.
16. The article of manufacture of claim 15, wherein scheduling the time to perform the memory copy operation further comprises: placing the memory copy operation in a pending state for execution at a subsequent time when it is determined that there is sufficient bandwidth to transfer the compressed image over the communication link; and initiating the pending memory copy operation upon the expiration of a predetermined period of time irrespective of whether there is sufficient bandwidth to transfer the compressed checkpoint image over the communication link.
17. The article of manufacture of claim 10, wherein the plurality of accelerator devices comprises graphics processing unit (GPU) devices.
18. A computing system, comprising: a server cluster comprising a plurality of server nodes, wherein the server nodes comprise accelerator devices; a control server node comprising a memory to store program instructions, and a processor to execute the stored program instructions to cause the control server node to perform a process which comprises: executing a distributed deep learning (DL) model training process to train a DL model using a plurality of accelerator devices residing on one or more server nodes of the computing system, wherein the distributed DL model training process comprises multiple iterations, wherein each iteration other than an initial iteration results in generation of an intermediate DL model which is an updated version of a previous intermediate DL model that is generated in conjunction with a previous iteration, and wherein a final iteration results in generation of a final DL model; and performing a checkpoint operation for the distributed DL model training process to generate and store a checkpoint image of a given intermediate DL model which is generated in conjunction with a given iteration of the distributed DL model training process prior to the final iteration, wherein performing the checkpoint operation comprises: accessing an in-memory image of the given intermediate DL model from a memory of a given accelerator device of the plurality of accelerator devices in which the given intermediate DL model is stored; compressing the in-memory image of the given intermediate DL model to generate a compressed checkpoint image and storing the compressed checkpoint image in the memory of the given accelerator device; scheduling a time to perform a memory copy operation to transfer a copy of the compressed checkpoint image from the memory of the given accelerator device to a host system memory, wherein the scheduling is performed based at least in part on a bandwidth usage of a communication link of the computing system, which is to be utilized to transfer the compressed checkpoint image from the memory of the given accelerator device to the host system memory to perform the memory copy operation; and performing the memory copy operation at the scheduled time.
19. The computing system of claim 18, wherein scheduling the time to perform the memory copy operation comprises: receiving notification that the compressed checkpoint image is stored in the memory of the given accelerator device; determining a current bandwidth usage of the communication link of the computing system, which is to be utilized to transfer the compressed checkpoint image to perform the memory copy operation; comparing the determined bandwidth usage of the communication link with a predefined bandwidth usage threshold to determine if there is sufficient bandwidth to transfer the compressed checkpoint image over the communication link to perform the memory copy operation; and initiating the memory copy operation responsive to a determination that there is sufficient bandwidth to transfer the compressed checkpoint image over the communication link.
20. The computing system of claim 19, wherein scheduling the time to perform the memory copy operation further comprises: placing the memory copy operation in a pending state for execution at a subsequent time when it is determined that there is sufficient bandwidth to transfer the compressed image over the communication link; and initiating the pending memory copy operation upon the expiration of a predetermined period of time irrespective of whether there is sufficient bandwidth to transfer the compressed checkpoint image over the communication link.
</claims>
</document>
