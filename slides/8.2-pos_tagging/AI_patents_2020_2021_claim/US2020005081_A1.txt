<document>

<filing_date>
2019-09-09
</filing_date>

<publication_date>
2020-01-02
</publication_date>

<priority_date>
2019-07-31
</priority_date>

<ipc_classes>
G06F3/0488,G06K9/00,G06K9/03,G06K9/62
</ipc_classes>

<assignee>
LG ELECTRONICS
</assignee>

<inventors>
KIM, DAESUNG
YUN, JAEWOONG
NAH, HEEYEON
</inventors>

<docdb_family_id>
67807768
</docdb_family_id>

<title>
METHOD AND APPARATUS FOR RECOGNIZING HANDWRITTEN CHARACTERS USING FEDERATED LEARNING
</title>

<abstract>
Provided is a method for recognizing handwritten characters in a terminal through federated learning. In the method, a first common prediction model for recognizing text from handwritten characters input from a user is applied, the handwritten characters are received from the user, feature values are extracted from an image including the handwritten characters, the feature values are input to the first common prediction mode, first text information is determined from an output of the first common prediction model, the first text information and a second text information received from the user for error correction of the first text information are cached, and the first common prediction model is learned using the image including the handwritten characters, the first text information, and the second text information. In this way, the terminal can determine the text from the handwritten characters input by the user, and can learn the first common prediction model through a feedback operation of the user.
</abstract>

<claims>
1. A method for recognizing handwritten characters in a terminal through federated learning, the method comprising: receiving the handwritten characters from a user; extracting a feature value from an image including the handwritten characters; inputting the feature value to a first common prediction model and determining first text information from an output of the first common prediction model; caching the first text information and second text information received from the user for error correction of the first text information; and learning the first common prediction model using the image including the handwritten characters, the first text information, and the second text information. wherein the first common prediction model is received through a server, and the first text information indicates text that is mapped by the image.
2. The method of claim 1, wherein in the learning of the first common prediction model, a weight parameter of the first common prediction model is updated using a hyper parameter received from the server.
3. The method of claim 2, further comprising: deleting the first text information and the second text information; transmitting the weight parameter to the server; and applying a second common prediction model received from the server, wherein the second common prediction model learns the first common prediction model using the weight parameter received from one or more terminals in the server.
4. The method of claim 1, wherein in the receiving of the handwritten characters from the user, the handwritten characters are received by a touch operation using a finger or a touch pen on an input region of the terminal.
5. The method of claim 1, wherein the second text information includes correct answer text input by the user through an input region of the terminal based on the first text information recognized by the user through an output screen of the terminal.
6. The method of claim 3, wherein in the applying of the second common prediction model, the first common prediction model is learned using the weight parameter extracted from the second common prediction model.
7. The method of claim 1, further comprising: transmitting the second text information to another terminal.
8. The method of claim 1, wherein the learning of the first common prediction model is performed when a condition set in the terminal is satisfied, and the condition includes a state where the terminal is being charged, a state in which the terminal is connected to WiFi, and a state where the terminal is in an idle mode.
9. The method of claim 3, wherein the applying of the second common prediction model is performed when the condition set in the terminal is satisfied, and the condition includes a state in which a permission is input from the user as a response to an update notification message output to a screen of the terminal, a state where the terminal is being charged, a state where the terminal is connected to WiFi, or a state where the terminal is in an idle mode.
10. The method of claim 3, wherein the second common prediction model learns the first common prediction model when a specific number or more of weight parameters set in the server are received.
11. A method for recognizing handwritten characters in a server through federated learning, the method comprising: transmitting, by a terminal, a first common prediction model for recognizing text from the handwritten characters received from a user; transmitting, by the terminal, a hyper-parameter for learning the first common prediction model; receiving a weight parameter from the terminal; and learning the first common prediction model using the weight parameter, wherein the first common prediction model is transmitted to one or more terminals.
12. The method of claim 11, further comprising: transmitting a second common prediction model to the terminal, wherein the second common prediction model learns the first common prediction model using the weight parameter.
13. The method of claim 11, wherein the learning of the first common prediction model is performed when a set specific number or more of the weight parameters are received.
14. The method of claim 12, wherein in the transmitting of the second common prediction model to the terminal, the weight parameter extracted from the second common prediction model is transmitted.
15. The method of claim 14, wherein the transmitting of the second common prediction model to the terminal is performed when a condition set in the terminal is satisfied, and the condition includes a state where the terminal is being charged, a state in which the terminal is connected to WiFi, and a state where the terminal is in an idle mode.
16. A terminal performing a method for recognizing handwritten characters through federated learning, comprising: a transceiver; a memory; a display; and a processor, wherein the processor receives the handwritten characters from a user through the display, extracts a feature value from an image including the handwritten characters, inputs the feature value to a first common prediction model and determining first text information from an output of the first common prediction model, caches the first text information and second text information received from the user for error correction of the first text information through the memory, and learns the first common prediction model using the image including the handwritten characters, the first text information, and the second text information, and the first common prediction model is received through a server, and the first text information indicates text that is mapped by the image.
17. The terminal of claim 16, wherein the processor updates a weight parameter of the first common prediction model is updated using a hyper-parameter received from the server through the communication model in order to learn the first common prediction model.
18. The terminal of claim 17, wherein the processor deletes the first text information and the second text information of the memory, transmits the weight parameter to the server through the transceiver, and applies a second common prediction model received from the server, and the second common prediction model learns the first common prediction model using the weight parameter received from one or more terminals in the server.
19. The terminal of claim 16, wherein the processor receives the handwritten characters through a touch operation using a finger or a touch pen on an input region to receive the handwritten characters from the user through the display.
20. The terminal of claim 16, wherein the second text information includes correct answer text input by the user through an input region of the terminal based on the first text information recognized by the user through the display.
</claims>
</document>
