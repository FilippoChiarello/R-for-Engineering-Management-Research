<document>

<filing_date>
2020-04-29
</filing_date>

<publication_date>
2020-11-05
</publication_date>

<priority_date>
2019-04-29
</priority_date>

<ipc_classes>
A61B5/11,A61B5/16,G06F16/70,G06F21/32,G06T15/08,G06T7/70
</ipc_classes>

<assignee>
SYLLABLE LIFE SCIENCES, INC.
</assignee>

<inventors>
CHAN, JOHN
WILTSCHKO, ALEXANDER
DATTA, Sandeep
</inventors>

<docdb_family_id>
73029198
</docdb_family_id>

<title>
SYSTEM AND METHOD OF FACIAL ANALYSIS
</title>

<abstract>
A system for facial analysis includes a camera, a data storage device and a data processing system. The camera takes video of a subject's face, and the data storage device receives and stores the video. The data processing system extracts a pose of the subject's face, and a representation of the subject's facial gesture state. The pose includes the angle and position of the subject's face. The representation includes facial keypoints that are a collection of points on the subject's face. The system then concatenates each data stream to align the data streams in time, extracts a plurality of facial syllables from the aligned data streams, and compiles the facial syllables into a series of state sequences. Based on the series of state sequences, the system extracts a behavioral fingerprint for the subject that provides a summary of the subject's state over a given period of time.
</abstract>

<claims>
1. A system for facial analysis comprising:
a camera configured to take a video of a subject's face, the video having a plurality of frames;
a data storage device configured to receive and store the video; and
a data processing system having a processor and program code which when executed:
(a) extracts a pose of the subject's face, the pose including the angle and position of the subject's face,
(b) extracts a representation of the subject's facial gesture state, the
representation including facial keypoints, the facial keypoints being a collection of points on the subject's face,
(c) concatenates each data stream to align the data streams in time,
(d) extracts a plurality of facial syllables from the aligned data streams,
(e) compiles the plurality of facial syllables into a series of state sequences, and
(f) extracts a behavioral fingerprint for the subject based on the series of state sequences, the behavioral fingerprint providing a summary of the subject's state over a given period of time.
2. A system according to claim 1, wherein the camera is a two-dimensional camera.
3. A system according to claim 1, wherein the camera is a three-dimensional camera.
4. A system according to claim 1, wherein the data processing system also has program code that extracts a face region from each of the frames of the video prior to extracting the pose of the subject's face.
5. A system according to claim 1, wherein the facial keypoints include at least one selected from the group consisting of the subject's nose, mouth, eyes and jaw line.
6. A system according to claim 1, wherein the data processing system uses latent embeddings derived from artificial neural networks and/or deep learning models to extract the facial gesture state.
7. A system according to claim 1, wherein the processing system is configured to analyze videos in aggregate to extract the facial syllables.
8. A system according to claim 1, wherein the processing system is configured to predict, based on the behavioral fingerprint, at least one selected from the group consisting of a level of pain, a level of anxiety, a level of depression, a level of hunger, a level of satiety, and a level of fatigue.
9. A system according to claim 1, wherein the processing system is configured to classify the behavioral summary as a pre-event summary or a post event summary.
10. A method of facial analysis comprising:
recording a video of a subject's face using a camera, the video having a plurality of frames;
storing the video in a data storage device;
extracting a pose of the subject's face, the pose including the angle and position of the subject's face;
extracting a representation of the subject's facial gesture state, the representation including facial keypoints, the facial keypoints being a collection of points on the subject's face;
concatenating each data stream to align the data streams in time;
extracting a plurality of facial syllables from the aligned data streams;
compiling the plurality of facial syllables into a series of state sequences; and extracting a behavioral fingerprint for the subject based on the series of state sequences, the behavioral fingerprint providing a summary of the subject's state over a given period of time.
11. A method according to claim 10, wherein the camera is a two-dimensional camera.
12. A method according to claim 10, wherein the camera is a three-dimensional camera.
13. A method according to claim 10, further comprising:
extracting a face region from each of the frames of the video prior to extracting the pose of the subject's face.
14. A method according to claim 10, wherein the facial keypoints include at least one selected from the group consisting of the subject's nose, mouth, eyes and jaw line.
15. A method according to claim 10, wherein extracting the facial gesture state includes using latent embeddings derived from artificial neural networks and/or deep learning models to extract the facial gesture state.
16. A method according to claim 10, wherein extracting the facial syllables includes analyzing videos in aggregate.
17. A method according to claim 10, further comprising predicting, based on the behavioral fingerprint, at least one selected from the group consisting of a level of pain, a level of anxiety, a level of depression, a level of hunger, a level of satiety, and a level of fatigue
18. A method according to claim 10, further comprising classifying the behavioral summary as a pre-event summary or a post event summary.
19. A system for subject analysis comprising:
a camera configured to take a video of a portion of a subject, the video having a plurality of frames;
a data storage device configured to receive and store the video; and
a data processing system having a processor and program code which when executed:
(a) extracts a pose of the portion of the subject, the pose including the angle and position of the portion of the subject,
(b) extracts a representation of the subject's gesture state, the representation including keypoints, the keypoints being a collection of points on the portion of the subject,
(c) concatenates each data stream to align the data streams in time,
(d) extracts a plurality of syllables from the aligned data streams,
(e) compiles the plurality of syllables into a series of state sequences, and
(f) extracts a behavioral fingerprint for the subject based on the series of state sequences, the behavioral fingerprint providing a summary of the subject's state over a given period of time.
20. A system according to claim 19, wherein the camera is a two-dimensional camera.
21. A system according to claim 19, wherein the camera is a three-dimensional camera.
22. A system according to claim 19, wherein the portion of the subject is the subject's face.
23. A system according to claim 22, wherein the pose of the portion of the subject is the pose of the subject's face, the pose including the angle and position of the subject's face.
24. A system according to claim 23, wherein the representation of the subject's gesture state is a representation of the subject's facial gesture state, the keypoints being facial keypoints that are a collection of points on the subject's face.
25. A system according to claim 24, wherein the plurality of syllables are a plurality of facial syllables.
26. A system according to claim 24, wherein the facial keypoints include at least one selected from the group consisting of the subject's nose, mouth, eyes and jaw line.
27. A system according to claim 19, wherein the data processing system use latent embeddings derived from artificial neural networks and/or deep learning models to extract the gesture state.
28. A system according to claim 19, wherein the processing system is configured to analyze videos in aggregate to extract the syllables.
29. A system according to claim 19, wherein the processing system is configured to predict, based on the behavioral fingerprint, at least one selected from the group consisting of a level of pain based, a level of anxiety, a level of depression, a level of hunger, a level of satiety, and a level of fatigue.
30. A system according to claim 19, wherein the processing system is configured to classify the behavioral summary as a pre-event summary or a post event summary.
31. A system according to claim 19, wherein the data processing system also has program code that extracts a region from each of the frames of the video prior to extracting the pose of the portion of the subject.
</claims>
</document>
