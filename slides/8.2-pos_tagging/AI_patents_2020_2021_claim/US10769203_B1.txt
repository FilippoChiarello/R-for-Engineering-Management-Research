<document>

<filing_date>
2019-12-10
</filing_date>

<publication_date>
2020-09-08
</publication_date>

<priority_date>
2019-10-17
</priority_date>

<ipc_classes>
G06F16/635,G06F16/638,G06F16/64,G06F16/65,G06F16/901,H04N21/439
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
SONASATH, MOIZ KAIZAR
JOSEPH, VINOD CHERIAN
</inventors>

<docdb_family_id>
72289899
</docdb_family_id>

<title>
System and method for prediction and recommendation using collaborative filtering
</title>

<abstract>
A method includes obtaining acoustic event information from at least one device, the acoustic event information associated with a first activity, at least a portion of the acoustic event information associated with sounds originating from a plurality of individuals in an identified group during a time period of the first activity. The method also includes categorizing the acoustic event information into a plurality of sound classes. The method also includes performing collaborative filtering on the plurality of sound classes. The method also includes determining one or more interests of the plurality of individuals in the identified group based on results of the collaborative filtering. The method also includes recommending a common activity for the identified group, based in part on the one or more interests of the plurality of individuals in the identified group.
</abstract>

<claims>
1. A method comprising: obtaining acoustic event information from at least one device, the acoustic event information associated with a first activity, at least a portion of the acoustic event information associated with sounds originating from a plurality of individuals in an identified group during a time period of the first activity; categorizing the acoustic event information into a plurality of sound classes; performing collaborative filtering on the plurality of sound classes; determining one or more interests of the plurality of individuals in the identified group based on results of the collaborative filtering; and recommending a common activity for the identified group, based in part on the one or more interests of the plurality of individuals in the identified group.
2. The method of claim 1, wherein the acoustic event information is categorized into the plurality of sound classes using at least one of automatic content recognition and natural language understanding.
3. The method of claim 1, wherein performing collaborative filtering on the plurality of sound classes comprises: filtering information from the sound classes to detect patterns associated with the plurality of individuals; and making one or more predictions about the one or more interests of the individuals.
4. The method of claim 1, wherein the one or more interests of the plurality of individuals are determined according to a level of engagement of each individual in the first activity.
5. The method of claim 4, further comprising: determining the level of engagement of each individual using contextual similarity detection.
6. The method of claim 5, further comprising: performing the contextual similarity detection based on a user access pattern model.
7. The method of claim 5, wherein recommending the common activity for the identified group comprises: generating an activity graph based on the level of engagement of each individual; determining and ranking one or more potential activities including the common activity based on the activity graph and the first activity; and selecting the common activity from the one or more potential activities based in part on the ranking.
8. The method of claim 1, further comprising: obtaining second acoustic event information from at least one second device, the second acoustic event information associated with the first activity, at least a portion of the second acoustic event information associated with sounds originating from a plurality of individuals in a second identified group during the time period of the first activity; categorizing the second acoustic event information into a plurality of second sound classes; performing collaborative filtering on the second sound classes; determining one or more interests of the plurality of individuals in the second identified group; and recommending a common activity for the second identified group, based in part on the one or more interests of the plurality of individuals in the second identified group.
9. An electronic device comprising: a transceiver; a processor configured to: obtain, via the transceiver, acoustic event information from at least one other device, the acoustic event information associated with a first activity, at least a portion of the acoustic event information associated with sounds originating from a plurality of individuals in an identified group during a time period of the first activity; categorize the acoustic event information into a plurality of sound classes; perform collaborative filtering on the plurality of sound classes; determine one or more interests of the plurality of individuals in the identified group based on results of the collaborative filtering; and recommend a common activity for the identified group, based in part on the one or more interests of the plurality of individuals in the identified group.
10. The electronic device of claim 9, wherein the processor is further configured to categorize the acoustic event information into the plurality of sound classes using at least one of automatic content recognition and natural language understanding.
11. The electronic device of claim 9, wherein to perform collaborative filtering on the plurality of sound classes, the processor is configured to: filter information from the sound classes to detect patterns associated with the plurality of individuals; and make one or more predictions about the one or more interests of the individuals.
12. The electronic device of claim 9, wherein the processor is further configured to determine the one or more interests of the plurality of individuals according to a level of engagement of each individual in the first activity.
13. The electronic device of claim 12, wherein the processor is further configured to determine the level of engagement of each individual using contextual similarity detection.
14. The electronic device of claim 13, wherein the processor is further configured to perform the contextual similarity detection based on a user access pattern model.
15. The electronic device of claim 13, wherein, to recommend the common activity for the identified group, the processor is further configured to: generate an activity graph based on the level of engagement of each individual; determine and rank one or more potential activities including the common activity based on the activity graph and the first activity; and select the common activity from the one or more potential activities based in part on the ranking.
16. The electronic device of claim 9, wherein the processor is further configured to: obtain second acoustic event information from at least one second device, the second acoustic event information associated with the first activity, at least a portion of the second acoustic event information associated with sounds originating from a plurality of individuals in a second identified group during the time period of the first activity; categorize the second acoustic event information into a plurality of second sound classes; perform collaborative filtering on the second sound classes; determine one or more interests of the plurality of individuals in the second identified group; and recommend a common activity for the second identified group, based in part on the one or more interests of the plurality of individuals in the second identified group.
17. A non-transitory computer readable medium containing computer readable program code that, when executed, causes at least one processor to: obtain acoustic event information from at least one device, the acoustic event information associated with a first activity, at least a portion of the acoustic event information associated with sounds originating from a plurality of individuals in an identified group during a time period of the first activity; categorize the acoustic event information into a plurality of sound classes; perform collaborative filtering on the plurality of sound classes; determine one or more interests of the plurality of individuals in the identified group based on results of the collaborative filtering; and recommend a common activity for the identified group, based in part on the one or more interests of the plurality of individuals in the identified group.
18. The non-transitory computer readable medium of claim 17, wherein the acoustic event information is categorized into the plurality of sound classes using at least one of automatic content recognition and natural language understanding.
19. The non-transitory computer readable medium of claim 17, wherein the computer readable program code that causes the at least one processor to perform collaborative filtering on the plurality of sound classes comprises computer readable program code that causes the at least one processor to: filter information from the sound classes to detect patterns associated with the plurality of individuals; and make one or more predictions about the one or more interests of the individuals.
20. The non-transitory computer readable medium of claim 17, wherein the one or more interests of the plurality of individuals are determined according to a level of engagement of each individual in the first activity.
</claims>
</document>
