<document>

<filing_date>
2020-07-22
</filing_date>

<publication_date>
2020-11-05
</publication_date>

<priority_date>
2018-08-14
</priority_date>

<ipc_classes>
G06N20/20,G06N3/02,G06N7/00,H04L29/12
</ipc_classes>

<assignee>
BEIJING DIDI INFINITY TECHNOLOGY AND DEVELOPMENT COMPANY
</assignee>

<inventors>
CHANG, YINHONG
HUANG TAO
LI, SHUAIJI
QIN, ZHIWEI
ZHANG, FANGFANG
</inventors>

<docdb_family_id>
69523589
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR DETECTING GENERATED DOMAIN
</title>

<abstract>
A computer-implemented method for domain analysis comprises: obtaining, by a computing device, a domain; and inputting, by the computing device, the obtained domain to a trained detection model to determine if the obtained domain was generated by one or more domain generation algorithms. The detection model comprises a neural network model, a n-gram-based machine learning model, and an ensemble layer. Inputting the obtained domain to the detection model comprises inputting the obtained domain to each of the neural network model and the n-gram-based machine learning model. The neural network model and the n-gram-based machine learning model both output to the ensemble layer. The ensemble layer outputs a probability that the obtained domain was generated by the domain generation algorithms.
</abstract>

<claims>
1. A computer-implemented method for training a detection model, comprising: training each of a neural network model and a n-gram-based machine learning model; generating a plurality of ensemble coefficients of an ensemble layer by inputting a plurality of outputs of both the trained neural network model and the trained n-gram-based machine learning model to the ensemble layer; and obtaining a trained detection model comprising the trained neural network model, the trained n-gram-based machine learning model, and the ensemble layer with the plurality of ensemble coefficients.
2. The method of claim 1, wherein training each of the neural network model and the n-gram-based machine learning model comprises: obtain training data comprising benign and generated domains; and training each of the neural network model and the n-gram-based machine learning mode based on the training data.
3. The method of claim 1, wherein the n-gram-based machine learning model comprises a gradient boosting based classifier trained based on n-gram features extracted from a training set.
4. The method of claim 1, wherein: the trained detection model comprises an extra feature layer; the extra feature layer outputs to the ensemble layer; and the extra feature layer comprises at least one of the following features: a length of an input domain name, a length of a top-level domain (TLD), whether the length of the domain name exceeds a domain name threshold, whether the length of the TLD exceeds a TLD threshold, a number of numerical characters in the domain name, whether the TLD contains any numerical character, a number of special characters contained in the domain name, and whether the TLD contains any special character.
5. The method of claim 1, wherein: the ensemble layer comprises a top logistic regression model; and generating the plurality of ensemble coefficients of the ensemble layer comprises solving the ensemble coefficients by inputting outputs of the trained neural network model and the trained n-gram-based machine learning model into the top logistic regression model.
6. The method of claim 1, wherein: the neural network model comprises a probability network, wherein the probability network comprises: a dense and batch normalization layer configured to output a predicted probability; a one-hot encoding layer configured to receive a first input of the probability network and couple to a recurrent neural network layer, the recurrent neural network layer coupled to the dense and batch normalization layer; and an embedding and batch normalization layer configured to receive a second input of the probability network and couple to the dense and batch normalization layer.
7. The method of claim 6, wherein: the recurrent neural network layer comprises long-short term memory (LSTM) units.
8. The method of claim 1, wherein: the neural network model comprises a representation network, wherein the representation network comprises: a dense and batch normalization layer configured to output a dense representation; an embedding and batch normalization layer configured to receive a first input of the representation network and couple to a recurrent neural network layer, recurrent neural network layer coupled to the dense and batch normalization layer; and an embedding and batch normalization layer configured to receive a second input of the representation network and couple to the dense and batch normalization layer.
9. The method of claim 8, wherein: the recurrent neural network layer comprises gated recurrent units (GRUs).
10. The method of claim 1, wherein: the n-gram-based machine learning model comprises a gradient boosting based classifier based on bigram features.
11. The method of claim 1, further comprising: inputting a domain to the trained detection model; and outputting from the trained detection model a probability that the domain was generated by a domain generation algorithm.
12. The method of claim 11, wherein inputting the obtained domain to the detection model comprises inputting the obtained domain to each of the neural network model and the n-gram-based machine learning model to obtain the plurality of outputs of both the trained neural network model and the trained n-gram-based machine learning model.
13. A system comprising a processor and a non-transitory computer-readable memory storing instructions that, when executed by the processor, cause the processor to perform operations comprising: training each of a neural network model and a n-gram-based machine learning model; generating a plurality of ensemble coefficients of an ensemble layer by inputting a plurality of outputs of both the trained neural network model and the trained n-gram-based machine learning model to the ensemble layer; and obtaining a trained detection model comprising the trained neural network model, the trained n-gram-based machine learning model, and the ensemble layer with the plurality of ensemble coefficients.
14. The system of claim 13, wherein training each of the neural network model and the n-gram-based machine learning model comprises: obtain training data comprising benign and generated domains; and training each of the neural network model and the n-gram-based machine learning mode based on the training data.
15. The system of claim 13, wherein the n-gram-based machine learning model comprises a gradient boosting based classifier trained based on n-gram features extracted from a training set.
16. The system of claim 13, wherein: the ensemble layer comprises a top logistic regression model; and generating the plurality of ensemble coefficients of the ensemble layer comprises solving the ensemble coefficients by inputting outputs of the trained neural network model and the trained n-gram-based machine learning model into the top logistic regression model.
17. The system of claim 13, wherein the operations further comprise: inputting a domain to the trained detection model; and outputting from the trained detection model a probability that the domain was generated by a domain generation algorithm.
18. The system of claim 17, wherein inputting the obtained domain to the detection model comprises inputting the obtained domain to each of the neural network model and the n-gram-based machine learning model to obtain the plurality of outputs of both the trained neural network model and the trained n-gram-based machine learning model.
19. A non-transitory computer-readable storage medium storing instructions that, when executed by a processor, cause the processor to perform operations comprising: training each of a neural network model and a n-gram-based machine learning model; generating a plurality of ensemble coefficients of an ensemble layer by inputting a plurality of outputs of both the trained neural network model and the trained n-gram-based machine learning model to the ensemble layer; and obtaining a trained detection model comprising the trained neural network model, the trained n-gram-based machine learning model, and the ensemble layer with the plurality of ensemble coefficients.
20. The non-transitory computer-readable storage medium of claim 19, wherein the operations further comprise: inputting a domain to the trained detection model; and outputting from the trained detection model a probability that the domain was generated by a domain generation algorithm.
</claims>
</document>
