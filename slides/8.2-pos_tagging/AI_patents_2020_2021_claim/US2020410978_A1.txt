<document>

<filing_date>
2019-06-25
</filing_date>

<publication_date>
2020-12-31
</publication_date>

<priority_date>
2019-06-25
</priority_date>

<ipc_classes>
G06N20/00,G10L13/033,G10L13/04,G10L13/047
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
CHILDRESS RHONDA L.
BENDER, MICHAEL
KEEN, MARTIN G.
TRIM, CRAIG M.
</inventors>

<docdb_family_id>
74043716
</docdb_family_id>

<title>
COGNITIVE MODIFICATION OF VERBAL COMMUNICATIONS FROM AN INTERACTIVE COMPUTING DEVICE
</title>

<abstract>
A method includes: determining, by a computer device, a current context associated with a user that is the target audience of an unprompted verbal output of an interactive computing device; determining, by the computer device, one or more parameters that are most effective in getting the attention of the user for the determined current context; and modifying, by the computer device, the unprompted verbal output of the interactive computing device using the determined one or more parameters.
</abstract>

<claims>
1. A method, comprising: creating, by a server, a corpus of data based on plural verbal interactions involving a user; analyzing, by the server, the corpus of data to determine output parameters that increase an attentiveness of the user in different contexts; determining, by the server, a verbal output of an interactive computing device to the user; determining, by the server, a current context; modifying, by the server, the verbal output based on the determined output parameters and the current context; and causing, by the server, the interactive computing device to output the modified verbal output to the user.
2. The method of claim 1, wherein the plural verbal interactions comprise verbal interactions between the user and the interactive computing device.
3. The method of claim 1, wherein the plural verbal interactions comprise verbal interactions between the user and another person.
4. The method of claim 1, wherein the corpus of data includes plural entries, wherein each entry includes: a context associated with a respective verbal output; a measure of effectiveness of the respective verbal output; and at least one parameter of the respective verbal output.
5. The method of claim 4, further comprising determining the context associated with the respective verbal output based on data from at least one selected from the group consisting of: a microphone that obtains audio data of an environment around the user; a video camera that obtains video data of the environment around the user; a biometric sensor that obtains biometric data of the user; a spatial sensor that obtains spatial data of the user; and a motion sensor or a proximity sensor that detects the presence of a person in a predefined area.
6. The method of claim 4, wherein context associated with the respective verbal output is selected from the group consisting of: an activity the user is performing at the time of the verbal interaction; a biometric state of the user at the time of the verbal interaction; and an amount of environmental noise at the time of the verbal interaction.
7. The method of claim 4, wherein the context associated with the respective verbal output comprises plural determined contexts.
8. The method of claim 4, wherein the measure of effectiveness of the respective verbal output comprises a combination of measures from plural detected reactions by the user.
9. The method of claim 4, wherein the at least one parameter of the respective verbal output comprises plural different parameters.
10. The method of claim 1, wherein the determining the current context comprises obtaining and analyzing data from at least one selected from the group consisting of: a microphone that obtains audio data of an environment around the user; a video camera that obtains video data of the environment around the user; a biometric sensor that obtains biometric data of the user; a spatial sensor that obtains spatial data of the user; and a motion sensor or a proximity sensor that detects the presence of a person in a predefined area.
11. The method of claim 1, wherein the modifying the verbal output comprises changing at least one selected from the group consisting of: name or names used to address the user in the verbal output; volume of the verbal output; cadence of the verbal output; specific words used in the verbal output; categories of words used in the verbal output; pronunciation of words used in the verbal output; and language used in the verbal output.
12. The method of claim 1, further comprising: observing a response of the user to the modified verbal output; and updating the corpus of data to include the current context, the modified verbal output, and the response of the user.
13. A method, comprising: creating, by an interactive computing device, a corpus of data based on plural verbal interactions involving a user; analyzing, by the interactive computing device, the corpus of data to determine output parameters that increase an attentiveness of the user in different contexts; determining, by the interactive computing device, a verbal output of the interactive computing device to the user; determining, by the interactive computing device, a current context; modifying, by the interactive computing device, the verbal output based on the determined output parameters and the current context; and outputting, by the interactive computing device, the modified verbal output to the user.
14. The method of claim 13, wherein: the current context is selected from the group consisting of: an activity the user is performing; a biometric state of the user; and an amount of environmental noise; and the modifying the verbal output comprises changing at least one selected from the group consisting of: name or names used to address the user in the verbal output; volume of the verbal output; cadence of the verbal output; specific words used in the verbal output; categories of words used in the verbal output; pronunciation of words used in the verbal output; and language used in the verbal output.
15. The method of claim 13, wherein the interactive computing device is a robot that performs at least one physical task.
16. A computer program product, the computer program product comprising a computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer device to cause the computer device to: analyze a corpus of data of a user, wherein the analyzing comprises using machine learning to determine parameters that are most effective in getting the user's attention in different contexts; determine a current context of the user based on the user being a target audience of a verbal output of an interactive computing device; modify the verbal output based on the current context and the determined parameters; and cause the interactive computing device to output the modified verbal output to the user.
17. The computer program product of claim 16, wherein: the current context is selected from the group consisting of: an activity the user is performing; a biometric state of the user; and an amount of environmental noise; and the modifying the verbal output comprises changing at least one selected from the group consisting of: name or names used to address the user in the verbal output; volume of the verbal output; cadence of the verbal output; specific words used in the verbal output; categories of words used in the verbal output; pronunciation of words used in the verbal output; and language used in the verbal output.
18. A system, comprising: a processor, a computer readable memory, and a computer readable storage medium; program instructions to determine a verbal output for an interactive computing device to present to a user; program instructions to determine a current context of the user; program instructions to modify the verbal output based on verbal output parameters determined to the most effective for the user for the current context; and program instructions to cause the interactive computing device to output the modified verbal output to the user, wherein the program instructions are stored on the computer readable storage medium for execution by the processor via the computer readable memory.
19. The system of claim 18, wherein the current context is selected from the group consisting of: an activity the user is performing; a biometric state of the user; and an amount of environmental noise.
20. The system of claim 18, wherein the modifying the verbal output comprises changing at least one selected from the group consisting of: name or names used to address the user in the verbal output; volume of the verbal output; cadence of the verbal output; specific words used in the verbal output; categories of words used in the verbal output; pronunciation of words used in the verbal output; and language used in the verbal output.
21. A method, comprising: determining, by a computer device, a current context associated with a user that is the target audience of an unprompted verbal output of an interactive computing device; determining, by the computer device, one or more parameters that are most effective in getting the attention of the user for the determined current context; and modifying, by the computer device, the unprompted verbal output of the interactive computing device using the determined one or more parameters.
22. The method of claim 21, wherein the computer device comprises a server that communicates with the interactive computing device via a network, and further comprising the server transmitting data defining the modified unprompted verbal output to the interactive computing device for output to the user by the interactive computing device.
23. The method of claim 21, wherein the interactive computing device is a robot that performs at least one physical task.
24. The method of claim 21, wherein the context comprises at least one selected from the group consisting of: an amount of environmental noise; at least one activity the user is engaged in; and a biometric state of the user.
25. The method of claim 21, wherein the parameter comprises at least one selected from the group consisting of: name or names used to address the user in the unprompted verbal output; volume of the unprompted verbal output; cadence of the unprompted verbal output; specific words used in the unprompted verbal output; categories of words used in the unprompted verbal output; pronunciation of words used in the unprompted verbal output; and language used in the unprompted verbal output.
</claims>
</document>
