<document>

<filing_date>
2019-02-19
</filing_date>

<publication_date>
2021-01-20
</publication_date>

<priority_date>
2018-03-13
</priority_date>

<ipc_classes>
G06K9/00
</ipc_classes>

<assignee>
TENCENT TECHNOLOGY (SHENZHEN) COMPANY
</assignee>

<inventors>
LUO, Wenhan
MA, Lin
LIU, Wei
ZHANG, Kaihao
</inventors>

<docdb_family_id>
63067656
</docdb_family_id>

<title>
IMAGE RECOGNITION METHOD AND APPARATUS, AND TERMINAL AND STORAGE MEDIUM
</title>

<abstract>
An image recognition method, comprising: obtaining a target video including a target object, extracting a target video frame image from the target video, according to object key point information and a plurality of video frames in the target video, generating a plurality of key point video frames, and combining the plurality of key point video frames into a key point video frame sequence; extracting dynamic timing feature information of the key point video frame sequence, and extracting static structure feature information of the target video frame image; according to the dynamic timing feature information of the key point video frame sequence and the static structure feature information of the target video frame image, recognizing the attribute type corresponding to the target object in the target video.
</abstract>

<claims>
1. A method for image recognition, performed by a terminal, the method comprising: obtaining a target video comprising a target object; extracting a target video frame image from the target video; generating a plurality of key point video frames according to key point information of the target object and a plurality of video frames in the target video, and combining the plurality of key point video frames into a key point video frame sequence; extracting dynamic timing feature information of the key point video frame sequence; extracting static structural feature information of the target video frame image by using a convolutional neural network model; and recognizing an attribute type corresponding to the target object in the target video based on the dynamic timing feature information of the key point video frame sequence and the static structural feature information of the target video frame image.
2. The method of claim 1, wherein the extracting dynamic timing feature information of the key point video frame sequence comprises: extracting key marker areas from each key point video frame in the key point video frame sequence, and respectively combining same key marker areas in all the key point video frames into unit key point video frame sequences; separately inputting the unit key point video frame sequences into a recurrent neural network model, to extract dynamic timing feature information of each unit key point video frame sequence; and connecting the dynamic timing feature information of the unit key point video frame sequences according to location relationships between the unit key point video frame sequences and the corresponding key marker areas, to obtain the dynamic timing feature information of the key point video frame sequence.
3. The method of claim 1, wherein the extracting static structural feature information of the target video frame image by using the convolutional neural network model comprises: inputting the target video frame image into an input layer of the convolutional neural network model; and extracting the static structural feature information of the target video frame image through convolution processing of a convolutional layer in the convolutional neural network model and pooling processing of a pooling layer in the convolutional neural network model.
4. The method of claim 1, wherein the recognizing the attribute type corresponding to the target object in the target video based on the dynamic timing feature information of the key point video frame sequence and the static structural feature information of the target video frame image comprises: recognizing, according to a classifier in a recurrent neural network model, matching degrees between the dynamic timing feature information of the key point video frame sequence and a plurality of attribute type features in the recurrent neural network model, and associating the matching degrees obtained through the dynamic timing feature information of the key point video frame sequence with label information corresponding to the plurality of attribute type features in the recurrent neural network model, to obtain a first label information set; recognizing, according to a classifier in the convolutional neural network model, matching degrees between the static structural feature information of the target video frame image and a plurality of attribute type features in the convolutional neural network model, and associating the matching degrees obtained through the static structural feature information of the target video frame image with label information corresponding to the plurality of attribute type features in the convolutional neural network model, to obtain a second label information set; and fusing the first label information set and the second label information set, to obtain the attribute type corresponding to the target object in the target video.
5. The method of claim 1, wherein the recognizing the attribute type corresponding to the target object in the target video based on the dynamic timing feature information of the key point video frame sequence and the static structural feature information of the target video frame image comprises: fusing the dynamic timing feature information of the key point video frame sequence and the static structural feature information of the target video frame image, to obtain fused feature information; recognizing, according to a classifier in a recurrent neural network model, matching degrees between the fused feature information and a plurality of attribute type features in the recurrent neural network model, and associating the matching degrees obtained through the recurrent neural network model with label information corresponding to the plurality of attribute type features in the recurrent neural network model, to obtain a first label information set; recognizing, according to a classifier in the convolutional neural network model, matching degrees between the fused feature information and a plurality of attribute type features in the convolutional neural network model, and associating the matching degrees obtained through the convolutional neural network model with label information corresponding to the plurality of attribute type features in the convolutional neural network model, to obtain a second label information set; and fusing the first label information set and the second label information set, to obtain the attribute type corresponding to the target object in the target video.
6. The method of claim 4 or 5, wherein the fusing the first label information set and the second label information set, to obtain the attribute type corresponding to the target object in the target video comprises: performing weighted averaging on matching degrees associated with the same label information in the first label information set and the second label information set, and associating the weighted averaged matching degrees with the label information, to obtain a target label information set; and extracting label information associated with a maximum matching degree from the target label information set, and determining the extracted label information as the attribute type corresponding to the target object in the target video.
7. The method of claim 3, further comprising: obtaining a first sample image and a second sample image; extracting static structural feature information of the first sample image, recognizing, according to a classifier in the convolutional neural network model, matching degrees between the static structural feature information of the first sample image and a plurality of attribute type features in the convolutional neural network model, and associating the matching degrees obtained through the static structural feature information of the first sample image with label information corresponding to the plurality of attribute type features in the convolutional neural network model, to obtain a third label information set; extracting static structural feature information of the second sample image, recognizing, according to the classifier in the convolutional neural network model, matching degrees between the static structural feature information of the second sample image and the plurality of attribute type features, and associating the matching degrees obtained through the static structural feature information of the second sample image with the label information corresponding to the plurality of attribute type features in the convolutional neural network model, to obtain a fourth label information set; and determining a model loss value based on the static structural feature information of the first sample image and the third label information set, as well as the static structural feature information of the second sample image and the fourth label information set, and adjusting a weight of a parameter in the convolutional neural network model based on the model loss value.
8. The method of claim 7, wherein the model loss value comprises recognition loss values and a verification loss value; and
the determining model loss values based on the static structural feature information of the first sample image and the third label information set, as well as the static structural feature information of the second sample image and the fourth label information set comprises: generating a recognition loss value of the first sample image according to the third label information set and a sample attribute type corresponding to the first sample image; generating a recognition loss value of the second sample image according to the fourth label information set and a sample attribute type corresponding to the second sample image; generating the verification loss value according to the static structural feature information of the first sample image, the sample attribute type corresponding to the first sample image, the static structural feature information of the second sample image, and the sample attribute type corresponding to the second sample image; and generating the model loss value based on the recognition loss value of the first sample image, the recognition loss value of the second sample image, and the verification loss value.
9. An apparatus for image recognition, comprising: a first obtaining module, configured to: obtain a target video comprising a target object; extract a target video frame image from the target video; and generate a plurality of key point video frames according to key point information of the target object and a plurality of video frames in the target video, and combine the plurality of key point video frames into a key point video frame sequence; a first extraction module, configured to extract dynamic timing feature information of the key point video frame sequence; a second extraction module, configured to extract static structural feature information of the target video frame image by using a convolutional neural network model; and a first recognition module, configured to recognize an attribute type corresponding to the target object in the target video based on the dynamic timing feature information of the key point video frame sequence and the static structural feature information of the target video frame image.
10. The apparatus of claim 9, wherein the first extraction module comprises: a combination unit, configured to extract key marker areas from each key point video frame in the key point video frame sequence, and respectively combine same key marker areas in all the key point video frames into unit key point video frame sequences; a first extraction unit, configured to separately input the unit key point video frame sequences into a recurrent neural network model, to extract dynamic timing feature information of each unit key point video frame sequence; and a connection unit, configured to connect the dynamic timing feature information of the unit key point video frame sequences according to location relationships between the unit key point video frame sequences and the corresponding key marker areas, to obtain the dynamic timing feature information of the key point video frame sequence.
11. The apparatus of claim 9, wherein the second extraction module comprises: an input unit, configured to input the target video frame image into an input layer of the convolutional neural network model; and a second extraction unit, configured to extract the static structural feature information of the target video frame image through convolution processing of a convolutional layer in the convolutional neural network model and pooling processing of a pooling layer in the convolutional neural network model.
12. The apparatus of claim 9, wherein the first recognition module comprises: a first recognition unit, configured to recognize, according to a classifier in a recurrent neural network model, matching degrees between the dynamic timing feature information of the key point video frame sequence and a plurality of attribute type features in the recurrent neural network model, and associate the matching degrees obtained through the dynamic timing feature information of the key point video frame sequence with label information corresponding to the plurality of attribute type features in the recurrent neural network model, to obtain a first label information set; a second recognition unit, configured to recognize, according to a classifier in the convolutional neural network model, matching degrees between the static structural feature information of the target video frame image and a plurality of attribute type features in the convolutional neural network model, and associate the matching degrees obtained through the static structural feature information of the target video frame image with label information corresponding to the plurality of attribute type features in the convolutional neural network model, to obtain a second label information set; and a first fusion unit, configured to fuse the first label information set and the second label information set, to obtain the attribute type corresponding to the target object in the target video.
13. The apparatus of claim 9, wherein the first recognition module comprises: a second fusion unit, configured to fuse the dynamic timing feature information of the key point video frame sequence and the static structural feature information of the target video frame image, to obtain fused feature information; a third recognition unit, configured to recognize, according to a classifier in a recurrent neural network model, matching degrees between the fused feature information and a plurality of attribute type features in the recurrent neural network model, and associate the matching degrees obtained through the recurrent neural network model with label information corresponding to the plurality of attribute type features in the recurrent neural network model, to obtain a first label information set; a fourth recognition unit, configured to recognize, according to a classifier in the convolutional neural network model, matching degrees between the fused feature information and a plurality of attribute type features in the convolutional neural network model, and associate the matching degrees obtained through the convolutional neural network model with label information corresponding to the plurality of attribute type features in the convolutional neural network model, to obtain a second label information set; and a first fusion unit, configured to fuse the first label information set and the second label information set, to obtain the attribute type corresponding to the target object in the target video.
14. The apparatus of claim 12 or 13, wherein the first fusion unit comprises: a calculation subunit, configured to perform weighted averaging on matching degrees associated with the same label information in the first label information set and the second label information set, and associate the weighted averaged matching degrees with the label information, to obtain a target label information set; and a determining subunit, configured to extract label information associated with a maximum matching degree from the target label information set, and determine the extracted label information as the attribute type corresponding to the target object in the target video.
15. The apparatus of claim 11, further comprising: a second obtaining module, configured to obtain a first sample image and a second sample image; a second recognition module, configured to extract static structural feature information of the first sample image, recognize, according to a classifier in the convolutional neural network model, matching degrees between the static structural feature information of the first sample image and a plurality of attribute type features in the convolutional neural network model, and associate the matching degrees obtained through the static structural feature information of the first sample image with label information corresponding to the plurality of attribute type features in the convolutional neural network model, to obtain a third label information set; a third recognition module, configured to extract static structural feature information of the second sample image, recognize, according to the classifier in the convolutional neural network model, matching degrees between the static structural feature information of the second sample image and the plurality of attribute type features, and associate the matching degrees obtained through the static structural feature information of the second sample image with the label information corresponding to the plurality of attribute type features in the convolutional neural network model, to obtain a fourth label information set; and a determining module, configured to determine a model loss value based on the static structural feature information of the first sample image and the third label information set, as well as the static structural feature information of the second sample image and the fourth label information set, and adjust a weight of a parameter in the convolutional neural network model according to the model loss value.
16. The apparatus of claim 15, wherein the model loss value comprises recognition loss values and a verification loss value; and
the determining module comprises: a first generation unit, configured to generate a recognition loss value of the first sample image according to the third label information set and a sample attribute type corresponding to the first sample image; a second generation unit, configured to generate a recognition loss value of the second sample image according to the fourth label information set and a sample attribute type corresponding to the second sample image; a third generation unit, configured to generate the verification loss value according to the static structural feature information of the first sample image, the sample attribute type corresponding to the first sample image, the static structural feature information of the second sample image, and the sample attribute type corresponding to the second sample image; and a fourth generation unit, configured to generate the model loss value based on the recognition loss value of the first sample image, the recognition loss value of the second sample image, and the verification loss value.
17. A terminal, comprising: a memory; and one or more processors, wherein the memory stores computer-readable instructions that, when executed by the one or more processors, cause the one or more processors to perform the following operations: obtaining a target video comprising a target object; extracting a target video frame image from the target video; generating a plurality of key point video frames according to key point information of the target object and a plurality of video frames in the target video, and combining the plurality of key point video frames into a key point video frame sequence; extracting dynamic timing feature information of the key point video frame sequence; extracting static structural feature information of the target video frame image by using a convolutional neural network model; and recognizing an attribute type corresponding to the target object in the target video based on the dynamic timing feature information of the key point video frame sequence and the static structural feature information of the target video frame image.
18. The terminal of claim 17, wherein the computer-readable instructions, when executed by the one or more processors to perform the operation of extracting dynamic timing feature information of the key point video frame sequence, cause the one or more processors to specifically perform the following operations: extracting key marker areas from each key point video frame in the key point video frame sequence, and respectively combining same key marker areas in all the key point video frames into unit key point video frame sequences; separately inputting the unit key point video frame sequences into a recurrent neural network model, to extract dynamic timing feature information of each unit key point video frame sequence; and connecting the dynamic timing feature information of the unit key point video frame sequences according to location relationships between the unit key point video frame sequences and the corresponding key marker areas, to obtain the dynamic timing feature information of the key point video frame sequence.
19. A non-transitory computer-readable storage medium having stored therein computer-readable instructions that, when executed by one or more processors, cause the one or more processors to perform the following operations: obtaining a target video comprising a target object; extracting a target video frame image from the target video; generating a plurality of key point video frames according to key point information of the target object and a plurality of video frames in the target video, and combining the plurality of key point video frames into a key point video frame sequence; extracting dynamic timing feature information of the key point video frame sequence; extracting static structural feature information of the target video frame image by using a convolutional neural network model; and recognizing an attribute type corresponding to the target object in the target video based on the dynamic timing feature information of the key point video frame sequence and the static structural feature information of the target video frame image.
20. The computer-readable storage medium of claim 19, wherein the computer-readable instructions, when executed by the one or more processors to perform the operation of extracting dynamic timing feature information of the key point video frame sequence, cause the one or more processors to specifically perform the following operations: extracting key marker areas from each key point video frame in the key point video frame sequence, and respectively combining same key marker areas in all the key point video frames into unit key point video frame sequences; separately inputting the unit key point video frame sequences into a recurrent neural network model, to extract dynamic timing feature information of each unit key point video frame sequence; and connecting the dynamic timing feature information of the unit key point video frame sequences according to location relationships between the unit key point video frame sequences and the corresponding key marker areas, to obtain the dynamic timing feature information of the key point video frame sequence.
</claims>
</document>
