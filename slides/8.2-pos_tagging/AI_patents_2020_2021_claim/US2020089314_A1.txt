<document>

<filing_date>
2019-11-20
</filing_date>

<publication_date>
2020-03-19
</publication_date>

<priority_date>
2015-10-06
</priority_date>

<ipc_classes>
G06F3/01,G06F3/0481
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
AMIHOOD, PATRICK M.
POUPYREV, IVAN
</inventors>

<docdb_family_id>
58446760
</docdb_family_id>

<title>
Fine-Motion Virtual-Reality or Augmented-Reality Control Using Radar
</title>

<abstract>
This document describes techniques for fine-motion virtual-reality or augmented-reality control using radar. These techniques enable small motions and displacements to be tracked, even in the millimeter or sub-millimeter scale, for user control actions even when those actions are small, fast, or obscured due to darkness or varying light. Further, these techniques enable fine resolution and real-time control, unlike conventional RF-tracking or optical-tracking techniques.
</abstract>

<claims>
We claim:
1. A computer-implemented method for fine-motion virtual-reality (VR) or augmented-reality (AR) control, the computer-implemented method comprising: presenting a VR/AR object within a VR world or an AR viewport; tracking a user interaction with the VR/AR object, the user interaction including a fine-motion in a millimeter or sub-millimeter range, the tracking comprising: receiving a radar signal representing a superposition of reflections of two or more spatially separated points over time within a radar field provided by a radar system; calculating a first radial distance for a first point of the points at a first time based on the received radar signal; calculating a first radial velocity for the first point at the first time using an observed Doppler frequency for the first point at the first time; calculating a second radial distance for the first point at a second time based on the received radar signal; calculating a second radial velocity for the first point at the second time using an observed Doppler frequency for the first point at the second time; determining a movement of the first point between the first and second times based on the first and second radial distances and first and second radial velocities; and establishing the user interaction based on the determined movement of the first point; altering the VR/AR object in real time corresponding to the user interaction; and passing a control input corresponding to the user interaction effective to control the VR world, the AR viewport, or an application or device external to the VR world or the AR viewport.
2. The computer-implemented method of claim 1, wherein: the VR/AR object comprises a number pad or virtual keyboard; and the user interaction is a button press on the number pad or the virtual keyboard.
3. The computer-implemented method of claim 1, wherein the method is performed by smart glasses or VR goggles.
4. The computer-implemented method of claim 1, wherein: the VR/AR object is a button, knob, or slider; and the user interaction is a press of the button, a turn of the knob, or a movement of the slider.
5. The computer-implemented method of claim 1, wherein: the VR/AR object is a steering wheel; and the user interaction is a turn of the steering wheel.
6. The computer-implemented method of claim 1, wherein the user interaction is a selection corresponding to the VR/AR object.
7. The computer-implemented method of claim 1, wherein the VR/AR object is superimposed on a real world object such that the user interaction corresponds to the real world object.
8. The computer-implemented method of claim 1, wherein the radar signal is received through fabric.
9. The computer-implemented method of claim 8, wherein the method is performed by a computing device that is disposed in a pocket or bag of a user performing the user interaction.
10. The computer-implemented method of claim 1, wherein a portion of a user that performs the user interaction is viewable by the user through the AR viewport.
11. The computer-implemented method of claim 10, wherein the portion of the user is one or more hands of the user.
12. The computer-implemented method of claim 1, further comprising displaying a visual indication of the user interaction with the VR/AR object in real time corresponding to the user interaction.
13. The computer-implemented method of claim 12, wherein the displaying the visual indication comprises altering an appearance of the VR/AR object.
14. The computer-implemented method of claim 13, wherein the altering the appearance of the VR/AR object comprises changing a color of a portion of the VR/AR object.
15. The computer-implemented method of claim 1, wherein: the tracking further comprises: calculating a first radial distance for a second point of the points at the first time based on the received radar signal; calculating a first radial velocity for the second point at the first time using an observed Doppler frequency for the second point at the first time; calculating a second radial distance for the second point at the second time based on the received radar signal; calculating a second radial velocity for the second point at the second time using an observed Doppler frequency for the second point at the second time; and determining a movement of the second point between the first and second times based on the first and second radial distances and first and second radial velocities for the second point; and the establishing the user interaction is further based on the determined movement of the second point.
16. The computer-implemented method of claim 15, wherein the first point is on a hand of a user that performs the user interaction and the second point is not on a portion of the user.
17. The computer-implemented method of claim 15, wherein the first and second points are on a hand of a user that performs the user interaction.
18. The computer-implemented method of claim 17, wherein the first point is on a first finger of the hand of the user and the second point is on a second finger of the hand of the user.
19. The computer-implemented method of claim 18, wherein the user interaction comprises the first finger crossing the second finger.
20. The computer-implemented method of claim 15, wherein the first point is visually obscured from a computing device that performs the method.
21. The computer-implemented method of claim 19, wherein the first point is visually obscured by the second point.
22. A computer-implemented method for fine-motion virtual-reality (VR) or augmented-reality (AR) control, the computer-implemented method comprising: presenting a VR/AR object within a VR world or an AR viewport; tracking a user interaction with the VR/AR object, the user interaction including a fine-motion in a millimeter or sub-millimeter range, the tracking comprising: receiving a radar signal representing a superposition of reflections of two or more spatially separated points over time within a radar field provided by a radar system; distinguishing the spatially separated points by determining respective Doppler centroids for the points; determining a movement of a first point of the points between first and second times based on spatially resolving the first point at the first and second times; and establishing the user interaction based on the determined movement of the first point; altering the VR/AR object in real time corresponding to the user interaction; and passing a control input corresponding to the user interaction effective to control the VR world, the AR viewport, or an application or device external to the VR world or the AR viewport.
23. The computer-implemented method of claim 22, wherein: the VR/AR object comprises a number pad or virtual keyboard; and the user interaction is a button press on the number pad or the virtual keyboard.
24. The computer-implemented method of claim 22, wherein the method is performed by smart glasses or VR goggles.
25. The computer-implemented method of claim 22, wherein: the VR/AR object is a button, knob, or slider; and the user interaction is a press of the button, a turn of the knob, or a movement of the slider.
26. The computer-implemented method of claim 22, wherein: the VR/AR object is a steering wheel; and the user interaction is a turn of the steering wheel.
27. The computer-implemented method of claim 22, wherein the user interaction is a selection corresponding to the VR/AR object.
28. The computer-implemented method of claim 22, wherein the VR/AR object is superimposed on a real world object such that the user interaction corresponds to the real world object.
29. The computer-implemented method of claim 22, wherein the radar signal is received through fabric.
30. The computer-implemented method of claim 29, wherein the method is performed by a computing device that is disposed in a pocket or bag of a user performing the user interaction.
31. The computer-implemented method of claim 22, wherein a portion of a user that performs the user interaction is viewable by the user through the AR viewport.
32. The computer-implemented method of claim 31, wherein the portion of the user is one or more hands of the user.
33. The computer-implemented method of claim 22, further comprising displaying a visual indication of the user interaction with the VR/AR object in real time corresponding to the user interaction.
34. The computer-implemented method of claim 33, wherein the displaying the visual indication comprises altering an appearance of the VR/AR object.
35. The computer-implemented method of claim 34, wherein the altering the appearance of the VR/AR object comprises changing a color of a portion of the VR/AR object.
36. The computer-implemented method of claim 22, wherein: the tracking further comprises determining a movement of a second point of the points between the first and second times based on spatially resolving the second point at the first and second times; and the establishing the user interaction is further based on the determined movement of the second point.
37. The computer-implemented method of claim 36, wherein the first point is on a hand of a user that performs the user interaction and the second point is not on a portion of the user.
38. The computer-implemented method of claim 36, wherein the first and second points are on a hand of a user that performs the user interaction.
39. The computer-implemented method of claim 38, wherein the first point is on a first finger of the hand of the user and the second point is on a second finger of the hand of the user.
40. The computer-implemented method of claim 39, wherein the user interaction comprises the first finger crossing the second finger.
41. The computer-implemented method of claim 36, wherein the first point is visually obscured from a computing device that performs the method.
42. The computer-implemented method of claim 41, wherein the first point is visually obscured by the second point.
</claims>
</document>
