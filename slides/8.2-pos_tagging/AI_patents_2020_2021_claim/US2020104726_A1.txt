<document>

<filing_date>
2019-09-30
</filing_date>

<publication_date>
2020-04-02
</publication_date>

<priority_date>
2018-09-29
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,G06N5/02
</ipc_classes>

<assignee>
ALVELDA, VII, PHILIP
</assignee>

<inventors>
ALVELDA, VII, PHILIP
</inventors>

<docdb_family_id>
69946902
</docdb_family_id>

<title>
MACHINE LEARNING DATA REPRESENTATIONS, ARCHITECTURES, AND SYSTEMS THAT INTRINSICALLY ENCODE AND REPRESENT BENEFIT, HARM, AND EMOTION TO OPTIMIZE LEARNING
</title>

<abstract>
A computer-implemented method, architecture and machine readable medium. The method includes receiving raw data and training data at an input of a neural network-based computing system (NNBCS) on a plurality of semantic concepts; and implementing a learning algorithm including: processing the raw data to generate processed output data; causing the processed output data to be stored in a data structure that corresponds to a continuous, differentiable vector space within a memory representing a Distributed Knowledge Graph (DKG) that reflects dimensions for the plurality of semantic concepts; comparing the processed output data with an output expected based on the training data to determine an error; and causing a weighted propagation of the error within the DKG as a function of one or more weights dependent on respective ones of one or more of the dimensions of the DKG corresponding to the error to generate an updated data structure of the DKG.
</abstract>

<claims>
1. A product comprising one or more tangible computer-readable non-transitory storage media comprising computer-executable instructions operable to, when executed by at least one computer processor, enable the at least one computer processor to perform operations including: receiving raw data and training data at an input of a neural network-based computing system (NNBCS) on a plurality of semantic concepts; and implementing a learning algorithm including a set of parameterizations, each of parameterization of the set including: processing the raw data to generate processed output data therefrom; causing the processed output data to be stored in a data structure that corresponds to a continuous, differentiable vector space within a memory, the continuous, differentiable vector space representing a Distributed Knowledge Graph (DKG) that reflects dimensions for the plurality of semantic concepts; comparing the processed output data with an output expected based on the training data to determine an error associated with the processed output data; and causing a weighted propagation of the error within the DKG as a function of one or more weights dependent on respective ones of one or more of the dimensions of the DKG corresponding to the error to generate an updated data structure of the DKG.
2. The product of claim 1, wherein the operations include, in response to a determination that error rates from a processing of raw data are above respective predetermined thresholds, performing a subsequent parameterization of the set, and otherwise generating a training model corresponding to the data structure from a last one of the set of parameterizations, the training model to be used by the NNBCS to process further raw data.
3. The product of claim 1, wherein the one or more weights pertain to information regarding one of harm or benefit associated with the respective ones of one or more of the dimensions.
4. The product of claim 1, wherein causing the weighted propagation includes determining one or more subspaces of the DKG for the weighted propagation based on the one or more weights, and causing the weighted propagation only in the one or more subspaces.
5. The product of claim 1, wherein causing the weighted propagation includes applying partial derivatives weighted by the one or more weights expressed as multidimensional vectors, and using at least one of a gradient ascent algorithm or a gradient descent algorithm based on the partial derivatives.
6. The product of claim 1, wherein the one or more weights are hard-coded within the NNBCS such that the one or more weights are fixed for the respective ones of one or more of the dimensions.
7. The product of claim 1, wherein the one or more weights are variable and subject to the learning algorithm, such that the raw data and the training data include data on the one or more weights, and such that the updated data structure includes updated data on the one or more weights.
8. The product of claim 1, wherein the learning algorithm is a first learning algorithm, and the NNBCS is a first NNBCS, the plurality of semantic concepts are a first plurality of semantic concepts, and the weighted propagation is a first weighted propagation, the method further including applying a second learning algorithm using a second NNBCS coupled to the DKG, the second NNBCS including a plurality of interconnected processing elements, the operations including: receiving raw data and training data at an input of the second NNBCS on a plurality of semantic concepts; using the plurality of processing elements of the second NNBCS to implement a second learning algorithm including a set of parameterizations, each of parameterization of the set of the second learning algorithm including: processing the raw data at the second NNBCS to generate processed output data therefrom; causing the processed output data from the second NNBCS to be stored in the DKG; comparing the processed output data from the NNBCS with an output expected based on the training data received at the second NNBCS to determine an error associated with the processed output data from the second NNBCS; and causing a second weighted propagation, within the DKG, of the error associated with the processed output data from the second NNBCS as a function of one or more weights dependent on respective ones of one or more of the dimensions of the DKG corresponding to the error associated with the processed output data from the second NNBCS to generate the updated data structure of the DKG.
9. The product of claim 8, wherein causing the first weighted propagation and causing the second weighted propagation occur simultaneously.
10. The product of claim 1, wherein: the DKG is defined by a plurality of nodes each representing a respective one of the plurality of semantic concepts; each of the nodes is represented by a characteristic distributed pattern of activity levels for respective meta-semantic nodes (MSNs), the MSNs for said each of the nodes defining a standard basis vector to designate a semantic concept, wherein standard basis vectors for respective ones of the nodes together define the continuous vector space; each MSN corresponds to an intersection of a plurality of the dimensions; and each activity level in the pattern of activity levels designates a value for a dimension of the plurality of dimensions.
11. The product of claim 1, wherein the operations further include implementing the weight propagation and storing the updated data structure within a memory coupled to the NNBCS.
12. A neural network-based computing system (NNBCS) including a plurality of interconnected processing elements and an input/output interface coupled to the processing elements, the processing elements to: receive raw data and training data at the input/output interface on a plurality of semantic concepts; implement a learning algorithm including a set of parameterizations, each of parameterization of the set including: processing the raw data to generate processed output data therefrom; causing the processed output data to be stored in a data structure that corresponds to a continuous, differentiable vector space within a memory, the continuous, differentiable vector space representing a Distributed Knowledge Graph (DKG) that reflects dimensions for the plurality of semantic concepts; comparing the processed output data with an output expected based on the training data to determine an error associated with the processed output data; and causing a weighted propagation of the error within the DKG as a function of one or more weights dependent on respective ones of one or more of the dimensions of the DKG corresponding to the error to generate an updated data structure of the DKG.
13. The neural network-based computing system of claim 12, wherein the processing elements are to, in response to a determination that error rates from a processing of raw data are above respective predetermined thresholds, perform a subsequent parameterization of the set, and otherwise generate a training model corresponding to the data structure from a last one of the set of parameterizations, the training model to be used by the processing elements to process further raw data.
14. The neural network-based computing system of claim 12, wherein the one or more weights pertain to information regarding one of harm or benefit associated with the respective ones of one or more of the dimensions.
15. The neural network-based computing system of claim 12, wherein the processing elements are to cause the weighted propagation by determining one or more subspaces of the DKG for the weighted propagation based on the one or more weights, and to cause the weighted propagation only in the one or more subspaces.
16. The neural network-based computing system of claim 12, wherein the processing elements are to cause the weighted propagation by applying partial derivatives weighted by the one or more weights expressed as multidimensional vectors, and by using at least one of a gradient ascent algorithm or a gradient descent algorithm based on the partial derivatives.
17. The neural network-based computing system of claim 12, wherein the one or more weights are hard-coded within the NNBCS such that the one or more weights are fixed for the respective ones of one or more of the dimensions.
18. The neural network-based computing system of claim 12, wherein the one or more weights are variable and subject to the learning algorithm, such that the raw data and the training data include data on the one or more weights, and such that the updated data structure includes updated data on the one or more weights.
19. The neural network-based computing system of claim 12, wherein the learning algorithm is a first learning algorithm, and the NNBCS is a first NNBCS, the plurality of semantic concepts are a first plurality of semantic concepts, and the weighted propagation is a first weighted propagation, the processing elements to further apply a second learning algorithm using a second NNBCS coupled to the DKG, the second NNBCS including a plurality of second interconnected processing elements, the processing elements to: receive raw data and training data at an input of the second NNBCS on a plurality of semantic concepts; use the plurality of second processing elements of the second NNBCS to implement the second learning algorithm including a set of parameterizations, each of parameterization of the set of the second learning algorithm including: processing the raw data at the second NNBCS to generate processed output data therefrom; causing the processed output data from the second NNBCS to be stored in the DKG; comparing the processed output data from the NNBCS with an output expected based on the training data received at the second NNBCS to determine an error associated with the processed output data from the second NNBCS; and causing a second weighted propagation, within the DKG, of the error associated with the processed output data from the second NNBCS as a function of one or more weights dependent on respective ones of one or more of the dimensions of the DKG corresponding to the error associated with the processed output data from the second NNBCS to generate the updated data structure of the DKG.
20. The neural network-based computing system of claim 19, wherein the processing elements are to cause the first weighted propagation and cause the second weighted propagation simultaneously.
21. The neural network-based computing system of claim 12, wherein: the DKG is defined by a plurality of nodes each representing a respective one of the plurality of semantic concepts; each of the nodes is represented by a characteristic distributed pattern of activity levels for respective meta-semantic nodes (MSNs), the MSNs for said each of the nodes defining a standard basis vector to designate a semantic concept, wherein standard basis vectors for respective ones of the nodes together define the continuous vector space; each MSN corresponds to an intersection of a plurality of the dimensions; and each activity level in the pattern of activity levels designates a value for a dimension of the plurality of dimensions.
22. A device including: means for receiving raw data and training data at an input of a neural network-based computing system (NNBCS) on a plurality of semantic concepts; means for implementing a learning algorithm including a set of parameterizations, each of parameterization of the set including: processing the raw data to generate processed output data therefrom; causing the processed output data to be stored in a data structure that corresponds to a continuous, differentiable vector space within a memory, the continuous, differentiable vector space representing a Distributed Knowledge Graph (DKG) that reflects dimensions for the plurality of semantic concepts; comparing the processed output data with an output expected based on the training data to determine an error associated with the processed output data; and causing a weighted propagation of the error within the DKG as a function of one or more weights dependent on respective ones of one or more of the dimensions of the DKG corresponding to the error to generate an updated data structure of the DKG.
23. The device of claim 22, further including, means for, in response to a determination that error rates from a processing of raw data are above respective predetermined thresholds, performing a subsequent parameterization of the set, and means for otherwise generating a training model corresponding to the data structure from a last one of the set of parameterizations, the training model to be used by the NNBCS to process further raw data.
24. The device of claim 22, wherein the one or more weights pertain to information regarding one of harm or benefit associated with the respective ones of one or more of the dimensions.
25. The device of claim 22, wherein causing the weighted propagation includes determining one or more subspaces of the DKG for the weighted propagation based on the one or more weights, and causing the weighted propagation only in the one or more subspaces.
</claims>
</document>
