<document>

<filing_date>
2020-05-08
</filing_date>

<publication_date>
2020-11-12
</publication_date>

<priority_date>
2019-05-10
</priority_date>

<ipc_classes>
G06T15/06,G06T19/20,G06T7/00,G06T7/62,G06T7/73,G06T7/80
</ipc_classes>

<assignee>
SMART PICTURE TECHNOLOGIES
</assignee>

<inventors>
GREFF, ANDREW KEVIN
JOVANOVIC, DEJAN
</inventors>

<docdb_family_id>
73045830
</docdb_family_id>

<title>
METHODS AND SYSTEMS FOR MEASURING AND MODELING SPACES USING MARKERLESS PHOTO-BASED AUGMENTED REALITY PROCESS
</title>

<abstract>
Described herein are platforms, systems, media, and methods for measuring a space by launching an active augmented reality (AR) session on a device comprising a camera and at least one processor; calibrating the AR session by establishing a fixed coordinate system, receiving a position and orientation of one or more horizontal or vertical planes in the space in reference to the fixed coordinate system, and receiving a position and orientation of the camera in reference to the fixed coordinate system; constructing a backing model; providing an interface allowing a user to capture at least one photo of the space during the active AR session; extracting camera data from the AR session for the at least one photo; extracting the backing model from the AR session; and storing the camera data and the backing model in association with the at least one photo.
</abstract>

<claims>
1. A system comprising a first processing device comprising a camera and at least one processor configured to perform at least the following: a) launch an active augmented reality (AR) session; b) calibrate the AR session by: i) establishing a fixed coordinate system, ii) receiving a position and orientation of one or more horizontal or vertical planes in a space in reference to the fixed coordinate system, and iii) receiving a position and orientation of the camera in reference to the fixed coordinate system; c) construct a backing model comprising the fixed coordinate system, the position and orientation of the camera, and the position and orientation of the one or more horizontal or vertical planes; d) present an interface allowing a user to capture at least one photo of the space during the active AR session; e) extract camera data from the AR session for the at least one photo; f) extract the backing model from the AR session; and g) store the camera data and the backing model in association with the at least one photo.
2. The system of claim 1, wherein the first processing device is further configured to: a) provide a user interface allowing the user to perform at least: i) viewing the at least one photo; and ii) identifying screen coordinates on the at least one photo to measure a feature of the space; b) access the camera data and the backing model for the at least one photo; c) build a conversion pipeline, using the camera data, to convert the screen coordinates to world coordinates using ray-casting, the conversion pipeline performing at least: i) using the screen coordinates to project a camera ray in world coordinates; ii) evaluate the ray for intersections with objects in the backing model; and iii) return any intersections as the world coordinates corresponding to the screen coordinates; d) convert the identified world coordinates to one or more lengths, one or more areas, or one or more volumes in the space; e) annotate the at least one photo with the one or more lengths, one or more areas, or one or more volumes; and f) store the measurements and annotations in association with the at least one photo.
3. The system of claim 2, wherein the user identifies screen coordinates by tapping on a touchscreen, tapping and dragging on a touch screen, clicking with a pointing device, or clicking and dragging with a pointing device.
4. The system of claim 2, wherein the measurements and annotations are stored in association with the at least one photo as metadata associated with the at least one photo.
5. The system of claim 2, wherein the measurements and annotations are stored in association with the at least one photo by linking the measurements and the annotations to that at least one photo via a stored token or key.
6. The system of claim 1, wherein the first processing device is further configured to: a) utilize one or more computer vision algorithms to detect one or more 3D geometries in the space, the one or more 3D geometries selected from: floor corners, walls, windows, doors, and other 3D geometries; and b) automatically add detected 3D geometries to the backing model.
7. The system of claim 1, wherein the first processing device is further configured to allow the user to make corrections to the backing model based on measurements taken in the at least one photo.
8. The system of claim 1, wherein the first processing device is further configured to transmit the stored camera data, the stored backing model, and the at least one photo.
9. The system of claim 1, further comprising a second processing device comprising at least one processor configured to perform at least the following: a) present a user interface allowing the user to perform at least: i) viewing the at least one photo; and ii) identifying screen coordinates on the at least one photo to measure a feature of the space; b) access the camera data and the backing model for the at least one photo; c) build a conversion pipeline, using the camera data, to convert the screen coordinates to world coordinates using ray-casting, the conversion pipeline performing at least: i) using the screen coordinates to project a camera ray in world coordinates; ii) evaluate the ray for intersections with objects in backing model; and iii) returning any intersections as the world coordinates corresponding to the screen coordinates; d) convert the identified world coordinates to one or more lengths, one or more areas, or one or more volumes in the space; e) annotate the at least one photo with the one or more lengths, one or more areas, or one or more volumes; and f) store the measurements and annotations in association with the at least one photo.
10. The system of claim 9, wherein the user interface is implemented in a web browser or a mobile application.
11. The system of claim 10, wherein the user identifies screen coordinates by tapping on a touchscreen, tapping and dragging on a touch screen, clicking with a pointing device, or clicking and dragging with a pointing device.
12. The system of claim 9, wherein the measurements and annotations are stored in association with the at least one photo as metadata associated with the at least one photo.
13. The system of claim 9, wherein the measurements and annotations are stored in association with the at least one photo by linking the measurements and the annotations to that at least one photo via a stored token or key.
14. The system of claim 9, wherein the second processing device is further configured to: a) utilize one or more computer vision algorithms to detect one or more 3D geometries in the space, the one or more 3D geometries selected from: floor corners, walls, windows, doors, and other 3D geometries; and b) automatically add detected 3D geometries to the backing model.
15. The system of claim 9, wherein the second processing device is further configured to allow the user to make corrections to the backing model based on measurements taken in the at least one photo.
16. The system of claim 1, wherein the camera data comprises: projection matrix, view matrix, view port, camera position, view angle, scale factor, or a combination thereof.
17. The system of claim 1, wherein the first processing device is further configured to allow the user to add one or more objects to the backing model by performing at least the following: a) provide an AR interface allowing the user to indicate the positions of corners of a floor of the space in reference to the fixed coordinate system, wherein the application is configured to project a reference point on the screen into a ray in world coordinates and determine an intersection point with the one or more horizontal or vertical planes plane via hit-testing thus detecting the corners of the floor of the space; b) assemble the detected corners into a floorplan of the space; c) generate virtual quasi-infinite vertical planes extending from each corner of the detected corners representing virtual walls of the space; d) provide an AR interface allowing the user to indicate the positions of intersection points between the ceiling and the virtual walls; e) truncate the virtual walls to reflect the ceiling height in the space; and f) optionally, provide an AR interface allowing the user to indicate the positions of corners openings in the virtual walls.
18. The system of claim 1, wherein the first processing device is further configured to convert the at least one photo to a transmittable format.
19. The system of claim 1, wherein the camera data and the backing model are stored in a structured or semi-structured data format.
20. The system of claim 1, wherein the camera data and the backing model are stored in association with the at least one photo as metadata associated with the at least one photo.
21. The system of claim 1, wherein the camera data and the backing model are stored in association with the at least one photo by linking the camera data and the backing model to that at least one photo via a stored token or key.
22. The system of claim 1, wherein the capture of the at least one photo of the space during the active AR session is triggered by a local user present in the space and with the first processing device.
23. The system of claim 1, wherein the capture of the at least one photo of the space during the active AR session is triggered by a remote user not present in the space.
24. The system of claim 1, further comprising a second processing device comprising at least one processor configured to provide an application allowing a user to edit the position or orientation of the one or more horizontal or vertical planes in the space in reference to the fixed coordinate system.
25. The system of claim 2, further comprising a second processing device comprising at least one processor configured to provide an application allowing a user to edit the screen coordinates identified on the at least one photo.
26. The system of claim 1, further comprising one or more computer vision algorithms configured to perform one or more of the following: a) identify or quantify one or more colors in space; b) identify or quantify one or more materials in the space; and c) identify or quantify one or more objects in the space.
27. The system of claim 1, further comprising an application allowing a user to rectify a floorplan by enforcing angles of all segments of the floorplan to fall into a predetermined set of angles selected from the group consisting of: 0 degrees, 45 degrees, 90 degrees, 135 degrees, and 180 degrees.
28. The system of claim 1, further comprising an application allowing a user to adjust a scale of a floorplan and 3D model by adjusting a virtual floor-plane height incrementally such that modeled object dimensions and aspect ratios match those of a known physical size of the space.
29. The system of claim 1, further comprising an application allowing a user to model ceiling geometries from the at least one photo of the space by hit-testing and identification of ceiling planes, facets, and boundaries.
30. A method comprising: a) launching an active augmented reality (AR) session on a first processing device comprising a camera and at least one processor; b) calibrating the AR session by establishing a fixed coordinate system, receiving a position and orientation of one or more horizontal or vertical planes in a space in reference to the fixed coordinate system, and receiving a position and orientation of the camera in reference to the fixed coordinate system; c) constructing a backing model comprising the fixed coordinate system, the position and orientation of the camera, and the position and orientation of the one or more horizontal or vertical planes; d) providing an interface allowing a user to capture at least one photo of the space during the active AR session; e) extracting camera data from the AR session for the at least one photo; f) extracting the backing model from the AR session; and g) storing the camera data and the backing model in association with the at least one photo.
</claims>
</document>
