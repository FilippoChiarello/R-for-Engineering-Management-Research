<document>

<filing_date>
2020-06-04
</filing_date>

<publication_date>
2020-12-10
</publication_date>

<priority_date>
2019-06-06
</priority_date>

<ipc_classes>
G06F3/00,G06T13/00,G10L15/00,G10L15/02
</ipc_classes>

<assignee>
ARTIE, INC.
</assignee>

<inventors>
EISENBERG, Josh
HORRIGAN, Ryan
MCINTYRE-KIRWIN, Armando
</inventors>

<docdb_family_id>
73652134
</docdb_family_id>

<title>
MULTI-MODAL MODEL FOR DYNAMICALLY RESPONSIVE VIRTUAL CHARACTERS
</title>

<abstract>
The disclosed embodiments relate to a method for controlling a virtual character (or 'avatar') using a multi-modal model. The multi-modal model may process various input information relating to a user and process the input information using multiple internal models. The multi-modal model may combine the internal models to make believable and emotionally engaging responses by the virtual character. The link to a virtual character may be embedded on a web browser and the avatar may be dynamically generated based on a selection to interact with the virtual character by a user. A report may be generated for a client, the report providing insights as to characteristics of users interacting with a virtual character associated with the client.
</abstract>

<claims>
I/We claim:
1. A method for controlling a virtual character, the method comprising:
receiving multi-modal input information from a device, the multi-modal input information including any of speech information, facial expression information, and environmental information representing an environment surrounding the device;
displaying the virtual character in a position in a display environment presented on the device;
implementing at least two internal models to identify characteristics of the multi modal input information;
inspecting the identified characteristics of the at least two internal models to determine whether a first identified characteristic of the identified characteristics includes a threshold number of similar features of a second identified characteristic of the identified characteristics;
comparing the first identified characteristic and the second identified characteristic against information specific to the virtual character included in a virtual character knowledge model to select a selected characteristic based on determining that the first identified characteristic includes the threshold number of similar features of the second identified characteristic of the identified characteristics;
accessing a library of potential actions associated with the virtual character to determine an action that matches the selected characteristic, the action including both an animation to be performed by the virtual character and associated audio; and
implementing the determined action by modifying the virtual character in the environment presented on the device and outputting the associated audio.
2. The method of claim 1 , wherein the at least two internal models includes a speech recognition model capable of parsing a speech sentiment from the speech information and a facial feature recognition model capable of detecting a facial feature sentiment based on the facial expression information, wherein the selected characteristic is a sentiment common among the speech sentiment and the facial feature sentiment, and wherein the determined action is determined based on the sentiment.
3. The method of claim 1 , wherein the at least two internal models include a prior knowledge model capable of retrieving prior knowledge information comprising information relating to previous engagement with a user, wherein the selected characteristic is selected based on the prior knowledge information processed using the prior knowledge model.
4. The method of claim 1 , wherein the internal models include a natural language understanding model configured to derive context and meaning from audio information, an awareness model configured to identify environmental information, and a social simulation model configured to identify data relating to a user and other virtual characters.
5. The method of claim 1 , further comprising:
instructing the virtual character to perform an initial action representing a query to a user on the device, wherein the input information represents a response by the user to the query.
6. The method of claim 1 , further comprising:
sharing an embedded link to a plurality of users via a network;
receiving a selection from any of a set of devices indicating that the embedded link has been selected; and
responsive to receiving the selection, transmitting a stream of data to the device of the set of devices that sent the selection to display the virtual character on the device.
7. The method of claim 6, further comprising:
transmitting a first batch of the stream of data at a first time, the first batch including information to initially generate the virtual character on the display of the device; and
transmitting a second batch of the stream of data at a second time after the first time, the second batch including information to output a first action by the virtual character, wherein the first batch is discarded at the second time.
8. The method of claim 1 , further comprising:
inspecting environmental information to identify a portion of the environment representative of a floor of the environment; and
positioning the virtual character at a first position above the portion of the environment representative of the floor of the environment.
9. A device configured to provide a response to a multi-modal input relatinger captured by the device, the device comprising:
at least one memory including:
at least two internal models configured to identify characteristics from multi-modal input information;
a virtual character knowledge model including information specific to a virtual character; and
a library of potential actions associated with the virtual character, each action is associated with an animation to be performed by the virtual character and associated audio; and
at least one processor configured to:
receive multi-modal input information including at least one of speech information, facial expression information, and environmental information representing an environment;
inspect the characteristics identified by the at least two internal models to determine whether a first identified characteristic is within a threshold similarity to a second identified characteristic; compare the first identified characteristic and the second identified characteristic against the virtual character knowledge model to identify a selected characteristic;
determine an action that matches the selected characteristic by inspecting the library of potential actions associated with the virtual character, the action including audio to be outputted on the device; and output the audio on the device.
10. The device of claim 9, wherein the at least one processor is further configured to:
display the virtual character on the display of the device in a position in the environment derived from the environmental information; and implement the action that includes both the audio to be outputted on the device and a selected animation to be performed by the virtual character by modifying the virtual character in the environment presented on the device.
1 1 . The device of claim 9, wherein the at least two internal models include a prior knowledge model capable of retrieving prior knowledge information comprising information relating to previous engagement with the user, wherein the selected characteristic is selected based on the prior knowledge information processed using the prior knowledge model.
12. The device of claim 9, wherein the at least two internal models includes a speech recognition model capable of parsing a speech sentiment from the speech information and a facial feature recognition model capable of detecting a facial feature sentiment based on the facial expression information, wherein the selected characteristic is a sentiment common among the speech sentiment and the facial feature sentiment, and wherein the determined action is determined based on the sentiment.
13. A computer-implemented method to dynamically generate a virtual character on a web browser of a user device, the computer-implemented method comprising:
embedding a link to the web browser of the user device, the link linking the web browser to an application executing on the user device;
receiving an indication from the user device that the link has been selected; transmitting a stream of data from the application representing information to the web browser to generate the virtual character;
displaying the virtual character on the web browser of the user device;
receiving multi-modal input information from the user device, the multi-modal input information including speech information, facial expression information, and environmental information representing an environment; implementing at least two internal models to identify characteristics of the multi modal input information;
inspecting the characteristics identified by the at least two internal models to determine whether a first identified characteristic is within a threshold similarity to a second identified characteristic;
comparing the first identified characteristic and the second identified characteristic against information specific to the virtual character included in a virtual character knowledge model to select a selected characteristic based on determining that the first identified characteristic includes the threshold number of similar features of the second identified characteristic of the identified characteristics;
accessing a library of potential actions associated with the virtual character to select an action that matches the selected characteristic, the action including an animation to be performed by the virtual character and associated audio; and
displaying the virtual character in the environment performing the action and outputting the associated audio.
14. The computer-implemented method of claim 13, wherein the web browser includes a page displayed on a mobile application executing on the user device.
15. The computer-implemented method of claim 13, wherein the at least two internal models includes a speech recognition model capable of parsing a speech sentiment from the speech information and a facial feature recognition model capable of detecting a facial feature sentiment based on the facial expression information, wherein the selected characteristic is a sentiment common among the speech sentiment and the facial feature sentiment, and wherein the determined action is determined based on the sentiment.
16. The computer-implemented method of claim 13, wherein the at least two internal models include a prior knowledge model capable of retrieving prior knowledge information comprising information relating to previous engagement with a user, wherein the selected characteristic is selected based on the prior knowledge information processed using the prior knowledge model.
17. The computer-implemented method of claim 13, further comprising:
sharing an embedded link to a plurality of users via a network;
receiving a selection from any of a set of devices indicating that the link has been selected; and
responsive to receiving the selection, transmitting the stream of data to the user device of the set of devices that sent the selection to display the virtual character on the user device.
18. The computer-implemented method of claim 13, further comprising:
transmitting a first batch of the stream of data at a first time, the first batch including information to initially generate the virtual character on a display of the user device; and
transmitting a second batch of the stream of data at a second time after the first time, the second batch including information to output a first action by the virtual character, wherein the first batch is discarded at the second time.
19. The computer-implemented method of claim 13, further comprising:
inspecting environmental information to identify a portion of the environment representative of a floor of the environment; and positioning the virtual character at a first position above the portion of the environment representative of the floor of the environment.
20. The computer-implemented method of claim 13, further comprising:
storing information relating to the selected characteristic and the action;
aggregating a series of selected characteristics and actions for a plurality of users;
processing the series of selected characteristics and actions for the plurality of users to derive a set of analytics relating to engagement with the virtual character with the plurality of users; and
presenting an analytics dashboard to display the set of analytics relating to engagement with the virtual character with the plurality of users.
</claims>
</document>
