<document>

<filing_date>
2018-03-02
</filing_date>

<publication_date>
2020-06-23
</publication_date>

<priority_date>
2017-03-03
</priority_date>

<ipc_classes>
G10L17/00,G10L17/02,G10L17/04,G10L17/06,G10L17/18,G10L19/02,G10L25/24
</ipc_classes>

<assignee>
PINDROP SECURITY
</assignee>

<inventors>
PATIL, KAILASH
GARLAND, MATTHEW
KHOURY, ELIE
NAGARSHETH, PARAV
</inventors>

<docdb_family_id>
63355275
</docdb_family_id>

<title>
Method and apparatus for detecting spoofing conditions
</title>

<abstract>
An automated speaker verification (ASV) system incorporates a first deep neural network to extract deep acoustic features, such as deep CQCC features, from a received voice sample. The deep acoustic features are processed by a second deep neural network that classifies the deep acoustic features according to a determined likelihood of including a spoofing condition. A binary classifier then classifies the voice sample as being genuine or spoofed.
</abstract>

<claims>
1. A method for detecting a spoofed voice source, the method comprising: receiving a voice sample; extracting at least deep acoustic features from the voice sample using a first deep neural network (DNN), wherein the first DNN comprises a max pooling layer that is configured to extract features of at least one audio artifact or channel artifact from the deep acoustic features; and calculating, via a second DNN that receives the extracted at least deep acoustic features, a likelihood that the voice sample includes a spoofing condition based in part on the features of at least one audio artifact or channel artifact in the deep acoustic features.
2. The method according to claim 1, further comprising: classifying the voice sample, using a binary classifier, as being either genuine or spoofed based on the likelihood from the second DNN.
3. The method according to claim 1, wherein the deep acoustic features are deep constant Q cepstral coefficients (CQCC).
4. The method according to claim 1, wherein the spoofing conditions include at least one of channel conditions and audio conditions.
5. The method according to claim 4, wherein the channel conditions include channel artifacts specific to at least one of different background environments, different acquisition devices, and different network infrastructures.
6. The method according to claim 1, further comprising: extracting other acoustic features from the voice sample; combining the deep acoustic features with the other acoustic features to provide tandem features, and classifying the tandem features using the second DNN, the second DNN configured to determine whether the tandem features include a non-spoofing condition or at least one spoofing condition, wherein said classifying at least the deep acoustic features is part of the classifying of the tandem features.
7. The method according to claim 6, wherein the other acoustic features are sub-band cepstral coefficient (SBCC) features, the method further comprising: sub-band filtering the voice sample before extracting the other features from the filtered sample, where said extracting the other, SBCC features includes: calculating a short-time Fourier transform (STFT) on a frame from the filtered sample, calculating a power spectrum from the STFT, calculating a log-amplitude from the power spectrum, calculating an inverse discrete cosine transform (IDCT) of the log-amplitude, and calculating dynamic features based on the IDCT.
8. The method according to claim 7, wherein said filtering the audio sample includes using a high pass filter, the filtered sample being limited to frequencies above a predetermined cutoff frequency.
9. The method according to claim 7, wherein said calculating dynamic features includes calculating at least one of delta and delta-delta (acceleration) features.
10. The method according to claim 1, wherein the second DNN is configured to extract multi-class features from the at least deep acoustic features.
11. The method according to claim 1, wherein the first DNN and the second DNN each include at least: an input layer, hidden layers, including one or more convolutional layers followed by the max-pooling layer, one or more fully-connected layers, and an output layer.
12. The method according to claim 11, wherein the max pooling layer of the first DNN is configured to extract bottleneck features from the deep acoustic features, the bottleneck features being sensitive to the at least one audio artifact or channel artifact.
13. The method according to claim 11, wherein batch normalization is applied, for at least one of the first DNN and the second DNN, to one or more of: the input layer, the hidden layers, the one or more fully-connected layers, and the output layer.
14. The method according to claim 1, wherein the second DNN is trained via at least one of gradient boosting and back propagation.
15. The method according to claim 1, wherein the second DNN is implemented using one or more graphics processors.
16. The method according to claim 1, wherein the configuration of the second DNN results from training the second DNN with a plurality of non-spoofed and known-spoofed voice samples.
17. An apparatus for detecting a spoofed voice source, the apparatus comprising: a receiving circuit configured to receive a voice sample; a first deep neural network (DNN) configured to extract at least deep acoustic features from the voice sample, wherein the first DNN comprises a max pooling layer that is configured to extract features of at least one audio artifact or channel artifact from the deep acoustic features; and a second DNN configured to calculate from the deep acoustic features a likelihood that the voice sample includes a spoofing condition based in part on the features of at least one audio artifact or channel artifact in the deep acoustic features.
18. The apparatus according to claim 17, further comprising: a binary classifier configured to classify the voice sample as being either genuine or spoofed based on the likelihood from the second DNN.
19. The apparatus according to claim 17, wherein the deep acoustic features are deep constant Q cepstral coefficients (CQCC).
20. The apparatus according to claim 17, wherein the spoofing conditions include at least one of channel conditions and audio conditions.
21. The apparatus according to claim 20, wherein the channel conditions include channel artifacts specific to at least one of: different background environments, different acquisition devices, and different network infrastructures.
22. The apparatus according to claim 17, further comprising: circuitry configured to extract other acoustic features from the voice sample; and a feature concatenation device configured to combine the deep acoustic features with the other acoustic features to provide tandem features, wherein the second DNN is further configured to classify the tandem features and to determine whether the tandem features include a non-spoofing condition or at least one spoofing condition and the configuration to classify the at least the deep acoustic features is included in said configuration to classify the tandem features.
</claims>
</document>
