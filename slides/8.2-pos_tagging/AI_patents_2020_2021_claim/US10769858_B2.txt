<document>

<filing_date>
2020-02-26
</filing_date>

<publication_date>
2020-09-08
</publication_date>

<priority_date>
2016-09-13
</priority_date>

<ipc_classes>
G02B27/01,G06F1/16,G06F3/01,G06F3/03,G06F3/16,G06F40/58,G06K9/00,G06K9/78,G06T19/00,G06T7/70
</ipc_classes>

<assignee>
MAGIC LEAP
</assignee>

<inventors>
RABINOVICH, ANDREW
WOODS, MICHAEL JANUSZ
BROWY, ERIC C.
</inventors>

<docdb_family_id>
61560263
</docdb_family_id>

<title>
Systems and methods for sign language recognition
</title>

<abstract>
A sensory eyewear system for a mixed reality device can facilitate user's interactions with the other people or with the environment. As one example, the sensory eyewear system can recognize and interpret a sign language, and present the translated information to a user of the mixed reality device. The wearable system can also recognize text in the user's environment, modify the text (e.g., by changing the content or display characteristics of the text), and render the modified text to occlude the original text.
</abstract>

<claims>
1. A computing system comprising: a hardware computer processor; a non-transitory computer readable medium having software instructions stored thereon, the software instructions executable by the hardware computer processor to cause the computing system to perform operations comprising: communicate with a plurality of wearable AR devices via one or more networks; receive image information captured by an outward facing imaging system of a first wearable AR device of the plurality of wearable AR devices, wherein the image information comprises images of one or more hands of a first wearer of the first wearable AR device; automatically detect sign language in the received image information, based at least on analysis of the images of the one or more hands of the first wearer; convert the detected sign language into text; and transmit, through the one or more networks, the converted text to a second wearable AR device in the plurality of wearable AR devices, wherein the second wearable AR device is configured to display the converted text to a second wearer of the second wearable AR device.
2. The system of claim 1, wherein the software instructions are further configured to cause the computing system to: transmit, through the one or more networks, a world map of the first wearer of the first wearable AR device.
3. The system of claim 2, wherein the world map of the first wearer comprises an avatar of the first wearer of the first wearable AR device.
4. The system of claim 1, wherein the software instructions are further configured to cause the computing system to identify a target language.
5. The system of claim 1, wherein the software instructions are further configured to cause the computing system to: identify a target language understood by the second wearer of the second AR device based on at least one of: speech as captured by the second AR device, the location of the second AR device, or an input from the wearer of the second AR device.
6. The system of claim 5, wherein the converted text is in the target language.
7. The system of claim 1, wherein said automatically detecting sign language comprises detecting a series of gestures in the received image information.
8. The system of claim 7, wherein said converting the detected sign language into text comprises accessing a sign language dictionary to translate a detected gesture into text.
9. The system of claim 1, wherein said converting the detected sign language into text comprises use of a machine learning algorithm.
10. The system of claim 1, wherein the software instructions are further configured to cause the computing system to: determine a source of the detected sign language, the source comprising a person, and convert the detected sign language based on the determined source.
11. The system of claim 10, wherein said determining the source of the detected sign language is based on at least the size of the one or more hands in the image information.
12. The system of claim 1, wherein the software instructions are further configured to cause the computing system to: transmit an audio stream to the second wearable AR device based on the converted text.
13. A method of facilitating communication between wearable AR devices, the method comprising: communicating with a plurality of wearable AR devices via one or more networks; receiving image information captured by an outward facing imaging system of a first wearable AR device of the plurality of wearable AR devices, wherein the image information comprises images of one or more hands of a first wearer of the first wearable AR device; automatically detecting sign language in the received image information, based at least on analysis of the images of the one or more hands of the first wearer; converting the detected sign language into text; and transmitting, through one or more networks, the converted text to a second wearable AR device in the plurality of wearable AR devices, wherein the second wearable AR device is configured to display the converted text to a second wearer of the second wearable AR device.
14. The method of claim 13, comprising transmitting, through the one or more networks, a world map of the first wearer of the first wearable AR device.
15. The method of claim 14, wherein the world map of the first wearer comprises an avatar of the first wearer of the first wearable AR device.
16. The method of claim 13, comprising identifying a target language.
17. The method of claim 13, comprising identifying a target language understood by the second wearer of the second AR device based on at least one of: speech as captured by the second AR device, the location of the second AR device, or an input from the wearer of the second AR device.
18. The method of claim 17, wherein the converted text is in the target language.
19. The method of claim 13, wherein said automatically detecting sign language comprises detecting a series of gestures in the received image information.
20. The method of claim 13, wherein said converting the detected sign language into text comprises accessing a sign language dictionary to translate a detected gesture into text.
21. The method of claim 13, wherein said converting the detected sign language into text comprises use of a machine learning algorithm.
22. The method of claim 13 comprising: determining a source of the detected sign language, the source comprising a person, and converting the detected sign language based on the determined source.
23. The method of claim 13 comprising determining a source of the detected sign language is the wearer of the first wearable AR device based on the size of the one or more hands in the image information.
24. The method of claim 13 comprising transmitting an audio stream to the second wearable AR device based on the converted text.
</claims>
</document>
