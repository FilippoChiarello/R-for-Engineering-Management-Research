<document>

<filing_date>
2018-12-18
</filing_date>

<publication_date>
2020-02-20
</publication_date>

<priority_date>
2018-08-16
</priority_date>

<ipc_classes>
G01N21/88,G01N21/94,G01N33/12,G06N20/00,G06T11/00
</ipc_classes>

<assignee>
MAIRHOFER, STEFAN
THAI UNION GROUP PUBLIC COMPANY
ZENTZ, BRADLEY, J.
</assignee>

<inventors>
MAIRHOFER, STEFAN
</inventors>

<docdb_family_id>
65003598
</docdb_family_id>

<title>
MULTI-VIEW IMAGING SYSTEM AND METHODS FOR NON-INVASIVE INSPECTION IN FOOD PROCESSING
</title>

<abstract>
An inline vision-based system (400) used for the inspection and processing of food material (408) and associated imaging methods are disclosed. The system includes a conveyor belt (402), a transparent plate (412), and an imaging system (422), wherein the imaging system includes a light source (424) and at least one camera. The imaging system produces image data from multiple views of light passing through an object on the transparent plate and captured by the camera. The image data corresponds to one of transmittance, interactance, or reflectance image data and is transmitted to a processor (428). The processor processes the data using machine learning to generate a three dimensional model of the geometry of a portion of material internal to the object so as to determine boundaries of the portion relative to the surrounding material.
</abstract>

<claims>
1. A system, comprising:
a first conveyor;
a second conveyor separated from the first conveyor by a gap; a transparent plate positioned in the gap and coupled to at least one of the first conveyor and the second conveyor;
a support ring positioned at least in part in the gap and coupled to at least one of the first conveyor and the second conveyor;
at least one imaging device coupled to the support ring;
a first light source coupled to the support ring; and
a control unit in electronic communication with the support ring and the at least one imaging device,
wherein during operation, the first light source emits light directed towards an object on the transparent plate and the control unit receives imaging data from the at least one imaging device, the control unit constructing a 3D model of a second portion of the object contained within a first portion of the object.
2. The system of claim 1 wherein the at least one imaging device transmits imaging data to the control unit, the imaging data including one of interactance imaging data and transmittance imaging data.
3. The system of claim 1 wherein the object is a tuna fillet and the first light source emits light at a wavelength that is equal to one of approximately 1260 nanometers, approximately 805 nanometers, or
approximately 770 nanometers.
4. The system of claim 1 wherein the processor uses machine learning in the form of a convolutional neural network to process the imaging data.
5. The system of claim 4 wherein the convolutional neural network receives the image data and outputs a plurality of silhouettes based on the image data corresponding to the second portion of the object, the processor projecting the silhouettes into a plurality of projections and analyzing an intersection between the plurality of projections to construct the 3D model.
8. The system of claim 1 wherein the support ring includes a plurality of cameras coupled to the support ring, each of the plurality of cameras capturing one of transmittance, interactance, or reflectance imaging data from the first light source.
7. The system of claim 8 wherein the support ring includes a second light source coupled to the support ring, wherein during operation, the second light source emits light directed to the transparent plate.
8. A device, comprising:
a conveyor having a space between a first portion and a second portion of the conveyor;
a plate positioned in the space and coupled to the conveyor;
a support ring positioned at least in part in the space and coupled to the conveyor;
at least one light source coupled to the support ring; an imaging device coupled to the support ring; and
a processor in electronic communication with the imaging device, wherein during operation, the support ring rotates between at least a first position and a second position and the at least one light source emits light directed towards an object on the plate and the imaging device receives light from the at least one light source after the light passes through the object,
wherein the processor receives a first set of image data from the imaging device when the support ring is in the first position and a second set of image data from the imaging device when the support ring is in the second position and outputs a 3D model of an inner portion of the object from the first set of image data and the second set of image data.
9. The device of claim 8 wherein the processor utilizes machine learning to process the first set of image data and the second set of image data into a plurality of silhouettes and to project the plurality of silhouettes into a plurality of projections, wherein the three-dimensional model is based on an intersection between each of the plurality of projections.
10. The device of claim 8 further comprising a second light source coupled to the support ring, the imaging device capturing a third set of image data from the second light source when the support ring is in the first or second position, the processor utilizing the third set of image data to clarify boundaries of the three-dimensional model.
11. The device of claim 8 wherein the imaging device is spectrograph and the at least one light source emits light at a wavelength selected from one of approximately 1260 nanometers, approximately 805 nanometers, or approximately 770 nanometers.
12. A method, comprising:
emitting light from a light source, the emitting including directing the light through an object having a first portion and second portion, the second portion enclosed within the first portion; capturing light from the light source after the light passes through the object with an imaging device, the captured light corresponding to image data of the first portion and the second portion received by the imaging device;
transmitting the image data to a processor; and
analyzing the image data with the processor to detect a boundary between the first portion and the second portion, wherein the analyzing includes utilizing machine learning to produce a three dimensional representation of the second portion.
13. The method of claim 12 wherein emitting light from the light source includes emitting the light with a wavelength selected from one of approximately 1260 nanometers, 805 nanometers, or 770 nanometers.
14. The method of claim 12 wherein utilizing machine learning to produce the three dimensional representation of the second portion includes the machine learning utilizing a deep convolutional neural network for processing the image data.
15. The method of claim 12 wherein analyzing the image data with the processor includes utilizing machine learning to output a plurality of two dimensional silhouettes corresponding to the image data of the second portion.
16. The method of claim 15 wherein analyzing the image data with the processor includes utilizing machine learning to create a plurality of projections, wherein each projection corresponds to a respective one of the plurality of two dimensional silhouettes.
17. The method of claim 15 wherein analyzing includes utilizing machine learning to produce a three dimensional representation further includes analyzing an intersection between each of the plurality of projections to output a three dimensional representation of the second portion of the object.
</claims>
</document>
