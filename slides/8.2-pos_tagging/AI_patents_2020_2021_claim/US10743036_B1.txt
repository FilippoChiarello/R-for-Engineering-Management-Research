<document>

<filing_date>
2018-05-30
</filing_date>

<publication_date>
2020-08-11
</publication_date>

<priority_date>
2018-05-30
</priority_date>

<ipc_classes>
H04N21/231,H04N21/235,H04N21/25
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
</assignee>

<inventors>
VERMA, PRASHANT
MOKASHI, RONIL SUDHIR
FARRIS, RYAN
</inventors>

<docdb_family_id>
71993878
</docdb_family_id>

<title>
Automatically augmenting user resources dedicated to serving content to a content delivery network
</title>

<abstract>
In some embodiments, a system is provided, and computer-executable instructions cause the system to: receive, at an edge server of a content delivery network (CDN), a request for a first video and a request for a second video; determine that the first video is not cached and that the second video is cached; request the first video from the origin server and log a cache miss; obtain the first video from the origin server and send responsive to the request; send the second video responsive to the request and log a cache hit; obtain a metric indicative of the volume of cache misses, including for the first and second videos, across edge servers of the CDN; and determine, based on the metric, an amount by which to scale resources implementing the origin.
</abstract>

<claims>
1. A system, comprising one or more hardware computing devices having a processor and memory storing specific computer-executable instructions that, when executed by the processor, cause the system to: obtain, at an edge server of a content delivery network, information identifying an origin server associated with a first user, wherein the origin server is implemented using an electronic data storage service to store a plurality of video files; obtain, from the first user associated with the origin server, permission to instruct a service provider to augment resources dedicated to implementing the origin server; obtain, at the edge server, a request for a first video from a first client device; determine that the first video is not cached by the edge server; in response to determining that the first video is not cached by the edge server, request the first video from the origin server based on the information identifying the origin server; in response to determining that the first video is not cached by the edge server, send first log information to a content delivery network log repository indicating a cache miss for the first video; obtain the first video from the origin server; in response to receiving the first video from the origin server, send the first video to the first client device; obtain, at the edge server, a request for a second video from a second client device; determine that the second video is cached by the edge server; in response to determining that the second video is cached by the edge server, send the second video to the second client device; in response to determining that the second video is cached by the edge server, send second log information to the content delivery network log repository indicating a cache hit for the second video; obtain a metric, based on log information sent to the content delivery network log repository, that is indicative of the volume of cache misses for videos, including the plurality of videos, associated with the origin server across a plurality of edge servers of the content delivery network; determine, based on the metric, that the volume of cache misses for videos associated with the origin server is likely to increase during a second period of time in the future; determine, based on the metric, an amount by which to scale compute resources used to implement the origin server; and send instructions to the electronic data storage service to increase the compute resources used to implement the origin server.
2. The system of claim 1, wherein the instructions, when executed by the processor, further cause the system to: obtain metadata associated with the first video; generate a portion of a feature vector based on the metadata associated with the first video; generate another portion of the feature vector based on the metric; provide the feature vector to a deep neural network, the deep neural network is trained to predict a change in requests for a video in a future time period of a particular length based on features corresponding to metadata associated with the video using metadata associated with videos served by the content delivery network from a plurality of origin servers; obtain, from the deep neural network, an output indicative of the predicted change in requests for the first video in a first future time period of the particular length; and determine the amount by which to scale the compute resources used to implement the origin server using the output of the deep neural network.
3. The system of claim 2, wherein the instructions, when executed by the processor, further cause the system to: obtain an updated metric, based on log information sent to the content delivery network log repository during the first future time period, that is indicative of the volume of cache misses for content associated with the origin server across the plurality of edge servers during the first future time period; and update the deep neural network based on a comparison of the updated metric and the predicted change in requests for the first video in the first future time period.
4. A system, comprising one or more hardware computing devices having a processor and memory storing specific computer-executable instructions that, when executed by the processor, cause the system to: obtain, from a first user associated with a first origin server, permission to instruct a service provider to augment resources dedicated to implementing the first origin server; obtain a metric, based on log information sent by edge servers of a content delivery network, indicative of a volume of requests for content items sent by the edge servers to an origin server associated with a first user during a first period of time in the past; determine, based on the metric, that the volume of requests is likely to increase during a second period of time in the future; and in response to determining that the volume of requests is likely to increase during the second period of time, instruct the service provider to increase the resources dedicated to implementing the first origin server by an amount based on a projected increase in the volume of requests during the second period of time.
5. The system of claim 4, wherein the instructions, when executed by the processor, further cause the system to: obtain metadata associated with content items served to the content delivery network from the first origin server; generate, for each content item, a plurality of features using the metadata; generate a feature based on the metric; provide, for each content item, the features and the feature based on the metric to a trained machine learning model that has been trained to predict a change in requests for a content item in a future period of time based on features generated from metadata associated with the content item, the trained machine learning model trained using metadata from a plurality of users; obtain, from the trained machine learning model, an output indicative of the predicted change in requests for each content item; and determine the amount by which to scale the compute resources used to implement the first origin server using the outputs of the trained machine learning model.
6. The system of claim 5, wherein the trained machine learning model comprises a deep neural network trained using at least one supervised learning techniques, wherein a label for each content item is indicative of a change in requests for that content item in a period of time following a period of time corresponding to the metadata associated with the content item.
7. The system of claim 5, wherein the trained machine learning model comprises a deep neural network trained using at least one reinforcement learning technique, wherein a reward metric provided to the deep neural network subsequent to a prediction for a particular content item of the content items is indicative of a change in requests for that content item in a period of time following the prediction.
8. The system of claim 5, wherein the plurality of features includes a first feature indicative of an author of the content item, a second feature indicative of the number of times that the content item was requested in the first period of time, and a third feature indicative of a length of the content item.
9. The system of claim 8, wherein the plurality of features includes a fourth feature indicative of identifying information of one or more of the following appearing in the content: a person; multiple different people; a landmark; and an object.
10. The system of claim 8, wherein the plurality of features includes a fifth feature indicative of a proportion of the author's content items for which an increase in requests for the content item within a predetermined time exceeded a threshold rate of increase.
11. The system of claim 5, wherein the plurality of users does not include the first user.
12. The system of claim 11, wherein the instructions, when executed by the processor, further cause the system to: obtain an updated metric, based on log information corresponding to the second time period, that is indicative of the volume of cache misses for content associated with the origin server across the plurality of edge servers during the first future time period; and update the trained machine learning model based on a comparison of the updated metric and the output indicative of the predicted change.
13. The system of claim 4, wherein the instructions, when executed by the processor, further cause the system to: in response to determining that the volume of requests is likely to increase during the second period of time, instruct each edge server of the content delivery network to inhibit serving of a most resource intensive version of a requested content item if a requested version of the content item from the first origin server is not cached; determine that the increased resources dedicated to implementing the first origin server are being used to implement the first origin server; and in response determining that the increased resources dedicated to implementing the first origin server are being used to implement the first origin server, instruct each edge server of the content delivery network to serve content from the first origin server normally.
14. The system of claim 4, wherein the instructions, when executed by the processor, further cause the system to: obtain information identifying the first origin server associated with the first user, wherein the first origin server is implemented using an electronic data storage service to store a plurality of video files; and send the information identifying the first origin server to each edge of the content delivery network.
15. The system of claim 4, wherein each of the plurality of content items includes video data.
16. A method, comprising: obtaining, from a first user associated with a first origin server, permission to instruct a service provider to augment resources dedicated to implementing the first origin server; obtaining a metric, based on log information sent by edge servers of a content delivery network, indicative of a volume of requests for content items sent by the edge servers to an origin server associated with a first user during a first period of time in the past; determining, based on the metric, that the volume of requests is likely to increase during a second period of time in the future; and in response to determining that the volume of requests is likely to increase during the second period of time, instructing the service provider to increase the resources dedicated to implementing the first origin server by an amount based on a projected increase in the volume of requests during the second period of time.
17. The method of claim 16, further comprising: obtaining metadata associated with content items served to the content delivery network from the first origin server; generating, for each content item, a plurality of features using the metadata; generating a feature based on the metric; providing, for each content item, the features and the feature based on the metric to a trained machine learning model that has been trained to predict a change in requests for a content item in a future period of time based on features generated from metadata associated with the content item, the trained machine learning model trained using metadata from a plurality of users; obtaining, from the trained machine learning model, an output indicative of the predicted change in requests for each content item; and determining the amount by which to scale the compute resources used to implement the first origin server using the outputs of the trained machine learning model.
18. The method of claim 17, wherein the plurality of features includes a first feature indicative of an author of the content item, a second feature indicative of the number of times that the content item was requested in the first period of time, and a third feature indicative of a length of the content item.
19. The method of claim 16, further comprising: in response to determining that the volume of requests is likely to increase during the second period of time, instructing each edge server of the content delivery network to inhibit serving of a most resource intensive version of a requested content item if a requested version of the content item from the first origin server is not cached; determining that the increased resources dedicated to implementing the first origin server are being used to implement the first origin server; and in response determining that the increased resources dedicated to implementing the first origin server are being used to implement the first origin server, instructing each edge server of the content delivery network to serve content from the first origin server normally.
20. The method of claim 16, further comprising: obtaining information identifying the first origin server associated with the first user, wherein the first origin server is implemented using an electronic data storage service to store a plurality of video files; and sending the information identifying the first origin server to each edge of the content delivery network.
</claims>
</document>
