<document>

<filing_date>
2020-04-21
</filing_date>

<publication_date>
2020-08-06
</publication_date>

<priority_date>
2017-02-10
</priority_date>

<ipc_classes>
G06F17/18,G06K9/62,G06N20/00,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
IOFFE, SERGEY
</inventors>

<docdb_family_id>
61283320
</docdb_family_id>

<title>
BATCH RENORMALIZATION LAYERS
</title>

<abstract>
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for implementing a neural network. In one aspect, the neural network includes a batch renormalization layer between a first neural network layer and a second neural network layer. The first neural network layer generates first layer outputs having multiple components. The batch renormalization layer is configured to, during training of the neural network on a current batch of training examples, obtain respective current moving normalization statistics for each of the multiple components and determine respective affine transform parameters for each of the multiple components from the current moving normalization statistics. The batch renormalization layer receives a respective first layer output for each training example in the current batch and applies the affine transform to each component of a normalized layer output to generate a renormalized layer output for the training example.
</abstract>

<claims>
1. A system comprising one or more computers and one or more storage devices storing instructions that when executed by one or more computers cause the one or more computers to implement a neural network, the neural network comprising: a batch renormalization layer between a first neural network layer and a second neural network layer, wherein the first neural network layer generates first layer outputs having a plurality of components, and wherein the batch renormalization layer is configured to, during training of the neural network on a current batch of training examples: obtain respective current moving normalization statistics for each of the plurality of components that are based on previous first layer outputs generated by the first neural network layer during training of the neural network on previous batches of training examples; receive a respective first layer output for each training example in the current batch; compute respective current batch normalization statistics for each of the plurality of components from the first layer outputs for the training examples in the current batch; determine respective transform function parameters for a transform function for each of the plurality of components from the current moving normalization statistics and the current batch normalization statistics; and for each of the first layer outputs for each of the training examples in the current batch: normalize each component of the first layer output using the current batch normalization statistics for the component to generate a normalized layer output for the training example, apply the transform function to each component of the normalized layer output in accordance with the transform function parameters for the component to generate a renormalized layer output for the training example, generate a batch renormalization layer output for the training example from the renormalized layer output, and provide the batch renormalization layer output as an input to the second neural network layer.
2. The system of claim 1, wherein the batch renormalization layer is further configured to: update the current moving normalization statistics for each component using the current batch normalization statistics for the component to generate updated moving normalization statistics for the component.
3. The system of claim 1, wherein, during the training of the neural network, the system is configured to backpropagate through the current batch normalization statistics as part of adjusting values of parameters of the neural network while treating the moving normalization statistics and the parameters of the transform function as a constant.
4. The system claim 1, wherein the plurality of components are respective dimensions of the first layer outputs.
5. The system of claim 1, wherein the first neural network layer is a convolutional layer, wherein the first layer outputs comprise a plurality of feature maps, and wherein each component is a respective feature map.
6. The system of claim 1, wherein the current moving normalization statistics comprise, for each of the components: a moving mean of the component for the previous first layer outputs, and a moving approximated standard deviation for the component of the first layer outputs; wherein computing a plurality of current batch normalization statistics for the first layer outputs comprises, for each of the components: computing a mean of the component for the first layer outputs in the current batch; and computing an approximated standard deviation for the component of the first layer outputs in the current batch.
7. The system of claim 6, wherein normalizing each component of each first layer output comprises: normalizing the component of the first layer output using the computed mean and computed approximate standard deviation for the component.
8. The system of claim 6, wherein determining respective parameters for a transform function for each of the components comprises, for each component: determining a first parameter for the component from a ratio between (i) a difference between the mean for the component and the moving mean for the component and (ii) the moving approximated standard deviation for the component; and determining a second parameter for the component from a ratio between the approximated standard deviation for the component and the moving approximated standard deviation for the component.
9. The system of claim 8, wherein applying the transform function to each component of the normalized layer output in accordance with the parameters comprises: multiplying the component of the normalized layer output by the second parameter for the component to generate a product; and adding the first transform for the component to the product to generate the component of the renormalized layer output.
10. The system of claim 8, wherein values of the first parameter and the second parameter are constrained to fall in a pre-determined range.
11. The system of claim 6, wherein an approximate standard deviation for a component is a square root of a sum of a variance for the component and a pre-determined constant value.
12. The system of claim 1, wherein generating the respective batch renormalization layer output for the training example from the renormalized layer outputs comprises: transforming, for each component, the component of the renormalized layer output for the training example in accordance with current values of a set of learnable parameters for the component.
13. The system of claim 12, wherein the batch renormalization layer is configured to, after the neural network has been trained to determine trained values of the learnable parameters for each of the components: receive a new first layer output generated by the first neural network layer for a new neural network input; normalize each component of the new first layer output using respective pre-computed normalization statistics for the component to generate a new renormalized layer output; generate a new batch renormalization layer output by transforming, for each component, the component of the new renormalized layer output in accordance with the trained values of the set of learnable parameters for the component; and provide the batch renormalization layer output as a new layer input to the second neural network layer.
14. The system of claim 13, wherein the pre-computed normalization statistics for the components are final moving normalization statistics after training of the neural network.
15. The system of claim 13, wherein the pre-computed normalization statistics for the components are computed from new first layer outputs generated by the first neural network layer after the neural network has been trained.
16. The system of claim 1, wherein the transform function parameters include a scale parameter and a bias parameter, and wherein determining respective transform function parameters comprises: determining the scale parameter value to be one and the bias parameter value to be zero if a number of completed training iterations is less than a predetermined threshold number of training iterations.
17. The system of claim 1, wherein generating a renormalized layer output for the training example further comprises: clipping each component of the renormalized layer output to cause the component to lie in a predetermined range.
18. The system of claim 15, wherein new neural network inputs processed by the neural network after the neural network has been trained are a different type of input than the training examples used to train the neural network.
19. The system of claim 1, wherein the first neural network layer generates the first layer outputs by modifying first layer inputs in accordance with current values of a set of parameters for the first neural network layer.
20. The system of claim 19, wherein the second neural network layer generates second layer outputs by applying a non-linear activation function to the batch renormalization layer outputs.
21. The system of claim 1, wherein the first neural network layer generates the first layer outputs by modifying first layer inputs in accordance with current values of a set of parameters to generate modified first layer inputs and then applying a non-linear activation function to the modified first layer inputs.
22. The system of claim 1, wherein the neural network is a feedforward neural network.
23. The system of claim 1, wherein the neural network is a recurrent neural network.
24. The system of claim 1, wherein the transform function is an affine function.
25. A method performed by one or more data processing apparatus, the method comprising: training a neural network on a current batch of training examples, wherein the neural network comprises a batch renormalization layer between a first neural network layer and a second neural network layer, wherein the first neural network layer generates first layer outputs having a plurality of components, and wherein the batch renormalization layer is configured to, during training of the neural network on the current batch of training examples: obtain respective current moving normalization statistics for each of the plurality of components that are based on previous first layer outputs generated by the first neural network layer during training of the neural network on previous batches of training examples; receive a respective first layer output for each training example in the current batch; compute respective current batch normalization statistics for each of the plurality of components from the first layer outputs for the training examples in the current batch; determine respective transform function parameters for a transform function for each of the plurality of components from the current moving normalization statistics and the current batch normalization statistics; and for each of the first layer outputs for each of the training examples in the current batch: normalize each component of the first layer output using the current batch normalization statistics for the component to generate a normalized layer output for the training example, apply the transform function to each component of the normalized layer output in accordance with the transform function parameters for the component to generate a renormalized layer output for the training example, generate a batch renormalization layer output for the training example from the renormalized layer output, and provide the batch renormalization layer output as an input to the second neural network layer.
26. One or more non-transitory computer-storage media storing instructions that when executed by one or more computers cause the one or more computers to perform operations comprising: training a neural network on a current batch of training examples, wherein the neural network comprises a batch renormalization layer between a first neural network layer and a second neural network layer, wherein the first neural network layer generates first layer outputs having a plurality of components, and wherein the batch renormalization layer is configured to, during training of the neural network on the current batch of training examples: obtain respective current moving normalization statistics for each of the plurality of components that are based on previous first layer outputs generated by the first neural network layer during training of the neural network on previous batches of training examples; receive a respective first layer output for each training example in the current batch; compute respective current batch normalization statistics for each of the plurality of components from the first layer outputs for the training examples in the current batch; determine respective transform function parameters for a transform function for each of the plurality of components from the current moving normalization statistics and the current batch normalization statistics; and for each of the first layer outputs for each of the training examples in the current batch: normalize each component of the first layer output using the current batch normalization statistics for the component to generate a normalized layer output for the training example, apply the transform function to each component of the normalized layer output in accordance with the transform function parameters for the component to generate a renormalized layer output for the training example, generate a batch renormalization layer output for the training example from the renormalized layer output, and provide the batch renormalization layer output as an input to the second neural network layer.
</claims>
</document>
