<document>

<filing_date>
2019-09-24
</filing_date>

<publication_date>
2020-04-02
</publication_date>

<priority_date>
2018-09-24
</priority_date>

<ipc_classes>
G06N3/08
</ipc_classes>

<assignee>
MICHELLE, ARCHULETA
</assignee>

<inventors>
MICHELLE, ARCHULETA
</inventors>

<docdb_family_id>
69949837
</docdb_family_id>

<title>
REINFORCEMENT LEARNING APPROACH TO APPROXIMATE A MENTAL MAP OF FORMAL LOGIC
</title>

<abstract>
Methods, systems, and apparatus, including computer programs language encoded on a computer storage medium for a logic correction system whereby input text is modified to a logical state using a reinforcement learning system with a real-time logic engine. The logic engine is able to extract the symmetry of word relationships and negate relationships into formal logical equations such that an automated theorem prover can evaluate the logical state of the input text and return a positive or negative reward. The reinforcement learning agent optimizes a policy creating a conceptual understanding of the logical system, a 'mental map' of word relationships.
</abstract>

<claims>
1. A reinforcement learning system, comprising:
one or more processors; and
one or more programs residing on a memory and executable by the
one or more processors, the one or more programs configured to:
perform actions on word classes within one or more sentence; select an action to maximize an expected future value of a reward function; and, wherein the reward function depends on: restoring the one or more sentence to a logical state.
2. The system of claim 1, wherein a one or more sentence and groups are used to construct a network.
3. The system of claim 2, wherein a word polarity score is defined between two nodes in the network whereby the nodes have symmetrical relation with respect to each other.
4. The system of claim 3, wherein the nodes share nodes.
5. The system of claim 3, wherein the nodes share antonym nodes.
6. The system of claim 3, wherein the network, antonyms, the word polarity score are used to create negated relationships among nodes in the network.
7. The system of claim 6, wherein the negated relationships are formulated as a formal logic, such that a set of logical equations is generated.
8. The system of claim 7, wherein the negated relationships are formulated as a propositional logic, such that a set of propositional logic equations is generated.
9. The system of claim 8, wherein an automated propositional logic theorem prover evaluates the propositional logic equations and returns a positive reward if the one or more sentence is logical and a negative reward if the one or more sentence is nonsensical.
10. The system of claim 7, wherein the negated relationships are formulated as a first-order logic, such that a set of logic equations is generated.
11. The system of claim 10, wherein an automated first-order logic
theorem prover evaluates the first-order logic equations and returns a positive reward if the one or more sentence is logical and a negative reward if the one or more sentence is nonsensical.
12. The system of claim 7, wherein the negated relationships are formulated as a second-order logic, such that a set of logic equations is generated.
13. The system of claim 12, wherein an automated second-order logic theorem prover evaluates the second-order logic equations and returns a positive reward if the one or more sentence is logical and a negative reward if the one or more sentence is nonsensical.
14. The system of claim 7, wherein the negated relationships are formulated as a higher-order logic, such that a set of logic equations is generated.
15. The system of claim 14, wherein an automated higher-order logic theorem prover evaluates the higher-order logic equations and returns a positive reward if the one or more sentence is logical and a negative reward if the one or more sentence is nonsensical.
16. The system of claim 7, wherein the logical equations are categorized into assumptions and conclusions.
17. The system of claim 16, wherein a user provides the logical equations that are categorized as assumptions.
18. A reinforcement learning system, comprising:
one or more processors; and
one or more programs residing on a memory and executable by the
one or more processors, the one or more programs configured to:
perform actions on word classes within a one or more sentence of sentences; wherein an optimized policy is obtained such that the reinforcement learning agent has generated a mental map of the logic of the one or more sentence.
19. The system of claim 18, wherein the mental map can be saved to a memory and transferred to a new system by saving the weights of the convolutional neural network used as a function approximator.
20. A logical correction system, comprising:
input sentence;
one or more processors; and
one or more programs residing on a memory and executable by the
one or more processors, the one or more programs configured to:
perform reinforcement learning wherein an optimal policy is achieved when a minimum number of edits result in a logical sentence;
21. A method for reinforcement learning, comprising the steps of:
receiving one or more sentences;
selecting an action to maximizing the expected future value of a reward function; wherein the reward function depends on at least partly on: restoring the sentences to a logical state.
22. The method of claim 21, wherein the sentence and groups are used to construct a network.
23. The method of claim 22, wherein a word polarity score is defined between two nodes in the network whereby the nodes have symmetrical relation with respect to each other.
24. The method of claim 23, wherein the nodes share nodes.
25. The method of claim 23, wherein the nodes share antonym nodes.
26. The method of claim 23, wherein the network, antonyms, the word polarity score are used to create negated relationships among nodes in the network.
27. The system of claim 26, wherein the negated relationships are formulated as a formal logic, such that a set of logical equations is generated.
28. A real-time logic engine, comprising:
one or more sentences;
a physical hardware device consisting of a memory unit and processor;
a software consisting of a computer program or computer programs;
an output signal that indicates that one or more sentences is logical or one or more sentences is nonsensical;
wherein one or more processors; and
one or more programs residing on a memory and executable by the one or more
processors, the one or more programs configured to:
receive one or more sentences;
generate a network from the one or more sentences;
identify symmetry in the network;
negate the symmetry of the network into
logical relationships;
generate logical equations by formalizing logical relationships into a formal logic;
infer a proof by evaluating the logical equations with an automated theorem prover;
wherein modifications made to one or more sentences can be evaluated to determine if the modifications results in a logical or nonsensical sentence. AMENDED CLAIMS
received by the International Bureau on 06 March 2020 (06.03.2020)
1. A reinforcement learning system, comprising:
one or more processors; and
one or more programs residing on a memory and executable by the
one or more processors, the one or more programs configured to:
perform actions from a set of available actions on a state wherein the state is a sentence; select an action to maximize an expected future value of a reward function; and, wherein the reward function depends on a logic-engine that upon receiving a logical sentence returns a positive reward and upon receiving an illogical sentence returns a negative reward; provide the reinforcement learning agent with a pool of states, actions, and rewards and a function approximator wherein using the function approximator the reinforcement learning agent predicts the best action to take resulting in maximum reward; wherein an agent optimizes a policy such the agent learns modifications to restore the sentence to a logical state.
2. The system of claim 1 , wherein the logic engine consisting of a automated theorem prover processes input sentences according to a set of logical equations derived from a discourse and the sentence, wherein the automated theorem prover takes the logical equations as the premise and infers a proof.
3. The system of claim 2, wherein the logic engine consisting of the automated theorem prover executes a logical equation or logical equations derived from the sentence stored in memory against a set of logical equations derived from a discourse stored in memory on a processor and returns the state of the sentence as logical or illogical.
4. The system of claim 3, wherein the logical equations consist of negated relationships that are determined by a symmetrical axis or a plurality of symmetrical axes in a network graph.
5. The system of claim 4, wherein one or more sentence and groups are used to construct a network.
6. The system of claim 5, wherein the symmetry of the network is used to create negated relationships among nodes in the network.
7. The system of claim 6, wherein a word polarity score is a measure of symmetry between two nodes in the network.
8. The system of claim 7, wherein a word polarity score is used to rank symmetrical node relationships wherein the top ranked symmetrical node relationships are used to generate the negated relationships.
9. The system of claim 4, wherein the negated relationships are formulated as a formal logic, such that a set of logical equations is generated.
10. The system of claim 9, wherein the negated relationships are formulated as a propositional logic, such that a set of propositional logic equations is generated.
11. The system of claim 10, wherein an automated propositional logic theorem prover evaluates the propositional logic equations and returns a positive reward if the one or more sentence is logical and a negative reward if the one or more sentence is nonsensical.
12. The system of claim 9, wherein the negated relationships are formulated as a first-order logic, such that a set of logic equations is generated.
13. The system of claim 12, wherein an automated first-order logic
theorem prover evaluates the first-order logic equations and returns a positive reward if the one or more sentence is logical and a negative reward if the one or more sentence is nonsensical.
14. The system of claim 9, wherein the negated relationships are formulated as a second-order logic, such that a set of logic equations is generated.
15. The system of claim 14, wherein an automated second-order logic theorem prover evaluates the second-order logic equations and returns a positive reward if the one or more sentence is logical and a negative reward if the one or more sentence is nonsensical.
16. The system of claim 9, wherein the negated relationships are formulated as a higher-order logic, such that a set of logic equations is generated.
17. The system of claim 16, wherein an automated higher-order logic theorem prover evaluates the higher-order logic equations and returns a positive reward if the one or more sentence is logical and a negative reward if the one or more sentence is nonsensical.
18. The system of claim 4, wherein the logical equations are categorized into
assumptions and conclusions.
19. The system of claim 18, wherein a user provides the logical equations categorized as assumptions.
20. A reinforcement learning system, comprising:
one or more processors; and
one or more programs residing on a memory and executable by the
one or more processors, the one or more programs configured to:
select an action to maximize an expected future value of a reward function; and, wherein the reward function depends on a logic-engine that upon receiving a logical sentence returns a positive reward and upon receiving an illogical sentence returns a negative reward; provide the reinforcement learning agent with a pool of states, actions, and rewards and a function approximator wherein using the function approximator the reinforcement learning agent predicts the best action to take resulting in maximum reward; upon obtaining an learning rate above a threshold save the weights of the function approximator to memory; wherein an optimized policy is obtained such that the reinforcement learning agent has generated a mental map of the logic of the one or more sentences.
21. The system of claim 20, wherein the mental map can be transferred to a new system by saving the weights of the convolutional neural network used as a function approximator.
22. A logical correction system, comprising:
input sentence;
one or more processors; and
one or more programs residing on a memory and executable by the
one or more processors, the one or more programs configured to:
perform reinforcement learning utilizing an automated theorem prover as the reward mechanism wherein an optimal policy is achieved when a minimum number of edits results in a logical sentence;
23. A method for reinforcement learning, comprising the steps of:
receiving one or more sentences;
performing actions from a set of available actions on a state wherein the state is a sentence; selecting an action to maximize the expected future value of a reward function; wherein the reward function depends on at least partly on: restoring the sentences to a logical state. providing the reinforcement learning agent with a pool of states, actions, and rewards and a function approximator wherein using the function approximator the reinforcement learning agent predicts the best action to take resulting in maximum reward;
24. The method of claim 23, wherein the logic engine consisting of a automated theorem prover that processes input sentences according to a set of logical equations derived from a discourse and the sentence, wherein the automated theorem prover takes the logical equations as the premise and infers a proof.
25. The system of claim 24, wherein the logic engine consisting of the automated theorem prover executes a logical equation derived from the sentence stored in memory against a set of logical equations derived from a discourse stored in memory on a processor and returns the state of the sentence as logical or illogical.
26. The system of claim 25, wherein the logical equations consist of negated
relationships that are determined by a symmetrical axis or a plurality of symmetrical axes in a network graph.
27. The system of claim 26, wherein the logical equations are categorized into assumptions and conclusions.
28. A real-time logic engine, comprising:
one or more sentences;
a physical hardware device consisting of a memory unit and processor;
a software consisting of a computer program or computer programs;
an output signal that indicates that one or more sentences is logical or one or more sentences is nonsensical;
wherein one or more processors; and
one or more programs residing on a memory and executable by the one or more
processors, the one or more programs configured to:
receive one or more sentences;
generate a network from the one or more sentences;
identify symmetry in the network; rank the symmetry in the network;
negate the ranked symmetry of the network into
logical relationships;
generate logical equations by formalizing logical relationships into a formal logic;
infer a proof by evaluating the logical equations with an automated theorem prover;
wherein modifications made to one or more sentences can be evaluated to determine if the modifications results in a logical or nonsensical sentence.
</claims>
</document>
