<document>

<filing_date>
2018-11-14
</filing_date>

<publication_date>
2020-05-14
</publication_date>

<priority_date>
2018-11-14
</priority_date>

<ipc_classes>
B25J9/16
</ipc_classes>

<assignee>
FETCH ROBOTICS
</assignee>

<inventors>
ELLIOTT, SARAH
KENT, DAVID
TORIS, RUSSELL
</inventors>

<docdb_family_id>
70551675
</docdb_family_id>

<title>
Method and system for selecting a preferred robotic grasp of an object-of-interest using pairwise ranking
</title>

<abstract>
A method for selecting a preferred robotic grasp of an object-of-interest using pairwise ranking includes: identifying a robotic candidate grasp usable to grasp the object-of-interest, the object-of-interest situated in an environment; receiving a grasp preference for a preferred candidate grasp of the object-of-interest; calculating a heuristic to describe a relationship of the candidate grasp to one or more of the object-of-interest and the environment; and using the heuristic and using the grasp preference, computing a pairwise ranking of two candidate grasps to determine an ordering of the at least two candidate grasps.
</abstract>

<claims>
We claim:
1. A method for selecting a preferred robotic grasp of an object-of-interest using pairwise ranking, comprising: identifying a robotic candidate grasp usable to grasp the object-of-interest, the object-of-interest situated in an environment; receiving a grasp preference for a preferred candidate grasp of the object-of-interest; calculating a heuristic to describe a relationship of the candidate grasp to one or more of the object-of-interest and the environment; and using the heuristic and using the grasp preference, computing a pairwise ranking of two candidate grasps.
2. The method of claim 1, wherein the robotic grasp is performed by a robotic manipulator.
3. The method of claim 2, wherein the robotic manipulator comprises one or more of a robotic hand and a vacuum gripper.
4. The method of claim 1, wherein the environment comprises one or more of a table and a wall.
5. The method of claim 1, wherein the identifying step comprises sampling the candidate grasp.
6. The method of claim 5, wherein the identifying step comprises using the sampling to identify the candidate grasp.
7. The method of claim 1, wherein the identifying step comprises excluding candidate grasps that do one or more of collide with the environment and come within a user-specified margin of colliding with the environment.
8. The method of claim 7, wherein the margin comprises one or more of a percentage and a distance.
9. The method of claim 1, wherein the identifying step comprises a sub-step of: receiving a grasp sampling strategy to be used in sampling.
10. The method of claim 9, wherein the sub-step of receiving the grasp sampling strategy comprises receiving the grasp sampling strategy from a user.
11. The method of claim 9, wherein the identifying step comprises using the grasp sampling strategy.
12. The method of claim 11, wherein the grasp sampling strategy comprises one or more of a height accumulated feature sampler, a handle-like sampler, and an antipodal grasp sampler.
13. The method of claim 12, wherein the antipodal grasp sampler comprises an antipodal grasp sampler included in an Agile grasp Robot Operating System (ROS) package.
14. The method of claim 1, wherein the receiving step comprises receiving the grasp preference from a user.
15. The method of claim 1, wherein the receiving step is performed by a graphical user interface (GUI).
16. The method of claim 15, wherein, using the GUI, the system generates a visualization of a set of candidate grasps.
17. The method of claim 16, wherein the system generates the visualization of the set of candidate grasps using the grasp sampling strategy.
18. The method of claim 17, wherein the system receives from the user a selection from the set of candidate grasps of a best candidate grasp of the object-of-interest in the environment.
19. The method of claim 18, wherein the method is conducted multiple times for at least one object in a trained object set.
20. The method of claim 19, wherein the method is conducted multiple times for each object in the trained object set.
21. The method of claim 1, wherein computing comprises finding a grasp characteristic describing a salient aspect of the grasp preference.
22. The method of claim 1, wherein the grasp characteristic comprises one or more of a preference to grasp a center of the object-of-interest, a preference to grasp a handle of the object-of-interest, a preference to grasp a side of the object-of-interest, a preference for a candidate grasp approximately perpendicular to a large surface in the environment, a preference to exclude a candidate grasp that does one or more of collide with the environment and come within a user-specified margin of colliding with the environment.
23. The method of claim 22, wherein the margin comprises one or more of a percentage and a distance.
24. The method of claim 1, wherein the heuristic describes a relationship of each candidate grasp to one of more of the object-of-interest and the environment.
25. The method of claim 1, wherein the heuristic describes a relationship of the candidate grasp to the object-of-interest, and wherein the heuristic further describes a relationship of the candidate grasp to the environment.
26. The method of claim 1, wherein the calculating step comprises calculating one or more of an object feature and an environment feature.
27. The method of claim 26, wherein the object feature comprises one or more of a size, a shape, a color, a surface texture, an object-of-interest label, an object-of-interest surface feature, a color histogram, a point feature histogram, and another feature.
28. The method of claim 27, wherein color is represented in the color space of the Commission Internationale de l'Eclairage Lab (CIELAB).
29. The method of claim 26, wherein the environment feature comprises one or more of a size, a shape, a color, a surface texture, a label of the environment, a surface feature of the environment, a color histogram, a point feature histogram, and another feature of the environment.
30. The method of claim 1, wherein the heuristic comprises an orientation of the candidate grasp relative to the object-of-interest.
31. The method of claim 1, wherein the heuristic comprises an orientation of the candidate grasp as the robot grasps the object-of-interest at a grasp point.
32. The method of claim 1, wherein the heuristic is scaled.
33. The method of claim 32, wherein the heuristic is scaled so that the heuristic is not less than zero, and wherein the heuristic is further scaled so that the heuristic is not greater than one.
34. The method of claim 1, wherein the heuristic comprises h1, wherein h1 is defined as a difference between the candidate grasp orientation and a normal to a principal plane of the environment.
35. The method of claim 34, wherein the principal plane of the environment is fit over PCE.
36. The method of claim 1, wherein the heuristic comprises h2, wherein h2 is defined as a difference between the candidate grasp orientation and a normal to a principal plane of the object-of-interest.
37. The method of claim 36, wherein the principal plane of the object-of-interest is fit over a local region of PCO.
38. The method of claim 37, wherein the local region has an approximate center at the grasp point.
39. The method of claim 1, wherein the heuristic comprises h3, wherein h3 is defined as a difference between the candidate grasp orientation and a principal axis of PCO.
40. The method of claim 1, wherein the heuristic comprises h4, wherein h4 is defined as a distance between the center of PCO and the grasp point.
41. The method of claim 1, wherein the heuristic comprises h5, wherein h5 is defined as a distance between the grasp point and the nearest point in PCO.
42. The method of claim 41, wherein candidate grasps centered on a point touching the object-of-interest are preferred over candidate grasps centered on a point not touching the object-of-interest.
43. The method of claim 1, wherein the heuristic comprises one or more of h1, h2, h3, h4, and h5, wherein h1 is defined as a difference between the candidate grasp orientation and a normal to a principal plane of the environment, wherein h2 is defined as a difference between the candidate grasp orientation and a normal to a principal plane of the object-of-interest, wherein h3 is defined as a difference between the grasp orientation and a principal axis of PCO, wherein h4 is defined as a distance between the center of PCO and the grasp point h5, and wherein h5 is defined as a distance between the grasp point and the nearest point in PCO.
44. The method of claim 43, wherein a combined heuristic hi calculates a quality of a candidate grasp i as a linear combination of heuristics h1, h2, h3, h4, and h5:
description="In-line Formulae" end="lead"?hi=w1hi1+w2hi2+w3hi3+w4hi4+w5hi5, (1)description="In-line Formulae" end="tail"? where 0≤wn≤1 and Σn wn=1, and where the weight wi represents an importance to the user of a heuristic hi.
45. The method of claim 1, wherein the computing step comprises producing a final ranked grasp list using a voting method in which pairwise rankings are tabulated and a highest rank candidate grasp comprises a candidate grasp that was a preferred candidate grasp in the highest number of pairwise rankings.
46. The method of claim 1, wherein the method comprises an additional step, performed before the calculating step, of: receiving context data regarding one or more of the object-of-interest and the environment.
47. The method of claim 46, wherein the step of receiving the context data comprises receiving the context data from the user.
48. The method of claim 46, wherein the step of receiving the context data comprises receiving one or more of the PCO and the PCE.
49. The method of claim 48, wherein the step of receiving the context data further comprises receiving the one or more of the PCO and the PCE from one or more of a stereo camera and a time-of-flight camera.
50. The method of claim 46, wherein the calculating step comprises using the context data to calculate the heuristic.
51. The method of claim 46, wherein the step of receiving the context data comprises receiving one or more of an object-of-interest point cloud (PCO) comprising one or more of an object point in the object-of-interest and an object color of the object-of-interest point, and an environment point cloud (PCE) comprising one or more of an environment point in the environment in which the object-of-interest is situated and an environment point color of the environment point.
52. The method of claim 1, wherein the calculating step comprises a sub-step of: representing an ith candidate grasp gi by an ith grasp vector xi={hi1, hi2, hi3, hi4, hi5}, wherein the ith grasp vector xi comprises a grasp characteristic of the ith candidate grasp gi in relation to one or more of the object-of-interest and the environment.
53. The method of claim 52, wherein the calculating step further comprises a sub-step of: representing a jth candidate grasp g; by a jth grasp vector xj={hj1, hj2, hj3, hj4, hj5}, wherein the jth grasp vector xj comprises a grasp characteristic of the jth candidate grasp gj in relation to one or more of the object-of-interest and the environment.
54. The method of claim 53, wherein the calculating step further comprises a sub-step, performed after both steps of representing candidate grasps by grasp vectors, of: computing a pairwise vector {circumflex over (x)}ij by taking a difference of the grasp vectors:
description="In-line Formulae" end="lead"?{circumflex over (x)}ij={xi−xj}. (2)description="In-line Formulae" end="tail"? wherein the pairwise vector {circumflex over (x)}ij indicates, for each of the five heuristics (h1, h2, h3, h4, and h5) how the candidate grasps differ.
55. The method of claim 54, wherein the calculating step further comprises, using the context data, computing from the object-of-interest point cloud PCO an object vector comprising object-of-interest features f0, f1, . . . , fn:
description="In-line Formulae" end="lead"?fij={f0, f1, . . . , fn} (3)description="In-line Formulae" end="tail"?
56. The method of claim 55, wherein the calculating step further comprises appending the pairwise vector {circumflex over (x)}ij to the object vector fij, to create a enhanced vector xij:
description="In-line Formulae" end="lead"?xij={f0, f1, . . . , fn, {circumflex over (x)}ij} (4)description="In-line Formulae" end="tail"? wherein the enhanced vector xij comprises information comparing two candidate grasps and relating them to the object-of-interest.
57. The method of claim 56, wherein the calculating step further comprises, using the grasp preference, creating a grasp preference label yij that denotes an ordering of the two candidate grasps gi and gj, wherein a grasp preference label yij of 1 indicates a grasp preference for gi over gj, and a grasp preference label yij of 0 indicates a grasp preference for gj over gi.
58. The method of claim 57, wherein the calculating step further comprises, using the enhanced vector xij, and using the grasp preference label yij, generating a training pair (xij, yij):
59. The method of claim 58, wherein the computing step further comprises using the training pair (xij, yij), training a binary classifier to predict a candidate grasp ordering.
60. The method of claim 1, further comprising an additional step, performed after the computing step, of: using the pairwise ranking, determining an ordering of the candidate grasps.
61. The method of claim 1, further comprising an additional step, performed after the computing step, of, using the trained binary classifier, predicting a pairwise ranking for a candidate grasp of a novel object.
62. The method of claim 61, wherein the predicting step comprises a sub-step of, using the candidate grasps, computing the enhanced vector xij.
63. The method of claim 62, wherein the predicting step comprises a sub-step, performed after the sub-step of computing the enhanced vector xij, using the trained binary classifier, predicting a grasp preference label yij for a candidate grasp of the novel object.
64. The method of claim 63, wherein the predicting step comprises a sub-step, performed after the sub-step of predicting the grasp preference label yij for a candidate grasp of the novel object, of, using the user grasp preference, predicting a preferred candidate grasp from new candidate grasps for which the user has not provided any grasp preference.
65. The method of claim 1, wherein the method comprises an additional step, performed after the computing step, of training a binary classifier to predict a grasp preference, thereby predicting a candidate grasp ordering.
66. The method of claim 65, wherein the binary classifier comprises one or more of a binary random forest classifier, a neural network classifier, and a k nearest neighbors classifier.
67. The method of claim 65, wherein the method comprises an additional step, performed after the training step, of, using a user grasp preference, predicting a preferred candidate grasp from new candidate grasps for which the user has not provided any grasp preference.
68. The method of claim 67, wherein the predicting step comprises producing a ranked candidate grasp list using a voting method in which pairwise rankings are tabulated and a highest rank candidate grasp comprises a candidate grasp that was a preferred candidate grasp in the highest number of pairwise rankings.
69. The method of claim 68, wherein the ranked candidate grasp list comprises feedback ranking data generated when a user performs one or more of selecting a candidate grasp, executing the candidate grasp, and ranking the candidate grasp, the feedback ranking data being applied to improve candidate grasp rankings.
70. The method of claim 69, wherein selecting the candidate grasp and ranking the candidate grasp are a combined single operation.
71. The method of claim 70, wherein the system is configured to suggest to the user a highest ranking candidate grasp.
72. The method of claim 1, wherein the system is configured to search using as a query the object-of-interest, returning as a search result a preferred candidate grasp.
73. The method of claim 46, wherein the step of receiving the context data is performed by a graphical user interface (GUI).
</claims>
</document>
