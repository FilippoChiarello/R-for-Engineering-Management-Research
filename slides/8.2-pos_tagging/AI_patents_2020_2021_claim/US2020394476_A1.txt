<document>

<filing_date>
2019-10-14
</filing_date>

<publication_date>
2020-12-17
</publication_date>

<priority_date>
2019-06-17
</priority_date>

<ipc_classes>
G06K9/54,G06K9/62,G06N3/04,G06T7/50
</ipc_classes>

<assignee>
HYUNDAI MOTOR COMPANY
KAIST (KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY)
KIA MOTORS CORPORATION
</assignee>

<inventors>
KIM, DO YEON
KIM, JUN MO
KIM, YANG SHIN
KIM, YOUNG HYUN
Jung, Hae Chang
Lee, Si Haeng
Lee, Jang Hyeon
Park, Min Woo
</inventors>

<docdb_family_id>
73546934
</docdb_family_id>

<title>
APPARATUS AND METHOD FOR RECOGNIZING OBJECT USING IMAGE
</title>

<abstract>
An apparatus for recognizing an object using an image includes a depth map generator that generates a depth map using a feature map of the image based on a dilated convolutional neural network (DCNN) and an object recognition device that recognizes the object using the depth map generated by the depth map generator and the image.
</abstract>

<claims>
1. An apparatus for recognizing an object using an image, the apparatus comprising: a depth map generator configured to generate a depth map using a feature map of the image based on a dilated convolutional neural network (DCNN); and an object recognition device configured to recognize the object using the depth map generated by the depth map generator and the image.
2. The apparatus of claim 1, further comprising: an input device configured to input a feature map of a red-green-blue (RGB) image to the depth map generator and input the RGB image to the object recognition device.
3. The apparatus of claim 2, wherein the input device includes: a first convolution module configured to generate a 16-channel feature map using the RGB image; a second convolution module configured to generate a 16-channel feature map using a gray image of the RGB image; and a concatenation module configured to generate a 32-channel feature map by concatenating the 16-channel feature map generated by the first convolution module and the 16-channel feature map generated by the second convolution module.
4. The apparatus of claim 3, wherein each of the first convolution module and the second convolution module uses a 3×3 filter.
5. The apparatus of claim 2, wherein the input device includes: a first convolution module configured to generate a 16-channel feature map using the RGB image; a second convolution module configured to generate an 8-channel feature map using a gray image of the RGB image; a third convolution module configured to generate an 8-channel feature map using a light detection and ranging (LiDAR) image; and a concatenation module configured to generate a 32-channel feature map by concatenating the 16-channel feature map generated by the first convolution module, the 8-channel feature map generated by the second convolution module, and the 8-channel feature map generated by the third convolution module.
6. The apparatus of claim 5, wherein each of the first convolution module, the second convolution module, and the third convolution module uses a 3×3 filter.
7. The apparatus of claim 1, wherein the depth map generator generates the depth map in a manner to gradually reduce resolution of the feature map and return the reduced resolution of the feature map.
8. The apparatus of claim 7, wherein the depth map generator applies a dilation rate corresponding to the resolution of the feature map.
9. The apparatus of claim 7, wherein the depth map generator reduces the resolution of the feature map by half.
10. The apparatus of claim 7, wherein the depth map generator includes a plurality of concatenation modules, each of which concatenates feature maps of the same channel in a process of gradually reducing the resolution of the feature map and returning the reduced resolution of the feature map.
11. A method for recognizing an object using an image, the method comprising: generating, by a depth map generator, a depth map using a feature map of the image based on a dilated convolutional neural network (DCNN); and recognizing, by an object recognition device, the object using the generated depth map and the image.
12. The method of claim 11, further comprising: inputting, by an input device, a feature map of a red-green-blue (RGB) image to the depth map generator; and inputting, by the input device, the RGB image to the object recognition device.
13. The method of claim 12, wherein the inputting includes: generating, by a first convolution module, a 16-channel feature map using the RGB image; generating, by a second convolution module, a 16-channel feature map using a gray image of the RGB image; and generating, by a concatenation module, a 32-channel feature map by concatenating the 16-channel feature map generated by the first convolution module and the 16-channel feature map generated by the second convolution module.
14. The method of claim 13, wherein each of the first convolution module and the second convolution module uses a 3×3 filter.
15. The method of claim 12, wherein the inputting includes: generating, by a first convolution module, a 16-channel feature map using the RGB image; generating, by a second convolution module, an 8-channel feature map using a gray image of the RGB image; generating, by a third convolution module, an 8-channel feature map using a light detection and ranging (LiDAR) image; and generating, by a concatenation module, a 32-channel feature map by concatenating the 16-channel feature map generated by the first convolution module, the 8-channel feature map generated by the second convolution module, and the 8-channel feature map generated by the third convolution module.
16. The method of claim 15, wherein each of the first convolution module, the second convolution module, and the third convolution module uses a 3×3 filter.
17. The method of claim 11, wherein the generating of the depth map includes: generating the depth map in a manner to gradually reduce resolution of the feature map and return the reduced resolution of the feature map.
18. The method of claim 17, wherein the generating of the depth map includes: applying a dilation rate corresponding to the resolution of the feature map.
19. The method of claim 17, wherein the generating of the depth map includes: reducing the resolution of the feature map by half.
20. The method of claim 17, wherein the generating of the depth map includes: concatenating feature maps of the same channel in the process of gradually reducing the resolution of the feature map and returning the reduced resolution of the feature map.
</claims>
</document>
