<document>

<filing_date>
2019-03-18
</filing_date>

<publication_date>
2020-09-24
</publication_date>

<priority_date>
2019-03-18
</priority_date>

<ipc_classes>
G06F7/483,G06N3/04,G06N3/063
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
ZHAO, RITCHIE
AKHLAGHI, Vahideh
CHUNG, Eric S.
OVTCHAROV, Kalin
</inventors>

<docdb_family_id>
70166157
</docdb_family_id>

<title>
QUANTIZATION-AWARE NEURAL ARCHITECTURE SEARCH
</title>

<abstract>
Quantization-aware neural architecture search ("QNAS") can be utilized to learn optimal hyperparameters for configuring an artificial neural network ("ANN") that quantizes activation values and/or weights. The hyperparameters can include model topology parameters, quantization parameters, and hardware architecture parameters. Model topology parameters specify the structure and connectivity of an ANN. Quantization parameters can define a quantization configuration for an ANN such as, for example, a bit width for a mantissa for storing activation values or weights generated by the layers of an ANN. The activation values and weights can be represented using a quantized-precision floating-point format, such as a block floating-point format ("BFP") having a mantissa that has fewer bits than a mantissa in a normal-precision floating-point representation and a shared exponent.
</abstract>

<claims>
1. A computer-implemented method, comprising: generating, by way of a recurrent neural network (RNN), hyperparameters for a child neural network, the hyperparameters comprising model topology parameters and quantization parameters for an artificial neural network (ANN); configuring the child neural network according to the hyperparameters; training the child neural network on a training data set; following training of the child neural network, computing one or more metrics for the child neural network; and performing reinforcement learning to train the RNN to generate second hyperparameters for a second child network using the one or more metrics as a reward signal for the RNN, the second hyperparameters comprising second quantization parameters.
2. The computer-implemented method of claim 1, wherein the model topology parameters define a number of filters for the ANN.
3. The computer-implemented method of claim 1, wherein the model topology parameters define a number of layers for the ANN.
4. The computer-implemented method of claim 1, wherein the quantization parameters define a bit width for a mantissa for storing activation values generated by layers of the ANN.
5. The computer-implemented method of claim 4, wherein the activation values are represented in a block floating-point format (BFP) comprising a mantissa having fewer bits than a mantissa in a normal-precision floating-point representation and a shared exponent.
6. The computer-implemented method of claim 1, wherein the quantization parameters define a bit width for a mantissa for storing weights for the ANN.
7. The computer-implemented method of claim 6, wherein the weights are represented in a block floating-point format (BFP) having a mantissa comprising fewer bits than a mantissa in a normal-precision floating-point representation and a shared exponent.
8. The computer-implemented method of claim 1, wherein the one or more metrics comprise one or more of accuracy, inference time, or cost.
9. The computer-implemented method of claim 1, wherein the hyperparameters further comprise hardware architecture parameters for configuring a field programmable gate array (FPGA).
10. A computer-implemented method, comprising: generating, by way of a recurrent neural network (RNN), hyperparameters for a child neural network, the hyperparameters comprising quantization parameters for an artificial neural network (ANN); configuring the child neural network according to the hyperparameters; training the child neural network on a training data set; following training of the child neural network, computing one or more metrics for the child neural network; and performing reinforcement learning to train the RNN to generate second hyperparameters for a second child network using the one or more metrics as a reward signal for the RNN, the second hyperparameters comprising second quantization parameters.
11. The computer-implemented method of claim 10, wherein the hyperparameters further comprise model topology parameters, and wherein the model topology parameters define a number of layers or a number of filters for the artificial neural network (ANN).
12. The computer-implemented method of claim 10, wherein the quantization parameters define a bit width for a mantissa for storing activation values generated by layers of the ANN or a bit width for a mantissa for storing weights for the ANN.
13. The computer-implemented method of claim 12, wherein the activation values and the weights are represented in a block floating-point format (BFP) having a mantissa comprising fewer bits than a mantissa in a normal-precision floating-point representation and a shared exponent.
14. The computer-implemented method of claim 10, wherein the one or more metrics comprise one or more of accuracy, inference time, or cost.
15. The computer-implemented method of claim 10, wherein the hyperparameters further comprise hardware architecture parameters for configuring a field programmable gate array (FPGA).
16. A computing device, comprising: one or more processors; and at least one computer storage media having computer-executable instructions stored thereupon which, when executed by the one or more processors, will cause the computing device to: generate, by way of a recurrent neural network (RNN), hyperparameters for a child neural network, the hyperparameters comprising model topology parameters and quantization parameters for an artificial neural network (ANN); configure the child neural network according to the hyperparameters; train the child neural network on a training data set; following training of the child neural network, compute one or more metrics for the child neural network; and perform reinforcement learning to train the RNN to generate second hyperparameters for a second child network using the one or more metrics as a reward signal for the RNN, the second hyperparameters comprising second quantization parameters.
17. The computing device of claim 16, wherein the quantization parameters define a bit width for a mantissa for storing activation values generated by layers of the ANN or a bit width for a mantissa for storing weights for the ANN.
18. The computing device of claim 17, wherein the activation values and the weights are represented in a block floating-point format (BFP) having a mantissa comprising fewer bits than a mantissa in a normal-precision floating-point representation and a shared exponent.
19. The computing device of claim 16, wherein the model topology parameters define a number of layers or a number of filters for the artificial neural network (ANN).
20. The computing device of claim 16, wherein the hyperparameters further comprise hardware architecture parameters for configuring a field programmable gate array (FPGA).
</claims>
</document>
