<document>

<filing_date>
2020-07-22
</filing_date>

<publication_date>
2021-01-28
</publication_date>

<priority_date>
2019-07-22
</priority_date>

<ipc_classes>
G06N20/20,G06T7/50,G06T7/73
</ipc_classes>

<assignee>
RAYTHEON COMPANY
</assignee>

<inventors>
Bongio Karrman, Anton
Fan, Ryan C.
</inventors>

<docdb_family_id>
71996071
</docdb_family_id>

<title>
CROSS-MODALITY IMAGE GENERATION
</title>

<abstract>
Systems, devices, methods, and computer-readable media for image processing by machine learning are provided. A method includes providing, as input to a first machine learning (ML) model trained based on image and corresponding depth data, data of a first image, the first image captured by a sensor of a first modality. The method includes receiving, from the ML model, an estimated depth per pixel of the first image. The method includes providing, as input to a second ML model trained based on the first image and a loss in constructing an estimated second image in a second modality, the first image and receiving, from the second ML model, estimated transformation parameters that transform the first image from the first modality to the second modality. The method includes using the estimated transformation parameters and the estimated depth to generate an estimated second image in the second modality.
</abstract>

<claims>
1. A method comprising: providing, as input to a first machine learning (ML) model trained based on image and corresponding depth data, data of a first image, the first image captured by a sensor of a first modality; receiving, from the ML model, an estimated depth per pixel of the first image; providing, as input to a second ML model trained based on the first image and a loss in constructing an estimated second image in a second modality, the first image; receiving, from the second ML model, estimated transformation parameters that transform the first image from the first modality to the second modality; and using the estimated transformation parameters and the estimated depth to generate an estimated second image in the second modality.
2. The method of claim 1, further comprising: providing, as input to a third ML model trained based on pairs of images of a same geographic area each pair of the pairs of images including the first image and a second image captured by a sensor of the second modality, the first image and the estimated second image; and receiving an estimated camera pose difference between the first image and the estimated second image, wherein the estimated second image is generated further using the estimated camera pose difference.
3. The method of claim 1, wherein the sensor of the first modality is a visible band sensor or an infrared sensor.
4. The method of claim 3, wherein the sensor of the second modality is an infrared sensor or a visible band sensor.
5. The method of claim 1, wherein training the first ML model or the second ML model includes using a differences between the second image and the estimated second image as the loss.
6. The method of claim 1, wherein the first image is captured at a first time and the second image is captured at a second time after the first time.
7. The method of claim 6, wherein the first time corresponds to a frame immediately preceding the second image.
8. A non-transitory machine-readable medium including instructions that, when executed by a machine, cause the machine to perform operations for cross-modality image generation, the operations comprising: providing, as input to a first machine learning (ML) model trained based on image and corresponding depth data, data of a first image, the first image captured by a sensor of a first modality; receiving, from the ML model, an estimated depth per pixel of the first image; providing, as input to a second ML model trained based on the first image and a loss in constructing an estimated second image in a different second modality, the first image; receiving, from the second ML model, estimated transformation parameters that transform the first image from the first modality to the second modality; and using the estimated transformation parameters and the estimated depth to generate an estimated second image in the second modality.
9. The non-transitory machine-readable medium of claim 8, wherein the operations further comprise: providing, as input to a third ML model trained based on pairs of images of a same geographic area each pair of the pairs of images including the first image and a second image captured by a sensor of the second modality, the first image and the estimated second image; and receiving an estimated camera pose difference between the first image and the estimated second image, wherein the estimated second image is generated further using the estimated camera pose difference.
10. The non-transitory machine-readable medium of claim 8, wherein the sensor of the first modality is a visible band sensor or an infrared sensor.
11. The non-transitory machine-readable medium of claim 10, wherein the sensor of the second modality is an infrared sensor or a visible band sensor.
12. The non-transitory machine-readable medium of claim 8, wherein training the first ML model or the second ML model includes using a differences between the second image and the estimated second image as the loss.
13. The non-transitory machine-readable medium of claim 8, wherein the first image is captured at a first time and the second image is captured at a second time after the first time.
14. The non-transitory machine-readable medium of claim 13, wherein the first time corresponds to a frame immediately preceding the second image.
15. A system comprising: processing circuitry; at least one memory including parameters of a first and second machine learning (ML) models and instructions stored thereon, the first ML model trained based on image and corresponding depth data, the second ML model trained based on the first image and a loss in constructing an estimated second image in a different second modality, the instructions, when executed by the processing circuitry, cause the processing circuitry to perform operations for cross-modality image generation, the operations comprising: providing, as input to the first ML model, data of a first image, the first image captured by a sensor of the first modality; receiving, from the ML model, an estimated depth per pixel of the first image; providing, as input to the second ML model, the first image; receiving, from the second ML model, estimated transformation parameters that transform the first image from the first modality to the second modality; and using the estimated transformation parameters and the estimated depth to generate an estimated second image in the second modality.
16. The system of claim 15, wherein the operations further comprise: providing, as input to a third ML model trained based on pairs of images of a same geographic area each pair of the pairs of images including the first image and a second image captured by a sensor of the second modality, the first image and the estimated second image; and receiving an estimated camera pose difference between the first image and the estimated second image, wherein the estimated second image is generated further using the estimated camera pose difference.
17. The system of claim 15, wherein the sensor of the first modality is a visible band sensor or an infrared sensor.
18. The system of claim 17, wherein the sensor of the second modality is an infrared sensor or a visible band sensor.
19. The system of claim 15, wherein training the first ML model or the second ML model includes using a differences between the second image and the estimated second image as the loss.
20. The system of claim 15, wherein the first image is captured at a first time and the second image is captured at a second time after the first time.
</claims>
</document>
