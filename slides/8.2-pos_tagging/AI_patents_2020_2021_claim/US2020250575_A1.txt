<document>

<filing_date>
2019-02-28
</filing_date>

<publication_date>
2020-08-06
</publication_date>

<priority_date>
2019-02-06
</priority_date>

<ipc_classes>
G06N20/00,G06N5/04
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
BOUTILIER, CRAIG EDGAR
IE, TZE WAY EUGENE
NARVEKAR, SANMIT SANTOSH
</inventors>

<docdb_family_id>
67690307
</docdb_family_id>

<title>
Systems and Methods for Simulating a Complex Reinforcement Learning Environment
</title>

<abstract>
A computing system for simulating allocation of resources to a plurality of entities is disclosed. The computing system can be configured to input an entity profile that describes a preference and/or demand of a simulated entity into a reinforcement learning agent model and receive, as an output of the reinforcement learning agent model, an allocation output that describes a resource allocation for the simulated entity. The computing system can select one or more resources based on the resource allocation described by the allocation output and provide the resource(s) to an entity model that is configured to simulate a simulated response output that describes a response of the simulated entity. The computing system can receive, as an output of the entity model, the simulated response output and update a resource profile that describes the at least one resource and/or the entity profile based on the simulated response output.
</abstract>

<claims>
1. A computing system for simulating allocation of resources to a plurality of entities, the computing system comprising: one or more processors; a reinforcement learning agent model configured to receive an entity profile that describes at least one of a preference or a demand of a simulated entity, and in response to receiving the entity profile, output an allocation output that describes a resource allocation for the simulated entity of the plurality of entities; an entity model configured to receive data descriptive of at least one resource, and in response to receiving the data descriptive of the at least one resource, simulate a simulated response output that describes a response of the simulated entity to the data descriptive of the at least one resource; one or more non-transitory computer-readable media that collectively store instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising: inputting the entity profile into the reinforcement learning agent model; receiving, as an output of the reinforcement learning agent model, the allocation output that describes the resource allocation for the simulated entity; selecting the at least one resource to provide to the entity model based on the resource allocation described by the allocation output; providing the at least one resource to the entity model; receiving, as an output of the entity model, the simulated response output that describes the response of the simulated entity to the at least one resource; and updating at least one of a resource profile that describes the at least one resource or the entity profile based on the simulated response output.
2. The computing system of claim 1, wherein the reinforcement learning agent model comprises a reinforcement learning agent that is learned based on a reward that is a function of the simulated response output.
3. The computing system of claim 1, wherein: the simulated entity comprises at least one of a computing task or a source of the computing task; and the at least one resource comprises a worker configured to execute the computing task.
4. The computing system of claim 1, wherein: the simulated entity comprises an industrial process; and the at least one resource comprises an input to the industrial process.
5. The computing system of claim 1, wherein the simulated entity comprises a simulated human user, and the entity profile comprises a user profile that describes at least one of interests or preferences of the simulated human user.
6. The computing system of claim 5, wherein the simulated response output describes an engagement metric that describes at least one of an interaction time, a consumption amount, a number of engagements, or a rating of the simulated human user with respect to the at least one resource.
7. The computing system of claim 1, wherein updating at least one of the resource profile or the entity profile based on the simulated response output comprises providing data that describes the simulated response output to a user transition model that generates an updated set of user hidden state features and updating the entity profile based on the user hidden state features.
8. The computing system of claim 1, wherein the at least one resource comprises at least one document that comprises at least one of text, audio, video, or graphical content.
9. The computing system of claim 1, further comprising a resource model configured to receive data descriptive of a plurality of resources including the at least one resource, and in response to receiving the data descriptive of the plurality of resources, output resource observable features, and wherein the reinforcement learning agent model is trained to select the allocation output based, at least in part, on the resource observable features, and wherein the operations further comprise: inputting the data descriptive of the plurality of resources into the resource model; receiving, as an output of the resource model, resource observable features; and inputting the resource observable features into the reinforcement learning agent model.
10. The computing system of claim 1, wherein: the at least one resource comprises a plurality of resource items; and the simulated response output describes a selection of fewer than all of the plurality of resource items.
11. The computing system of claim 10, wherein the entity model comprises a discrete choice model.
12. The computing system of claim 11, wherein the discrete choice model comprises at least one a multinomial proportion function, multinomial logit function, or exponential cascade function.
13. A method for simulating allocation of resources to a plurality of entities, the method comprising: inputting, by one or more computing devices, an entity profile that describes at least one of a preference or a demand of a simulated entity into a reinforcement learning agent model, the reinforcement learning agent model configured to receive the entity profile, and in response to receiving the entity profile, output an allocation output that describes a resource allocation for the simulated entity; receiving, by the one or more computing devices, as an output of the reinforcement learning agent model, the allocation output that describes the resource allocation for the simulated entity; selecting, by the one or more computing devices, at least one resource to simulate providing to an entity model based on the resource allocation described by the allocation output, the entity model being configured to receive data descriptive of the at least one resource, and in response to receiving the data descriptive of the at least one resource, simulate a simulated response output that describes a response of the simulated entity to the data descriptive of the at least one resource; providing, by the one or more computing devices, data descriptive of the at least one resource to an entity model; receiving, by the one or more computing devices, as an output of the entity model, the simulated response output that describes the response of the simulated entity to the at least one resource; and updating, by the one or more computing devices, at least one of a resource profile that describes the at least one resource or the entity profile based on the simulated response output.
14. The method of claim 13, wherein the agent model comprises a reinforcement learning agent that is learned based on a reward that is a function of the simulated response output.
15. The method of claim 13, wherein the simulated entity comprises a simulated human user, and the entity profile comprises a user profile that describes at least one of interests or preferences of the simulated human user.
16. The method of claim 13, wherein the simulated response output describes an engagement metric that describes at least one of an interaction time, a consumption amount, a number of engagements, or a rating of the simulated human user with respect to the at least one resource.
17. The method of claim 13, wherein updating, by the one or more computing devices, at least one of the resource profile or the entity profile based on the simulated response output comprises providing, by the one or more computing devices, data that describes the simulated response output to a user transition model that generates an updated set of user hidden state features and updating, by the one or more computing devices, the entity profile based on the user hidden state features.
18. The method of claim 13, wherein the at least one resource comprises at least one document that comprises at least one of text, audio, video, or graphical content.
19. The method of claim 13, further comprising: inputting, by the one or more computing devices, the data descriptive of the plurality of resources into a resource model that is configured to receive data descriptive of a plurality of resources including the at least one resource, and in response to receiving the data descriptive of the plurality of resources, output resource observable features; receiving, by the one or more computing devices, as an output of the resource model, resource observable features; and inputting, by the one or more computing devices, the resource observable features into the reinforcement learning agent model, wherein the reinforcement learning agent model is trained to select the allocation output based, at least in part, on the resource observable features.
20. The method of claim 13, further comprising receiving, by one or more computing devices, the reinforcement learning agent model before inputting, by the one or more computing devices, the entity profile into the reinforcement learning agent model.
</claims>
</document>
