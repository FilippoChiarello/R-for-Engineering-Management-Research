<document>

<filing_date>
2018-01-18
</filing_date>

<publication_date>
2020-10-29
</publication_date>

<priority_date>
2017-12-29
</priority_date>

<ipc_classes>
G06F9/30,G06F9/38,G06N3/063
</ipc_classes>

<assignee>
NATIONZ TECHNOLOGIES
</assignee>

<inventors>
XIE, HUA
LUO, Cong
WAN, Wentao
LIANG, Jie
</inventors>

<docdb_family_id>
62586926
</docdb_family_id>

<title>
Convolutional Neural Network Hardware Acceleration Device, Convolutional Calculation Method, and Storage Medium
</title>

<abstract>
A convolutional neural network hardware acceleration device, a convolutional calculation method, and a storage medium are provided. The device includes an instruction processing element (1), a hardware acceleration component (2), and an external data memory element (3). The instruction processing element (1) decodes instructions, and controls the hardware acceleration component (2) to perform operations corresponding to decoded instructions. The hardware acceleration component (2) includes: an input buffer element (21), a data calculation element (22) and an output buffer element (23). The external data memory element (3) stores the calculation result output by the output buffer element (23) and transmits the data to the input buffer element (21).
</abstract>

<claims>
1. A convolutional neural network hardware acceleration device, comprising an instruction processing element, a hardware acceleration component, and an external data memory element, wherein the instruction processing element is configured to decode instructions, and control the hardware acceleration component to perform operations corresponding to decoded instructions; the hardware acceleration component comprises an input buffer element, an output buffer element, and a data calculation element; the input buffer element is configured to perform a memory access operation controlled by the instruction processing element to store data read from the external data memory element; the data calculation element is configured to perform an operational execution operation controlled by the instruction processing element to process a data calculation of an inference part of a convolutional neural network, and control a data calculation process and a data flow direction according to calculation instructions; the output buffer element is configured to perform a memory access operation controlled by the instruction processing element to store a calculation result which is to be written into the external data memory element and is output by the data calculation element; and the external data memory element is configured to store the calculation result output by the output buffer element and transmit the data to the input buffer element.
2. The device as claimed in claim 1, wherein the instruction processing element comprises: an instruction queue component, configured to store the instructions; an instruction fetch component, configured to sequentially fetch the instructions in the instruction queue component; a decoding component, configured to decode the instructions fetched from the instruction fetch component; an instruction sequence component, configured to buffer the instructions decoded by the decoding component; and an execution component, configured to execute the decoded instructions buffered by the instruction sequence component to control the hardware acceleration component to perform corresponding operations.
3. The device as claimed in claim 1, wherein the data calculation element comprises at least one calculation element, and each calculation element is configured to select calculation operations containing at least one of multiply-accumulation, addition, subtraction, multiplication, and division according to an instruction decoding result.
4. The device as claimed in claim 3, wherein the calculation element comprises a multiplier or multiply accumulator, an adder, a subtractor, a divider, a comparator, and an output data selection element, wherein the multiplier or multiply accumulator is configured to implement a multiply-accumulate or multiply operation, and the multiplier or multiply-accumulator comprises a temporary storage register for storing a temporary value after the multiply-accumulate or multiply operation; the adder is configured to implement an addition operation, and the adder comprises a temporary storage register for storing a temporary value after the addition operation; the subtractor is configured to implement a subtraction operation, and the subtractor comprises a temporary storage register for storing a temporary value after the subtraction operation; the divider is configured to implement a division operation, and the divider comprises a temporary storage register for storing a temporary value after the division operation; the comparator is configured to implement a comparison operation, and the comparator comprises a temporary storage register for storing a temporary value after the comparison operation; and the output data selection element is configured to select and output data of one of the multiplier or multiply accumulator, the adder, the subtractor, the divider, and the comparator according to an output data selection signal of a decoding result obtained by the instruction processing element.
5. A method for convolutional calculation using the convolutional neural network hardware acceleration device as claimed in claim 1, comprising the following steps: Step S1, decoding, by an instruction processing element, instructions, and sending, by the instruction processing element, decoded instructions to a hardware acceleration component; Step S2, reading, by an input buffer element, weight data and input data from an external data memory element according to the decoded instructions; Step S3, transporting, by the input buffer element, the input data to one end of a selected operator of a calculation element and the weight data to the other end of the selected operator according to a selection signal of the calculation element; Step S4, performing, by a data calculation element, a calculation operation according to the decoded instructions, adding, by the data calculation element, a calculation result of each selected operator to data stored in a temporary storage register of this selected operator to obtain an addition result, and storing, by the data calculation element, the addition result to the corresponding temporary storage register, Step S5, after the calculation operation performed each selected operator is completed, storing, by an on-chip storage element, a result of one feature point of an output feature map on one channel obtained by the temporary storage register at a corresponding address, and after the calculation operation of each selected operator is completed, a convolutional calculation between a feature map and a weight of a channel is completed; and Step S6, when data of all channels is calculated and stored on the on-chip storage element after the operation of all calculation elements is completed, a convolutional operation of multi-channel data and a convolution sum is completed, and storing, by the data calculation element, output data to an external data memory element through an output buffer element.
6. The method as claimed in claim 5, wherein in Step S1, a manner of decoding the instructions by the instruction processing element comprises: storing, by an instruction queue component, the instructions; sequentially fetching, by an instruction fetch component, an instruction combination in the instruction queue component according to content of convolutional neural calculation; decoding, by a decoding component, the instruction combination fetched from the instruction fetch component; buffering, by an instruction sequence component, decoded instructions in the instruction combination decoded by the decoding component; and sequentially executing, by an execution component, the decoded instructions buffered by the instruction sequence component, and controlling, by the execution component, the hardware acceleration component to perform operations corresponding to the decoded instructions.
7. The method as claimed in claim 5, wherein in Step S1, the instructions are obtained from an instruction set, the instruction set comprising: data interaction instructions, which are used for performing data transportation operations comprising data reading, data storage, and data transfer and assignment in a device; data calculation instructions, which are used for performing data parallel batch calculation operations comprising multiply-accumulation, addition, subtraction, multiplication, and division in a data convolutional calculation process; control logic instructions, which are used for performing control logic operations to control a data calculation process, wherein the data interaction instructions, the data calculation instructions, and the control logic instructions form different simple instruction combinations according to different convolutional neural calculations, so as to control the hardware acceleration component performs convolutional calculation.
8. The method as claimed in claim 5, wherein in Step S1, the instruction processing element sends a current instruction, when the current instruction has a dependency relationship with a previous instruction, the current instruction is performed after the previous instruction has been executed, when the instruction queue component is fully buffered, the instruction queue component returns a full writing status to, forbid the instruction fetch component from continuously reading any subsequent instruction, and the instruction fetch component continuously reads the subsequent instruction after the instructions in the instruction queue component are executed; each instruction in the instruction processing element in Step S1 at least comprises one operation code, one status register, and one operation domain, wherein the operation code is used for indicating different kinds of operations; the status register is configured to indicate different operation modes under the same operation code; the operation domain is represented by an immediate value or a register, and the operation code, the status register, and the operation domain are sequentially coded into one instruction.
9. A computer-readable storage medium, storing a processor program that, when executed by a computer, implements the following steps: Step S1, decoding, by an instruction processing element, instructions, and sending, by the instruction processing element, decoded instructions to a hardware acceleration component; Step S2, reading, by an input buffer element, weight data and input data from an external data memory element according to the decoded instructions; Step S3, transporting, by the input buffer element, the input data to one end of a selected operator of a calculation element and the weight data to the other end of the selected operator according to a selection signal of the calculation element; Step S4, performing, by a data calculation element, a calculation operation according to the decoded instructions, adding, by the data calculation element, a calculation result of each selected operator to data stored in a temporary storage register of this selected operator to obtain an addition result, and storing, by the data calculation element, the addition result to the corresponding temporary storage register, Step S5, after the calculation operation performed each selected operator is completed, storing, by an on-chip storage element, a result of one feature point of an output feature map on one channel obtained by the temporary storage register at a corresponding address, and after the calculation operation of each selected operator is completed, a convolutional calculation between a feature map and a weight of a channel is completed; and Step S6, when data of all channels is calculated and stored on the on-chip storage element after the operation of all calculation elements is completed, a convolutional operation of multi-channel data and a convolution sum is completed, and storing, by the data calculation element, output data to an external data memory element through an output buffer element.
10. The computer-readable storage medium as claimed in claim 9, wherein in Step S1, a manner of decoding the instructions by the instruction processing element comprises: storing, by an instruction queue component, the instructions; sequentially fetching, by an instruction fetch component, an instruction combination in the instruction queue component according to content of convolutional neural calculation; decoding, by a decoding component, the instruction combination fetched from the instruction fetch component; buffering, by an instruction sequence component, decoded instructions in the instruction combination decoded by the decoding component; and sequentially executing, by an execution component, the decoded instructions buffered by the instruction sequence component, and controlling, by the execution component, the hardware acceleration component to perform operations corresponding to the decoded instructions, wherein in Step S1, the instruction processing element sends a current instruction, when the current instruction has a dependency relationship with a previous instruction, the current instruction is performed after the previous instruction has been executed, when the instruction queue component is fully buffered, the instruction queue component returns a full writing status to, forbid the instruction fetch component from continuously reading any subsequent instruction, and the instruction fetch component continuously reads the subsequent instruction after the instructions in the instruction queue component are executed; in Step S1, the instructions are obtained from an instruction set, the instruction set comprising: data interaction instructions, which are used for performing data transportation operations comprising data reading, data storage, and data transfer and assignment in a device; data calculation instructions, which are used for performing data parallel batch calculation operations comprising multiply-accumulation, addition, subtraction, multiplication, and division in a data convolutional calculation process; control logic instructions, which are used for performing control logic operations to control a data calculation process, wherein the data interaction instructions, the data calculation instructions, and the control logic instructions form different simple instruction combinations according to different convolutional neural calculations, so as to control the hardware acceleration component performs convolutional calculation; each instruction in the instruction processing element in Step S1 at least comprises one operation code, one status register, and one operation domain, wherein the operation code is used for indicating different kinds of operations; the status register is configured to indicate different operation modes under the same operation code; the operation domain is represented by an immediate value or a register; and the operation code, the status register, and the operation domain are sequentially coded into one instruction.
</claims>
</document>
