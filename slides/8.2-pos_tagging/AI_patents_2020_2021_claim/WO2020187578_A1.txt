<document>

<filing_date>
2020-03-05
</filing_date>

<publication_date>
2020-09-24
</publication_date>

<priority_date>
2019-03-21
</priority_date>

<ipc_classes>
G03F1/36,G03F7/20,G06N20/10
</ipc_classes>

<assignee>
ASML NETHERLANDS
</assignee>

<inventors>
MOON, Jaiin
</inventors>

<docdb_family_id>
69784421
</docdb_family_id>

<title>
TRAINING METHOD FOR MACHINE LEARNING ASSISTED OPTICAL PROXIMITY ERROR CORRECTION
</title>

<abstract>
Described herein is a method of determining representative patterns for training a machine learning model to predict optical proximity corrections. The method includes obtaining a design layout comprising a set of group of patterns, each group of patterns includes one or more sub-groups; determining a set of representative patterns of the set of group of patterns, a representative pattern being a sub-group whose instances appear in the set of group patterns; obtaining, via simulating an optical proximity correction process using the set of representative patterns, reference optical proximity correction data associated with the set of representative patterns; and training a machine learning model to predict optical proximity corrections for the design layout based on the set of representative patterns and the set of reference optical proximity correction data.
</abstract>

<claims>
1. A method of determining representative patterns for training a machine learning model to predict optical proximity corrections, the method comprising:
obtaining a design layout comprising a set of group of patterns, each group of patterns includes one or more sub-groups;
determining a set of representative patterns of the set of group of patterns, a representative pattern being a sub-group whose instances appear in the set of group patterns;
obtaining, via simulating an optical proximity correction process using the set of
representative patterns, reference optical proximity correction data associated with the set of representative patterns; and
training a machine learning model to predict optical proximity corrections for the design layout based on the set of representative patterns and the set of reference optical proximity correction data.
2. The method of claim 1, wherein the group of patterns are arranged in a hierarchy in which each group of patterns includes the one or more sub-groups of pattern.
3. The method of claim 2, wherein the determining the set of representative patterns is an iterative process, an iteration comprises:
searching for the instances of a given sub-group of pattern within the hierarchy of the set of group of patterns;
categorizing the instances of the given sub-group as the representative pattern; and extracting, from the design layout, pattern information associated with the representative pattern.
4. The method of claim 1 , wherein each group of the set of group of patterns is associated with a first identifier, and the one or more sub-group of patterns is associated with a second identifier.
5. The method of claim 4, wherein the determining the set of representative patterns comprises:
comparing the second identifier associated with the given sub-group with identifiers within the hierarchy of each group of the set of patterns; and
identifying, based on the comparison, instances of sub-groups of patterns within the set of group of patterns having the same second identifier; and
categorizing the instances of the given sub-group as the representative pattern.
6. The method of claim 1 , wherein the obtaining of the reference optical proximity correction comprises:
simulating the optical proximity correction process using the pattern information associated with the representative pattern; and
providing the optical proximity corrections associated with the representative pattern for a patterning process.
7. The method of claim 1, wherein the searching for the sub-group of pattern does not directly compare the given sub-group's pattern shapes and sizes with pattern shapes and sizes within the set of group of patterns.
8. The method of claim 1, wherein the optical proximity corrections comprise placement of assist features associated with a desired pattern of the design layout.
9. The method of claim 1, wherein the optical proximity corrections are in the form of images and the training is based on the images or pixel data of the images.
10. The method of claim 9, wherein the images are continuous transmission mask (CTM) images, and/or assist feature guidance maps, wherein the CTM images and the guidance maps provide locations of assist feature associated with the set of representative patterns, and/or
wherein the assist feature guidance maps are generated by model-based OPC simulation, or rule -based OPC simulation.
11. The method of claim 1 , further comprising:
determining, via the trained machine learning model, mask pattern data associated with a given design layout;
outputting the mask pattern data to be used in a patterning process to image a substrate.
12. The method of claim 11, further comprising:
fabricating, via a mask-making apparatus using the mask pattern data, a mask to be used in the patterning process to image the substrate, and/or
wherein the mask pattern data comprises characteristics upon which the patterning process adjusts one or more of process parameters including dose, focus, illumination intensity, and/or illumination pupil.
13. The method of claim 1, further comprising:
determining, via simulating a process model in cooperation with the training machine learning model, a process condition associated with a desired pattern of the given design layout; and exposing, via a lithographic apparatus configured according to the process condition employing a mask corresponding to the design layout, a substrate.
14. The method of claim 13, wherein the process condition comprises values of one or more of process parameters including dose, focus, illumination intensity, and/or illumination pupil.
15. A computer program product comprising a non-transitory computer readable medium having instructions recorded thereon, the instructions when executed by a computer implementing the method of claim 1.
</claims>
</document>
