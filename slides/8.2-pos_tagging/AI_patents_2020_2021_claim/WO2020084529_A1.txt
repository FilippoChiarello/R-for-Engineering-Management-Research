<document>

<filing_date>
2019-10-23
</filing_date>

<publication_date>
2020-04-30
</publication_date>

<priority_date>
2018-10-23
</priority_date>

<ipc_classes>
G06N20/00,G16H50/20
</ipc_classes>

<assignee>
MMODAL IP
</assignee>

<inventors>
POLZIN, THOMAS S.
</inventors>

<docdb_family_id>
70279663
</docdb_family_id>

<title>
APPLYING MACHINE LEARNING TO SCRIBE INPUT TO IMPROVE DATA ACCURACY
</title>

<abstract>
A computerized system learns a mapping from the speech of a physician and patient in a physician-patient encounter to discrete information to be input into the patient's Electronic Medical Record (EMR). The system learns this mapping based on a transcript of the physician-patient dialog, an initial state of the EMR (before the EMR was updated based on the physician-patient dialogue), and a final state of the EMR (after the EMR was updated based on the physician-patient dialog). The learning process is enhanced by taking advantage of knowledge of the differences between the initial EMR state and the final EMR state.
</abstract>

<claims>
1. A method performed by at least one computer processor executing computer program instructions tangibly stored on at least one non-transitory computer-readable medium, the method comprising, at a transcription job routing engine:
(A) saving an initial state of an electronic medical record (EMR) of a first person;
(B) saving a final state of the EMR of the first person after the EMR of the first person has been modified based on speech of the first person and speech of a second person;
(C) identifying differences between the initial state of the EMR of the third person and the final state of the EMR of the third person; and
(D) applying a machine learning module to: (1) a transcript of the speech of the first person and the speech of the second person; and (2) the differences between the initial state of the EMR of the first person and the final state of the EMR of the first person, to generate a mapping between: (a) the transcript of the speech of the first person and the speech of the second person; and (b) the differences between the initial state of the EMR and the final state of the EMR.
2. The method of claim 1, further comprising, before (B):
(E) capturing the speech of the first person and the speech of a second person to produce at least one audio signal representing the speech of the first person and the speech of the second person; and
(F) applying automatic speech recognition to the at least one audio signal to
produce the transcript of the speech of the first person and the speech of the second person.
3. The method of claim 2, further comprising, before (B):
(G) identifying an identity of the first person;
(H) identifying an identity of the second person; and
wherein (F) comprises producing the transcript of the speech of the first person and the speech of the second person based on the identity of the first person, the identity of the second person, and the speech of the first person and the speech of the second person.
4. The method of claim 3, wherein (F) further comprises associating the identity of the first person with a first portion of the transcript and associating the identity of the second person with a second portion of the transcript.
5. The method of claim 1, wherein (A) comprises converting the initial state of the EMR into a text file.
6. The method of claim 1, wherein (A) comprises converting the initial state of the EMR of the first person into a list of discrete medical domain model instances.
7. The method of claim 1, wherein (B) comprises converting the final state of the EMR of the first person into a text file.
8. The method of claim 1, wherein (B) comprises converting the final state of the EMR of the first person into a list of discrete medical domain model instances.
9. The method of claim 1, wherein (C) comprises using non-linear alignment techniques to identify the differences between the initial state of the EMR of the first person and the final state of the EMR of the first person.
10. The method of claim 1, further comprising:
(E) saving an initial state of an electronic medical record (EMR) of a third person;
(F) saving a final state of the EMR of the third person after the EMR of the third person has been modified based on speech of the third person and speech of a fourth person;
(G) identifying differences between the initial state of the EMR of the third person and the final state of the EMR of the third person; and
(H) applying the machine learning module to:
(1) the transcript of the speech of the first person and the speech of the second person;
(2) the differences between the initial state of the EMR of the first person and the final state of the EMR of the first person;
(3) the transcript of the speech of the third person and the speech of the fourth person; (4) the differences between the initial state of the EMR of the third person and the final state of the EMR of the third person;
thereby generating a mapping between text and EMR state differences.
11. A system comprising at least one non-transitory computer-readable medium having computer program instructions stored thereon for causing at least one computer processor to perform a method, the method comprising, at a transcription job routing engine:
(A) saving an initial state of an electronic medical record (EMR) of a first person;
(B) saving a final state of the EMR of the first person after the EMR of the first person has been modified based on speech of the first person and speech of a second person;
(C) identifying differences between the initial state of the EMR of the third person and the final state of the EMR of the third person; and
(D) applying a machine learning module to: (1) a transcript of the speech of the first person and the speech of the second person; and (2) the differences between the initial state of the EMR of the first person and the final state of the EMR of the first person, to generate a mapping between: (a) the transcript of the speech of the first person and the speech of the second person; and (b) the differences between the initial state of the EMR and the final state of the EMR.
12. The system of claim 11, wherein the method further comprises, before (B):
(E) capturing the speech of the first person and the speech of a second person to produce at least one audio signal representing the speech of the first person and the speech of the second person; and
(F) applying automatic speech recognition to the at least one audio signal to produce the transcript of the speech of the first person and the speech of the second person.
13. The system of claim 12, wherein the method further comprises, before (B):
(G) identifying an identity of the first person;
(H) identifying an identity of the second person; and
wherein (F) comprises producing the transcript of the speech of the first person and the speech of the second person based on the identity of the first person, the identity of the second person, and the speech of the first person and the speech of the second person.
14. The system of claim 13, wherein (F) further comprises associating the identity of the first person with a first portion of the transcript and associating the identity of the second person with a second portion of the transcript.
15. The system of claim 11, wherein (A) comprises converting the initial state of the EMR into a text file.
16. The system of claim 11, wherein (A) comprises converting the initial state of the EMR of the first person into a list of discrete medical domain model instances.
17. The system of claim 11, wherein (B) comprises converting the final state of the EMR of the first person into a text file.
18. The system of claim 11, wherein (B) comprises converting the final state of the EMR of the first person into a list of discrete medical domain model instances.
19. The system of claim 11, wherein (C) comprises using non-linear alignment techniques to identify the differences between the initial state of the EMR of the first person and the final state of the EMR of the first person.
20. The system of claim 11, wherein the method further comprises:
(E) saving an initial state of an electronic medical record (EMR) of a third person;
(F) saving a final state of the EMR of the third person after the EMR of the third person has been modified based on speech of the third person and speech of a fourth person;
(G) identifying differences between the initial state of the EMR of the third person and the final state of the EMR of the third person; and
(H) applying the machine learning module to:
(1) the transcript of the speech of the first person and the speech of the second person;
(2) the differences between the initial state of the EMR of the first person and the final state of the EMR of the first person;
(3) the transcript of the speech of the third person and the speech of the fourth person; (4) the differences between the initial state of the EMR of the third person and the final state of the EMR of the third person;
thereby generating a mapping between text and EMR state differences.
</claims>
</document>
