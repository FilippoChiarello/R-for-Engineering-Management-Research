<document>

<filing_date>
2019-05-06
</filing_date>

<publication_date>
2020-01-16
</publication_date>

<priority_date>
2018-07-16
</priority_date>

<ipc_classes>
G06K9/00,G06T7/246,G06T7/292
</ipc_classes>

<assignee>
ACCEL ROBOTICS CORPORATION
</assignee>

<inventors>
PETRE, CSABA
BUIBAS, MARIUS
QUINN, JOHN
FEIGUM, KAYLEE
MASEDA, MICHAEL BRANDON
CSEH, MARTIN ALAN
</inventors>

<docdb_family_id>
69138454
</docdb_family_id>

<title>
PROJECTED IMAGE ITEM TRACKING SYSTEM
</title>

<abstract>
A projected image item tracking system that analyzes projected camera images to determine items taken from, placed on, or moved on a shelf or other area in an autonomous store. The items and actions performed on them may then be attributed to a shopper near the area. Projected images may be combined to generate a 3D volume difference between the state of the area before and after shopper interaction. The volume difference may be calculated using plane-sweep stereo, or using convolutional neural networks. Because these methods may be computationally intensive, the system may first localize a change volume where items appear to have been displaced, and then generate a volume difference only within that change volume. This optimization results in significant savings in power consumption and in more rapid identification of items. The 3D volume difference may also indicate the quantity of items displaced, for example from a vertical stack.
</abstract>

<claims>
1. A projected image item tracking system comprising: a processor coupled to a sensor configured to generate an enter signal when a shopper reaches into or towards an item storage area in a store, wherein said item storage area contains items; and, an exit signal when said shopper retracts from said item storage area; and, a plurality of cameras oriented to view said item storage area; wherein said processor is configured to obtain a plurality of before images captured by said plurality of cameras, each before image of said plurality of before images corresponding to a camera of said plurality of cameras, wherein said each before image is captured at a time before said enter signal; obtain a plurality of after images captured by said plurality of cameras, each after image of said plurality of after images corresponding to said camera of said plurality of cameras, wherein said each after image is captured at a time after said exit signal; project said plurality of before images onto two or more planes in said item storage area to generate a plurality of projected before images corresponding to each combination of a plane of said two or more planes and said camera of said plurality of cameras; project said plurality of after images onto said two or more planes to generate a plurality of projected after images corresponding to said each combination of said plane of said two or more planes and said camera of said plurality of cameras; analyze said plurality of projected before images, and said plurality of projected after images, to identify an item of said items taken from or put into said item storage area between said enter signal and said exit signal; and, associate said item with said shopper; and, wherein said analyze said plurality of projected before images and said plurality of projected after images comprises calculate a 3D volume difference between contents of said item storage area at said time before said enter signal and contents of said item storage area at said time after said exit signal; when said 3D volume difference indicates that said contents of said item storage area at said time after said exit signal is smaller than said contents of said item storage area at said time before said enter signal, input at least a portion of one or more of said plurality of projected before images that intersects said 3D volume difference into a classifier; when said 3D volume difference indicates that said contents of said item storage area at said time after said exit signal is larger than said contents of said item storage area at said time before said enter signal, input at least a portion of one or more of said plurality of projected after images that intersects said 3D volume difference into said classifier; and, identify said item of said items taken from or put into said item storage area as an output of said classifier.
2. (canceled)
3. The system of claim 1, wherein said classifier comprises a neural network trained to recognize images of said items.
4. The system of claim 1, wherein said processor is further configured to calculate a quantity of said item of said items taken from or put into said item storage area based on said 3D volume difference; and, associate said quantity with said shopper and with said item.
5. The system of claim 4, wherein said calculate said quantity of said item comprises obtain a size of said item; and, compare said 3D volume difference with said size of said item to determine said quantity of said item.
6. The system of claim 1, wherein said processor is further configured to: when said 3D volume difference indicates that said contents of said item storage area at said time after said exit signal is smaller than said contents of said item storage area at said time before said enter signal, associate a take action with said shopper and with said item; and, when said 3D volume difference indicates that said contents of said item storage area at said time after said exit signal is larger than said contents of said item storage area at said time before said enter signal, associate a put action with said shopper and with said item.
7. The system of claim 1, wherein said calculate said 3D volume difference comprises calculate a before 3D surface of said contents of said item storage area from said plurality of projected before images; calculate an after 3D surface of said contents of said item storage area from said plurality of projected after images; and, calculate said 3D volume difference as a volume between said before 3D surface and said after 3D surface.
8. The system of claim 7, wherein said calculate said before 3D surface comprises apply a plane sweep stereo algorithm to said plurality of projected before images; and, said calculate said after 3D surface comprises apply said plane sweep stereo algorithm to said plurality of projected after images.
9. The system of claim 8, wherein said two or more planes are parallel to a surface of said item storage area.
10. The system of claim 8, wherein at least one plane of said two or more planes is not parallel to a surface of said item storage area.
11. The system of claim 7, wherein said processor is further configured to compare said plurality of projected before images with said plurality of projected after images to calculate a change region of each plane of said two or more planes; combine said change region of each plane of said two or more planes to calculate a change volume in said item storage area; calculate said before 3D surface and said after 3D surface only in said change volume.
12. The system of claim 11, wherein said compare said plurality of projected before images to said plurality of projected after images comprises calculate an image difference between each projected before image of said plurality of projected before images, and a corresponding projected after image of said plurality of projected after images that is associated with a same camera of said plurality of cameras and with a same plane of said two or more planes; and, for each plane of said two or more planes, combine the image difference associated with each camera of said plurality of cameras and associated with said each plane to form said change region of said each plane.
13. The system of claim 11, wherein said combine the image difference associated with each camera of said plurality of cameras and associated with said each plane comprises weight each pixel in said image difference based on a distance between a point in said each plane corresponding to said pixel and a location of said each camera, to form a weighted image difference; calculate an average weighted image difference for said each plane as an average of said weighted image difference corresponding to said each camera; and, calculate said change region of said each plane based on said average weighted image difference.
14. The system of claim 11, wherein said calculate said image difference comprises calculate an absolute value of a difference between a pixel value of said each projected before image and a corresponding pixel value of said corresponding projected after image.
15. The system of claim 11, wherein said calculate said image difference comprises input said each projected before image and said corresponding projected after image into a neural network trained to identify image differences between pairs of images.
16. The system of claim 1, further comprising a modular shelf comprising said plurality of cameras oriented to view said item storage area; a right-facing camera mounted on or proximal to a left edge of said modular shelf; a left-facing camera mounted on or proximal to a right edge of said modular shelf; a shelf processor; and, a network switch; wherein said processor comprises a network of computing devices, said computing devices comprising a store processor; and, said shelf processor.
17. The system of claim 16, wherein said sensor comprises said right-facing camera, said left-facing camera; and, said processor is further configured to analyze images from said right-facing camera and said left-facing camera to detect when said shopper reaches into or towards said item storage area, and to generate said enter signal; and, analyze images from said right-facing camera and said left-facing camera to detect when said shopper retracts from said item storage area, and to generate said exit signal.
18. The system of claim 16, wherein said shelf processor comprises or is coupled to a memory; and, said shelf processor is configured to receive images from said plurality of cameras and store said images in said memory; when said shelf processor receives or generates said enter signal, retrieve said plurality of before images from said memory.
19. The system of claim 18, wherein said shelf processor is further configured to when said shelf processor receives or generates said enter signal, transmit said plurality of before images from said memory to said store processor; and, when said shelf processor receives or generates said exit signal, receive said plurality of after images from said plurality of cameras or obtain said plurality of after images from said memory and transmit said plurality of after images to said store processor.
20. The system of claim 1, wherein said analyze said plurality of projected before images and said plurality of projected after images further comprises input at least a portion of said plurality of projected before images and at least a portion of said plurality of projected after images into a neural network trained to output said item of said items taken from or put into said item storage area between said enter signal and said exit signal.
21. The system of claim 20, wherein said neural network is further trained to output an action that indicates whether said item of said items is taken from or is put into said item storage area between said enter signal and said exit signal.
22. The system of claim 21, wherein said neural network comprises a feature extraction layer applied to each of said at least a portion of said plurality of projected before images and at least a portion of said plurality of projected after images, wherein said feature extraction layer outputs image features; a differencing layer applied to each pair of said image features associated with a portion of a projected before image from said camera and said image features associated with a portion of a projected after image from said camera, wherein said differencing layer outputs feature differences associated with said camera; one or more convolutional layers applied to said feature differences associated with each camera of said plurality of cameras; an item classifier layer applied to an output of said one or more convolutional layers; and, an action classifier layer applied to said output of said one or more convolutional layers.
23. A projected image item tracking system comprising: a processor coupled to a sensor configured to generate an enter signal when a shopper reaches into or towards an item storage area in a store, wherein said item storage area contains items; and, an exit signal when said shopper retracts from said item storage area; and, a plurality of cameras oriented to view said item storage area; wherein said processor is configured to obtain a plurality of before images captured by said plurality of cameras, each before image of said plurality of before images corresponding to a camera of said plurality of cameras, wherein said each before image is captured at a time before said enter signal; obtain a plurality of after images captured by said plurality of cameras, each after image of said plurality of after images corresponding to said camera of said plurality of cameras, wherein said each after image is captured at a time after said exit signal; project said plurality of before images onto two or more planes in said item storage area to generate a plurality of projected before images corresponding to each combination of a plane of said two or more planes and said camera of said plurality of cameras; project said plurality of after images onto said two or more planes to generate a plurality of projected after images corresponding to said each combination of said plane of said two or more planes and said camera of said plurality of cameras; analyze said plurality of projected before images, and said plurality of projected after images, to identify an item of said items taken from or put into said item storage area between said enter signal and said exit signal; and, associate said item with said shopper; and, wherein said analyze said plurality of projected before images and said plurality of projected after images comprises input at least a portion of said plurality of projected before images and at least a portion of said plurality of projected after images into a neural network trained to output said item of said items taken from or put into said item storage area between said enter signal and said exit signal, wherein said neural network is further trained to output an action that indicates whether said item of said items is taken from or is put into said item storage area between said enter signal and said exit signal; and, wherein said neural network comprises a feature extraction layer applied to each of said at least a portion of said plurality of projected before images and at least a portion of said plurality of projected after images, wherein said feature extraction layer outputs image features; a differencing layer applied to each pair of said image features associated with a portion of a projected before image from said camera and said image features associated with a portion of a projected after image from said camera, wherein said differencing layer outputs feature differences associated with said camera; one or more convolutional layers applied to said feature differences associated with each camera of said plurality of cameras; an item classifier layer applied to an output of said one or more convolutional layers; and, an action classifier layer applied to said output of said one or more convolutional layers.
</claims>
</document>
