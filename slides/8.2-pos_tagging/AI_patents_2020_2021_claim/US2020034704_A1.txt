<document>

<filing_date>
2018-07-30
</filing_date>

<publication_date>
2020-01-30
</publication_date>

<priority_date>
2018-07-30
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
AGRAVANTE, DON JOVEN RAVOY
DE MAGISTRIS, GIOVANNI DE
PHAM, TU-HOA
TACHIBANA, RYUKI
</inventors>

<docdb_family_id>
69178198
</docdb_family_id>

<title>
SEQUENTIAL LEARNING OF CONSTRAINTS FOR HIERARCHICAL REINFORCEMENT LEARNING
</title>

<abstract>
A computer-implemented method, computer program product, and computer processing system are provided for Hierarchical Reinforcement Learning (HRL) with a target task. The method includes obtaining, by a processor device, a sequence of tasks based on hierarchical relations between the tasks, the tasks constituting the target task. The method further includes learning, by a processor device, a sequence of constraints corresponding to the sequence of tasks by repeating, for each of the tasks in the sequence, reinforcement learning and supervised learning with a set of good samples and a set of bad samples and by applying an obtained constraint for a current task to a next task.
</abstract>

<claims>
1. A computer-implemented method for Hierarchical Reinforcement Learning (HRL) with a target task, comprising: obtaining, by a processor device, a sequence of tasks based on hierarchical relations between the tasks, the tasks constituting the target task; and learning, by a processor device, a sequence of constraints corresponding to the sequence of tasks by repeating, for each of the tasks in the sequence, reinforcement learning and supervised learning with a set of good samples and a set of bad samples and by applying an obtained constraint for a current task to a next task.
2. The computer-implemented method of claim 1, wherein each performance of the repeating step comprises training a neural network to predict a constraint corresponding to the current task with the set of good samples and the set of bad samples.
3. The computer-implemented method of claim 2, wherein the each performance of the repeating step further comprises training a protagonist policy and an antagonist policy by reinforcement learning by restricting predicted actions according to the trained neural network.
4. The computer-implemented method of claim 2, wherein the constraint is an inequality constraint.
5. The computer-implemented method of claim 1, wherein said learning step comprises applying a respective reward as an input to a non-antagonistic reinforcement learning neural network and an antagonist reinforcement learning neural network, and applying an output of each of the non-antagonistic reinforcement learning and antagonistic reinforcement learning neural networks to a supervised learning neural network to obtain a respective one of the constraints of the sequence.
6. The computer-implemented method of claim 1, wherein the constraints are used to prioritize the tasks by imposing a particular execution order on the tasks.
7. The computer-implemented method of claim 1, further comprising disabling one or more of the constraints based on a current state of a value function, the value function indicating a maximum expected future reward an agent will get at a given state.
8. The computer-implemented method of claim 1, wherein each of the tasks in the sequence corresponds to a respective hierarchy level.
9. A computer program product for Hierarchical Reinforcement Learning (HRL) with a target task, the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to perform a method comprising: obtaining, by a processor device, a sequence of tasks based on hierarchical relations between the tasks, the tasks constituting the target task; and learning, by a processor device, a sequence of constraints corresponding to the sequence of tasks by repeating, for each of the tasks in the sequence, reinforcement learning and supervised learning with a set of good samples and a set of bad samples and by applying an obtained constraint for a current task to a next task.
10. The computer program product of claim 9, wherein each performance of the repeating step comprises training a neural network to predict a constraint corresponding to the current task with the set of good samples and the set of bad samples.
11. The computer program product of claim 10, wherein the each performance of the repeating step further comprises training a protagonist policy and an antagonist policy by reinforcement learning by restricting predicted actions according to the trained neural network.
12. The computer program product of claim 10, wherein the constraint is an inequality constraint.
13. The computer program product of claim 9, wherein said learning step comprises applying a respective reward as an input to a non-antagonistic reinforcement learning neural network and an antagonist reinforcement learning neural network, and applying an output of each of the non-antagonistic reinforcement learning and antagonistic reinforcement learning neural networks to a supervised learning neural network to obtain a respective one of the constraints of the sequence.
14. The computer program product of claim 9, wherein the constraints are used to prioritize the tasks by imposing a particular execution order on the tasks.
15. The computer program product of claim 9, wherein the method further comprises disabling one or more of the constraints based on a current state of a value function, the value function indicating a maximum expected future reward an agent will get at a given state.
16. The computer program product of claim 9, wherein each of the tasks in the sequence corresponds to a respective hierarchy level.
17. A computer processing system for Hierarchical Reinforcement Learning (HRL) with a target task, comprising: a memory for storing program code; and a processor device operatively coupled to the memory for running the program code to obtain a sequence of tasks based on hierarchical relations between the tasks, the tasks constituting the target task; and learn a sequence of constraints corresponding to the sequence of tasks by repeating, for each of the tasks in the sequence, reinforcement learning and supervised learning with a set of good samples and a set of bad samples and by applying an obtained constraint for a current task to a next task.
18. The computer processing system of claim 17, wherein the processor device repeats the reinforcement learning and the supervised learning by training a neural network to predict a constraint corresponding to the current task with the set of good samples and the set of bad samples.
19. The computer processing system of claim 18, wherein the processor device repeats the reinforcement learning and the supervised learning by training a protagonist policy and an antagonist policy by reinforcement learning by restricting predicted actions according to the trained neural network.
20. The computer processing system of claim 18, wherein the constraint is an inequality constraint.
</claims>
</document>
