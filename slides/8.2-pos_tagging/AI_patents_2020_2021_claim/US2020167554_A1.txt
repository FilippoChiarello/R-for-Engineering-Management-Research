<document>

<filing_date>
2020-01-29
</filing_date>

<publication_date>
2020-05-28
</publication_date>

<priority_date>
2017-08-01
</priority_date>

<ipc_classes>
G06F3/01,G06K9/00,G06K9/62,G06N20/10,G06N20/20,G06N3/08
</ipc_classes>

<assignee>
HUAWEI TECHNOLOGIES COMPANY
</assignee>

<inventors>
WANG LIANG
HE JUN
LIU, CHUANJIAN
XU, SONGCEN
</inventors>

<docdb_family_id>
65232224
</docdb_family_id>

<title>
Gesture Recognition Method, Apparatus, And Device
</title>

<abstract>
This application provides a gesture recognition method, and relates to the field of man-machine interaction technologies. The method includes: extracting M images from a first video segment in a video stream; performing gesture recognition on the M images by using a deep learning algorithm, to obtain a gesture recognition result corresponding to the first video segment; and performing result combination on gesture recognition results of N consecutive video segments including the first video segment, to obtain a combined gesture recognition result. In the foregoing recognition process, a gesture in the video stream does not need to be segmented or tracked, but phase actions are recognized by using a deep learning algorithm with a relatively fast calculation speed, and then the phase actions are combined, so as to improve a gesture recognition speed, and reduce a gesture recognition delay.
</abstract>

<claims>
1. A gesture recognition device, wherein the device comprises a processor and a memory; the processor is configured to obtain M images, wherein the M images are extracted from a first video segment in a video stream, the first video segment is a video segment in the video stream, and M is an integer greater than or equal to 2; the processor is configured to perform gesture recognition on the M images by using a deep learning algorithm, to obtain a gesture recognition result corresponding to the first video segment; and the processor is configured to: after obtaining gesture recognition results of N consecutive video segments in the video stream that comprise the first video segment, combine the gesture recognition results of the N consecutive video segments, to obtain a combined gesture recognition result, wherein N is an integer greater than or equal to 2.
2. The device according to claim 1, wherein the processor is configured to: input the gesture recognition results of the N consecutive video segments into a pre-trained first machine learning model, to obtain the combined gesture recognition result, wherein the first machine learning model is used to determine an overall gesture motion trend composed of the input N consecutive gesture recognition results, and to output a gesture corresponding to the overall gesture motion trend as the combined gesture recognition result.
3. The device according to claim 2, wherein the first machine learning model is a neural network model, and the neural network model has N neurons; or the first machine learning model is a support vector machine(SVM) model.
4. The device according to claim 1, wherein the processor is configured to: obtain preset weight coefficients respectively corresponding to the gesture recognition results of the N consecutive video segments; and perform weighted averaging on the gesture recognition results of the N consecutive video segments based on the weight coefficients respectively corresponding to the gesture recognition results of the N consecutive video segments, to obtain the combined gesture recognition result.
5. The device according to claim 1, wherein the processor is configured to: perform image processing on the M images, to obtain an optical flow information image corresponding to the first video segment, wherein the optical flow information image comprises optical flow information between a first image in the M images and a pth image before the first image, the first image is one of the M images, and the optical flow information comprises instantaneous speed vector information of a pixel in the image, and perform gesture recognition on the optical flow information image by using a first deep learning algorithm, to obtain a first recognition result, wherein p is an integer greater than or equal to 1; perform image processing on the M images, to obtain a color information image corresponding to the first video segment, wherein the color information image comprises color information of the M images, and the color information comprises a color value of each pixel in the image, and perform gesture recognition on the color information image by using a second deep learning algorithm, to obtain a second recognition result; and combine the first recognition result and the second recognition result, to obtain the gesture recognition result of the first video segment.
6. The device according to claim 5, wherein the processor is configured to: for the first image, obtain, based on a preset rule, the pth image before the first image in the video stream, calculate the optical flow information between the first image and the pth image, and generate the optical flow information image comprising the optical flow information between the first image and the pth image, wherein a time interval between the first image and the pth image is not less than a forward calculation time of the first deep learning algorithm or a time required for calculating the optical flow information image; or for the first image, obtain, based on a preset rule, p images before the first image in the video stream, calculate optical flow information between every two adjacent images in the first image and the P images, and after accumulating the optical flow information between every two adjacent images, generate an optical flow information image comprising accumulated optical flow information, wherein a time interval between the first image and the pth image before the first image is not less than a forward calculation time of the first deep learning algorithm or a time required for calculating the optical flow information image.
7. The device according to claim 5, wherein the processor is configured to: extract color information of m images in the M images, generate, based on the extracted color information, color information images respectively corresponding to the m images, and obtain the color information images respectively corresponding to the m images as the color information image corresponding to the first video segment, wherein the m images are m random images in the M images, or the m images are m images that are in the M images and that each have a largest variation relative to a previous image in the video stream, and m is an integer greater than or equal to 1; or detect a pixel location in the M images at which image content changes with time, calculate an average value of color information in the M images that is corresponding to recognized pixel locations, to obtain new color information at the recognized pixel locations, and generate, based on the new color information at the recognized pixel locations, the color information image corresponding to the first video segment.
8. The device according to claim 1, wherein before obtaining the M images, the processor is further configured to: determine a time window with a preset time length in the video stream, wherein an end moment of the time window is within a time period corresponding to the first video segment; determine, based on a last image and at least one reference image in the time window, whether an action is performed in the first video segment, wherein the reference image is an image in the time window other than the last image; and if a determining result is that an action is performed in the first video segment, perform the step of obtaining M images.
9. The device according to claim 8, wherein the processor is configured to: for each of the at least one reference image, calculate a partial derivative image of the last image, wherein a value of each pixel in the partial derivative image is a partial derivative of a value of a corresponding pixel in the last image relative to a value of a corresponding pixel in the reference image; normalize the value of each pixel in the partial derivative image, to obtain a normalized partial derivative image; binarize the normalized partial derivative image based on a preset binarization threshold, to obtain a binarized image of the partial derivative image, wherein a value of each pixel in the binarized image is 0 or 1; calculate a sum of gray scale values of the pixels in the binarized image; and when the sum of the grayscale values is greater than 0, determine that an action is performed in the first video segment.
10. The device according to claim 5, wherein the processor is configured to: perform average value calculation on the first recognition result and the second recognition result, to obtain the gesture recognition result of the first video segment based on a calculation result of the average value calculation; or input the first recognition result and the second recognition result into a pre-trained second machine learning model, to obtain the gesture recognition result of the first video segment.
11. 11-20. (canceled)
21. A gesture recognition method, wherein the method comprises: obtaining M images, wherein the M images are extracted from a first video segment in a video stream, the first video segment is a video segment in the video stream, and M is an integer greater than or equal to 2; performing gesture recognition on the M images by using a deep learning algorithm, to obtain a gesture recognition result corresponding to the first video segment; and after gesture recognition results of N consecutive video segments in the video stream that comprise the first video segment are obtained, combining the gesture recognition results of the N consecutive video segments, to obtain a combined gesture recognition result, wherein Nâ‰¥2 and N is an integer.
22. The method according to claim 21, wherein the combining the gesture recognition results of the N consecutive video segments, to obtain a combined gesture recognition result comprises: inputting the gesture recognition results of the N consecutive video segments into a pre-trained first machine learning model, to obtain the combined gesture recognition result, wherein the first machine learning model is used to determine an overall gesture motion trend composed of the input N consecutive gesture recognition results, and to output a gesture corresponding to the overall gesture motion trend as the combined gesture recognition result.
23. The method according to claim 22, wherein the first machine learning model is a neural network model, and the neural network model has N neurons; or the first machine learning model is a support vector machine SVM model.
24. The method according to claim 21, wherein the combining the gesture recognition results of the N consecutive video segments, to obtain a combined gesture recognition result comprises: obtaining preset weight coefficients respectively corresponding to the gesture recognition results of the N consecutive video segments; and performing weighted averaging on the gesture recognition results of the N consecutive video segments based on the weight coefficients respectively corresponding to the gesture recognition results of the N consecutive video segments, to obtain the combined gesture recognition result.
25. The method according to claim 21, wherein the performing gesture recognition on the M images by using a deep learning algorithm, to obtain a gesture recognition result corresponding to the first video segment comprises: performing image processing on the M images, to obtain an optical flow information image corresponding to the first video segment, wherein the optical flow information image comprises optical flow information between a first image in the M images and a pth image before the first image, the first image is one of the M images, and the optical flow information comprises instantaneous speed vector information of a pixel in the image, and performing gesture recognition on the optical flow information image by using a first deep learning algorithm, to obtain a first recognition result, wherein p is an integer greater than or equal to 1; performing image processing on the M images, to obtain a color information image corresponding to the first video segment, wherein the color information image comprises color information of the M images, and the color information comprises a color value of each pixel in the image, and performing gesture recognition on the color information image by using a second deep learning algorithm, to obtain a second recognition result; and combining the first recognition result and the second recognition result, to obtain the gesture recognition result of the first video segment.
26. The method according to claim 25, wherein the performing image processing on the M images, to obtain an optical flow information image corresponding to the first video segment comprises: for the first image, obtaining, based on a preset rule, the pth image before the first image in the video stream, calculating the optical flow information between the first image and the pth image, and generating the optical flow information image comprising the optical flow information between the first image and the pth image, wherein a time interval between the first image and the pth image is not less than a forward calculation time of the first deep learning algorithm or a time required for calculating the optical flow information image; or for the first image, obtaining, based on a preset rule, p images before the first image in the video stream, calculating optical flow information between every two adjacent images in the first image and the P images, and after the optical flow information between every two adjacent images is accumulated, generating an optical flow information image comprising the accumulated optical flow information, wherein a time interval between the first image and the pth image before the first image is not less than a forward calculation time of the first deep learning algorithm or a time required for calculating the optical flow information image.
27. The method according to claim 25, wherein the performing image processing on the M images, to obtain a color information image corresponding to the first video segment comprises: extracting color information of m images in the M images, generating, based on the extracted color information, color information images respectively corresponding to the m images, and obtaining the color information images respectively corresponding to the m images as the color information image corresponding to the first video segment, wherein the m images are m random images in the M images, or the m images are m images that are in the M images and that each have a largest variation relative to a previous image in the video stream, and m is an integer greater than or equal to 1; or detecting a pixel location in the M images at which image content changes with time, calculating an average value of color information in the M images that is corresponding to recognized pixel locations to obtain new color information at the recognized pixel locations, and generating, based on the new color information at the recognized pixel locations, the color information image corresponding to the first video segment.
28. The method according to claim 21, wherein before the obtaining M images, the method further comprises: determining a time window with a preset time length in the video stream, wherein an end moment of the time window is within a time period corresponding to the first video segment; determining, based on a last image and at least one reference image in the time window, whether an action is performed in the first video segment, wherein the at least one reference image is an image in the time window other than the last image; and if a determining result is that an action is performed in the first video segment, performing the step of obtaining M images.
29. The method according to claim 28, wherein the determining, based on a last image and at least one reference image in the time window, whether an action is performed in the first video segment comprises: for each of the at least one reference image, calculating a partial derivative image of the last image, wherein a value of each pixel in the partial derivative image is a partial derivative of a value of a corresponding pixel in the last image relative to a value of a corresponding pixel in the reference image; normalizing the value of each pixel in the partial derivative image, to obtain a normalized partial derivative image; binarizing the normalized partial derivative image based on a preset binarization threshold, to obtain a binarized image of the partial derivative image, wherein a value of each pixel in the binarized image is 0 or 1; calculating a sum of grayscale values of the pixels in the binarized image; and when the sum of the grayscale values is greater than 0, determining that an action is performed in the first video segment.
30. The method according to claim 25, wherein the combining the first recognition result and the second recognition result, to obtain the gesture recognition result of the first video segment comprises: performing average value calculation on the first recognition result and the second recognition result, to obtain the gesture recognition result of the first video segment based on a calculation result of the average value calculation; or inputting the first recognition result and the second recognition result into a pre-trained second machine learning model, to obtain the gesture recognition result of the first video segment.
</claims>
</document>
