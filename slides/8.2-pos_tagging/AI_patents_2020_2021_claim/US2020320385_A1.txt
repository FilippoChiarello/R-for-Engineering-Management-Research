<document>

<filing_date>
2019-04-30
</filing_date>

<publication_date>
2020-10-08
</publication_date>

<priority_date>
2019-04-05
</priority_date>

<ipc_classes>
G06N3/063,G06N3/08
</ipc_classes>

<assignee>
GYRFALCON TECHNOLOGY
</assignee>

<inventors>
YANG LIN
SUN, BAOHUA
ZHANG, WENHAN
DONG, PATRICK ZENG
REN, YONGXIONG
</inventors>

<docdb_family_id>
72662682
</docdb_family_id>

<title>
USING QUANTIZATION IN TRAINING AN ARTIFICIAL INTELLIGENCE MODEL IN A SEMICONDUCTOR SOLUTION
</title>

<abstract>
A system for training an artificial intelligence (AI) model for an AI chip may include a forward network and a backward propagation network. The AI model may be a convolution neural network (CNN). The forward network may infer the output of the AI chip based on the training data. The backward network may use the output of the AI chip and the ground truth data to train the weights of the AI model. In some examples, the system may train the AI model using a gradient descent method. The system may quantize the weights and update the weights during the training. In some examples, the system may perform a uniform quantization over the weights. The system may also determine the distribution of the weights. If the weight distribution is not symmetric, the system may group the weights and quantize the weights based on the grouping.
</abstract>

<claims>
We claim:
1. A system comprising: a processor; and a non-transitory computer readable medium containing programming instructions that, when executed, will cause the processor to: determine weights of an artificial intelligence (AI) model; repeat in one or more iterations, until a stopping criteria is met, operations comprising: quantizing the weights into one or more quantization levels; determining output of the AI model based at least on a training data set and the quantized weights of the AI model; determining a change of weights based on the output of the AI model; and updating the weights of the AI model based on the change of weights; upon the stopping criteria being met, upload the quantized weights of the AI model to an AI chip for performing an AI task.
2. The system of claim 1, wherein the programming instructions for quantizing the weights comprise programming instructions configured to: clip the weights of a convolution neural network (CNN) model to a maximum value of a corresponding convolution layer in an embedded cellular neural network of the AI chip; and quantize the clipped weights of the AI model.
3. The system of claim 1, wherein the programming instructions for quantizing the weights comprise programming instructions configured to cluster the weights of the AI model and assign a weight to a quantization level of the one or more quantization levels based on which cluster to which the weight belongs.
4. The system of claim 1, wherein the programming instructions for quantizing the weights comprise programming instructions configured to: determine a distribution of the weights of the AI model; and upon determining the distribution of the weights: if the distribution of the weights is symmetric, apply an uniform quantization over the weights of the AI model; otherwise, group the weights of the AI model and quantize the weights to the one or more quantization levels based on the grouping.
5. The system of claim 1, wherein the weights of the AI model are stored in floating point and the quantized weights of the AI model are stored in fixed point.
6. The system of claim 1, wherein the programming instructions for determining the change of weights contain programming instructions configured to use a gradient descent method, wherein a loss function in the gradient descent method is based on a sum of loss values over a plurality of training instances in the training data set, wherein the loss value of each of the plurality of training instances is a difference between an output of the AI model for a training instance and a ground truth of the training instance.
7. The system of claim 6, wherein the stopping criteria is met when a value of the loss function at an iteration is greater than a value of the loss function at a preceding iteration.
8. The system of claim 1, wherein the programming instructions comprise additional programming instructions configured to, by the AI chip: perform the AI task to generate output based on the quantized weights of the AI model; and transmit the output of the AI task to an output device; wherein the quantized weights of the AI model are uploaded into an embedded cellular neural network architecture in the AI chip.
9. A method comprising, at a processing device: determining initial weights of a convolution neural network (CNN) model; repeating in one or more iterations, until a stopping criteria is met, operations comprising: quantizing the weights into one or more quantization levels; determining output of the CNN model based at least on a training data set and the quantized weights of the CNN model; determining a change of weights based on the output of the CNN model; and updating the weights of the CNN model based on the change of weights; upon the stopping criteria being met, uploading the quantized weights of the CNN model to an artificial intelligence (AI) chip configured to perform an AI task.
10. The method of claim 9, wherein quantizing the weights comprises clustering the weights of the CNN model and quantizing a weight to a quantization level of the one or more quantization levels based on which cluster to which the weight belongs.
11. The method of claim 9, wherein quantizing the weights comprises: determining a distribution of the weights of the CNN model; upon determining the distribution of the weights: if the distribution of the weights is symmetric, applying an uniform quantization over the weights of the CNN model; and otherwise, grouping the weights of the CNN model and quantizing the weights based on the grouping.
12. The method of claim 9, wherein the weights of the CNN model are stored in floating point and the quantized weights of the CNN model are stored in fixed point.
13. The method of claim 9, wherein determining the change of weights of the CNN model is based on a gradient descent method, wherein a loss function in the gradient descent method is based on a sum of loss values over a plurality of training instances in the training data set, wherein the loss value of each of the plurality of training instances is a difference between an output of the CNN model for a training instance and a ground truth of the training instance.
14. The method of claim 13, wherein determining the change of weights of the CNN model is further based on a stochastic gradient of the quantized weights of the CNN model.
15. The method of claim 13, wherein the stopping criteria is met when a value of the loss function at an iteration is greater than a value of the loss function at a preceding iteration.
16. The method of claim 9 further comprising, by the AI chip: performing the AI task to generate output based on the quantized weights of the CNN model; and transmitting the output of the AI task to an output device; wherein the quantized weights of the CNN model are uploaded into an embedded cellular neural network architecture in the AI chip.
17. A system comprising: a processor; and a non-transitory computer readable medium containing programming instructions that, when executed, will cause the processor to: repeat in one or more iterations, until a stopping criteria is met, operations comprising: quantizing weights of an artificial intelligence (AI) model into one or more quantization levels; determining output of the AI model based at least on a training data set and the quantized weights of the AI model; determining a change of weights based on the output of the AI model; and updating the weights of the AI model based on the change of weights; and upon the stopping criteria being met, upload the quantized weights of the AI model to an embedded cellular neural network architecture in an AI chip configured to: perform an AI task to generate output based on the quantized weights; and transmit the output of the AI task to an output device.
18. The system of claim 17, wherein the programming instructions for quantizing the weights comprise programming instructions configured to: determine a distribution of the weights of the AI model; and upon determining the distribution of the weights: if the distribution of the weights is symmetric, apply an uniform quantization over the weights of the AI model; otherwise, group the weights of the AI model and quantize the weights to the one or more quantization levels based on the grouping.
19. The system of claim 17, wherein the weights of the AI model are stored in floating point and the quantized weights of the AI model are stored in fixed point.
20. The system of claim 17, wherein the programming instructions for determining the change of weights of contain programming instructions configured to use a gradient descent method, wherein a loss function in the gradient descent method is based on a sum of loss values over a plurality of training instances in the training data set, wherein the loss value of each of the plurality of training instances is a difference between an output of the AI model for a training instance and a ground truth of the training instance.
</claims>
</document>
