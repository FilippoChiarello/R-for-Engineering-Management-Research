<document>

<filing_date>
2019-03-15
</filing_date>

<publication_date>
2020-02-11
</publication_date>

<priority_date>
2019-03-15
</priority_date>

<ipc_classes>
G06F16/901,G06N20/00,G06N7/00,G10L15/26,H03M7/42
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
</assignee>

<inventors>
GROVER, SACHIN
STRIMEL, GRANT
</inventors>

<docdb_family_id>
69410848
</docdb_family_id>

<title>
Compression of machine learned models
</title>

<abstract>
Devices and techniques are generally described for compression of natural language processing models. A first index value to a first address of a weight table may be stored in a hash table. The first address may store a first weight associated with a first feature of a natural language processing model. A second index value to a second address of the weight table may be stored in the hash table. The second address may store a second weight associated with a second feature of the natural language processing model. A first code associated with the first feature and comprising a first number of bits may be generated. A second code may be generated associated with the second feature and comprising a second number of bits greater than the first number of bits based on a magnitude of the second weight being greater than a magnitude of the first weight.
</abstract>

<claims>
1. A method of generating a compressed natural language model, the method comprising: determining a plurality of quantized weight values from weights of a natural language model, wherein the plurality of quantized weight values includes a first quantized weight value representing an average value of a first set of the weights; storing the plurality of quantized weight values in a quantized weight table, wherein the first quantized weight value is stored at a first address in the quantized weight table and a second quantized weight value of the plurality of quantized weight values is stored at a second address in the quantized weight table; generating, using a minimal perfect hash function, a perfect hash table comprising: a first portion of memory associated with a first feature of the natural language model, the first portion of memory storing a first index value associated with the quantized weight table, the first index value identifying the first address storing the first quantized weight value, the first quantized weight value having a first magnitude; and a second portion of memory associated with a second feature of the natural language model, the second portion of memory storing a second index value associated with the quantized weight table, the second index value identifying the second address storing the second quantized weight value, the second quantized weight value having a second magnitude that is less than the first magnitude; generating first code data using first feature string data of the first feature, wherein the first code data identifies the first feature from among other features and comprises a first number of bits; storing, in association with the first portion of memory, the first code data; generating a second code data using second feature string data of the second feature, wherein the second code data identifies the second feature from among other features and comprises a second number of bits, the second number of bits being less than the first number of bits; and storing, in association with the second portion of memory, the second code data.
2. The method of claim 1, further comprising: determining that a majority of the weights of the natural language model are within one standard deviation of zero; determining, using Huffman encoding, a first prefix code for the first index value, wherein storing the first index value in the first portion of memory of the perfect hash table comprises storing the first prefix code in the first portion of memory; and determining, using Huffman encoding, a second prefix code for the second index value, wherein storing the second index value in the second portion of memory of the perfect hash table comprises storing the second prefix code in the second portion of memory, wherein a third number of bits of the second prefix code is fewer than a fourth number of bits of the first prefix code based on the second quantized weight value being used more frequently than the first quantized weight value by the compressed natural language model.
3. The method of claim 1, further comprising: receiving, by at least one computing device, a request for a weight value associated with first feature string data, wherein the first feature string data is undefined with respect to the compressed natural language model; generating a third code data for the first feature string data; generating an index associated with the second portion of memory of the perfect hash table by inputting the first feature string data into the minimal perfect hash function; determining the second code data; determining that the second code data differs from the third code data; and determining that the weight value associated with the first feature string data is zero.
4. A method comprising: generating, using a hash function, a hash table, wherein a first element of the hash table is associated with a first feature of a machine learned model and a second element of the hash table is associated with a second feature of the machine learned model; storing first data in the hash table, the first data representing where a first weight is stored in a weight table, the first weight associated with the first feature; storing second data in the hash table, the second data representing where a second weight is stored in the weight table, the second weight associated with the second feature of the machine learned model; generating a first identifying data associated with the first feature, the first identifying data identifying the first feature from among other features of the machine learned model, wherein the first identifying data comprises a first number of bits based at least in part on the first weight; generating a second identifying data associated with the second feature, the second identifying data identifying the second feature from among the other features of the machine learned model, wherein the second identifying data comprises a second number of bits greater than the first number of bits, wherein the second number of bits is based at least in part on the second weight; and storing the hash table in a non-transitory computer-readable memory.
5. The method of claim 4, further comprising: determining a first number of occurrences of the first data in the hash table; determining a second number of occurrences of the second data in the hash table, wherein the first number of occurrences is greater than the second number of occurrences; encoding the first data with a first prefix code having a third number of bits; and encoding the second data with a second prefix code having a fourth number of bits, wherein the third number of bits is less than the fourth number of bits.
6. The method of claim 4, further comprising: determining a plurality of quantized weights for the machine learned model, wherein the plurality of quantized weights includes a first quantized weight value representing first weights of the machine learned model and a second quantized weight value representing second weights of the machine learned model; and storing the plurality of quantized weights in the weight table, wherein the first weight corresponds to the first quantized weight value and the second weight corresponds to the second quantized weight value.
7. The method of claim 6, further comprising: determining a minimum weight value of the machine learned model; determining a maximum weight value of the machine learned model; determining a set of evenly spaced clusters between the minimum weight value and the maximum weight value; determining the first quantized weight value for a first cluster of the set of evenly spaced clusters; and determining the second quantized weight value for a second cluster of the set of evenly spaced clusters.
8. The method of claim 4, wherein the first identifying data is a first variable length fingerprint code and the second identifying data is a second variable length fingerprint code, the method further comprising: determining a first bit allocation for the first variable length fingerprint code by solving an optimization problem; and determining a second bit allocation for the second variable length fingerprint code by solving the optimization problem, wherein the first bit allocation is different from the second bit allocation.
9. The method of claim 4, further comprising: receiving a request for a weight associated with a third feature, wherein the third feature is undefined with respect to the machine learned model; generating, by a first function, third identifying data for the third feature; generating an index to the hash table by inputting the third feature into the hash function; determining the first identifying data associated with the first feature; determining that the third identifying data matches the first identifying data; and determining that the weight associated with the third feature is the first weight.
10. The method of claim 4, further comprising: receiving a request for a weight associated with a third feature, wherein the third feature is undefined with respect to the machine learned model; generating, by a first function, third identifying data for the third feature; generating an index to the second element of the hash table by inputting the third feature into the hash function; determining the second identifying data associated with the second feature; determining that the third identifying data differs from the second identifying data; and determining that the weight associated with the third feature is zero.
11. The method of claim 4, further comprising: receiving audio data representing user utterance; determining, by an speech recognition component, text data from the audio data; determining feature data represented in the text data; determining, for the feature data, the first data, by inputting the feature data into the hash function; performing a lookup operation on the weight table using the first data, wherein the lookup operation returns the first weight; sending the feature data and the first weight to a compressed machine learned model; and determining, based at least in part on the first weight and the feature data, that the user utterance invokes a first speech-processing domain.
12. The method of claim 4, further comprising: generating the first identifying data comprising the first number of bits and the second identifying data comprising the second number of bits based at least in part on a first magnitude of the first weight being less than a second magnitude of the second weight.
13. A system, comprising: at least one processor; and at least one non-transitory computer-readable memory storing instructions that, when executed by the at least one processor, programs the at least one processor to perform a method comprising: generating, using a hash function, a hash table, wherein a first element of the hash table is associated with a first feature of a machine learned model and a second element of the hash table is associated with a second feature of the machine learned model; storing first data in the hash table, the first data representing where a first weight is stored in a weight table, the first weight associated with the first feature; storing second data in the hash table, the second data representing where a second weight is stored in the weight table, the second weight associated with the second feature of the machine learned model; generating a first identifying data associated with the first feature, the first identifying data identifying the first feature from among other features of the machine learned model, wherein the first identifying data comprises a first number of bits based at least in part on the first weight; generating a second identifying data associated with the second feature, the second identifying data identifying the second feature from among the other features of the machine learned model, wherein the second identifying data comprises a second number of bits greater than the first number of bits, wherein the second number of bits is based at least in part on the second weight; and storing the hash table in the at least one non-transitory computer-readable memory.
14. The system of claim 13, wherein the instructions, when executed by the at least one processor, are effective to program the at least one processor to perform the method further comprising: determining a first number of occurrences of the first data in the hash table; determining a second number of occurrences of the second data in the hash table, wherein the first number of occurrences is greater than the second number of occurrences; encoding the first data with a first prefix code having a third number of bits; and encoding the second data with a second prefix code having a fourth number of bits, wherein the third number of bits is less than the fourth number of bits.
15. The system of claim 13, wherein the instructions, when executed by the at least one processor, are effective to program the at least one processor to perform the method further comprising: determining a plurality of quantized weights for the machine learned model, wherein the plurality of quantized weights includes a first quantized weight value representing first weights of the machine learned model and a second quantized weight value representing second weights of the machine learned model; and storing the plurality of quantized weights in the weight table, wherein the first weight corresponds to the first quantized weight value and the second weight corresponds to the second quantized weight value.
16. The system of claim 15, wherein the instructions, when executed by the at least one processor, are effective to program the at least one processor to perform the method further comprising: determining a minimum weight value of the machine learned model; determining a maximum weight value of the machine learned model; determining a set of evenly spaced clusters between the minimum weight value and the maximum weight value; determining the first quantized weight value for a first cluster of the set of evenly spaced clusters; and determining the second quantized weight value for a second cluster of the set of evenly spaced clusters.
17. The system of claim 13, wherein the first identifying data is a first variable length fingerprint code and the second identifying data is a second variable length fingerprint code, and wherein the instructions, when executed by the at least one processor, are effective to program the at least one processor to perform the method further comprising: determining a first bit allocation for the first variable length fingerprint code by solving an optimization problem; and determining a second bit allocation for the second variable length fingerprint code by solving the optimization problem, wherein the first bit allocation is different from the second bit allocation.
18. The system of claim 13, wherein the instructions, when executed by the at least one processor, are effective to program the at least one processor to perform the method further comprising: receiving a request for a weight associated with a third feature, wherein the third feature is undefined with respect to the machine learned model; generating, by a first function, third identifying data for the third feature; generating an index to the hash table by inputting the third feature into the hash function; determining the first identifying data associated with the first feature; determining that the third identifying data matches the first identifying data; and determining that the weight associated with the third feature is the first weight.
19. The system of claim 13, wherein the instructions, when executed by the at least one processor, are effective to program the at least one processor to perform the method further comprising: receiving a request for a weight associated with a third feature, wherein the third feature is undefined with respect to the machine learned model; generating, by a first function, third identifying data for the third feature; generating an index to the second element of the hash table by inputting the third feature into the hash function; determining the second identifying data associated with the second feature; determining that the third identifying data differs from the second identifying data; and determining that the weight associated with the third feature is zero.
20. The system of claim 13, wherein the instructions, when executed by the at least one processor, are effective to program the at least one processor to perform the method further comprising: generating the first identifying data comprising the first number of bits and the second identifying data comprising the second number of bits based at least in part on a first magnitude of the first weight being less than a second magnitude of the second weight.
</claims>
</document>
