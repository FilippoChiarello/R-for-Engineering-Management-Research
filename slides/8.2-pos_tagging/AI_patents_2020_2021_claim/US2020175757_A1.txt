<document>

<filing_date>
2019-12-04
</filing_date>

<publication_date>
2020-06-04
</publication_date>

<priority_date>
2018-12-04
</priority_date>

<ipc_classes>
G06T17/20,G06T19/20,G06T7/10
</ipc_classes>

<assignee>
UNIVERSITY OF SOUTHERN CALIFORNIA
</assignee>

<inventors>
HU, LIWEN
LI, HAO
SAITO, SHUNSUKE
</inventors>

<docdb_family_id>
70849766
</docdb_family_id>

<title>
3D HAIR SYNTHESIS USING VOLUMETRIC VARIATIONAL AUTOENCODERS
</title>

<abstract>
Devices and methods for single-view 3D hair modeling are disclosed. The method for single-view 3D hair modeling includes training, by a neural network processor, a volumetric autoencoder to encode a plurality of 3D hairstyles into latent features, and to generate an output based on the latent features. The method for single-view 3D hair modeling includes training, by the neural network processor, an embedding network to determine hair coefficients of a single hairstyle from an input image. The method for single-view 3D hair modeling includes receiving, by the neural network processor, the input image. The method for single-view 3D hair modeling includes synthesizing, by the neural network processor, hair strands to generate a single-view 3D model of the single hairstyle based on the volumetric autoencoder, the embedding network, and the input image.
</abstract>

<claims>
1. A method for single-view 3D hair modeling, the method comprising: training, by a neural network processor, a volumetric autoencoder to encode a plurality of 3D hairstyles into latent features, and to generate an output based on the latent features; training, by the neural network processor, an embedding network to determine hair coefficients of a single hairstyle from an input image; receiving, by the neural network processor, the input image; and synthesizing, by the neural network processor, hair strands to generate a single-view 3D model of the single hairstyle based on the volumetric autoencoder, the embedding network, and the input image.
2. The method of claim 1, wherein training the volumetric autoencoder includes training the volumetric autoencoder to encode the plurality of 3D hairstyles by representing the plurality of 3D hairstyles using an occupancy field representing a volume of a specific hairstyle and a flow field representing an orientation of hair strands of the specific hairstyle.
3. The method of claim 2, wherein the flow field is determined by averaging orientations of nearby strands and diffusing the flow field into the volume represented by the occupancy field.
4. The method of claim 1, wherein training the volumetric autoencoder includes training the volumetric autoencoder to include a volumetric encoder to encode the plurality of 3D hairstyles into the latent features and an occupancy field decoder to decode an occupancy field representing a volume of the single hairstyle and an orientation field decoder to decode a flow field representing an orientation of hair strands of the single hairstyle to generate the output.
5. The method of claim 1, further comprising performing Principled Component Analysis (PCA) on the hair coefficients.
6. The method of claim 1, further comprising performing, by the neural network processor, post-processing on the single-view 3D model of the single hairstyle to place the single-view 3D model of the single hairstyle on at least one of a face or a head.
7. The method of claim 6, wherein the post-processing includes: segmenting a pixel-level hair mask and a digitized head model starting from an input image; and applying a mask-based deformation method to improve alignment with the hair segmentation mask.
8. A device for single-view 3D hair modeling, the device comprising: a neural network processor; and a memory, coupled to the neural network processor, the memory including instructions causing the neural network processor to: train a volumetric autoencoder to encode a plurality of 3D hairstyles into latent features, and to generate an output based on the latent features; train an embedding network to determine hair coefficients of a single hairstyle from an input image; receive the input image; and synthesize hair strands to generate a single-view 3D model of the single hairstyle based on the volumetric autoencoder, the embedding network, and the input image.
9. The device of claim 8, wherein training the volumetric autoencoder includes training the volumetric autoencoder to encode the plurality of 3D hairstyles by representing the plurality of 3D hairstyles using an occupancy field representing a volume of a specific hairstyle and a flow field representing an orientation of hair strands of the specific hairstyle.
10. The device of claim 9, wherein the flow field is determined by averaging orientations of nearby strands and diffusing the flow field into the volume represented by the occupancy field.
11. The device of claim 8, wherein training the volumetric autoencoder includes training the volumetric autoencoder to include a volumetric encoder to encode the plurality of 3D hairstyles into the latent features and an occupancy field decoder to decode an occupancy field representing a volume of the single hairstyle and an orientation field decoder to decode a flow field representing an orientation of hair strands of the single hairstyle to generate the output.
12. The device of claim 8, the memory including instructions further causing the neural network processor to perform Principled Component Analysis (PCA) on the hair coefficients.
13. The device of claim 8, the memory including instructions further causing the neural network processor to perform post-process on the single-view 3D model of the single hairstyle to place the single-view 3D model of the single hairstyle on at least one of a face or a head.
14. The device of claim 13, wherein the post-processing includes: segmenting a pixel-level hair mask and a digitized head model starting from an input image; and applying a mask-based deformation method to improve alignment with the hair segmentation mask.
15. A method for single-view 3D hair modeling, the method comprising: encoding a plurality of 3D hairstyles into latent features; generating an output based on the latent features; determining hair coefficients of a single hairstyle from an input image; receiving the input image; and generating a single-view 3D model of the single hairstyle based on the volumetric autoencoder, the embedding network, and the input image.
16. The method of claim 1, wherein encoding a plurality of 3D hairstyles into latent features includes training the volumetric autoencoder to encode the plurality of 3D hairstyles by representing the plurality of 3D hairstyles using an occupancy field representing a volume of a specific hairstyle and a flow field representing an orientation of hair strands of the specific hairstyle.
17. The method of claim 2, wherein the flow field is determined by averaging orientations of nearby strands and diffusing the flow field into the volume represented by the occupancy field.
18. The method of claim 1, wherein encoding a plurality of 3D hairstyles into latent features includes training the volumetric autoencoder to include a volumetric encoder to encode the plurality of 3D hairstyles into the latent features and an occupancy field decoder to decode an occupancy field representing a volume of the single hairstyle and an orientation field decoder to decode a flow field representing an orientation of hair strands of the single hairstyle to generate the output.
19. The method of claim 1, further comprising performing Principled Component Analysis (PCA) on the hair coefficients.
20. The method of claim 1, further comprising placing the single-view 3D model of the single hairstyle on at least one of a face or a head.
</claims>
</document>
