<document>

<filing_date>
2020-03-10
</filing_date>

<publication_date>
2020-08-20
</publication_date>

<priority_date>
2013-05-09
</priority_date>

<ipc_classes>
F41G3/26
</ipc_classes>

<assignee>
SHOOTING SIMULATOR
</assignee>

<inventors>
NORTHRUP; JAMES L.
</inventors>

<docdb_family_id>
72041366
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR MARKSMANSHIP TRAINING
</title>

<abstract>
A system for and method of use of an augmented reality display is provided. The preferred system is implemented by one or more tactical units calculating a target path from a weapon position and a range. A lead is calculated. A phantom target is displayed at the lead. A virtual laser and virtual tracer are provided to assist in target tracking. A spotter unit is also provided to supplement target path and range data. A neural network is provided to learn from tracking and successful lead data and to predict lead in the tactical theatre.
</abstract>

<claims>
1. An augmented reality ranging and lead determination system comprising: a set of processors, operatively connected to a set of memories; an augmented reality display, connected to the set of processors; the set of memories further comprising instructions that when executed by the set of processors cause the system to: derive a weapon path, from a movement of a weapon trained on a target; derive a range distance from the weapon to the target; extend a ray object, from the weapon to the range distance, based on the weapon path; derive a target trajectory from the ray object; derive a lead position from the range and the target trajectory; and, render a phantom, at lead position, on the augmented reality display.
2. The system of claim 1 further comprising instructions that when executed by the set of processors cause the system to: calculate a virtual laser position from the weapon path; and, render a virtual laser, at the virtual laser position, on the augmented reality display.
3. The system of claim 2 wherein the virtual laser position is generally coaxial with a barrel of the weapon.
4. The system of claim 2 further comprising instructions that when executed by the set of processors cause the system to: compare the phantom and the virtual laser for a coincident condition; and, send an alert signal upon the coincident condition.
5. The system of claim 1 further comprising instructions that when executed by the set of processors cause the system to: calculate a virtual tracer path from the weapon position; and, render a virtual tracer, at the virtual tracer path, on the augmented reality display.
6. The system of claim 1 further comprising instructions that when executed by the set of processors cause the system to: monitor the target for a hit event; and, record the hit event in the memory.
7. The system of claim 1 wherein the weapon further comprises: a forward motion sensor, positioned adjacent a barrel of the weapon, operatively connected to the set of processors; a rear motion sensor, positioned adjacent a stock of the weapon, operatively connected to the set of processors; and, wherein the movement of the weapon is derived from the forward motion sensor and the rear motion sensor.
8. The system of claim 1 further comprising a laser range finder, attached to the weapon, operatively connected to the set of processors; and, wherein the range distance is derived from the laser range finder.
9. The system of claim 1 further comprising a stereoscopic camera, operatively connected to the set of processors; and, wherein the range distance is derived from the stereoscopic camera.
10. An augmented reality ranging and lead determination system for a target comprising: a first remote unit, having a first processor, and a first memory and a first augmented reality display, operatively connected to the first processor; a second remote unit, having a second processor, and a second memory, and a second augmented reality display, operatively connected to the second processor; the first memory and the second memory including a set of instructions that, when executed, cause the system to perform the steps of: initiating, by the first remote unit, a track of the target; determining, by the first remote unit, a first range to the target; determining, by the first remote unit, a first weapon position; determining, by the first remote unit, a first remote unit position; calculating, by the first remote unit, a target path based on the first weapon position, the first remote unit position and the first range to target; sending, from the first remote unit to the second remote unit, the target path; determining, by the second remote unit, a second range to target; calculating, by the second remote unit, a first time to target; determining, by the second remote unit, a second weapon position; displaying, on the second augmented reality display, a first virtual laser based on the second weapon position; calculating, by the second remote unit, a first lead distance based on the first time to target; displaying, on the second augmented reality display, a first phantom target at the first lead distance; comparing, by the second remote unit, the first phantom target to the first virtual laser to determine a first coincidence condition; and, generating a fire alert message, by the second remote unit, upon receipt of the first coincidence condition.
11. The system of claim 10 further comprising instructions that, when executed, cause the system to perform the steps of: displaying, at the second remote unit, the fire alert message, on the second augmented reality display.
12. The system of claim 10 further comprising instructions that, when executed, cause the system to perform the steps of: sensing, at the second remote unit, a shot signal; generating, by the second remote unit, a virtual tracer path upon receipt of the shot signal; and, displaying, on the second augmented reality display, a virtual tracer on the virtual tracer path.
13. The system of claim 12 further comprising instructions that, when executed, cause the system to perform the steps of: sending, from the second remote unit to the first remote unit, the shot signal; and, displaying, on the first augmented reality display, the shot signal.
14. The system of claim 10 further comprising instructions that, when executed, cause the system to perform the steps of: recording, at the second remote unit, one of a target hit condition and a target miss condition.
15. The system of claim 10 further comprising instructions that, when executed, cause the system to perform the steps of: determining, by the second remote unit, a shot drop based on the first time to target; and, translating the target path based on the shot drop.
16. The system of claim 10 further comprising instructions that, when executed, cause the system to perform the steps of: determining, by the second remote unit, a change in position of the second remote unit; and, translating and rotating the target path based on the change in position.
17. The system of claim 10 further comprising instructions that, when executed, cause the system to perform the steps of: setting, by the first remote unit, an origin position; and, sending, by the first remote unit to the second remote unit, the origin position.
18. The system of claim 10 further comprising instructions that, when executed, cause the system to perform the steps of: receiving, by the first remote unit, a set of cardinal directions; and, setting, by the first remote unit, a set of cartesian coordinates based on the origin and the set of cardinal directions.
19. An augmented reality ranging and lead determination system for a target comprising: a tactical computer, having a first processor and a first memory; a spotter unit, having a second processor, a second memory, and a camera, operatively connected to the tactical computer; a remote unit, having a third processor, a third memory, and an augmented reality display, operatively connected to the tactical computer; the first memory, the second memory and the third memory including a set of instructions, that when executed, cause the system to perform the steps of: deriving a weapon path from a movement of a weapon trained on the target; deriving a first range to target from the remote unit; deriving a first target trajectory from the weapon path and the first range to target; deriving a camera path from a movement of the camera trained on the target; deriving a second range to target from the spotter unit; deriving a second target trajectory from the camera path and the second range to target; deriving a third target trajectory from the first target trajectory and the second target trajectory; calculating a lead from the third target trajectory; and, displaying a phantom target, on the third target trajectory, at the first lead, on the augmented reality display.
20. The system of claim 19 wherein the spotter unit further comprises an airborne platform and the set of instructions include further instructions that, when executed, cause the system to perform the steps of: moving the airborne platform on a flight path; and, the step of deriving the second trajectory further comprises the step of correcting the second trajectory for the flight path.
21. The system of claim 20 wherein and the set of instructions include further instructions that, when executed, cause the system to perform the step of: controlling the flight path by the tactical computer.
22. The system of claim 19 wherein the spotter unit further comprises a fixed platform, supporting the camera.
23. The system of claim 22 wherein and the set of instructions include further instructions that, when executed, cause the system to perform the step of: controlling the camera path by the tactical computer.
24. An augmented reality ranging and lead determination system for a target comprising: a tactical computer, having a first processor and a first memory; a neural network, having an input layer and an output layer, operatively connected to the tactical computer and the first memory; a remote unit, having a second processor and a second memory, and an augmented reality display, operatively connected to the tactical computer; the first memory and the second memory including a set of instructions, that when executed, cause the system to perform the steps of: deriving a weapon path from a movement of a weapon trained on the target; deriving a range to target from the remote unit; deriving a target trajectory from the weapon path and the range to target; training the neural network with the target trajectory and the range to target; submitting a current set of target path data to the input layer; and, reading a predictive set of lead data from the output layer.
25. The system of claim 24 wherein the step of training further comprises training the neural network with a set of successful lead data.
</claims>
</document>
