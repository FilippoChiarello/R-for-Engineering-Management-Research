<document>

<filing_date>
2020-04-23
</filing_date>

<publication_date>
2020-08-06
</publication_date>

<priority_date>
2017-07-25
</priority_date>

<ipc_classes>
G01S17/89,G01S7/48,G01S7/497,G06K9/00,G06K9/62
</ipc_classes>

<assignee>
WAYMO
</assignee>

<inventors>
TISDALE, JOHN
CHATHAM, ANDREW
MONTEMERLO, MICHAEL
</inventors>

<docdb_family_id>
65038209
</docdb_family_id>

<title>
Determining Yaw Error from Map Data, Lasers, and Cameras
</title>

<abstract>
The present disclosure relates to methods and systems that facilitate determination of a pose of a vehicle based on various combinations of map data and sensor data received from light detection and ranging (LIDAR) devices and/or camera devices. An example method includes receiving point cloud data from a (LIDAR) device and transforming the point cloud data to provide a top-down image. The method also includes comparing the top-down image to a reference image and determining, based on the comparison, a yaw error. An alternative method includes receiving camera image data from a camera and transforming the camera image data to provide a top-down image. The method also includes comparing the top-down image to a reference image and determining, based on the comparison, a yaw error.
</abstract>

<claims>
1. A method comprising: capturing point cloud data using a light detection and ranging (LIDAR) device, wherein the point cloud data comprises information indicative of an environment; transforming the point cloud data to provide a top-down image; comparing the top-down image to a reference image; and determining, based on the comparison, a yaw error.
2. The method of claim 1, wherein capturing point cloud data comprises capturing information indicative of a 360 degree environment.
3. The method of claim 1, wherein transforming the point cloud data to provide a top-down image comprises a scale-invariant feature transform, image flattening, image perspective rotation, or a 3D-to-2D image transform.
4. The method of claim 1, wherein transforming the point cloud data into a top-down image comprises culling or ignoring at least a portion of the point cloud data.
5. The method of claim 1, wherein the reference image comprises at least a portion of at least one of: one or more prior camera images, a prior LIDAR point cloud, prior RADAR data, satellite imagery, map data, or other overhead imagery.
6. The method of claim 1, further comprising forming an aggregate image, wherein the aggregate image comprises a combination, overlay, superposition, or image comparison between the top-down image and the reference image.
7. The method of claim 6, wherein the aggregate image comprises an indication of the yaw error and wherein the yaw error comprises an angle difference between an estimated heading and an actual heading.
8. The method of claim 1, further comprising: providing an adjustment signal to a pose estimator based on the determined yaw error.
9. A method comprising: capturing camera image data using a camera, wherein the camera image data comprises information indicative of an environment; transforming the camera image data to provide a top-down image; comparing the top-down image to a reference image; and determining, based on the comparison, a yaw error.
10. The method of claim 9, wherein capturing camera image data comprises capturing the camera image data using a plurality of cameras configured to capture a plurality of images comprising information indicative of a 360 degree field of view of the environment.
11. The method of claim 10, wherein transforming the camera image data comprises combining the plurality of images to form the top-down image, wherein the combining comprises one or more of projective geometry transforms or perspective morphing techniques applied to the plurality of images.
12. The method of claim 9, further comprising forming a forward aggregate image based on a combination, overlay, or superposition image comparison between a forward reference image and forward camera image data.
13. The method of claim 12, wherein determining the yaw error comprises comparing an actual yaw direction in the forward camera image data to an estimated yaw direction in the forward reference image.
14. A method comprising: capturing information indicative of a representation of a three dimensional environment; transforming the information to provide a top-down representation of the three dimensional environment; comparing the top-down representation to reference information; and determining, based on the comparison, a rotational angle difference.
15. The method of claim 14, wherein transforming the information to provide a top-down representation of the three dimensional environment comprises transforming the information with at least one of: a scale-invariant feature transform, image flattening, image perspective rotation, or 3D-to-2D image transform.
16. The method of claim 14, wherein the reference information comprises at least one of: a previously captured camera image, previously captured LIDAR point cloud data, previously captured RADAR data, map data, satellite imagery, or other overhead imagery.
17. The method of claim 14, wherein transforming the information to provide the top-down representation comprises forming an aggregated image based on at least two top-down representations, wherein forming the aggregated image comprises applying at least one of: a combination, an overlay, a superposition, a juxtaposition, or a comparison of the at least two top-down representations.
18. The method of claim 14, wherein comparing the top-down representation to the reference information comprises performing at least one of: a normalized cross-correlation, grayscale matching, gradient matching, histogram matching, edge detection, scale-invariant feature transform (SIFT), or speeded-up robust features (SURF).
19. The method of claim 14, further comprising: providing an adjustment signal to a pose estimator of a vehicle based on the determined rotational angle difference.
20. The method of claim 19, further comprising: determining, with the pose estimator, a yaw orientation of the vehicle; and determining a refined yaw orientation based on the adjustment signal and the determined yaw orientation of the vehicle.
</claims>
</document>
