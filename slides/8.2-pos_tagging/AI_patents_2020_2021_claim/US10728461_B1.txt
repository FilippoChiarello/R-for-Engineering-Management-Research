<document>

<filing_date>
2020-01-10
</filing_date>

<publication_date>
2020-07-28
</publication_date>

<priority_date>
2019-01-31
</priority_date>

<ipc_classes>
G06K9/00,G06N3/04,G06T7/73,H04N5/232
</ipc_classes>

<assignee>
STRADVISION
</assignee>

<inventors>
BOO, SUKHOON
CHO, HOJIN
JANG, TAEWOONG
JE, HONGMO
JEONG, KYUNGJOONG
KIM, HAK-KYOUNG
KIM, KYE-HYEON
KIM, YONGJOONG
LEE, HYUNG SOO
LEE, MYEONG-CHUN
NAM, WOONHYUN
RYU, WOOJU
SHIN, DONGSOO
SUNG, MYUNGCHUL
YEO, DONGHUN
</inventors>

<docdb_family_id>
69187605
</docdb_family_id>

<title>
METHOD FOR CORRECTING MISALIGNMENT OF CAMERA BY SELECTIVELY USING INFORMATION GENERATED BY ITSELF AND INFORMATION GENERATED BY OTHER ENTITIES AND DEVICE USING THE SAME
</title>

<abstract>
A method for correcting an incorrect angle of a camera is provided. And the method includes steps of: (a) a computing device, generating first reference data or second reference data according to circumstance information by referring to a reference image; (b) the computing device generating a first angle error or a second angle error by referring to the first reference data or the second reference data with vehicle coordinate data; and (c) the computing device instructing a physical rotation module to adjust the incorrect angle by referring to the first angle error or the second angle error.
</abstract>

<claims>
1. A method for correcting an incorrect angle of an unwantedly yawed camera on a subject vehicle by using at least part of primary information generated by the subject vehicle itself and secondary information generated by other entities, wherein at least one of the primary information and the secondary information is selected by referring to circumstance information on surroundings of the subject vehicle, to be used for correcting the incorrect angle, comprising steps of: (a) a computing device, if at least one reference image is acquired through a camera on the subject vehicle, performing, (i) a process of instructing, if the circumstance information corresponds to a first condition related to lanes on roads, a first Convolutional Neural Network (CNN) to apply at least one first CNN operation to the reference image, to thereby generate first reference data including information on reference lanes of the reference image, and (ii) a process of instructing, if the circumstance information corresponds to a second condition related to other vehicles on the roads, a second CNN to apply at least one second CNN operation to the reference image, to thereby generate second reference data including information on one or more reference vehicles of the reference image; (b) the computing device instructing a correction module to perform (i) a process of generating, if the circumstance information corresponds to the first condition, at least one first angle error on the camera corresponding to the reference lanes by referring to the first reference data and vehicle coordinate data, and (ii) a process of generating, if the circumstance information corresponds to the second condition, at least one second angle error on the camera corresponding to the reference vehicles by referring to the second reference data and the vehicle coordinate data; and (c) the computing device instructing a physical rotation module on the subject vehicle to perform (i) a process of adjusting, if the circumstance information corresponds to the first condition, the incorrect angle by referring to the first angle error corresponding to the reference lanes and (ii) a process of adjusting, if the circumstance information corresponds to the second condition, the incorrect angle by referring to the second angle error corresponding to the reference vehicles.
2. The method of claim 1, wherein, at the step of (a), the computing device, if the circumstance information corresponds to the first condition, (i) instructs at least one first convolutional layer in the first CNN to apply at least one first convolutional operation to the reference image, to thereby generate at least one first reference feature map, (ii) instructs at least one first pooling layer in the first CNN to apply at least one first pooling operation to the first reference feature map, to thereby generate at least one first reference pooled feature map, and (iii) instructs at least one first Fully-Connected (FC) layer in the first CNN to apply at least one first FC operation to the first reference pooled feature map, to thereby generate detection result on the reference lanes of the reference image as the first reference data.
3. The method of claim 2, wherein, at the step of (b), the computing device, if the circumstance information corresponds to the first condition, instructs the correction module to (i) map the reference lanes onto a coordinate plane corresponding to the vehicle coordinate data, by referring to the vehicle coordinate data, (ii) calculate at least one first difference angle between a reference axis on the coordinate plane and at least one of the reference lanes, and (iii) output the first difference angle as the first angle error.
4. The method of claim 1, wherein, at the step of (a), the computing device, if the circumstance information corresponds to the second condition, (i) instructs at least one second convolutional layer in the second CNN to apply at least one second convolutional operation to the reference image, to thereby generate at least one second reference feature map, (ii) instructs at least one second pooling layer in the second CNN to apply at least one second pooling operation to the second reference feature map, to thereby generate at least one second reference pooled feature map, and (iii) instructs at least one second Fully-Connected (FC) layer in the second CNN to apply at least one second FC operation to the second reference pooled feature map, to thereby generate detection result on the reference vehicles of the reference image as the second reference data.
5. The method of claim 4, wherein, at the step of (a), the computing device, if the circumstance information corresponds to the second condition, in parallel with said process of generating the second reference data, acquires comparable data from one or more V2V communication vehicles located closer than a first threshold from the subject vehicle by performing wireless communications with the V2V communication vehicles, wherein K-th specific comparable data, among the comparable data, acquired from a K-th specific V2V communication vehicle among a first to an N-th specific V2V communication vehicles in the V2V communication vehicles, includes information, generated by the K-th specific V2V communication vehicle, on locations of its K-th specific surrounding vehicles located closer than a second threshold from the subject vehicle, and wherein, at the step of (b), the computing device, if the circumstance information corresponds to the second condition, instructs the correction module to (i) perform (i-1) a process of generating one or more image-based coordinates on a coordinate plane corresponding to the vehicle coordinate data representing one or more relative locations of the reference vehicles in relation to the subject vehicle by referring to the second reference data, and (i-2) a process of generating one or more communication-based coordinates on the coordinate plane representing one or more relative locations of surrounding vehicles of the V2V communication vehicles in relation to the subject vehicle by referring to the comparable data, and then to (ii) generate the second angle error by referring to the image-based coordinates and the communication-based coordinates, wherein N denotes the number of the V2V communication vehicles, and K denotes an integer from 1 to N.
6. The method of claim 5, wherein, at the step of (b), the computing device, if the circumstance information corresponds to the second condition, instructs the correction module, by referring to information on a Field-Of-View (FOV) of the camera and the K-th specific comparable data acquired from the K-th specific V2V communication module, to (i) map the locations of said its K-th specific surrounding vehicles estimated by the K-th specific V2V communication vehicle onto the coordinate plane, to thereby generate one or more K-th initial coordinates, and (ii) generate the communication-based coordinates of the surrounding vehicles by referring to one or more first to one or more N-th initial coordinates including the K-th initial coordinates.
7. The method of claim 5, wherein, at the step of (b), the computing device instructs the correction module to calculate at least one second difference angle between (i) at least one first specific direct line including at least one specific communication-based coordinate of at least one specific reference vehicle which is included in both of the reference vehicles and the surrounding vehicles, and an origin point of the coordinate plane and (ii) at least one second specific direct line including at least one specific image-based coordinate of at least one specific reference vehicle and the origin point of the coordinate plane, and output the second difference angle as the second angle error.
8. The method of claim 7, wherein, at the step of (b), the computing device, if there are a plurality of (i) specific reference vehicles included in both of the reference vehicles and the surrounding vehicles, (ii) specific image-based coordinates corresponding to the specific reference vehicles and (iii) specific communication-based coordinates corresponding to the specific reference vehicles, generates the second angle error by referring to a following formula: wherein M denotes the number of the specific reference vehicles, (xik,yik) denotes a K-th specific image-based coordinate of a K-th specific reference vehicle, (xck,yck) denotes a K-th specific communication-based coordinate thereof, and Î±k denotes a weight assigned thereto.
9. The method of claim 1, wherein, at the step of (a), the computing device, if driving information of the subject vehicle is acquired from a Controller Area Network (CAN) of the subject vehicle, determines whether the driving information corresponds to a condition A for a straight driving or a condition B for a non-straight driving, and instructs the first CNN or the second CNN to apply the first CNN operation or the second CNN operation to the reference image if the driving information corresponds to the condition A.
10. A computing device for correcting an incorrect angle of an unwantedly yawed camera on a subject vehicle by using at least part of primary information generated by the subject vehicle itself and secondary information generated by other entities, wherein at least one of the primary information and the secondary information is selected by referring to circumstance information on surroundings of the subject vehicle, to be used for correcting the incorrect angle, comprising: at least one memory that stores instructions; and at least one processor configured to execute the instructions to perform processes of: (I) if at least one reference image is acquired through a camera on the subject vehicle, instructing, if the circumstance information corresponds to a first condition related to lanes on roads, a first Convolutional Neural Network (CNN) to apply at least one first CNN operation to the reference image, to thereby generate first reference data including information on reference lanes of the reference image, and instructing, if the circumstance information corresponds to a second condition related to other vehicles on the roads, a second CNN to apply at least one second CNN operation to the reference image, to thereby generate second reference data including information on one or more reference vehicles of the reference image; (II) instructing an correction module to generate, if the circumstance information corresponds to the first condition, at least one first angle error on the camera corresponding to the reference lanes by referring to the first reference data and vehicle coordinate data, and generate, if the circumstance information corresponds to the second condition, at least one second angle error on the camera corresponding to the reference vehicles by referring to the second reference data and the vehicle coordinate data; and (III) instructing a physical rotation module on the subject vehicle to adjust, if the circumstance information corresponds to the first condition, the incorrect angle by referring to the first angle error corresponding to the reference lanes and adjust, if the circumstance information corresponds to the second condition, the incorrect angle by referring to the second angle error corresponding to the reference vehicles.
11. The device of claim 10, wherein, at the process of (I), the processor, if the circumstance information corresponds to the first condition, (i) instructs at least one first convolutional layer in the first CNN to apply at least one first convolutional operation to the reference image, to thereby generate at least one first reference feature map, (ii) instructs at least one first pooling layer in the first CNN to apply at least one first pooling operation to the first reference feature map, to thereby generate at least one first reference pooled feature map, and (iii) instructs at least one first Fully-Connected (FC) layer in the first CNN to apply at least one first FC operation to the first reference pooled feature map, to thereby generate detection result on the reference lanes of the reference image as the first reference data.
12. The device of claim 11, wherein, at the process of (II), the processor, if the circumstance information corresponds to the first condition, instructs the correction module to (i) map the reference lanes onto a coordinate plane corresponding to the vehicle coordinate data, by referring to the vehicle coordinate data, (ii) calculate at least one first difference angle between a reference axis on the coordinate plane and at least one of the reference lanes, and (iii) output the first difference angle as the first angle error.
13. The device of claim 10, wherein, at the process of (I), the processor, if the circumstance information corresponds to the second condition, (i) instructs at least one second convolutional layer in the second CNN to apply at least one second convolutional operation to the reference image, to thereby generate at least one second reference feature map, (ii) instructs at least one second pooling layer in the second CNN to apply at least one second pooling operation to the second reference feature map, to thereby generate at least one second reference pooled feature map, and (iii) instructs at least one second Fully-Connected (FC) layer in the second CNN to apply at least one second FC operation to the second reference pooled feature map, to thereby generate detection result on the reference vehicles of the reference image as the second reference data.
14. The device of claim 13, wherein, at the process of (I), the processor, if the circumstance information corresponds to the second condition, in parallel with said process of generating the second reference data, acquires comparable data from one or more V2V communication vehicles located closer than a first threshold from the subject vehicle by performing wireless communications with the V2V communication vehicles, wherein K-th specific comparable data, among the comparable data, acquired from a K-th specific V2V communication vehicle among a first to an N-th specific V2V communication vehicles in the V2V communication vehicles, includes information, generated by the K-th specific V2V communication vehicle, on locations of its K-th specific surrounding vehicles located closer than a second threshold from the subject vehicle, and wherein, at the process of (II), the processor, if the circumstance information corresponds to the second condition, instructs the correction module to (i) perform (i-1) a process of generating one or more image-based coordinates on a coordinate plane corresponding to the vehicle coordinate data representing one or more relative locations of the reference vehicles in relation to the subject vehicle by referring to the second reference data, and (i-2) a process of generating one or more communication-based coordinates on the coordinate plane representing one or more relative locations of surrounding vehicles of the V2V communication vehicles in relation to the subject vehicle by referring to the comparable data, and then to (ii) generate the second angle error by referring to the image-based coordinates and the communication-based coordinates, wherein N denotes the number of the V2V communication vehicles, and K denotes an integer from 1 to N.
15. The device of claim 14, wherein, at the process of (II), the processor, if the circumstance information corresponds to the second condition, instructs the correction module, by referring to information on a Field-Of-View (FOV) of the camera and the K-th specific comparable data acquired from the K-th specific V2V communication module, to (i) map the locations of said its K-th specific surrounding vehicles estimated by the K-th specific V2V communication vehicle onto the coordinate plane, to thereby generate one or more K-th initial coordinates, and (ii) generate the communication-based coordinates of the surrounding vehicles by referring to one or more first to one or more N-th initial coordinates including the K-th initial coordinates.
16. The device of claim 14, wherein, at the process of (II), the processor instructs the correction module to calculate at least one second difference angle between (i) at least one first specific direct line including at least one specific communication-based coordinate of at least one specific reference vehicle which is included in both of the reference vehicles and the surrounding vehicles, and an origin point of the coordinate plane and (ii) at least one second specific direct line including at least one specific image-based coordinate of at least one specific reference vehicle and the origin point of the coordinate plane, and output the second difference angle as the second angle error.
17. The device of claim 16, wherein, at the process of (II), the processor, if there are a plurality of (i) specific reference vehicles included in both of the reference vehicles and the surrounding vehicles, (ii) specific image-based coordinates corresponding to the specific reference vehicles and (iii) specific communication-based coordinates corresponding to the specific reference vehicles, generates the second angle error by referring to a following formula: wherein M denotes the number of the specific reference vehicles, (xik,yik) denotes a K-th specific image-based coordinate of a K-th specific reference vehicle, (xck,yck) denotes a K-th specific communication-based coordinate thereof, and Î±k denotes a weight assigned thereto.
18. The device of claim 10, wherein, at the process of (I), the processor, if driving information of the subject vehicle is acquired from a Controller Area Network (CAN) of the subject vehicle, determines whether the driving information corresponds to a condition A for a straight driving or a condition B for a non-straight driving, and instructs the first CNN or the second CNN to apply the first CNN operation or the second CNN operation to the reference image if the driving information corresponds to the condition A.
</claims>
</document>
