<document>

<filing_date>
2018-04-17
</filing_date>

<publication_date>
2020-01-30
</publication_date>

<priority_date>
2017-04-20
</priority_date>

<ipc_classes>
G06N3/063,G06N3/08
</ipc_classes>

<assignee>
SHANGHAI CAMBRICON INFORMATION TECHNOLOGY COMPANY
</assignee>

<inventors>
CHEN, TIANSHI
CHEN, XIAOBIN
LIU, SHAOLI
ZHUANG, YIMIN
LIU, DAOFU
WANG, ZAI
</inventors>

<docdb_family_id>
63856484
</docdb_family_id>

<title>
COMPUTING APPARATUS AND RELATED PRODUCT
</title>

<abstract>
The present application provides an operation device and related products. The operation device is configured to execute operations of a network model, wherein the network model includes a neural network model and/or non-neural network model; the operation device comprises an operation unit, a controller unit and a storage unit, wherein the storage unit includes a data input unit, a storage medium and a scalar data storage unit. The technical solution provided by this application has advantages of a fast calculation speed and energy-saving.
</abstract>

<claims>
1. An operation device, comprising: a storage unit configured to store data and instructions; a controller unit configured to extract, from the storage unit, a first instruction including sorting instructions or sparse processing instructions and a first data corresponding to the first instruction including input neuron data and weight data; and an operation unit configured to, in response to the first instruction, perform an operation corresponding to the first instruction on the input neuron data and the weight data, to obtain an operation result.
2. The device according to claim 1, wherein the controller unit comprises an instruction buffer unit configured to buffer the instructions and an instruction processing unit configured to implement decoding function.
3. The device according to claim 2, further comprising: a configuration parsing unit and a mapping unit, and when the first instruction is the sparse processing instruction and the first data further includes preset configuration data, the configuration parsing unit is configured to set a mapping mode according to the preset configuration data; the mapping unit is configured to perform mapping processing on the input neuron and the weight data according to the mapping mode to obtain an input neuron-weight pair which is a mapping relationship between the input neuron data and the weight data after the mapping processing; the instruction buffer unit is configured to receive a target instruction transmitted by the controller unit; the instruction processing unit is configured to decode the target instruction into an operation instruction; and the operation unit performs a calculating operation on the input neuron-weight pair to obtain an operation result.
4. The device according to claim 1, wherein the first data further comprises sparse parameters; the device further comprises: a sparse unit configured to perform a sparse processing on the operation result according to the sparse parameters to obtain an operation result after the sparse processing.
5. The device according to claim 4, wherein the sparse parameters include a sparse mode; the mapping unit performs a mapping processing on the input neuron and the weight according to the mapping mode, by: obtaining, when the sparse mode is a sparse mode 1, a weight sparse sequence corresponding to the sparse mode 1 and performing a mapping processing on the weight according to that weight sparse sequence; obtaining, when the sparse mode is a sparse mode 2, a neuron sparse sequence corresponding to the sparse mode 2 and performing a mapping processing on the input neuron according to the neuron sparse sequence; and obtaining, when the sparse mode is a sparse mode 3, a weight sparse sequence and a neuron sparse sequence corresponding to the sparse mode 3 and performing a mapping processing on the input neuron and the weight data according to the weight sparse sequence and the neuron sparse sequence.
6. The device according to claim 5, wherein the sparse parameters further comprises a sparse rate, and the sparse unit performs sparse processing on the operation result according to the sparse parameters, by: sorting the absolute values of the elements of the neuron data, calculating the number of elements that need to be taken sparse processing according to the sparse rate, then performing a sparse processing on the elements of the sorted neuron data according to the number of elements that need to be taken sparse processing, and transmitting the sparse neuron data after being taken sparse processing and neuron sparse sequences to the controller unit.
7. The device according to claim 5, wherein the sparse unit performs a sparse processing on the operation result according to the sparse parameters, in such a way that the elements whose neuron data is 0 remains unchanged, and the elements whose neuron data is within the preset value interval are set to 0.
8. The device according to claim 2, wherein if the input neuron data is a vector; the first instruction is a vector sorting instruction and the first data is an data vector to be sorted and an intermediate result of the data vector to be sorted; the controller unit comprises an instruction processing unit, the instruction processing unit is configured to decode the vector sorting instruction into a microinstruction performed by the operation unit; and the operation unit is further configured to sort the data victor to be sorted or the intermediate result according to the microinstruction to obtain a sorted vector having the same length as the data vector to be sorted.
9. The device according to claim 8, wherein the operation unit sorting the data vector to be sorted or the intermediate result according to the microinstruction comprises: step A of, if what obtained after the sorting is the intermediate result of the data vector to be sorted, writing the intermediate result of the data vector to be sorted back to the source address of the storage unit, and repeatedly executing step A until obtaining the final result of the data vector to be sorted, then going to step B; and step B of, if what obtained after the sorting is the final result of the data vector to be sorted, writing the final result of the data vector to be sorted back to the data I/O unit of the storage unit according to the destination operand address provided by the vector sorting instruction, and the operation ending.
10. The device according to claim 9, wherein the operation unit includes n vector merging units, where n is an integer greater than or equal to 2; the n vector merging units are configured to read no more than 2n sub-vectors having been merged or ordered sub-vectors from the storage unit, merge them and store the merged result to the storage unit until the length of the sub-vectors having been merged is equal to the length of the data vector to be sorted to form a sorted vector.
11. The device according to claim 10, wherein the step A performed by the operation unit comprises: step A1 of initializing the number of merging i to 1; step A2 of performing calculation by the n vector merging units, and when merging the data vector to be sorted or the intermediate result at the ith time, obtaining the data vector to be sorted or the intermediate result from the storage unit, dividing the data vector to be sorted or the intermediate result into [m/2i-1] portions in order, and merging the vectors into pairs, each vector except for the last one having a length of 2i-1, where m is the length of the data vector to be sorted; and step A3 of increasing the number of merging by one if the number of merging i<[log 2m] and writing the processed intermediate result back to a source address of the storage unit, repeatedly executing steps A2 to A3 until i=[log 2m], then going to step B; the step B performed by the operation unit comprises: if the number of merging i=[log 2m] and if there are only two data vectors to be sorted after assignment, setting the obtained vector as a vector having been sorted after being merged by the first vector merging unit of the n vector merging units, writing the sorted result in the data output unit according to the destination operand address provided by the vector sorting instruction, and the operation ending.
12. The device according to claim 11, wherein the operation unit performing a pairwise merging of the vectors comprises: numbering 1, 2, . . . , [m/2i-1] in order according to the source operand address provided by the vector sorting instruction, and assigning the vectors numbered 2*j−1, 2*j to the ((j−1) mod n)+1)th vector merging unit for processing, where j>0.
13. The device according to claim 12, wherein the data vector to be sorted is a characteristic value vector and a probability vector of the classification result corresponding to the matrix for testing data characteristics at the preprocessing stage.
14. The device according to claim 13, wherein the first instruction comprises one or any combination of the following instructions: inter-vector AND instruction VAV, intra-vector AND instruction VAND, inter-vector OR instruction VOV, intra-vector OR instruction VOR, vector index instruction VE, vector logarithmic instruction VL, vector greater-than decision instruction VGT, vector equal-to decision instruction VEQ, vector NOT instruction VINV, vector selection and merge instruction VMER, vector maximum instruction VMAX, scalar extension instruction STV, scalar-vector replacement instruction STVPN, vector-scalar replacement instruction VPNTS, vector retrieval instruction VR, vector dot-product instruction VP, random vector instruction RV, cyclic shift instruction VCS, vector load instruction VLOAD, vector storage instruction VS, vector moving instruction VMOVE, matrix retrieval instruction MR, matrix load instruction ML, matrix storage instruction MS and matrix moving instruction MMOVE.
15. The device according to claim 14, wherein the device is used for a sparse neural network operation or a dense neural network operation.
16. A neural network calculating device, comprising: one or more operation devices configured to acquire data to be operated and control information from other processing devices, execute a specified neural network operation and transmit the execution result to other processing devices through an IO interface; wherein, when the neural network calculating device includes a plurality of the operation devices, the plurality of the operation devices are capable of being connected through a specific structure and transmitting data; the plurality of operation devices interconnect through Peripheral Component Interconnect-Express (PCIE) bus and transmit data to support operation on a larger scale neural network; the plurality of operation devices share the same control system or have their own control systems; the plurality of operation devices share a memory or have their own memories; the interconnection manner of the plurality of operation devices is an arbitrary interconnection topology.
17. An operation method applicable to an operation device, wherein the operation device comprises a storage unit, a controller unit, and an operation unit, the operation method comprises: storing, by the storage unit, data and instructions; extracting, by the controller unit, from the storage unit a first instruction including sorting instructions or sparse processing instructions and a first data corresponding to the first instruction including input neuron data and weight data; and in response to the first instruction, performing, by the operation unit, an operation corresponding to the first instruction on the input neuron data and the weight data, to obtain an operation result.
18. The method according to claim 17, wherein the controller unit comprises an instruction buffer unit configured to buffer the instructions and an instruction processing unit configured to implement decoding function.
19. The method according to claim 18, wherein the operation device further comprises a configuration parsing unit and a mapping unit and when the first instruction is the sparse processing instruction and the first data further includes preset configuration data, the configuration parsing unit sets a mapping mode according to the preset configuration data; the mapping unit performs mapping processing on the input neuron and the weight data according to the mapping mode to obtain an input neuron-weight pair which is a mapping relationship between the input neuron data and the weight data after the mapping processing; and the instruction buffer unit receives a target instruction transmitted by the controller unit; the instruction processing unit decodes the target instruction into an operation instruction; and the operation unit performs a calculating operation on the input neuron-weight pair to obtain an operation result.
20. The method according to claim 17, wherein the first data further comprises sparse parameters; the method further comprises performing a sparse processing on the operation result by a sparse unit according to the sparse parameters to obtain an operation result after the sparse processing.
21. The method according to claim 20, wherein the sparse parameters include a sparse mode; the mapping unit performs a mapping processing on the input neuron and the weight according to the mapping mode, by: obtaining, when the sparse mode is a sparse mode 1, a weight sparse sequence corresponding to the sparse mode 1 and performing a mapping processing on the weight according to that weight sparse sequence; obtaining, when the sparse mode is a sparse mode 2, a neuron sparse sequence corresponding to the sparse mode 2 and performing a mapping processing on the input neuron according to the neuron sparse sequence; and obtaining, when the sparse mode is a sparse mode 3, a weight sparse sequence and a neuron sparse sequence corresponding to the sparse mode 3 and performing a mapping processing on the input neuron and the weight data according to the weight sparse sequence and the neuron sparse sequence.
22. The method according to claim 21, wherein the sparse parameters further comprises a sparse rate, the sparse unit performs sparse processing on the operation result according to the sparse parameters, by: sorting the absolute values of the elements of the neuron data, calculating the number of elements that need to be taken sparse processing according to the sparse rate, then performing a sparse processing on the elements of the sorted neuron data according to the number of elements that need to be taken sparse processing, and transmitting the sparse neuron data after being taken sparse processing and neuron sparse sequences to the controller unit.
23. The method according to claim 21, wherein the sparse unit performs a sparse processing on the operation result according to the sparse parameters, in such a way that the elements whose neuron data is 0 remains unchanged, and the elements whose neuron data is within the preset value interval are set to 0.
24. The method according to claim 18, wherein if the input neuron data is a vector; the first instruction is a vector sorting instruction and the first data is an data vector to be sorted and an intermediate result of the data vector to be sorted; the controller unit comprises an instruction processing unit, the instruction processing unit is configured to decode the vector sorting instruction into a microinstruction performed by the operation unit; the operation unit is further configured to sort the data vector to be sorted or the intermediate result according to the microinstruction to obtain a sorted vector having the same length as the data vector to be sorted.
25. The method according to claim 24, wherein the operation unit sorting the data vector to be sorted or the intermediate result according to the microinstruction comprises: step A of, if what obtained after the sorting is the intermediate result of the data vector to be sorted, writing the intermediate result of the data vector to be sorted back to the source address of the storage unit, and repeatedly executing step A until obtaining the final result of the data vector to be sorted, then going to step B; and step B of, if what obtained after the sorting is the final result of the data vector to be sorted, writing the final result of the data vector to be sorted back to the data I/O unit of the storage unit according to the destination operand address provided by the vector sorting instruction, and the operation ending.
26. The method according to claim 25, wherein the operation unit includes n vector merging units, where n is an integer greater than or equal to 2; the n vector merging units is configured to read no more than 2n sub-vectors having been merged or ordered sub-vectors from the storage unit, merge them and store the merged result to the storage unit until the length of the sub-vectors having been merged is equal to the length of the data vector to be sorted to form a sorted vector.
27. The method according to claim 26, wherein the step A performed by the operation unit comprises: step A1 of initializing the number of merging i to 1; step A2 of performing calculation by the n vector merging units, and when merging the data vector to be sorted or the intermediate result at the ith time, obtaining the data vector to be sorted or the intermediate result from the storage unit, dividing the data vector to be sorted or the intermediate result into [m/2i-1] portions in order, and merging the vectors into pairs, each vector except for the last one having a length of 2i-1, where m is the length of the data vector to be sorted; and step A3 of increasing the number of merging by one if the number of merging i<[log 2m] and writing the processed intermediate result back to a source address of the storage unit, repeatedly executing steps A2 to A3 until i=[log2m], then going to step B; the step B performed by the operation unit comprises: if the number of merging i=[log2m] and if there are only two data vectors to be sorted after assignment, the obtained vector is a vector having been sorted after being merged by the first vector merging unit of the n vector merging units, writing the sorted result in the data output unit according to the destination operand address provided by the vector sorting instruction, and the operation ends.
28. The method according to claim 27, wherein the operation unit performs a pairwise merging of the vectors, by: numbering 1, 2, . . . , [m/2i-1] in order according to the source operand address provided by the vector sorting instruction, assigning the vectors numbered 2*j−1, 2*j to the ((j−1) mod n)+1)th vector merging unit for processing, where j>0.
29. The method according to claim 28, wherein the data vector to be sorted is a characteristic value vector and a probability vector of the classification result corresponding to the matrix for testing data characteristics at the preprocessing stage.
30. The method according to claim 29, wherein the first instruction comprises one or any combination of the following instructions: inter-vector AND instruction VAV, intra-vector AND instruction VAND, inter-vector OR instruction VOV, intra-vector OR instruction VOR, vector index instruction VE, vector logarithmic instruction VL, vector greater-than decision instruction VGT, vector equal-to decision instruction VEQ, vector NOT instruction VINV, vector selection and merge instruction VMER, vector maximum instruction VMAX, scalar extension instruction STV, scalar-vector replacement instruction STVPN, vector-scalar replacement instruction VPNTS, vector retrieval instruction VR, vector dot-product instruction VP, random vector instruction RV, cyclic shift instruction VCS, vector load instruction VLOAD, vector storage instruction VS, vector moving instruction VMOVE, matrix retrieval instruction MR, matrix load instruction ML, matrix storage instruction MS and matrix moving instruction MMOVE.
31. The device according to claim 30, wherein the device is used for a sparse neural network operation or a dense neural network operation.
</claims>
</document>
