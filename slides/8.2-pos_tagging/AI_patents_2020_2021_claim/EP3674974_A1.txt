<document>

<filing_date>
2019-12-17
</filing_date>

<publication_date>
2020-07-01
</publication_date>

<priority_date>
2018-12-31
</priority_date>

<ipc_classes>
G06K9/00
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
HAN, JAEJOON
CHOI, CHANGKYU
CHANG, WONSUK
RHEE, SEONMIN
HAN, SEUNGJU
KO, MINSU
</inventors>

<docdb_family_id>
68944163
</docdb_family_id>

<title>
APPARATUS AND METHOD WITH USER VERIFICATION
</title>

<abstract>
A processor-implemented verification method includes: detecting a characteristic of an input image; acquiring input feature transformation data and enrolled feature transformation data by respectively transforming input feature data and enrolled feature data based on the detected characteristic, wherein the input feature data is extracted from the input image using a feature extraction model; and verifying a user corresponding to the input image based on a result of comparison between the input feature transformation data and the enrolled feature transformation data.
</abstract>

<claims>
1. A processor-implemented verification method comprising: detecting a characteristic of an input image; acquiring input feature transformation data and enrolled feature transformation data by respectively transforming input feature data and enrolled feature data based on the detected characteristic, wherein the input feature data is extracted from the input image using a feature extraction model; and verifying a user corresponding to the input image based on a result of comparison between the input feature transformation data and the enrolled feature transformation data.
2. The method of claim 1, wherein the input image includes a facial image of the user.
3. The method of claim 1 or 2, wherein the verifying of the user comprises determining whether the user is an enrolled user corresponding to the enrolled feature data, and the enrolled feature data was previously extracted using the feature extraction model.
4. The method of any of the previous claims, wherein the characteristic is an illuminance characteristic of the input image.
5. The method of any of the previous claims, wherein the detecting of the characteristic comprises:
detecting at least one of a landmark characteristic associated with a landmark of an object appearing in the input image and an environment characteristic associated with a capturing environment of the input image, wherein, preferably, the object is a face of the user and the landmark is one of a facial feature and an item disposed on the face.
6. The method of any of the previous claims, wherein the feature extraction model is a trained neural network, preferably comprising a convolutional neural network (CNN).
7. The method of any of the previous claims, wherein the acquiring of the input feature transformation data and the enrolled feature transformation data comprises: generating the input feature transformation data by applying a transformation function corresponding to the characteristic to the input feature data; and generating the enrolled feature transformation data by applying the transformation function to the enrolled feature data, wherein the transformation function is preferably determined differently for the user than for another user, and/or wherein the transformation function is determined in a user-enrollment process comprising enrolling the enrolled feature data; and/or, respectively transforming the input feature data and the enrolled feature data using a linear transformation or a nonlinear transformation determined based on the characteristic; and/or, respectively performing a projection operation corresponding to the characteristic on the input feature data and the enrolled feature data; and/or, determining target dimensional component information based on the characteristic; and generating the input feature transformation data and the enrolled feature transformation data respectively based on the input feature data and the enrolled feature data by maintaining a target dimensional component and excluding a remaining dimensional component.
8. The method of any of the previous claims, wherein: the detecting of the characteristic comprises detecting a plurality of characteristics of the input image, including the characteristic, the acquiring of the input feature transformation data comprises calculating, as the input feature transformation data, a weighted sum of results obtained by applying transformation functions corresponding to the plurality of characteristics to the input feature data, and the acquiring of the enrolled feature transformation data comprises calculating, as the enrolled feature transformation data, a weighted sum of results obtained by applying the transformation functions to the enrolled feature data, wherein the method preferably further comprises selecting a reference user to be verified from a plurality of enrolled users, wherein the determining of the target dimensional component information comprises loading target dimensional component information corresponding to the characteristic from a database based on transformation-related information mapped to enrolled feature data of the reference user.
9. The method of the previous claims, further comprising: selecting a reference user to be verified from a plurality of enrolled users, wherein the acquiring of the input feature transformation data and the enrolled feature transformation data comprises loading a transformation function corresponding to the characteristic from a database, among a plurality of transformation functions mapped to enrolled feature data of the reference user.
10. The method of any of the previous claims, further comprising: extracting reference feature data from a reference image of a reference user in response to the reference user being enrolled; extracting augmented feature data from an augmented image acquired by augmenting the reference image based on an augmentation characteristic; and determining a transformation function associated with the augmentation characteristic based on a comparison between the reference feature data and the augmented feature data, mapping the determined transformation function to the reference user, and storing the mapped transformation function; wherein, preferably: the augmentation characteristic corresponds to the detected characteristic of the input image, and wherein the acquiring of the input feature transformation data and the enrolled feature transformation data comprises:
generating the input feature transformation data by applying the transformation function to the input feature data; and generating the enrolled feature transformation data by applying the transformation function to the enrolled feature data.
11. The method of claim 10, wherein the determining of the transformation function comprises: calculating a variation score for each dimensional component of the reference feature data and the augmented feature data; and determining a transformation function associated with the augmentation characteristic based on the variation score; wherein the determining of the transformation function preferably comprises: determining, to be a target dimensional component, a dimensional component, among the dimensional components, corresponding to a dimension index indicating a variation score less than or equal to a change threshold among a plurality of dimension indices in the variation score; and determining, to be a remaining dimensional component, a dimensional component, among the dimensional components, corresponding to a dimension index indicating a variation score greater than the change threshold in the variation score.
12. The method of any of the previous claims, wherein the verifying of the user corresponding to the input image comprises: calculating a similarity between the input feature transformation data and the enrolled feature transformation data, preferably comprising calculating a cosine similarity between a feature indicated by the input feature transformation data and a feature indicated by the enrolled feature transformation data; and determining that a verification for the user is successful in response to the similarity being greater than a threshold similarity.
13. The method of any of the previous claims, wherein the verifying of the user corresponding to the input image comprises: indicating a result of the verifying of the user; wherein the indicating of the result of the verifying of the user preferably comprises:
unlocking a device in response to the result of the verifying of the user being a successful verification.
14. A non-transitory computer-readable storage medium storing instructions that, when executed by one or more processors, configure the one or more processors to perform the method of any of the previous claims.
15. A verification apparatus comprising: an image acquirer configured to acquire an input image; and one or more processors configured to execute the steps of the method of any of the previous claims, wherein the apparatus is preferably any one of an image processing device, a smartphone, a wearable device, a tablet computer, a netbook, a laptop, a desktop, a personal digital assistant (PDA), a set-top box, a home appliance, a biometric door lock, a security device, and a vehicle starter, and the image acquirer comprises a camera.
</claims>
</document>
