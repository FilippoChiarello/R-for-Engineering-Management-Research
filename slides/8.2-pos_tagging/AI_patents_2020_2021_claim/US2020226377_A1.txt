<document>

<filing_date>
2020-03-25
</filing_date>

<publication_date>
2020-07-16
</publication_date>

<priority_date>
2020-03-25
</priority_date>

<ipc_classes>
G06K9/00,G06K9/62
</ipc_classes>

<assignee>
INTEL CORPORATION
</assignee>

<inventors>
CAMPOS MACIAS, LEOBARDO
DE LA GUARDIA GONZALEZ, RAFAEL
GOMEZ GUTIERREZ, DAVID
GUZMAN LEGUEL, ANTHONY KYUNG
PARRA VILCHIS, JOSE IGNACIO
</inventors>

<docdb_family_id>
71517597
</docdb_family_id>

<title>
ROBUST OBJECT DETECTION AND CLASSIFICATION USING STATIC-BASED CAMERAS AND EVENTS-BASED CAMERAS.
</title>

<abstract>
Techniques are disclosed to facilitate, in autonomous vehicles, the robust detection and classification of objects in a scene using a static sensors in conjunction with event-based sensors. A trained system architecture may be implemented, and the fusion of both sensors thus allows for the consideration of scenes with overexposure, scenes with underexposure, as well as scenes in which there is no movement. In doing so, the autonomous vehicle may detect and classify objects in conditions in which each sensor, if operating separately, would not otherwise be able to classify (or classify with high uncertainty) due to the sensing environment.
</abstract>

<claims>
1. An object detection and classification system of an autonomous vehicle (AV), the object detection and classification system comprising: one or more processors; and a memory configured to store instructions that, when executed by the one or more processors, cause the one or more processors to: obtain a first set of images associated with a scene, the first set of images being encoded using static-based camera sensor data; obtain a second set of images associated with the scene, the second set of images including encoded information representing events occurring within a time window using event-based camera sensor data; receive the first set of images and the second set of images as separate respective channel inputs; process (i) frames of the first set of images including the static-based camera sensor data, and (ii) the encoded information included in the second set of images, received via the channel inputs, to determine a location and type of one or more objects included in the scene; and provide the location and type of the one or more objects included in the scene to a control system of the AV to perform one or more autonomous navigation tasks.
2. The object detection and classification system of claim 1, wherein the encoded information included in the second set of images represent events occurring within the time window that is based upon consecutive frames of the received static-based camera sensor data.
3. The object detection and classification system of claim 1, wherein the first set of images includes a cyan wavelength image, a magenta wavelength image, and a yellow wavelength image, and wherein the second set of images includes a positive event image and a negative event image.
4. The object detection and classification system of claim 3, wherein the one or more processors are configured to: encode the received event-based camera sensor data into (i) the positive event image using a number of a positive events occurring within the time window, and (ii) the negative event image using a number of a negative events occurring within the time window.
5. The object detection and classification system of claim 3, wherein the one or more processors are configured to encode the received event-based camera sensor data into the positive event image and the negative event image by: assigning, to each pixel of the positive event image, an intensity gradient value that is based upon a number of aggregated positive events detected within the time window by each respective pixel of an event camera image as indicated by the event-based camera sensor data; and assigning, to each pixel of the negative event image, an intensity gradient value that is based upon a number of aggregated negative events detected within the time window by each respective pixel of an event camera image as indicated by the event-based camera sensor data.
6. The object detection and classification system of claim 1, wherein the scene is a static road scene that causes the received event-based camera sensor data to indicate no positive events and no negative events, and wherein the one or more processors are configured to determine the location and type of the one or more objects included in the static scene using the combination of the first set of images and the second set of images received via the channel inputs.
7. The object detection and classification system of claim 1, wherein the one or more processors are configured to encode the received event-based camera sensor data into the positive event image and the negative event image by: applying a kernel of m-by-m pixel dimensions centered on each respective pixel coordinate within the positive event image to encode the positive event image in accordance with a time surface encoding, with m being an integer value; and applying a kernel of m-by-m pixel dimensions centered on each respective pixel coordinate within the negative event image to encode the negative event image in accordance with a time surface encoding, with m being an integer value.
8. An autonomous vehicle (AV), comprising: a media control unit configured to obtain (i) a first set of images associated with a scene, the first set of images being encoded using static-based camera sensor data, and (ii) a second set of images associated with the scene, the second set of images including encoded information representing events occurring within a time window using event-based camera sensor data; and an electronic control unit (ECU) configured to (i) receive the first set of images and the second set of images as separate respective channel inputs, (ii) process frames of the first set of images, received via the channel inputs, including the static-based camera sensor data, (iii) process the encoded information included in the second set of images, received via the channel inputs, (iv) determine a location and type of one or more objects included in the scene, and (v) provide the location and type of the one or more objects included in the scene to a control system of the AV to perform one or more autonomous navigation tasks.
9. The AV of claim 8, wherein the encoded information included in the second set of images represent events occurring within the time window based upon consecutive frames of the received static-based camera sensor data.
10. The AV of claim 8, wherein the first set of images includes a cyan wavelength image, a magenta wavelength image, and a yellow wavelength image, and wherein the second set of images includes a positive event image and a negative event image.
11. The AV of claim 10, wherein the media control unit is configured to encode the received event-based camera sensor data into (i) the positive event image using a number of a positive events occurring within the time window, and (ii) the negative event image using a number of a negative events occurring within the time window.
12. The AV of claim 11, wherein the media control unit is configured to encode the received event-based camera sensor data into the positive event image and the negative event image by: assigning, to each pixel of the positive event image, an intensity gradient value based upon a number of aggregated positive events detected within the time window by each respective pixel of an event camera image as indicated by the event-based camera sensor data; and assigning, to each pixel of the negative event image, an intensity gradient value based upon a number of aggregated negative events detected within the time window by each respective pixel of an event camera image as indicated by the event-based camera sensor data.
13. The AV of claim 8, wherein the scene is a static road scene that causes the received event-based camera sensor data to indicate no positive events and no negative events, and wherein the ECU is configured to determine the location and type of the one or more objects included in the static scene using the combination of the first set of images and the second set of images received via the channel inputs.
14. The AV of claim 11, wherein the media control unit is configured to encode the received event-based camera sensor data into the positive event image and the negative event image by: applying a kernel of m-by-m pixel dimensions centered on each respective pixel coordinate within the positive event image to encode the positive event image in accordance with a time surface encoding, with m being an integer value; and applying a kernel of m-by-m pixel dimensions centered on each respective pixel coordinate within the negative event image to encode the negative event image in accordance with a time surface encoding, with m being an integer value.
15. A non-transitory computer-readable medium having instructions stored thereon that, when executed by one or more processors of a control system associated with an autonomous vehicle (AV), cause the AV to: obtain a first set of images associated with a scene, the first set of images being encoded using static-based camera sensor data; obtain a second set of images associated with the scene, the second set of images including encoded information representing events occurring within a time window using event-based camera sensor data; receive the first set of images and the second set of images as separate respective channel inputs; process (i) frames of the first set of images including the static-based camera sensor data, and (ii) the encoded information included in the second set of images, received via the channel inputs, to determine a location and type of one or more objects included in the scene; and provide the location and type of the one or more objects included in the scene to a control system of the AV to perform one or more autonomous navigation tasks.
16. The non-transitory computer-readable medium of claim 15, wherein encoded information included in the second set of images represent events occurring within the time window based upon consecutive frames of the received static-based camera sensor data.
17. The non-transitory computer-readable medium of claim 15, wherein the first set of images includes a cyan wavelength image, a magenta wavelength image, and a yellow wavelength image, and wherein the second set of images includes a positive event image and a negative event image.
18. The non-transitory computer-readable medium of claim 17, wherein the instructions, when executed by the one or more processors of the control system associated with the AV, cause the AV to encode the received event-based camera sensor data into (i) the positive event image using a number of a positive events occurring within the time window, and (ii) the negative event image using a number of a negative events occurring within the time window.
19. The non-transitory computer-readable medium of claim 18, wherein the instructions, when executed by the one or more processors of the control system associated with the AV, cause the AV to encode the received event-based camera sensor data into the positive event image and the negative event image by: assigning, to each pixel of the positive event image, an intensity gradient value based upon a number of aggregated positive events detected within the time window by each respective pixel of an event camera image as indicated by the event-based camera sensor data; and assigning, to each pixel of the negative event image, an intensity gradient value based upon a number of aggregated negative events detected within the time window by each respective pixel of an event camera image as indicated by the event-based camera sensor data.
20. The non-transitory computer-readable medium of claim 15, wherein the scene is a static road scene that causes the received event-based camera sensor data to indicate no positive events and no negative events, and wherein the instructions, when executed by the one or more processors of the control system associated with the AV, cause the AV to determine the location and type of the one or more objects included in the static scene using the combination of the first set of images and the second set of images received via the channel inputs.
21. The non-transitory computer-readable medium of claim 15, wherein the instructions, when executed by the one or more processors of the control system associated with the AV, cause the AV to encode the received event-based camera sensor data into the positive event image and the negative event image by: applying a kernel of m-by-m pixel dimensions centered on each respective pixel coordinate within the positive event image to encode the positive event image in accordance with a time surface encoding, with m being an integer value; and applying a kernel of m-by-m pixel dimensions centered on each respective pixel coordinate within the negative event image to encode the negative event image in accordance with a time surface encoding, with m being an integer value.
</claims>
</document>
