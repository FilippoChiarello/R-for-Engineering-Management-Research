<document>

<filing_date>
2018-09-11
</filing_date>

<publication_date>
2020-02-19
</publication_date>

<priority_date>
2017-09-11
</priority_date>

<ipc_classes>
G06F16/583,G06K9/46,G06K9/62,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
TENCENT TECHNOLOGY (SHENZHEN) COMPANY
</assignee>

<inventors>
JIANG, WENHAO
LIU, WEI
MA, LIN
</inventors>

<docdb_family_id>
62869573
</docdb_family_id>

<title>
IMAGE RECOGNITION METHOD, TERMINAL AND STORAGE MEDIUM
</title>

<abstract>
A method for recognizing an image, a terminal, and a storage medium belong to the field of machine learning. The method includes: performing feature extraction on a to-be-recognized image by using an encoder, to obtain a feature vector and a first annotation vector set (101); performing initialization processing on the feature vector to obtain first initial input data; generating first guiding information based on the first annotation vector set by using a first guiding network model, where the first guiding network model is configured to generate guiding information according to an annotation vector set of an image; and determining a descriptive statement of the image based on the first guiding information, the first annotation vector set and the first initial input data by using a decoder. A guiding network model that can generate guiding information according to an annotation vector set of an image is added between the encoder and the decoder. Therefore, the guiding information generated using the guiding network model is more accurate, thereby accurately guiding an encoding process, and improving quality of the generated descriptive statement.
</abstract>

<claims>
1. A method for recognizing an image, performed by a terminal, and comprising: performing feature extraction on a to-be-recognized target image by using an encoder, to obtain a feature vector and a first annotation vector set; performing initialization processing on the feature vector, to obtain first initial input data; generating first guiding information based on the first annotation vector set using a first guiding network model, with the first guiding network model being configured to generate guiding information according to an annotation vector set of an image; and determining a descriptive statement of the target image based on the first guiding information, the first annotation vector set and the first initial input data by a decoder.
2. The method according to claim 1, wherein the generating the first guiding information based on the first annotation vector set using the first guiding network model comprises: performing linear transformation on the first annotation vector set based on a first matrix constructed by model parameters in the first guiding network model, to obtain a second matrix; and determining the first guiding information based on a maximum value of each row in the second matrix.
3. The method according to claim 1, wherein the first guiding network model is configured to generate guiding information according to an annotation vector set and attribute information of an image, and the attribute information is used for indicating a predicted occurrence probability of a word in a descriptive statement of the image, and
the generating the first guiding information based on the first annotation vector set using the first guiding network model comprises: inputting the target image to a multi-instance model, processing the target image by using the multi-instance model, to obtain attribute information of the target image; performing linear transformation on the first annotation vector set based on a third matrix constructed by model parameters in the first guiding network model, to obtain a fourth matrix; generating a fifth matrix based on the fourth matrix and the attribute information of the target image; and determining the first guiding information based on a maximum value of each row in the fifth matrix.
4. The method according to claim 1, wherein the determining the descriptive statement of the target image based on the first guiding information, the first annotation vector set and the first initial input data by using the decoder comprises:
decoding the first annotation vector set and the first initial input data based on the first guiding information by using the decoder, to obtain the descriptive statement of the target image.
5. The method according to claim 4, wherein the decoding the first annotation vector set and the first initial input data based on the first guiding information by using the decoder, to obtain the descriptive statement of the target image comprises: determining, when a first recurrent neural network, RNN model, is used in the decoder, and the first RNN model is configured to perform M first sequential steps, for each of the first sequential steps performed by the first RNN model, input data of the first sequential step based on the first guiding information, wherein M denotes a number of circularly processing the input data by the first RNN model, M is a positive integer, and each of the first sequential steps is a step of processing the input data by the first RNN model; determining output data of each of the first sequential steps based on the input data of the first sequential step, the first annotation vector set and output data of a previous sequential step of the first sequential step, wherein when the first sequential step is the first one in the M first sequential steps, the output data of the previous sequential step of the first sequential step is determined based on the first initial input data; and determining the descriptive statement of the target image based on all output data of the M first sequential steps.
6. The method according to claim 5, wherein the determining input data of the first sequential step based on the first guiding information comprises: determining the input data of the first sequential step based on the first guiding information according to a formula as follows. xt = Eyt + Qv wherein t denotes the first sequential step, xt denotes the input data of the first sequential step, E denotes a word embedding matrix and denotes a model parameter of the first RNN model, yt denotes an one-hot vector of a word corresponding to the first sequential step, and the word corresponding to the first sequential step is determined based on output data of a previous sequential step of the first sequential step, Q denotes a sixth matrix and denotes a model parameter of the first RNN model, and v denotes the first guiding information.
7. The method according to any one of claims 1 to 6, further comprising: before the performing feature extraction on the target image by the encoder to obtain the feature vector and the first annotation vector set,
combining a to-be-trained first encoder, a to-be-trained first guiding network model and a to-be-trained first decoder, to obtain a first cascaded network model; and
training the first cascaded network model based on a plurality of sample images and descriptive statements of the plurality of sample images using a gradient descent method, to obtain the encoder, the first guiding network model and the decoder.
8. The method according to claim 1, wherein the determining the descriptive statement of the target image based on the first guiding information, the first annotation vector set and the first initial input data by using the decoder comprises: determining a second annotation vector set and second initial input data based on the first guiding information, the first annotation vector set and the first initial input data by using a reviewer; generating second guiding information based on the second annotation vector set by using a second guiding network model, wherein the second guiding network model is configured to generate guiding information based on an annotation vector set; and encoding the second annotation vector set and the second initial input data based on the second guiding information by using the encoder, to obtain the descriptive statement of the target image.
9. The method according to claim 8, wherein the determining the second annotation vector set and the second initial input data based on the first guiding information, the first annotation vector set, and the first initial input data by using the reviewer comprises: determining, when a second RNN model is used in the first reviewer, and the second RNN model is configured to perform N second sequential steps, for each of the second sequential steps performed by the second RNN model, input data of the second sequential step based on the first guiding information, wherein N denotes a number of circularly processing the input data by the second RNN model, N is a positive integer, and each of the second sequential steps is a step of processing the input data by the second RNN model; determining output data of each of the second sequential steps based on the input data of the second sequential step, the first annotation vector set and output data of a previous second sequential step of the second sequential step, wherein when the second sequential step is the first one of the N second sequential steps, the output data of the previous second sequential step of the second sequential step is determined based on the first initial input data; determining the second initial input data based on the output data of a last second sequential step in the N second sequential steps; and determining the second annotation vector set based on all output data of the N second sequential steps.
10. The method according to claim 8 or 9, further comprising: before the performing feature extraction on the target image by using the encoder to obtain the feature vector and the first annotation vector set,
combining a to-be-trained second encoder, a to-be-trained second guiding network model, a to-be-trained reviewer, a to-be-trained third guiding network model and a to-be-trained second decoder, to obtain a second cascaded network model; and
training the second cascaded network model based on a plurality of sample images and descriptive statements of the plurality of sample images using a gradient descent method, to obtain the encoder, the first guiding network model, the reviewer, the second guiding network model and the decoder.
11. A terminal comprising a processor and a memory, with the memory storing thereon at least one instruction, at least one program, and a code set or an instruction set, wherein the instruction, the program, and the code set or the instruction set is loaded and executed by the processor to implement operations of: performing feature extraction on a to-be-recognized target image by using an encoder, to obtain a feature vector and a first annotation vector set; performing initialization processing on the feature vector, to obtain first initial input data; generating first guiding information based on the first annotation vector set using a first guiding network model, with the first guiding network model being configured to generate guiding information based on an annotation vector set of an image; and determining a descriptive statement of the target image based on the first guiding information, the first annotation vector set and the first initial input data by using a decoder.
12. The terminal according to claim 11, wherein the instruction, the program, and the code set or the instruction set are loaded and executed by the processor to implement operations of: performing linear transformation on the first annotation vector set based on a first matrix constructed by model parameters in the first guiding network model, to obtain a second matrix; and determining the first guiding information based on a maximum value of each row in the second matrix.
13. The terminal according to claim 11, wherein the first guiding network model is configured to generate guiding information according to an annotation vector set and attribute information of an image, and the attribute information is used for indicating a predicted occurrence probability of a word in a descriptive statement of the image, and
the instruction, the program, and the code set or the instruction set are loaded and executed by the processor to implement operations of: inputting the target image to a multi-instance model, processing the target image by using the multi-instance model, to obtain attribute information of the target image; performing linear transformation on the first annotation vector set based on a third matrix constructed by model parameters in the first guiding network model, to obtain a fourth matrix; generating a fifth matrix based on the fourth matrix and the attribute information of the target image; and determining the first guiding information based on a maximum value of each row in the fifth matrix.
14. The terminal according to claim 11, wherein the instruction, the program, and the code set or the instruction set are loaded and executed by the processor to implement operations of
decoding the first annotation vector set and the first initial input data based on the first guiding information by using the decoder, to obtain the descriptive statement of the target image.
15. The terminal according to claim 14, wherein the instruction, the program, and the code set or the instruction set are loaded and executed by the processor to implement operations of
determining, when a second RNN model is used in the first reviewer, and the second RNN model is configured to perform N second sequential steps, for each of the second sequential steps performed by the second RNN model, input data of the second sequential step based on the first guiding information,
wherein N denotes a number of circularly processing the input data by the second RNN model, N is a positive integer, and each of the second sequential steps is a step of processing the input data by the second RNN model;
determining output data of each of the second sequential steps based on the input data of the second sequential step, the first annotation vector set and output data of a previous second sequential step of the second sequential step, wherein
when the second sequential step is the first one of the N second sequential steps, the output data of the previous second sequential step of the second sequential step is determined based on the first initial input data;
determining the second initial input data based on the output data of a last second sequential step in the N second sequential steps; and
determining the second annotation vector set based on all output data of the N second sequential steps.
16. The terminal according to any one of claims 11 to 15, wherein the instruction, the program, and the code set or the instruction set are loaded and executed by the processor to implement operations of: combining a to-be-trained first encoder, a to-be-trained first guiding network model and a to-be-trained first decoder, to obtain a first cascaded network model; and training the first cascaded network model based on a plurality of sample images and descriptive statements of the plurality of sample images using a gradient descent method, to obtain the encoder, the first guiding network model and the decoder.
17. The terminal according to claim 11, wherein the instruction, the program, and the code set or the instruction set are loaded and executed by the processor to implement operations of: determining a second annotation vector set and second initial input data based on the first guiding information, the first annotation vector set and the first initial input data by using a reviewer; generating second guiding information based on the second annotation vector set by using a second guiding network model, wherein the second guiding network model is configured to generate guiding information according to an annotation vector set; and encoding the second annotation vector set and the second initial input data based on the second guiding information by using the encoder, to obtain the descriptive statement of the target image.
18. The terminal according to claim 17, wherein the instruction, the program, and the code set or the instruction set are loaded and executed by the processor to implement operations of: determining, when a second RNN model is used in the first reviewer, and the second RNN model is configured to perform N second sequential steps, for each of the second sequential steps performed by the second RNN model, input data of the second sequential step based on the first guiding information, wherein N denotes a number of circularly processing the input data by the second RNN model, N is a positive integer, and each of the second sequential steps is a step of processing the input data by the second RNN model; determining output data of each of the second sequential steps based on the input data of the second sequential step, the first annotation vector set and output data of a previous second sequential step of the second sequential step, wherein when the second sequential step is the first one of the N second sequential steps, the output data of the previous second sequential step of the second sequential step is determined based on the first initial input data; determining the second initial input data based on the output data of a last second sequential step in the N second sequential steps; and determining the second annotation vector set based on all output data of the N second sequential steps.
19. The terminal according to claim 17 or 18, wherein the instruction, the program, and the code set or the instruction set are loaded and executed by the processor to implement operations of: combining a to-be-trained second encoder, a to-be-trained second guiding network model, a to-be-trained reviewer, a to-be-trained third guiding network model and a to-be-trained second decoder, to obtain a second cascaded network model; and training the second cascaded network model based on a plurality of sample images and descriptive statements of the plurality of sample images by using a gradient descent method, to obtain the encoder, the first guiding network model, the reviewer, the second guiding network model and the decoder.
20. A computer readable storage medium having stored thereon at least one instruction, at least one program, and a code set or an instruction set, wherein the instruction, the program, and the code set or the instruction set is loaded and executed by a processor to implement the method for recognizing an image according to any one of claims 1 to 10.
</claims>
</document>
