<document>

<filing_date>
2019-04-24
</filing_date>

<publication_date>
2020-07-21
</publication_date>

<priority_date>
2016-11-04
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
MAZIARZ, KRZYSZTOF STANISLAW
MIRHOSEINI, AZALIA
SHAZEER, NOAM M.
</inventors>

<docdb_family_id>
60452764
</docdb_family_id>

<title>
Mixture of experts neural networks
</title>

<abstract>
A system includes a neural network that includes a Mixture of Experts (MoE) subnetwork between a first neural network layer and a second neural network layer. The MoE subnetwork includes multiple expert neural networks. Each expert neural network is configured to process a first layer output generated by the first neural network layer to generate a respective expert output. The MoE subnetwork further includes a gating subsystem that selects, based on the first layer output, one or more of the expert neural networks and determine a respective weight for each selected expert neural network, provides the first layer output as input to each of the selected expert neural networks, combines the expert outputs generated by the selected expert neural networks in accordance with the weights for the selected expert neural networks to generate an MoE output, and provides the MoE output as input to the second neural network layer.
</abstract>

<claims>
1. A system comprising: a main neural network implemented by one or more computers, the main neural network comprising a Mixture of Experts (MoE) subnetwork between a first neural network layer and a second neural network layer in the main neural network, wherein the MoE subnetwork comprises: a plurality of expert neural networks, wherein each expert neural network is configured to process a first layer output generated by the first neural network layer in accordance with a respective set of expert parameters of the expert neural network to generate a respective expert output, and a gating subsystem configured to: generate an initial gating output by applying a set of gating parameters to the first layer output, apply a sparsifying function to the initial gating output to generate a sparsified initial gating output, apply a softmax function to the sparsified initial gating output to generate a weight vector that includes a respective weight for each of the plurality of expert neural networks, select, based on the weights in the weight vector, one or more of the expert neural networks and determine a respective weight for each selected expert neural network, provide the first layer output as input to each of the selected expert neural networks, combine the expert outputs generated by the selected expert neural networks in accordance with the weights for the selected expert neural networks to generate an MoE output, and provide the MoE output as input to the second neural network layer.
2. The system of claim 1, wherein the expert neural networks have the same or similar architectures but different parameter values.
3. The system of claim 1, wherein combining the expert outputs generated by the selected expert neural network comprises: weighting the expert output generated by each of the selected expert neural networks by the weight for the selected expert neural network to generate a weighted expert output, and summing the weighted expert outputs to generate the MoE output.
4. The system of claim 1, wherein the weight vector is a sparse vector that includes non-zero weights for only a few of the expert neural networks.
5. The system of claim 1, wherein selecting one or more of the expert neural networks comprises: selecting only expert neural networks that have non-zero weights in the weight vector.
6. The system of claim 1, wherein generating the initial gating output comprises: applying the set of gating parameters to the first layer output to generate a modified first layer output; and adding tunable Gaussian noise to the modified first layer output to generate the initial gating output.
7. The system of claim 6, wherein adding tunable Gaussian noise to the modified first layer output to generate the initial gating output comprises: applying a set of trainable noise parameters to the first layer output to generate an initial noise output; element-wise multiplying the initial noise output by a vector of noise values sampled from a normal distribution to generate a final noise output; and adding the final noise output to the modified first layer output.
8. The system of claim 1, wherein the sparsifying function sets all values in the initial gating output other than the k highest values to a value that is mapped to zero by the softmax function.
9. The system of claim 1, wherein the gating subsystem comprises a parent gating subnetwork and a plurality of child gating subnetworks, and wherein each of the child gating subnetworks manages a disjoint subset of the plurality of expert neural networks from each other child gating subnetwork.
10. A method comprising: receiving a network input; and processing the network input using a main neural network to generate a network output for the network input, wherein the main neural network is implemented by one or more computers and comprises a Mixture of Experts (MoE) subnetwork between a first neural network layer and a second neural network layer in the main neural network, wherein the network input is a first layer output generated by the first neural network layer preceding the MOE subnetwork, wherein the MoE subnetwork comprises: a plurality of expert neural networks, wherein each expert neural network is configured to process a first layer output generated by the first neural network layer in accordance with a respective set of expert parameters of the expert neural network to generate a respective expert output, and wherein processing the network input using the main neural network comprises: generating an initial gating output by applying a set of gating parameters to the first layer output, applying a sparsifying function to the initial gating output to generate a sparsified initial gating output, applying a softmax function to the sparsified initial gating output to generate a weight vector that includes a respective weight for each of the plurality of expert neural networks, selecting, based on the weights in the weight vector, one or more of the expert neural networks and determining a respective weight for each selected expert neural network, providing the first layer output as input to each of the selected expert neural networks, combining the expert outputs generated by the selected expert neural networks in accordance with the weights for the selected expert neural networks to generate an MoE output, and providing the MoE output as input to the second neural network layer.
11. The method of claim 10, wherein combining the expert outputs generated by the selected expert neural network comprises: weighting the expert output generated by each of the selected expert neural networks by the weight for the selected expert neural network to generate a weighted expert output, and summing the weighted expert outputs to generate the MoE output.
12. The method of claim 10, wherein the weight vector is a sparse vector that includes non-zero weights for only a few of the expert neural networks.
13. The method of claim 10, wherein selecting one or more the expert neural networks comprises: selecting only expert neural networks that have non-zero weights in the weight vector.
14. The method of claim 10, wherein generating the initial gating output comprises: applying the set of gating parameters to the first layer output to generate a modified first layer output; and adding tunable Gaussian noise to the modified first layer output to generate the initial gating output.
15. The method of claim 14, wherein adding tunable Gaussian noise to the modified first layer output to generate the initial gating output comprises: applying a set of trainable noise parameters to the first layer output to generate an initial noise output; element-wise multiplying the initial noise output by a vector of noise values sampled from a normal distribution to generate a final noise output; and adding the final noise output to the modified first layer output.
16. The method of claim 10, wherein the sparsifying function sets all values in the initial gating output other than the k highest values to a value that is mapped to zero by the softmax function.
17. One or more computer storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising: receiving a network input; and processing the network input using a main neural network to generate a network output for the network input, wherein the main neural network is implemented by one or more computers and comprises a Mixture of Experts (MoE) subnetwork between a first neural network layer and a second neural network layer in the main neural network, wherein the network input is a first layer output generated by the first neural network layer preceding the MOE subnetwork, wherein the MoE subnetwork comprises: a plurality of expert neural networks, wherein each expert neural network is configured to process a first layer output generated by the first neural network layer in accordance with a respective set of expert parameters of the expert neural network to generate a respective expert output, and wherein processing the network input using the main neural network comprises: generating an initial gating output by applying a set of gating parameters to the first layer output, applying a sparsifying function to the initial gating output to generate a sparsified initial gating output, applying a softmax function to the sparsified initial gating output to generate a weight vector that includes a respective weight for each of the plurality of expert neural networks, selecting, based on the weights in the weight vector, one or more of the expert neural networks and determining a respective weight for each selected expert neural network, providing the first layer output as input to each of the selected expert neural networks, combining the expert outputs generated by the selected expert neural networks in accordance with the weights for the selected expert neural networks to generate an MoE output, and providing the MoE output as input to the second neural network layer.
</claims>
</document>
