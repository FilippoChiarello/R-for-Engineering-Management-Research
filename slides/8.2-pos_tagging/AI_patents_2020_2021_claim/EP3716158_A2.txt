<document>

<filing_date>
2020-03-24
</filing_date>

<publication_date>
2020-09-30
</publication_date>

<priority_date>
2019-03-25
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,H04L29/08,H04N19/124
</ipc_classes>

<assignee>
NOKIA TECHNOLOGIES
</assignee>

<inventors>
AYTEKIN, CAGLAR
CRICRI, FRANCESCO
LAM, Yat Hong
</inventors>

<docdb_family_id>
69960343
</docdb_family_id>

<title>
COMPRESSING WEIGHT UPDATES FOR DECODER-SIDE NEURAL NETWORKS
</title>

<abstract>
A method, apparatus, and computer program product are provided for training a neural network or providing a pre-trained neural network with the weight-updates being compressible using at least a weight-update compression loss function and/or task loss function. The weight-update compression loss function can comprise a weight-update vector defined as a latest weight vector minus an initial weight vector before training. A pre-trained neural network can be compressed by pruning one or more small-valued weights. The training of the neural network can consider the compressibility of the neural network, for instance, using a compression loss function, such as a task loss and/or a weight-update compression loss. The compressed neural network can be applied within a decoding loop of an encoder side or in a post-processing stage, as well as at a decoder side.
</abstract>

<claims>
1. An apparatus comprising: means for training a neural network on a training dataset, wherein the training comprises applying at least a task loss function and a weight-update compression loss function, the weight-update compression loss function comprising a weight-update vector defined as a latest weight vector minus an initial weight vector before training, the weight-update compression loss function comprising a ratio of an L1-norm of the weight-update vector to an L2-norm of the weight-update vector; and means for pruning weight-update values which are within a predetermined range from zero, and quantizing the weight-update values and the pruned weight-update values.
2. The apparatus of claim 1, further comprising:
means for entropy encoding the resulting weight-update vector to obtain a compressed weight-update vector.
3. The method of claim 2, wherein the weight-update compression loss function is minimized to increase a sparsity and a quantizability of non-zero weight-update values.
4. The method of claim 3, wherein minimizing the weight-update compression loss function lowers an entropy of the weight-update vector of the trained neural network.
5. An apparatus configured to pretrain and use a neural network, the apparatus comprising: means for a) temporarily overfitting the neural network on a first image of a plurality of images for a first predetermined number of times to generate a first temporarily overfitted neural network; means for, after iteratively temporarily-overfitting the neural network on the first image of the plurality of images, b) resetting one or more weights to an initial weight value; means for c) iteratively temporarily-overfitting the first temporarily overfitted and reset neural network to a second image of the plurality of images for a second predetermined number of times to generate a second temporarily overfitted neural network; means for d) computing an average loss over all the temporarily overfitted neural networks, based on the performance of the first temporarily overfitted neural network relative to the first image and the performance of the second temporarily overfitted neural network relative to the second image; means for e) computing one or more gradient values based on the average loss value and one or more neural network weights; and means for f) updating the neural network using the gradient values.
6. The apparatus of claim 5, further comprising: means for repeating each of a-f a number of times to generate a pretrained neural network; and means for overfitting the pretrained neural network to a small set of data to be encoded.
7. A method comprising: training a neural network on a training dataset, wherein the training comprises applying at least a task loss function and a weight-update compression loss function, the weight-update compression loss function comprising a weight-update vector defined as a latest weight vector minus an initial weight vector before training, the weight-update compression loss function comprising a ratio of an L1-norm of the weight-update vector to an L2-norm of the weight-update vector; and pruning weight-update values which are within a predetermined range of zero, and quantizing the weight-update values and the pruned weight-update values.
8. The method of claim 7, further comprising:
entropy encoding the weight-update vector to obtain a compressed weight-update vector.
9. The method of claim 7, wherein: the weight-update compression loss function is minimized to increase a sparsity and a quantizability of non-zero weight-update values, or minimizing the weight-update values lowers an entropy of a weight-update vector of the trained neural network.
10. A method comprising: a) temporarily overfitting a neural network on a first image of a plurality of images for a first predetermined number of times to generate a first temporarily overfitted neural network; b) after iteratively temporarily-overfitting the neural network on the first image of the plurality of images, resetting one or more weights to an initial weight value; c) iteratively temporarily-overfitting the first temporarily overfitted and reset neural network to a second image of the plurality of images for a second predetermined number of times to generate a second temporarily overfitted neural network; d) computing an average loss over all the temporarily overfitted neural networks, based on the performance of the first temporarily overfitted neural network relative to the first image and the performance of the second temporarily overfitted neural network relative to the second image; e) computing one or more gradient values based on the average loss value and one or more neural network weights; and f) updating the neural network using the gradient values.
11. The method of claim 10, further comprising: repeating each of a-f a number of times to generate a pretrained neural network; and overfitting the pretrained neural network to a small set of data to be encoded.
12. A computer-readable storage medium having program code instructions stored therein that are configured, upon execution, to: train a neural network on a training dataset, wherein the training comprises applying at least a task loss function and a weight-update compression loss function, the weight-update compression loss function comprising a weight-update vector defined as a latest weight vector minus an initial weight vector before training, the weight-update compression loss function comprising a ratio of an L1-norm of the weight-update vector to an L2-norm of the weight-update vector; prune weight-update values which are within a predetermined range of zero; and quantize the weight-update values and the pruned weight-update values.
13. The computer-readable storage medium of claim 12, wherein the program code instructions are further configured, upon execution, to:
entropy encode the weight-update vector to obtain a compressed weight-update vector.
14. A computer-readable storage medium having program code instructions stored therein that are configured, upon execution, to: a) temporarily overfitting a neural network on a first image of a plurality of images for a first predetermined number of times to generate a first temporarily overfitted neural network; b) after iteratively temporarily-overfitting the neural network on the first image of the plurality of images, resetting one or more weights to an initial weight value; c) iteratively temporarily-overfitting the first temporarily overfitted and reset neural network to a second image of the plurality of images for a second predetermined number of times to generate a second temporarily overfitted neural network; d) computing an average loss over all the temporarily overfitted neural networks, based on the performance of the first temporarily overfitted neural network relative to the first image and the performance of the second temporarily overfitted neural network relative to the second image; e) computing one or more gradient values based on the average loss value and one or more neural network weights; and f) updating the neural network using the gradient values.
15. The computer-readable storage medium of claim 14, wherein the program code instructions are further configured, upon execution, to: repeat each of a-f a number of times to generate a pretrained neural network; and overfit the pretrained neural network to a small set of data to be encoded.
</claims>
</document>
