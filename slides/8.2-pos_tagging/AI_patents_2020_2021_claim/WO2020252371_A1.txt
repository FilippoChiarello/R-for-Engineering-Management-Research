<document>

<filing_date>
2020-06-12
</filing_date>

<publication_date>
2020-12-17
</publication_date>

<priority_date>
2019-06-14
</priority_date>

<ipc_classes>
G06K9/36,G06K9/46,G06K9/60
</ipc_classes>

<assignee>
CHOUDHARY, SIDDHARTH
MAGIC LEAP
SINGHAL, PRATEEK
GANGWAR, Manushree
MAHENDRAN, Siddarth
DONG, Shiyu
SEKHAR, Nitesh
GUPTA, Khushi
RAMNATH, Divya
KANNAN, Arumugam Kalai
</assignee>

<inventors>
CHOUDHARY, SIDDHARTH
SINGHAL, PRATEEK
GANGWAR, Manushree
MAHENDRAN, Siddarth
DONG, Shiyu
SEKHAR, Nitesh
GUPTA, Khushi
RAMNATH, Divya
KANNAN, Arumugam Kalai
</inventors>

<docdb_family_id>
73745180
</docdb_family_id>

<title>
SCALABLE THREE-DIMENSIONAL OBJECT RECOGNITION IN A CROSS REALITY SYSTEM
</title>

<abstract>
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for scalable three-dimensional (3-D) object recognition in a cross reality system. One of the methods includes maintaining object data specifying objects that have been recognized in a scene. A stream of input images of the scene is received, including a stream of color images and a stream of depth images. A color image is provided as input to an object recognition system. A recognition output that identifies a respective object mask for each object in the color image is received. A synchronization system determines a corresponding depth image for the color image. A 3-D bounding box generation system determines a respective 3-D bounding box for each object that has been recognized in the color image. Data specifying one or more 3-D bounding boxes is received as output from the 3-D bounding box generation system.
</abstract>

<claims>
1. A computer-implemented method, the method comprising:
maintaining object data specifying objects that have been recognized in a scene in an environment;
receiving a stream of input images of the scene, wherein the stream of input images comprises a stream of color images and a stream of depth images;
for each of a plurality of color images in the stream of color images:
providing the color image as input to an object recognition system;
receiving, as output from the object recognition system, a recognition output that identifies a respective object mask in the color image for each of one or more objects that have been recognized in the color image;
providing the color image and a plurality of depth images in the stream of depth images as input to a synchronization system that determines a corresponding depth image for the color image based on a timestamp of the corresponding depth image and a timestamp of the color image;
providing the object data, the recognition output identifying the object masks, and the corresponding depth image as input to a three-dimensional (3-D) bounding box generation system that determines, from the object data, the object masks, and the corresponding depth image, a respective 3-D bounding box for each of one or more of the objects that have been recognized in the color image; and
receiving, as output from the 3-D bounding box generation system, data specifying one or more 3-D bounding boxes for one or more of the objects recognized in the color image; and
providing, as output, data specifying the one or more 3-D bounding boxes.
2. The method of claim 1, wherein the 3-D bounding box generation system comprises: a multi-view fusion system that generates an initial set of 3-D object masks.
3. The method of claim 2, wherein the object recognition system, the synchronization system, the multi-view fusion system operate in a stateless manner and independently from one another.
4. The method of any one of claims 2-3, wherein the multi-view fusion system comprises:
an association system that identifies, from the maintained object data, matched object data specifying a corresponding object with the respective object mask of each recognized object in the color image; and
a fusion system that generates, for each recognized object in the color image, an initial 3-D object mask by combining the object mask in the color image with the matched object data.
5. The method of any preceding claim, wherein the 3-D bounding box generation system further comprises an object refinement system that refines the initial set of 3-D object masks to generate an initial set of 3-D bounding boxes.
6. The method of any preceding claim, wherein the 3-D bounding box generation system further comprises a bounding box refinement system that refines the initial set of 3-D bounding boxes to generate the one or more 3-D bounding boxes.
7. The method of any preceding claim, wherein the object recognition system comprises a trained deep neural network (DNN) model that takes the color image as input and generates a respective two-dimensional (2-D) object mask for each of the one or more objects that have been recognized in the color image.
8. The method of any preceding claim, wherein determining, by the synchronization system, a corresponding depth image for the color image based on timestamps of the corresponding depth images and timestamp of the color image comprises:
identifies a candidate depth image which has a closest timestamp to the timestamp of the color image;
determining that a time difference between the candidate depth image and the color image is less than a threshold; and
in response, determining the candidate depth image as the corresponding depth image for the color image.
9. The method of any preceding claim, wherein the 3-D bounding box generation system determines, from the object masks and the corresponding depth image, a respective 3-D object mask for each of the one or more of the objects that have been recognized in the color image, and wherein the method further comprises:
receiving, as output from the 3-D bounding box generation system, data specifying one or more 3-D object masks for the one or more of the objects recognized in the color image; and
providing, as output, data specifying the one or more 3-D object masks.
10. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to perform respective operations of the method of any preceding claim.
11. One or more non-transitory computer-readable storage media storing instructions that when executed by one or more computers cause the one or more computers to perform respective operations of the method of any preceding claim.
12. A computer-implemented method, the method comprising:
maintaining object data specifying objects that have been recognized in a scene in an environment;
receiving a stream of input images of the scene;
for each of a plurality of input images in the stream of input images:
providing the input image as input to an object recognition system;
receiving, as output from the object recognition system, a recognition output that identifies a respective bounding box in the input image for each of one or more objects that have been recognized in the input image;
providing data identifying the bounding boxes as input to a three-dimensional (3-D) bounding box generation system that determines, from the object data and the bounding boxes, a respective 3-D bounding box for each of one or more of the objects that have been recognized in the input image; and
receiving, as output from the 3-D bounding box generation system, data specifying one or more 3-D bounding boxes for one or more of the objects recognized in the input image; and
providing, as output, data specifying the one or more 3-D bounding boxes.
13. The method of claim 12, wherein the 3-D bounding box generation system comprises: a multi-view fusion system that generates an initial set of 3-D bounding boxes; and a bounding box refinement system that refines the initial set of 3-D bounding boxes to generate the one or more 3-D bounding boxes.
14. The method of claim 13, wherein the object recognition system, the multi-view fusion system, and the bounding box refinement system operate in a stateless manner and independently from one another.
15. The method of any one of claims 13-14, wherein the maintained object data comprises an ellipsoid that is generated from a plurality of two-dimensional (2-D) bounding boxes of each object that have been recognized in the scene, wherein the multi-view fusion system generates the initial set of 3-D bounding boxes by performing at least the following steps: for each 2-D bounding box identified in the input image,
determining whether the 2-D bounding box identified in the input image is associated with one or more 2-D bounding boxes of an object that has been recognized in the maintained object data;
in response to determining that the 2-D bounding box identified in the input image is associated with one or more 2-D bounding boxes of an object that has been recognized, updating the maintained object data by calculating an updated ellipsoid of the object using the 2-D bounding box identified in the input image;
in response to determining that the 2-D bounding box identified in the input image is not associated with any objects that have been recognized, creating a new object by generating an ellipsoid from at least the 2-D bounding box identified in the input image; and generating the initial set of 3-D bounding boxes using the ellipsoids of the objects that have been recognized in the input image.
16. The method of any one of claims 12-15, wherein the object recognition system comprises a trained deep neural network (DNN) model that takes the input image and generates a respective two-dimensional (2-D) object bounding box for each of the one or more objects that have been recognized in the input image.
17. The method of any one of claims 12-16, wherein the stream of input images of the scene are captured from two or more user devices.
18. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to perform respective operations of the method of any one of claims 12-17.
19. One or more non-transitory computer-readable storage media storing instructions that when executed by one or more computers cause the one or more computers to perform respective operations of the method of any one of claims 12-17.
</claims>
</document>
