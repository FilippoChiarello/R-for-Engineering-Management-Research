<document>

<filing_date>
2019-01-31
</filing_date>

<publication_date>
2020-06-11
</publication_date>

<priority_date>
2018-12-11
</priority_date>

<assignee>
SALESFORCE.COM
</assignee>

<inventors>
SOCHER, RICHARD
HASHIMOTO, KAZUMA
BRADBURY, JAMES
XIONG, CAIMING
BUSCHIAZZO, RAFFAELLA
MARSHALL, TERESA
</inventors>

<docdb_family_id>
70970997
</docdb_family_id>

<title>
Structured Text Translation
</title>

<abstract>
Approaches for the translation of structured text include an embedding module for encoding and embedding source text in a first language, an encoder for encoding output of the embedding module, a decoder for iteratively decoding output of the encoder based on generated tokens in translated text from previous iterations, a beam module for constraining output of the decoder with respect to possible embedded tags to include in the translated text for a current iteration using a beam search, and a layer for selecting a token to be included in the translated text for the current iteration. The translated text is in a second language different from the first language. In some embodiments, the approach further includes scoring and pointer modules for selecting the token based on the output of the beam module or copied from the source text or reference text from a training pair best matching the source text.
</abstract>

<claims>
1. A system for translating structured text, the system comprising: an embedding module for encoding and embedding structured source text in a first language; a multi-layer attention-based encoder for encoding output of the embedding module; a multi-layer attention-based decoder for iteratively decoding output of the multi-layer attention-based encoder based on generated tokens in structured translated text from previous iterations, the structured translated text being in a second language different from the first language; a beam module for constraining, according to a beam search, output of the multi-layer attention-based decoder with respect to possible embedded tags to include in the structured translated text for a current iteration; and a softmax layer for selecting a token to be included in the structured translated text for the current iteration based on output from the beam module.
2. The system of claim 1, wherein the beam module limits the possible embedded tags to one or more of: an opening embedded tag selected from one or more embedded tags in the structured source text; a closing embedded tag corresponding to an opening embedded tag last selected for inclusion in the structured translated text; or an end of sequence embedded tag after each of the one or more embedded tags in the structured source text is selected for inclusion in the structured translated text.
3. The system of claim 1, further comprising: a scoring module for determining whether the token selected for inclusion in the structured translated text for the current iteration is selected from the output of the beam module or copied from other structured text; and a pointer module including the softmax layer for selecting the token to be included in the structured translated text for the current iteration from the output of the beam module or copied from the other structured text based on the determination of the scoring module.
4. The system of claim 3, wherein the other structured text is the structured source text.
5. The system of claim 4, further comprising a second beam module for constraining, according to a second beam search, output of the multi-layer attention-based encoder with respect to possible embedded tags to include in the structured translated text for the current iteration when the token to be included in the structured translated text for the current iteration is selected from the structured source text.
6. The system of claim 3, wherein: the other structured text is a structured reference text corresponding to a translation in the second language of a structured retrieved text in the first language; the structured retrieved text and the structure reference text being selected from a training pair used to train the system; and the structured retrieved text being a closest match to the structured source text from among each training pair used to train the system.
7. The system of claim 6, further comprising: a second multi-layer attention-based decoder for decoding the structured reference text; and a second beam module for constraining, according to a second beam search, output of the second multi-layer attention-based decoder with respect to possible embedded tags to include in the structured translated text for the current iteration when the token to be included in the structured translated text for the current iteration is selected from the structured reference text.
8. The system of claim 1, wherein the structured source text or the structured translated text include one or more embedded XML tags or one or more embedded HTML tags.
9. A method for translating structured text, the method comprising: encoding and embedding, by an embedding module, structured source text in a first language; encoding output of the embedding module using a multi-layer attention-based encoder; iteratively decoding, by a multi-layer attention-based decoder, output of the multi-layer attention-based encoder based on generated tokens in structured translated text from previous iterations, the structured translated text being in a second language different from the first language; constraining, by a beam module according to a beam search, output of the multi-layer attention-based decoder with respect to possible embedded tags to include in the structured translated text for a current iteration; and selecting, by a softmax layer, a token to be included in the structured translated text for the current iteration based on output from the beam module.
10. The method of claim 9, wherein the possible embedded tags are limited to one or more of: an opening embedded tag selected from one or more embedded tags in the structured source text; a closing embedded tag corresponding to an opening embedded tag last selected for inclusion in the structured translated text; or an end of sequence embedded tag after each of the one or more embedded tags in the structured source text is selected for inclusion in the structured translated text.
11. The method of claim 9, further comprising: determining, by a scoring module, whether the token selected for inclusion in the structured translated text for the current iteration is selected from the output of the beam module or copied from other structured text; and selecting, by a pointer module including the softmax layer, the token to be included in the structured translated text for the current iteration from the output of the beam module or copied from the other structured text based on the determination of the scoring module.
12. The method of claim 11, wherein the other structured text is the structured source text.
13. The method of claim 12, further comprising constraining, by a second beam module according to a second beam search, output of the multi-layer attention-based encoder with respect to possible embedded tags to include in the structured translated text for the current iteration when the token to be included in the structured translated text for the current iteration is selected from the structured source text.
14. The method of claim 11, wherein: the other structured text is a structured reference text corresponding to a translation in the second language of a structured retrieved text in the first language; the structured retrieved text and the structure reference text being selected from a training pair used to train for training; and the method further comprises: selecting the structured retrieved text as a closest match to the structured source text from among each training pair used for training; decoding, by a second multi-layer attention-based decoder, the structured reference text; and constraining, by a second beam module according to a second beam search, output of the second multi-layer attention-based decoder with respect to possible embedded tags to include in the structured translated text for the current iteration when the token to be included in the structured translated text for the current iteration is selected from the structured reference text.
15. A non-transitory machine-readable medium comprising executable code which when executed by one or more processors associated with a computing device are adapted to cause the one or more processors to perform a method comprising: encoding and embedding, by an embedding module, structured source text in a first language; encoding output of the embedding module using a multi-layer attention-based encoder; iteratively decoding, by a multi-layer attention-based decoder, output of the multi-layer attention-based encoder based on generated tokens in structured translated text from previous iterations, the structured translated text being in a second language different from the first language; constraining, by a beam module according to a beam search, output of the multi-layer attention-based decoder with respect to possible embedded tags to include in the structured translated text for a current iteration; and selecting, by a softmax layer, a token to be included in the structured translated text for the current iteration based on output from the beam module.
16. The non-transitory machine-readable medium of claim 15, wherein the possible embedded tags are limited to one or more of: an opening embedded tag selected from one or more embedded tags in the structured source text; a closing embedded tag corresponding to an opening embedded tag last selected for inclusion in the structured translated text; or an end of sequence embedded tag after each of the one or more embedded tags in the structured source text is selected for inclusion in the structured translated text.
17. The non-transitory machine-readable medium of claim 15, wherein the method further comprises: determining, by a scoring module, whether the token selected for inclusion in the structured translated text for the current iteration is selected from the output of the beam module or copied from other structured text; and selecting, by a pointer module including the softmax layer, the token to be included in the structured translated text for the current iteration from the output of the beam module or copied from the other structured text based on the determination of the scoring module.
18. The non-transitory machine-readable medium of claim 17, wherein the other structured text is the structured source text.
19. The non-transitory machine-readable medium of claim 18, wherein the method further comprises constraining, by a second beam module according to a second beam search, output of the multi-layer attention-based encoder with respect to possible embedded tags to include in the structured translated text for the current iteration when the token to be included in the structured translated text for the current iteration is selected from the structured source text.
20. The non-transitory machine-readable medium of claim 17, wherein: the other structured text is a structured reference text corresponding to a translation in the second language of a structured retrieved text in the first language; the structured retrieved text and the structure reference text being selected from a training pair used for training; and the method further comprises: selecting the structured retrieved text as a closest match to the structured source text from among each training pair used for training; decoding, by a second multi-layer attention-based decoder, the structured reference text; and constraining, by a second beam module according to a second beam search, output of the second multi-layer attention-based decoder with respect to possible embedded tags to include in the structured translated text for the current iteration when the token to be included in the structured translated text for the current iteration is selected from the structured reference text.
</claims>
</document>
