<document>

<filing_date>
2018-03-27
</filing_date>

<publication_date>
2020-06-30
</publication_date>

<priority_date>
2018-03-27
</priority_date>

<ipc_classes>
G06F3/16,H04N5/04,H04N5/60,H04S7/00
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
WANG, OLIVER
LANGLOIS, TIMOTHY
MORGADO, PEDRO
</inventors>

<docdb_family_id>
68057476
</docdb_family_id>

<title>
Generating spatial audio using a predictive model
</title>

<abstract>
Certain embodiments involve generating and providing spatial audio using a predictive model. For example, a generates, using a predictive model, a visual representation of visual content provideable to a user device by encoding the visual content into the visual representation that indicates a visual element in the visual content. The system generates, using the predictive model, an audio representation of audio associated with the visual content by encoding the audio into the audio representation that indicates an audio element in the audio. The system also generates, using the predictive model, spatial audio based at least in part on the audio element and associating the spatial audio with the visual element. The system can also augment the visual content using the spatial audio by at least associating the spatial audio with the visual content.
</abstract>

<claims>
1. A method for generating spatial audio, wherein the method includes one or more processing devices performing operations comprising: encoding visual content from a 360-degree video into a visual representation that indicates a visual element in the visual content; encoding non-spatial audio associated with the 360-degree video into an audio representation that indicates an audio element in the non-spatial audio; applying, to the visual representation and the audio representation, a predictive model that is trained to recognize relationships among training 360-degree video, training ambisonic audio, and training non-spatial audio corresponding to the training ambisonic audio, wherein applying the predictive model identifies an association between the audio element and the visual element, wherein training of the predictive model comprises: receiving data indicating content of the training 360-degree video; receiving data indicating the training non-spatial audio, wherein the training non-spatial audio comprises stereo audio signals associated with the content of the training 360-degree video; generating the training ambisonic audio for the content based on the stereo audio signals associated with the content; receiving data indicating ground-truth spatial audio associated with the content; training a machine-learning algorithm using the ground-truth spatial audio and the training ambisonic audio generated from the stereo audio signals; and generating spatial audio by modifying an intensity of the audio element with a weight based at least in part on the association between the audio element and the visual element; and augmenting the visual content using the spatial audio by at least associating the spatial audio with the visual content.
2. The method of claim 1 wherein the visual representation or the audio representation indicates a visual location of the visual element in the visual content, the method further comprising: determining the weight to be applied to the audio element based on the visual location of the visual element.
3. The method of claim 1, wherein the visual representation or the audio representation indicates a visual location of the visual element and wherein generating the spatial audio comprises: generating the spatial audio based on the visual location of the visual element; and outputting the spatial audio such that the spatial audio is perceivable as originating from the visual location of the visual element.
4. The method of claim 1, wherein generating the spatial audio comprises: applying a linear model to the audio element and the weight associated with the audio element.
5. The method of claim 1, wherein the non-spatial audio associated with the visual content comprises at least one of stereo audio or mono audio.
6. The method of claim 1, wherein the non-spatial audio associated with the visual content comprises first order spatial audio, the method further comprising converting the first order spatial audio to second order spatial audio.
7. A system comprising: a processing device; and a non-transitory computer-readable medium communicatively coupled to the processing device, wherein the processing device is configured to perform operations comprising: encoding visual content from a 360-degree video into a visual representation that indicates a visual element in the visual content; encoding non-spatial audio associated with the 360-degree video into an audio representation that indicates an audio element in the non-spatial audio; applying, to the visual representation and the audio representation, a predictive model that is trained to recognize relationships among training 360-video, training ambisonic audio, and training non-spatial audio corresponding to the training ambisonic audio, wherein applying the predictive model identifies an association between the audio element and the visual element, wherein the training of the predictive model comprises: receiving data indicating content of the training 360-degree video; receiving data indicating the training non-spatial audio, wherein the training non-spatial audio comprises stereo audio signals associated with the content of the training 360-degree video; generating the training ambisonic audio for the content based on the stereo audio signals associated with the content; receiving data indicating ground-truth spatial audio associated with the content; training a machine-learning algorithm using the ground-truth spatial audio and the training ambisonic audio generated from the stereo audio signals; generating spatial audio by modifying an intensity of the audio element with a weight based at least in part on the association between the audio element and the visual element; and augmenting the visual content using the spatial audio by at least associating the spatial audio with the visual content.
8. The system of claim 7, wherein the visual representation or the audio representation indicates a visual location of the visual element in the visual content and the processing device is further configured to: determine, using the predictive model, the weight to be applied to the audio element based on the visual location of the visual element.
9. The system of claim 7, wherein the visual representation or the audio representation indicates a visual location of the visual element and wherein the processing device is further configured to generate the spatial audio by performing operations comprising: generating the spatial audio based on the visual location of the visual element; and outputting the spatial audio such that the spatial audio is perceivable as originating from the visual location of the visual element.
10. The system of claim 7, wherein the processing device is further configured to generate the spatial audio by performing operations comprising: applying, using the predictive model, a linear model to the audio element and the weight associated with the audio element.
11. The system of claim 7, wherein the non-spatial audio associated with the visual content comprises first order spatial audio, wherein the processing device is further configured to convert the first order spatial audio to second order spatial audio.
12. A non-transitory computer-readable medium storing program code executable by a processor for generating spatial audio, the program code comprising: program code for generating encoding visual content from a 360-degree video into a visual representation that indicates a visual element in the visual content; program code for encoding non-spatial audio associated with the 360-degree video into an audio representation that indicates an audio element in the non-spatial audio; program code for applying, to the visual representation and the audio representation, a predictive model that is trained to recognize relationships among training 360-video, training ambisonic audio, and training non-spatial audio corresponding to the training ambisonic audio, wherein applying the predictive model identifies an association between the audio element and the visual element, wherein the training of the predictive model comprises: receiving data indicating content of the training 360-degree video; receiving data indicating the training non-spatial audio, wherein the training non-spatial audio comprises stereo audio signals associated with the content of the training 360-degree video; generating the training ambisonic audio for the content based on the stereo audio signals associated with the content; receiving data indicating ground-truth spatial audio associated with the content; training a machine-learning algorithm using the ground-truth spatial audio and the training ambisonic audio generated from the stereo audio signals; program code for generating spatial audio by modifying an intensity of the audio element based at least in part on the association between the audio element and the visual element; and program code for augmenting, by the processor, the visual content using the spatial audio by at least associating the spatial audio with the visual content.
13. The method of claim 1, wherein training the machine-learning algorithm using the ground-truth spatial audio and the training ambisonic audio generated from the stereo audio signals comprises iteratively modifying the machine-learning algorithm to minimize an average squared error between the training ambisonic audio and the ground-truth spatial audio, wherein the average squared error is computed with a means-squared error function.
14. The method of claim 13, wherein training the machine-learning algorithm using the ground-truth spatial audio and the training ambisonic audio generated from the stereo audio signals further comprises decomposing the training ambisonic audio and the ground-truth spatial audio with a short-time Fourier transform.
15. The method of claim 1, wherein generating the training ambisonic audio comprises: determining an individual weight of an individual non-spatial audio track from the stereo audio signals; and encoding each non-spatial audio track from the stereo audio signals into ambisonic audio at a particular time frame of the individual non-spatial audio track.
16. The non-transitory computer-readable medium of claim 12, wherein training the machine-learning algorithm using the ground-truth spatial audio and the training ambisonic audio generated from the stereo audio signals comprises iteratively: computing, with a means-squared error function, an average squared error between the training ambisonic audio and the ground-truth spatial audio, and modifying the machine-learning algorithm to reduce the average squared error.
17. The non-transitory computer-readable medium of claim 16 wherein training the machine-learning algorithm using the ground-truth spatial audio and the training ambisonic audio generated from the stereo audio signals further comprises decomposing the training ambisonic audio and the ground-truth spatial audio with a short-time Fourier transform.
18. The non-transitory computer-readable medium of claim 12, wherein generating the training ambisonic audio comprises: determining an individual weight of an individual non-spatial audio track from the stereo audio signals; and encoding each non-spatial audio track from the stereo audio signals into ambisonic audio at a particular time frame of the individual non-spatial audio track.
</claims>
</document>
