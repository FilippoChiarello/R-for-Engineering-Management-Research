<document>

<filing_date>
2020-02-19
</filing_date>

<publication_date>
2020-06-11
</publication_date>

<priority_date>
2016-12-22
</priority_date>

<ipc_classes>
G06F15/78,G06F16/00,G06F16/2455,G06F17/00,G06F21/60,G06F21/72,G06F21/76,G06F21/85,G06F3/06,G06F9/445,G06N20/00
</ipc_classes>

<assignee>
IP RESERVOIR
</assignee>

<inventors>
CHAMBERLAIN, ROGER D.
INDECK, RONALD S.
</inventors>

<docdb_family_id>
62627278
</docdb_family_id>

<title>
Method and apparatus for hardware-accelerated machine learning
</title>

<abstract>
A multi-functional data processing pipeline for use with machine learning is disclosed. The multi-functional pipeline may comprise a plurality of pipelined data processing engines, the plurality of pipelined data processing engines being configured to perform processing operations, and the pipelined data processing engines can include correlation logic. The multi-functional pipeline can be configured to controllably activate or deactivate each of the pipelined data processing engines in the pipeline in response to control instructions and thereby define a function for the pipeline, each pipeline function being the combined functionality of each activated pipelined data processing engine in the pipeline. In example embodiments, such pipelines can be used to accelerate convolutional layers in machine-learning technology such as convolutional neural networks.
</abstract>

<claims>
1. A machine-learning apparatus comprising: a feature extractor for a convolutional neural network, wherein the feature extractor is deployed on a member of the group consisting of (1) a reconfigurable logic device, (2) a graphics processing unit (GPU), and (3) a chip multi-processor (CMP), wherein the member comprises a plurality of data processing engines arranged as a multi-functional pipeline through which data is streamed, the pipelined data processing engines configured for operation in parallel with each other; each pipelined data processing engine being configured to (1) receive streaming data and perform a processing operation on the received streaming data, and (2) be responsive to a control instruction that defines whether that pipelined data processing engine is an activated data processing engine or a deactivated data processing engine, wherein an activated data processing engine is configured to perform its processing operation on streaming data received thereby, and wherein a deactivated data processing engine remains in the pipeline but does not perform its processing operation on streaming data received thereby, the multi-functional pipeline thereby being configured to provide a plurality of different pipeline functions in response to control instructions that are configured to selectively activate and deactivate the pipelined data processing engines, each pipeline function being the combined functionality of each activated pipelined data processing engine in the pipeline at a given time; wherein each of a plurality of the data processing engines is configured as a convolution engine that convolves first data with second data via correlation logic; wherein each of another plurality of the data processing engines is configured as a data reduction engine that performs a data reduction operation on data received thereby; and wherein the multi-functional pipeline is configured to activate a plurality of the convolution engines and a plurality of the data reduction engines at the same time in response to control instructions in order to configure the multi-functional pipeline as the feature extractor for the convolutional neural network.
2. The apparatus of claim 1 wherein the multi-functional pipeline comprises a plurality of pairs of the convolution engines and the data reduction engines arranged within the multi-functional pipeline in an interleaved order.
3. The apparatus of claim 1 wherein the multi-functional pipeline is configured to deactivate at least one of the convolution engines or data reduction engines while a plurality of the convolution engines and a plurality of the data reduction engines are activated.
4. The apparatus of claim 3 wherein the multi-functional pipeline is further configured to selectively activate and deactivate different mixes of the convolution engines and data reduction engines based on whether the multi-functional pipeline is to operate in a training mode or a classification mode.
5. The apparatus of claim 3 wherein the multi-functional pipeline is further configured to disconnect power from a deactivated convolution engine or data reduction engine while retaining power to the activated convolution engines and data reduction engines.
6. The apparatus of claim 1 wherein the correlation logic of each convolution engine is configured to operate on second data over a sliding window of first data.
7. The apparatus of claim 6 wherein a first convolutional engine in the multi-functional pipeline is configured to process pixel data from an image as the first data and a plurality of weights as the second data.
8. The apparatus of claim 6 wherein each convolution engine includes a data shift register through which first data is streamed and a register that holds second data, and wherein the correlation logic of each convolution engine comprises a plurality of multipliers and summation logic, wherein the multipliers are configured to multiply values in a plurality of cells of the data shift register and the register, and wherein the summation logic is connected to a plurality of outputs of the multipliers to sum the outputs from the multipliers.
9. The apparatus of claim 1 wherein at least one of the data reduction engines is configured to perform a max pooling operation.
10. The apparatus of claim 1 wherein at least one of the data reduction engines is configured to perform an averaging operation.
11. The apparatus of claim 1 wherein at least one of the data reduction engines is configured to perform a sampling operation.
12. The apparatus of claim 1 wherein at least one of the data reduction engines comprises at least two of (1) max pooling logic, (2) averaging logic, and (3) sampling logic, and wherein the at least one data reduction engine is configured to select which of the at least two is used to process data received thereby in response to a data reduction control instruction.
13. The apparatus of claim 1 wherein the activated data processing engines further comprise at least one of (1) an encryption engine, (2) a decryption engine, (3) a compression engine, (4) a decompression engine, and (5) a search engine.
14. The apparatus of claim 1 wherein the multi-functional pipeline further comprises a plurality of parallel paths, each parallel path comprising at least one data processing engine.
15. The apparatus of claim 1 wherein the member comprises the reconfigurable logic device, wherein at least a portion of the multi-functional pipeline resides on the reconfigurable logic device.
16. The apparatus of claim 15 wherein the member comprises the reconfigurable logic device and at least one of a GPU and a CMP, and wherein another portion of the multifunctional pipeline resides on the at least one GPU or CMP.
17. The apparatus of claim 15 wherein the re-configurable logic device comprises a field programmable gate array (FPGA), wherein at least a portion of the multi-functional pipeline resides on the FPGA.
18. The apparatus of claim 1 wherein the member comprises the GPU, wherein at least a portion of the multi-functional pipeline resides on the GPU.
19. The apparatus of claim 18 wherein the member comprises the GPU and at least one of a reconfigurable logic device and a CMP, and wherein another portion of the multifunctional pipeline resides on the at least one reconfigurable logic device or CMP.
20. The apparatus of claim 1 wherein the member comprises the CMP, wherein at least a portion of the multi-functional pipeline resides on the CMP.
21. A machine-learning method comprising: selectively activating a plurality of data processing engines in a multi-functional pipeline in response to a control instruction to define a feature extractor for a convolutional neural network, the multi-functional pipeline being resident on a member of the group consisting of (1) a reconfigurable logic device, (2) a graphics processing unit (GPU), and (3) a chip multi-processor (CMP); wherein the multi-functional pipeline comprises a plurality of data processing engines through which data is streamed, the pipelined data processing engines configured for operation in parallel with each other, each pipelined data processing engine being configured to (1) receive streaming data and perform a processing operation on the received streaming data, and (2) be responsive to the control instruction that defines whether that pipelined data processing engine is an activated data processing engine or a deactivated data processing engine; wherein an activated data processing engine is configured to perform its processing operation on streaming data received thereby; wherein a deactivated data processing engine remains in the pipeline but does not perform its processing operation on streaming data received thereby, the multi-functional pipeline thereby being configured to provide a plurality of different pipeline functions in response to the control instructions that are configured to selectively activate and deactivate the pipelined data processing engines, each pipeline function being the combined functionality of each activated pipelined data processing engine in the pipeline at a given time; wherein each of a plurality of the data processing engines is configured as a convolution engine that convolves first data with second data via correlation logic; wherein each of another plurality of the data processing engines is configured as a data reduction engine that performs a data reduction operation on data received thereby; and wherein a plurality of the selectively activated data processing engines comprise a plurality of the convolution engines and a plurality of the data reduction engines; streaming data into a first activated convolution engine in the multi-functional pipeline, the streaming data comprising input data to be classified via the convolutional neural network as the first data and weight data as the second data; and the activated pipelined data processing engines in the multi-functional pipeline performing their data processing operations on data received thereby to perform feature extraction on the input data as part of the convolutional neural network.
22. The method of claim 21 further comprising: selectively deactivating at least one of the convolution engines or data reduction engines while a plurality of the convolution engines and a plurality of the data reduction engines are activated.
23. The method of claim 22 wherein the selectively activating and deactivating steps comprise selectively activating and deactivating different mixes of the convolution engines and data reduction engines based on whether the multi-functional pipeline is to operate in a training mode or a classification mode.
24. The method of claim 22 wherein the selectively deactivating step comprises disconnecting power from a deactivated convolution engine or data reduction engine while retaining power to the activated convolution engines and data reduction engines.
25. The method of claim 21 wherein at least one of the activated data reduction engines is configured to perform a max pooling operation, an averaging operation, and/or a sampling operation.
26. The method of claim 21 wherein the activated data processing engines further comprise at least one of (1) an encryption engine, (2) a decryption engine, (3) a compression engine, (4) a decompression engine, and (5) a search engine.
27. The method of claim 21 wherein the multi-functional pipeline further comprises a plurality of parallel paths, each parallel path comprising at least one data processing engine, and wherein the selectively activating step comprises activating a plurality of data processing engines in different parallel paths so that those data processing engines operate in parallel.
28. A machine-learning apparatus comprising: a member of the group consisting of (1) a reconfigurable logic device, (2) a graphics processing unit (GPU), and (3) a chip multi-processor (CMP), wherein the member comprises a plurality of data processing engines arranged as a multi-functional pipeline through which data is streamed, the pipelined data processing engines configured for operation in parallel with each other; each pipelined data processing engine being configured to (1) receive streaming data and perform a processing operation on the received streaming data, and (2) be responsive to a control instruction that defines whether that pipelined data processing engine is an activated data processing engine or a deactivated data processing engine, wherein an activated data processing engine is configured to perform its processing operation on streaming data received thereby, and wherein a deactivated data processing engine remains in the pipeline but does not perform its processing operation on streaming data received thereby, the multi-functional pipeline thereby being configured to provide a plurality of different pipeline functions in response to control instructions that are configured to selectively activate and deactivate the pipelined data processing engines, each pipeline function being the combined functionality of each activated pipelined data processing engine in the pipeline at a given time; and wherein the multi-functional pipeline is configured to selectively activate a plurality of the data processing engines at the same time in response to control instructions in order to configure the multi-functional pipeline to support machine-learning operations.
29. The apparatus of claim 28 wherein each of a plurality of the data processing engines comprises correlation logic, and wherein at least one of the data processing engines that comprises the correlation logic is configured as a convolution engine.
30. A data processing system comprising: a processor comprising a plurality of data processing engines arranged as a multi-functional pipeline through which data is streamed, the pipelined data processing engines configured for operation in parallel with each other; each pipelined data processing engine being configured to (1) receive streaming data and perform a processing operation on the received streaming data, and (2) be responsive to a control instruction that defines whether that pipelined data processing engine is an activated data processing engine or a deactivated data processing engine, wherein an activated data processing engine is configured to perform its processing operation on streaming data received thereby, and wherein a deactivated data processing engine remains in the pipeline but does not perform its processing operation on streaming data received thereby, the multi-functional pipeline thereby being configured to provide a plurality of different pipeline functions in response to control instructions that are configured to selectively activate and deactivate the pipelined data processing engines, each pipeline function being the combined functionality of each activated pipelined data processing engine in the pipeline at a given time; wherein the multi-functional pipeline is configured to provide a plurality of convolutional layers for a convolutional neural network; wherein a plurality of the pipelined data processing engines include correlation logic that convolves first data with second data to thereby serve as convolution engines within the multi-functional pipeline; and wherein the multi-functional pipeline is configured to selectively activate a plurality of the convolutional engines at the same time in response to the control instructions to support operations of the convolutional neural network.
</claims>
</document>
