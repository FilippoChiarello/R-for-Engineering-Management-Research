<document>

<filing_date>
2018-09-20
</filing_date>

<publication_date>
2020-03-26
</publication_date>

<priority_date>
2018-09-20
</priority_date>

<ipc_classes>
B25J11/00,B25J13/00,B25J13/08,B25J9/00,B25J9/16
</ipc_classes>

<assignee>
SONY CORPORATION
</assignee>

<inventors>
DEMERCHANT, MARVIN
MILLER, LINDSAY
WINGO, LOBRENZO
YOUNG, DAVID
</inventors>

<docdb_family_id>
69885237
</docdb_family_id>

<title>
AUTONOMOUS ROBOT
</title>

<abstract>
A robot in a location interacts with a user. The robot includes a camera, an image recognition processor, a microphone and a loudspeaker, a voice assistant, and a wireless transceiver. The robot moves around and creates a model of the location, and recognizes changes. It recognizes objects of interest, beings, and situations. The robot monitors the user and recognizes body language and gesture commands, as well as voice commands. The robot communicates with the user, the TV, and other devices. It may include environment sensors and health status sensors. It acts as a user companion by answering queries, executing commands, and issuing reminders. It may monitor to determine if the user is well. The robot may monitor objects of interest, their placement and their status. When necessary, it communicates with the user.
</abstract>

<claims>
We claim:
1. A robot capable of interacting with a user in a location, the robot comprising: (a) a camera configured to capture local images; (b) an image recognition processor coupled to the camera; (c) a microphone configured to capture local sounds and a loudspeaker; (d) one or more environment sensors for at least one of ambient temperature, infrared light, ultra-violet light, electromagnetic waves of another frequency, pressure, smoke, carbon monoxide, humidity, location, and movement; (e) a voice assistant coupled to the microphone and the loudspeaker; and (f) a wireless transceiver capable of performing two-way communication, wherein the wireless transceiver at least one of communicates over a radio-frequency (RF) electromagnetic field and communicates optically; wherein the robot is configured to: (i) autonomously move around in the location; (ii) create a model of the location and recognize changes in the location; (iii) recognize objects of interest and situations based on results from the image recognition processor; (iv) recognize beings based on at least one of image recognition results from the image recognition processor and voice recognition, wherein the beings include at least one of the user, a family member, a friend, an acquaintance, a visitor, a coworker, another human, a pet, and another animal; (v) recognize the beings based on results from the voice assistant; (vi) monitor the user; (vii) monitor camera images and recognize user body language and gesture commands using the image recognition processor; (viii) monitor sounds and recognize voice commands using the voice assistant; (ix) communicate with the user via at least one of the voice assistant and the wireless transceiver; (x) communicate with and control a television (TV) via the wireless transceiver; (xi) using one of a TV display and a handheld device display as a primary display to communicate with the user; and (xii) communicate with, receive information from, and control other devices via the wireless transceiver, wherein the other devices include at least one of a television, a camera, a home alarm system, a home appliance, consumer electronics, a telephone, lighting, a heater, and an air conditioner.
2. The robot of claim 1, further comprising a video projector.
3. The robot of claim 1, wherein the robot accepts commands from an authorized being and rejects commands from an unauthorized being, and configured to allow the user to at least temporarily authorize another being.
4. The robot of claim 1, further configured to answer user queries using the voice assistant and execute user commands.
5. The robot of claim 4, wherein a user command includes at least one of making a telephone call, controlling the TV, and providing a voice assistant function.
6. The robot of claim 4, configured to move around the location as a response to a user command, and to stream local images to the TV.
7. The robot of claim 1, configured to follow the user in the location using at least one of a camera, a microphone, and another sensor and configured to determine if the user is well.
8. The robot of claim 1, further comprising one or more health status sensors, wherein a health status sensor includes at least one of a heart rate sensor, a temperature sensor, a blood pressure sensor, and a pulse oximeter; and wherein the robot is configured to determine if the user is well by measuring at least one of a user's heart rate, a user's temperature, and a user's blood pressure.
9. The robot of claim 1, further configured to dispense medication.
10. The robot of claim 1, further configured to issue reminders.
11. The robot of claim 1, wherein communication shown on the primary display includes at least one of a reminder, an alert, a text message, an email, an electronic document, a video call, health status information, security information, and entertainment.
12. The robot of claim 1, configured to be taught by the user, through voice commands and local images, that an object comprises an object of interest.
13. The robot of claim 1, configured to determine that an object comprises an object of interest by applying deep-learning techniques on at least one of local images and local sounds.
14. The robot of claim 1, wherein the robot further is configured to determine, remember and report a placement of an object of interest, wherein determining is performed using at least one of a location sensor, an infrared sensor, sonar, radar, LIDAR, another laser scanner, and the image recognition processor, and wherein remembering is performed using a non-transitory memory.
15. The robot of claim 14, using the model of the location to determine if the placement is regular, and upon determining that the placement is not regular, alerting the user.
16. The robot of claim 15, wherein alerting the user comprises showing the user an image with a timestamp and a marker of the placement, and giving the user an audio alert.
17. The robot of claim 15, further comprising leading the user to the placement.
18. The robot of claim 1, further comprising monitoring a state of an object of interest using at least one of local images, remote images, and local sounds.
19. The robot of claim 18, wherein the object of interest is an appliance and wherein the robot determines a priority for displaying the state on the TV and a priority for displaying content on the TV.
20. The robot of claim 19, wherein the state includes at least one of a finished task and a problem that requires user attention, and wherein the robot immediately displays the state on the TV if the priority for displaying the state on the TV is higher than the priority for displaying the content on the TV, and wherein the robot delays displaying the state on the TV if the priority for displaying the state on the TV is lower than the priority for displaying the content on the TV.
</claims>
</document>
