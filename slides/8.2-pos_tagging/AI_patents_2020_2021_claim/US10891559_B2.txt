<document>

<filing_date>
2014-12-12
</filing_date>

<publication_date>
2021-01-12
</publication_date>

<priority_date>
2013-12-12
</priority_date>

<ipc_classes>
G06K9/62,G06N20/00,G06N5/02,G06N99/00,G10L17/06
</ipc_classes>

<assignee>
INDIAN INSTITUTE OF TECHNOLOGY
</assignee>

<inventors>
JAYADEVA
</inventors>

<docdb_family_id>
53370681
</docdb_family_id>

<title>
Classifying test data based on a maximum margin classifier
</title>

<abstract>
Systems and methods for classifying binary data based training data having a predefined sample size is obtained. The training data is composed of separable binary datasets. An exact bound on Vapnik-Chervonenkis (VC) dimension of a classifier for the training data is determined. The exact bound is based one or more variables defining the hyperplane. The exact bound may be minimized for generating a classifier for predicting one class to which a given data sample of the training data belongs.
</abstract>

<claims>
I claim:
1. A method for classifying binary data, the method comprising: obtaining training data having a predefined sample size, wherein the training data is composed of separable binary datasets; determining an exact bound on Vapnik-Chervonenkis (VC) dimension of a classifier for the training data, wherein the exact bound is based one or more variables defining the hyperplane; and minimizing the exact bound on the VC dimension; based on the minimizing of the exact bound, determining optimal values of the one or more variables defining the hyperplane; and generating a classifier, based on minimized exact bound, for predicting one class to which a given data sample of the training data belongs, wherein the exact bound is a function of the distances of closest and furthest from amongst the training data from the hyperplane, wherein the hyperplane classifies plurality of points within the training data with zero error, and wherein for a notional hyperplane depicted by the following relation:
description="In-line Formulae" end="lead"?uTx+v=0,description="In-line Formulae" end="tail"? the exact bound on the VC dimension for the hyperplane is a function of h, being defined by: wherein, xi, i=1, 2, . . . , M depict data points within training data.
2. The method as claimed in claim 1, wherein the binary datasets are one of linearly separable datasets and non-linearly separable datasets.
3. The method as claimed in claim 1, wherein the function to be minimized is another function of h added to a misclassification error parameter.
4. The method as claimed in claim 1, wherein the minimizing the exact bound further comprises: reducing the linear fractional programming problem of minimizing the h to obtain a linear programming problem; by solving the linear programming problem so obtained, obtaining a decision function for classifying the test data.
5. The method as claimed in claim 4, wherein the decision function has a low VC dimension.
6. The method as claimed in claim 4, wherein the objective of the linear programming problem includes a function of the misclassification error.
7. A system for classifying test data, the system comprising: a processor; and a data classification nodule, wherein the data classification module is to, obtaining training data having a predefined sample size, wherein the training data is composed of separable binary datasets having; determining an exact bound on the Vapnik-Chervonenkis (VC) dimension of a hyperplane for the training data, wherein the exact bound depends on one or more variables defining the hyperplane minimizing the exact bound on the VC dimension based on the minimizing of the exact bound, determining optimal values of the one or more variables defining the hyperplane; and generating a classifier based on the minimized exact bound for predicting one class to which a given data sample of the training data belongs, wherein the exact bound is a function of the distances of closest and furthest from amongst the training data from the hyperplane, wherein the hyperplane classifies plurality of points within the training data with zero error; and wherein for a notional hyperplane depicted by the following relation:
description="In-line Formulae" end="lead"?uTx+v=0,description="In-line Formulae" end="tail"? the exact bound on the VC dimension for the hyperplane is a function of h, being defined by: wherein, xi, i=1, 2, . . . , M depict data points within training data.
8. The system as claimed in claim 7, wherein the data classification module for nonlinearly separable datasets in a first dimension, is to map samples of the training data from the first dimension to a higher dimension using a mapping function Φ.
9. The system as claimed in claim 7, wherein for the notional hyperplane depicted by the relation uT Φ(x)+v=0, the data classification module is to: minimize an exact bound on the VC dimension of the hyperplane wherein the said classifier separates samples that have been transformed from the input dimension to a higher dimension by means of the mapping function (Φ); wherein the minimization task is achieved by solving a fractional programming problem that has been reduced to a linear programming problem.
10. The system as claimed in claim 7, where data classification module utilizes a Kernel function K, wherein, K is a function of two input vectors 'a' and 'b', with K being positive definite; and K(a,b)=Φ(a)T Φ(b) with K(a,b) being an inner product of the vectors obtained by transforming vectors 'a' and 'b' into a higher dimensional space by using the mapping function Φ.
11. The system as claimed in claim 7, wherein alternatively the data classification module is to further: obtain a tolerance regression parameter, for a plurality of points within the training data; obtain the value of a hypothetical function or measurement at each of said training samples derive a classification problem in which the samples of each of the two classes are determined by using the given data and the tolerance parameter define a notional hyperplane, wherein the notional hyperplane classifies the plurality of points within the derived classification problem with minimal error; and based on the notional hyperplane, generates a regressor corresponding to the plurality of points.
12. The system as claimed in claim 11, wherein for the notional hyperplane is defined by, wTx+ηy+b=0, the data classification module generates the regressor defined by,
13. The system as claimed in claim 12, wherein for the points forming a linearly separable dataset, the regressor is a linear regressor.
14. The system as claimed in claim 12, wherein for the points forming a nonlinearly separable dataset, the regressor is a kernel regressor.
15. The system as claimed in claim 12, wherein the regressor further includes an error parameter.
16. The method as claimed in claim 6, in which the solution of the linear programming problem yields a set of weights or co-efficients, with each weight corresponding to an input feature, attribute, or co-ordinate, and wherein the set of input features with non-zero weights constitutes a set of selected features to allow feature selection.
17. The method as claimed in claim 16, in which only the selected features are used to next compute a classifier, thus eliminating the noise or confusion introduced by features that are less discriminative.
18. The method as claimed in claim 3, in which the constraints are modified so that one of the terms of the objective function is non-essential and can be removed.
19. The method as claimed in claim 18, in which the removal of a term in the objective function removes the need to choose a hyper-parameter weighting the misclassification error, thus simplifying the use of the said method.
20. The method as claimed in claim 15, in which the constraints are modified so that one of the terms of the objective function is non-essential and can be removed.
21. The method as claimed in claim 1, wherein the Max function is replaced by a "soft Max" function in which distance is measured as a weighted function of distances from a plurality of hyperplanes, and in which the Min function is replaced by a "soft Min" function.
22. The system as claimed in claim 12, in which the Max function is replaced by a "soft Max" function in which distance is measured as a weighted function of distances from a plurality of hyperplanes, and in which the Min function is replaced by a "soft Min" function.
</claims>
</document>
