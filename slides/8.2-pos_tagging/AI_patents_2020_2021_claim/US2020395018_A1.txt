<document>

<filing_date>
2019-06-13
</filing_date>

<publication_date>
2020-12-17
</publication_date>

<priority_date>
2019-05-06
</priority_date>

<ipc_classes>
G06F3/16,G10L15/22,G10L15/26
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
VLASYUK, BOHDAN
COTTING, DANIEL
ANDREICA, MUGUREL IONUT
MIRELMANN, LUCAS
CHENG, STEVE
BEHZADI, BEHSHAD
SABUR, ZAHEED
BURAKOV, DENIS
NOWAK-PRZYGODZKI, MARCIN
Golikov, Michael
Voroneanu, Radu
Nazarov, Sergey
Bertschler, Mario
</inventors>

<docdb_family_id>
67138078
</docdb_family_id>

<title>
INITIALIZING NON-ASSISTANT BACKGROUND ACTIONS, VIA AN AUTOMATED ASSISTANT, WHILE ACCESSING A NON-ASSISTANT APPLICATION
</title>

<abstract>
Implementations set forth herein relate to a system that employs an automated assistant to further interactions between a user and another application, which can provide the automated assistant with permission to initialize relevant application actions simultaneous to the user interacting with the other application. Furthermore, the system can allow the automated assistant to initialize actions of different applications, despite being actively operating a particular application. Available actions can be gleaned by the automated assistant using various application-specific schemas, which can be compared with incoming requests from a user to the automated assistant. Additional data, such as context and historical interactions, can also be used to rank and identify a suitable application action to be initialized via the automated assistant.
</abstract>

<claims>
1. A method implemented by one or more processors, the method comprising: determining, by a computing device while an application is executing at the computing device, that a user has provided a spoken utterance that is directed to an automated assistant but does not explicitly identify any application that is accessible via the computing device, wherein the spoken utterance is received at an automated assistant interface of the computing device, and the automated assistant is separately accessible from the application; accessing, based on determining that the user has provided the spoken utterance, application data characterizing multiple different actions capable of being performed by the application that is executing at the computing device; determining, based on the application data, a correlation between content of the spoken utterance provided by the user and the application data; selecting an action, from the multiple different actions characterized by the application data, for initializing via the automated assistant, wherein the action is selected based on the correlation between the content of the spoken utterance and the application data; and when the selected action corresponds to one of the multiple different actions capable of being performed by the application that is executing at the computing device: causing, via the automated assistant, the application to initialize performance of the selected action.
2. The method of claim 1, wherein the application data further characterizes other actions capable of being performed via one or more other applications that are separately accessible from the automated assistant and the application.
3. The method of claim 2, further comprising: when the selected action corresponds to one of the other actions capable of being performed by another application that is different from the application that is executing at the computing device: causing, via the automated assistant, the other application to initialize performance of the selected action.
4. The method of claim 1, wherein the application data identifies one or more contextual actions of the multiple different actions based on one or more features of a current application status of the application when the user provided the spoken utterance.
5. The method of claim 4, wherein the one or more contextual actions are identified by the application and the one or more features characterize a graphical user interface of the application rendered when the user provided the spoken utterance.
6. The method of claim 4, wherein the one or more contextual actions are identified by the application based on a status of an ongoing action that is being performed at the computing device when the user provided the spoken utterance.
7. The method of claim 1, further comprising: determining a success metric for one or more actions of the multiple different actions, wherein the success metric for a particular action is based at least on a number of times the particular action has been completely performed in response to the user, and/or one or more other users, initializing the particular action via the automated assistant, and wherein the action is selected based further on the success metric for the action relative to other actions of the multiple different actions.
8. The method of claim 7, further comprising: subsequent to causing the application and/or another application to initialize performance of the action: determining whether the action was completely performed by the application and/or the other application, and causing, based on whether the action was completely performed by the application and/or the other application, a corresponding success metric for the action to be modified.
9. The method of claim 1, prior to determining that the user has provided the spoken utterance while the application is executing at the computing device: determining that another application provided a notification to the user via an interface of the computing device while the application is executing at the computing device, wherein the application data includes other data that characterizes another action capable of being performed by the other application, and wherein the other data is requested from other application in response to the user providing the spoken utterance.
10. The method of claim 1, wherein the application data identifies the multiple different actions capable of being performed by the application, and also identifies descriptive data for each action of the multiple different actions, wherein particular descriptive data for a particular action of the multiple different actions characterizes two or more properties of the particular action.
11. (canceled)
12. The method of claim 1, wherein determining that the user has provided the spoken utterance that is directed to the automated assistant but does not explicitly identify any application that is accessible via the computing device includes: generating, at the computing device, audio data that embodies the spoken utterance provided by the user, and processing, at the computing device, the audio data according to a speech-to-text process and/or a natural language understanding process.
13. The method of claim 12, wherein the computing device includes one or more processors, and the speech-to-text and/or the natural language understanding process are performed using the one or more processors of the computing device.
14. A method implemented by one or more processors, the method comprising: determining, by a computing device that provides access to an automated assistant, that a user has provided one or more inputs for invoking the automated assistant, wherein the one or more inputs are provided by the user while an application is exhibiting a current application status; accessing, based on determining that the user has provided the one or more inputs, application data characterizing multiple different actions capable of being performed via one or more applications that include the application, wherein the application data characterizes contextual actions that can be performed by the application when the application is exhibiting the current application status; determining, based on the application data, a correlation between content of a spoken utterance that the user provided while the application is exhibiting the current application status, wherein the spoken utterance does not explicitly identify any application that is accessible via the computing device; selecting, based on the correlation between the content of the spoken utterance and the application data, an action from the contextual actions characterized by the application data; and when the selected action corresponds to one of the contextual actions that can be performed by the application when the application is exhibiting the current application status: causing, via the automated assistant, the application to initialize performance of the selected action.
15. The method of claim 14, wherein the current application status is exhibited by the application when the application is being rendered in a foreground of a display interface of the computing device and/or another computing device.
16. The method of claim 14, further comprising: when the selected action corresponds to another application that is different from the application that is exhibiting the current application status: causing, via the automated assistant, the other application to initialize performance of the selected action.
17. The method of claim 15, wherein the application data characterizes another action that is initialized via selection of one or more graphical interface elements omitted from the display interface when the current application status is exhibited by the application.
18. The method of claim 14, wherein determining that the user has provided the spoken utterance that is directed to the automated assistant but does not explicitly identify any application that is accessible via the computing device includes: generating, at the computing device, audio data that embodies the spoken utterance provided by the user, and processing, at the computing device, the audio data according to a speech-to-text process and/or a natural language understanding process.
19. (canceled)
20. A method implemented by one or more processors, the method comprising: receiving, from an automated assistant and while a user is accessing an application that is available via a computing device, an indication that the user has provided a spoken utterance, wherein the spoken utterance does not explicitly identify any application that is accessible via the computing device, and wherein the automated assistant is separately accessible from the application; providing, in response to receiving the indication that the user has provided the spoken utterance, application data that characterizes one or more contextual actions, wherein the one or more contextual actions are identified by the application based on an ability of the application to initialize performance of the one or more contextual actions while the application is in a current state, and wherein the one or more contextual actions are selected from multiple different actions based on the current state of the application; causing, based on providing the application data, the automated assistant to determine whether the spoken utterance corresponds to a particular action of the one or more contextual actions characterized by the application data; and when the automated assistant determines that the spoken utterance corresponds to the particular action of the one or more contextual actions characterized by the application data: causing, based on the spoken utterance corresponding to the particular action, the automated assistant to initialize performance of the particular action via the application.
21. The method of claim 20, wherein the application data also identifies descriptive data for each contextual action of the one or more contextual actions, wherein particular descriptive data for a particular contextual action characterizes two or more properties of the particular contextual action.
22. The method of claim 21, wherein the two or more properties include an action type name that characterizes a type of action corresponding to the particular contextual action, and/or an interface type name corresponding to a type of interface that renders content during execution of the particular contextual action.
23. 23-29. (canceled)
</claims>
</document>
