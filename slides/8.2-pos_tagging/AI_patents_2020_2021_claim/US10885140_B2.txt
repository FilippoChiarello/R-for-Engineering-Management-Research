<document>

<filing_date>
2020-04-08
</filing_date>

<publication_date>
2021-01-05
</publication_date>

<priority_date>
2019-04-11
</priority_date>

<ipc_classes>
G06F16/9536,G06F16/9537,G06F16/9538,G06N20/00,G06N5/04
</ipc_classes>

<assignee>
VAEAENAENEN, MIKKO
</assignee>

<inventors>
VAEAENAENEN, MIKKO
</inventors>

<docdb_family_id>
72749097
</docdb_family_id>

<title>
Intelligent search engine
</title>

<abstract>
A search engine (200, 500, 800), method and a system for performing a search is provided. The search engine (200, 500, 800) is connected to at least one mobile device (210, 510, 810) and at least one web crawler (222, 522, 822). The web crawler (222, 522, 822) is configured to index documents and classify said documents. The search engine (200, 500, 800) receives a query from the mobile device (210, 510, 810) which is determined to be best answered by a crowd-sourced answer. The search engine (200, 500, 800) searches the documents and delivers at least one crowd-sourced answer (318, 618, 918). The search engine (200, 500, 800) displays the crowd-sourced answer (318, 618, 918) to a user.
</abstract>

<claims>
1. A search engine connected to at least one mobile device and at least one web crawler, characterized in that, the web crawler is configured to index documents and classify said documents, the determination to seek a crowd-sourced answer is done for queries that are determined not to have a factual answer, the determination to seek a crowd-sourced answer is not done for queries that are determined to have a factual answer, the factuality of the answer and/or the query is determined based on the dispersion of different answers to the query, so that a greater dispersion among different answers to the query exceeding a numerical threshold is determined to imply non-factuality of the query and/or the answer, and a lesser dispersion of different answers not exceeding the numerical threshold to the query is determined to imply a greater factuality of the query and/or the answer, the search engine receives a non-factual query from the user of the mobile device which is determined to be best answered by a crowd-sourced answer, the search engine searches the documents and delivers at least one crowd-sourced answer, and the crowd-sourced answer is displayed to user on a display of the mobile device, so that a most popular crowd-sourced answer is ranked first and displayed to the user first and/or a breakdown or selection of possible answers is shown to the user on the display.
2. A search engine as claimed in claim 1, characterized in that, a most popular crowd-sourced answer is subjected to a veracity test, and if the veracity test is failed, the most popular search result passing the veracity test is ranked first.
3. A search engine as claimed in claim 1, characterized in that, the web crawler is configured to crawl and index any of the following individually or in a mix: text, voice, image and/or video.
4. A search engine as claimed in claim 1, characterized in that, a crowd-sourced answer may be sought to a query that is contextual, and/or context data required to answer the query is derived from the mobile device of the user.
5. A search engine as claimed in claim 4, characterized in that, a most popular crowd-sourced answer is calculated by assigning different context weights to different results.
6. A search engine as claimed in claim 5, characterized in that, the said context weights are user location dependent and/or user time dependent.
7. A search engine as claimed in claim 1, characterized in that, the search engine is trained with a training set of queries and a validation set of queries.
8. A search engine as claimed in claim 1, characterized in that, the search engine is trained with a training set of web crawler and/or index syntaxes and a validation set of web crawler and/or index syntaxes.
9. A method of performing a search by a search engine, the search engine connected to at least one mobile device and at least one web crawler, characterized in that, configuring the web crawler to index documents and classify said documents, determining to seek a crowd-sourced answer for queries that are determined not to have an unambiguous factual answer, determining to seek a crowd-sourced answer is not done for queries that are determined to have an unambiguous factual answer, the factuality of the answer and/or the query is determined based on the dispersion of different answers to the query, so that a greater dispersion among different answers to the query exceeding a numerical threshold is determined to imply non-factuality of the query and/or the answer, and a lesser dispersion of different answers to the query not exceeding the numerical threshold is determined to imply a greater factuality of the query and/or the answer, receiving a query from the user of the mobile device which is determined to be best answered by a crowd-sourced answer, searching said documents and delivering at least one crowd-sourced answer, and displaying the crowd-sourced answer to a user so that the most popular crowd-sourced answer is ranked first and displayed to the user first and/or displaying a breakdown or selection of possible answers to the user on the display.
10. The method as claimed in claim 9, characterized in that, subjecting the most popular crowd-sourced answer to a veracity test, and if the veracity test is failed, the most popular search result passing the veracity test is ranked first.
11. The method as claimed in claim 9, characterized in that, the web crawler crawls and indexes any of the following individually or in a mix: text, voice, image and/or video.
12. The method as claimed in claim 9, characterized in that, seeking a crowd-sourced answer to a query that is contextual, and/or context data required to answer the query is derived from the mobile device of the user.
13. The method as claimed in claim 12, characterized in that, calculating the most popular crowd-sourced answer by assigning different context weights to different results.
14. The method as claimed in claim 13, characterized in that, the said context weights are user location dependent and/or user time dependent.
15. The method as claimed in claim 9, characterized in that, training the search engine with a training set of queries and a validation set of queries.
16. The method as claimed in claim 9, characterized in that, training the search engine with a training set of web crawler and/or index syntaxes and a validation set of web crawler and/or index syntaxes.
17. A search engine system the search engine connected to at least one mobile device and at least one web crawler, characterized in that, a configuration module of the search engine configures the web crawler to index documents and classify said documents, the AI module is configured to seek a crowd-sourced answer for queries that are determined not to have an unambiguous factual answer, the AI module is configured to not seek a crowd-sourced answer for queries that are determined to have an unambiguous factual answer, the factuality of the answer and/or the query is determined based on the dispersion of different answers to the query, so that a greater dispersion among different answers to the query exceeding a numerical threshold is determined to imply non-factuality of the query and/or the answer, and a lesser dispersion of different answers to the query not exceeding the numerical threshold is determined to imply a greater factuality of the query and/or the answer, a receiving module of the search engine is configured to receive a query from the user of the mobile device which is determined to be best answered by a crowd-sourced answer, an Artificial Intelligence (AI) module of the search engine is configured to search the documents and deliver at least one crowd-sourced answer, and a display module of the search engine is configured to display the crowd-sourced answer to a user so that the ranking module ranks first the most popular crowd-sourced answer and this answer is displayed first and/or a breakdown or selection of possible answers is shown to the user on the display.
18. The system as claimed in claim 17, characterized in that, a veracity module configured to subject the most popular crowd-sourced answer to a veracity test, and if the veracity test fails, a most popular search result that passes the veracity test is ranked first.
19. The system as claimed in claim 17, characterized in that, the web crawler is configured to crawl and index any of the following individually or in a mix: text, voice, image and/or video.
20. The system as claimed in claim 17, characterized in that, a determination module configured to seek a crowd-sourced answer to a query that is contextual and/or a context module derives context data required to answer the query from the mobile device of the user.
21. The system as claimed in claim 20, characterized in that, a calculation module is configured to calculate the most popular crowd-sourced answer by assigning different context weights to different results.
22. The system as claimed in claim 21, characterized in that, the said context weights are user location dependent and/or user time dependent.
23. The system as claimed in claim 17, characterized in that, a training module is configured to train the search engine with a training set of queries and a validation set of queries.
24. The system as claimed in claim 17, characterized in that, the training module is configured to train the search engine with a training set of web crawler and/or index syntaxes and a validation set of web crawler and/or index syntaxes.
</claims>
</document>
