<document>

<filing_date>
2018-09-06
</filing_date>

<publication_date>
2020-03-12
</publication_date>

<priority_date>
2018-09-06
</priority_date>

<ipc_classes>
G06K9/62,G06N3/08
</ipc_classes>

<assignee>
SAP
</assignee>

<inventors>
REISSWIG, CHRISTIAN
HOEHNE, JOHANNES
KATTI, ANOOP RAVEENDRA
</inventors>

<docdb_family_id>
69719901
</docdb_family_id>

<title>
OPTICAL CHARACTER RECOGNITION USING END-TO-END DEEP LEARNING
</title>

<abstract>
Disclosed herein are system, method, and computer program product embodiments for optical character recognition using end-to-end deep learning. In an embodiment, an optical character recognition system may train a neural network to identify characters of pixel images and to assign index values to the characters. The neural network may also be trained to identify groups of characters and to generate bounding boxes to group these characters. The optical character recognition system may then analyze documents to identify character information based on the pixel data and produce a segmentation mask and one or more bounding box masks. The optical character recognition system may supply these masks as an output or may combine the masks to generate a version of the received document having optically recognized characters.
</abstract>

<claims>
1. A computer implemented method, comprising: receiving a document image; analyzing pixels of the document image using a neural network to identify characters of the document image; mapping the characters to index values using mapping functions of the neural network; combining the index values to generate a segmentation mask including position information for the characters; and overlaying the segmentation mask on the document image to generate a document having optically recognized characters.
2. The computer implemented method of claim 1, further comprising: generating one or more bounding boxes indicating groups of characters in the document image.
3. The computer implemented method of claim 2, further comprising: combining the one or more bounding boxes into a bounding box mask, wherein the one or more bounding boxes are located in positions corresponding to the groups of characters in the document image.
4. The computer implemented method of claim 3, further comprising: overlaying the bounding box mask on the document image.
5. The computer implemented method of claim 1, wherein the mapping functions are a result of training data supplied to the neural network.
6. The computer implemented method of claim 1, wherein the mapping functions are configured to map characters from two different languages to the index values.
7. The computer implemented method of claim 1, wherein the segmentation mask includes a unique color for each of the index values.
8. A system, comprising: a memory; and at least one processor coupled to the memory and configured to: receive a document image; analyze pixels of the document image using a neural network to identify characters of the document image; map the characters to index values using mapping functions of the neural network; combine the index values to generate a segmentation mask including position information for the characters; and overlay the segmentation mask on the document image to generate a document having optically recognized characters.
9. The system of claim 8, wherein the at least one processor is further configured to: generate one or more bounding boxes indicating groups of characters in the document image.
10. The system of claim 9, wherein the at least one processor is further configured to: combine the one or more bounding boxes into a bounding box mask, wherein the one or more bounding boxes are located in positions corresponding to the groups of characters in the document image.
11. The system of claim 10, wherein the at least one processor is further configured to: overlay the bounding box mask on the document image.
12. The system of claim 8, wherein the mapping functions are a result of training data supplied to the neural network.
13. The system of claim 8, wherein the mapping functions are configured to map characters from two different languages to the index values.
14. The system of claim 8, wherein the segmentation mask includes a unique color for each of the index values.
15. A non-transitory computer-readable device having instructions stored thereon that, when executed by at least one computing device, cause the at least one computing device to perform operations comprising: receiving a document image; analyzing pixels of the document image using a neural network to identify characters of the document image; mapping the characters to index values using mapping functions of the neural network; combining the index values to generate a segmentation mask including position information for the characters; and overlaying the segmentation mask on the document image to generate a document having optically recognized characters.
16. The non-transitory computer-readable device of claim 15, the operations further comprising: generating one or more bounding boxes indicating groups of characters in the document image; and combining the one or more bounding boxes into a bounding box mask, wherein the one or more bounding boxes are located in positions corresponding to the groups of characters in the document image.
17. The non-transitory computer-readable device of claim 16, the operations further comprising: overlaying the bounding box mask on the document image.
18. The non-transitory computer-readable device of claim 15, wherein the mapping functions are a result of training data supplied to the neural network.
19. The non-transitory computer-readable device of claim 15, wherein the mapping functions are configured to map characters from two different languages to the index values.
20. The non-transitory computer-readable device of claim 15, wherein the segmentation mask includes a unique color for each of the index values.
</claims>
</document>
