<document>

<filing_date>
2019-01-18
</filing_date>

<publication_date>
2020-07-23
</publication_date>

<priority_date>
2019-01-18
</priority_date>

<ipc_classes>
G06K9/00,G06T13/80,G06T7/174
</ipc_classes>

<assignee>
SNAP
</assignee>

<inventors>
SAVCHENKOV, PAVEL
KROKHALEV, EUGENE
MASHRABOV, ALEKSANDR
</inventors>

<docdb_family_id>
69726709
</docdb_family_id>

<title>
SYSTEMS AND METHODS FOR PHOTOREALISTIC REAL-TIME PORTRAIT ANIMATION
</title>

<abstract>
Provided are systems and methods for photorealistic real-time portrait animation. An example method includes receiving a scenario video with at least one input frame. The input frame includes a first face. The method further includes receiving a target image with a second face. The method further includes determining, based on the at least one input frame and the target image, two-dimensional (2D) deformations, wherein the 2D deformations, when applied to the second face, modify the second face to imitate at least a facial expression and a head orientation of the first face. The method further includes applying, by the computing device, the 2D deformations to the target image to obtain at least one output frame of an output video.
</abstract>

<claims>
1. A method for portrait animation, the method comprising: receiving, by a computing device, a scenario video, the scenario video including at least one input frame, the at least one input frame including a first face; receiving, by the computing device, a target image, the target image including a second face; determining, by the computing device and based on the at least one input frame and the target image, two-dimensional (2D) deformations, wherein the 2D deformations when applied to the second face, modify the second face to imitate at least a facial expression and a head orientation of the first face; and applying, by the computing device, the 2D deformations to the target image to obtain at least one output frame of an output video.
2. The method of claim 1, further comprising, prior to applying 2D deformations: performing, by the computing device and using a deep neural network (DNN), segmentation of the target image to obtain an image of the second face and a background; and wherein applying, by the computing device, the 2D deformations includes applying the 2D deformations to the image of the second face to obtain the deformed face while keeping the background unchanged.
3. The method of claim 2, further comprising, upon applying 2D deformations: inserting, by the computing device, the deformed face into the background; and predicting, by the computing device and using the DNN, a portion of the background in gaps between the deformed face and the background; and filling, by the computing device, the gaps with the predicted portions.
4. The method of claim 1, wherein the determining 2D deformations includes: determining, by the computing device, first control points on the first face; determining, by the computing device, second control points on the second face; and defining, by the computing device, 2D deformations or affine transformations for aligning the first control points to the second control points.
5. The method of claim 4, wherein the determining of the 2D deformations includes building, by the computing device, a triangulation of the second control points.
6. The method of claim 5, wherein the determining the 2D deformations further includes: determining, by the computing device, displacements of the first control points in the at least one input frame; projecting, by the computing device and using the affine transformation, the displacements on the target image, to obtain expected displacements of the second control points; and determining, by the computing device and based on the expected displacements, a warp field to be used as the 2D deformations.
7. The method of claim 6, wherein the warp field includes a set of a piecewise linear transformations defined by changes of triangles in the triangulation of the second control points.
8. The method of claim 1, further comprising: generating, by the computing device, one of a mouth region and an eyes region; and inserting, by the computing device, the one of the mouth region and the eyes region into the at least output frame.
9. The method of claim 8, wherein the generation of the one of the mouth region and the eyes region includes transferring, by the computing device, the one of the mouth region and the eyes region from the first face.
10. The method of claim 8, wherein generating the one of the mouth region and the eyes region includes: fitting, by the computing device, a 3D face model to the first control points to obtain a first set of parameters, the first set of parameters including at least a first facial expression; fitting, by the computing device, the 3D face model to the second control points to obtain a second set of parameters, the second set of parameters including at least second facial expression; transferring, by the computing device, the first facial expression from the first set of parameters to the second set of parameters; and synthesizing, by the computing device and using the 3D face model, the one of the mouth region and the eyes region.
11. A system for portrait animation, the system comprising at least one processor, a memory storing processor-executable codes, wherein the at least one processor is configured to implement the following operations upon executing the processor-executable codes: receiving a scenario video, the scenario video including at least one input frame, the at least one input frame including a first face; receiving a target image, the target image including a second face; determining, based on the at least one input frame and the target image, two-dimensional (2D) deformations, wherein the 2D deformations, when applied to the second face, modify the second face to imitate at least a facial expression and a head orientation of the first face; and applying the 2D deformations to the target image to obtain at least one output frame of an output video.
12. The system of claim 11, further comprising, prior to applying 2D deformations: performing, using a deep neural network (DNN), segmentation of the target image to obtain an image of the second face and a background; and wherein applying the 2D deformations includes applying the 2D deformations to the image of the second face to obtain the deformed face while keeping the background unchanged.
13. The system of claim 12, further comprising, upon applying 2D deformations: inserting, by the computing device, the deformed face into the background; and predicting, using the DNN, a portion of the background in gaps between the deformed face and the background; and filling the gaps with the predicted portions.
14. The system of claim 11, wherein the determining 2D deformations includes: determining first control points on the first face; determining second control points on the second face; and defining 2D deformations or affine transformations for aligning the first control points to the second control points.
15. The system of claim 14, wherein the determining 2D deformations includes building a triangulation of the second control points.
16. The method of claim 15, wherein the determining the 2D deformations further includes: determining displacements of the first control points in the at least one input frame; projecting, using the affine transformation, the displacements on the target image, to obtain expected displacements of the second control points; and determining, based on the expected displacements, a warp field to be used as the 2D deformations.
17. The system of claim 16, wherein the warp field includes a set of a piecewise linear transformations defined by changes of triangles in the triangulation of the second control points.
18. The system of claim 11, wherein the method id further comprising: generating one of a mouth region and an eyes region; and inserting the one of the mouth region and the eyes region into the at least output frame.
19. The system of claim 18, wherein generating the one of the mouth region and the eyes region includes: fitting a 3D face model to the first control points to obtain a first set of parameters, the first set of parameters including at least a first facial expression; fitting the 3D face model to the second control points to obtain a second set of parameters, the second set of parameters including at least second facial expression; transferring the first facial expression from the first set of parameters to the second set of parameters; and synthesizing, using the 3D face model, the one of the mouth region and the eyes region.
20. A non-transitory processor-readable medium having instructions stored thereon, which when executed by one or more processors, cause the one or more processors to implement a method for portrait animation, the method comprising: receiving a scenario video, the scenario video including at least one input frame, the at least one input frame including a first face; receiving a target image, the target image including a second face; determining based on the at least one input frame and the target image, two-dimensional (2D) deformations, wherein the 2D deformations, when applied to the second face, modify the second face to imitate at least a facial expression and a head orientation of the first face; and applying the 2D deformations to the target image to obtain at least one output frame of an output video.
</claims>
</document>
