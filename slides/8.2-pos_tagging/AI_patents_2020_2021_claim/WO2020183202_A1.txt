<document>

<filing_date>
2020-03-13
</filing_date>

<publication_date>
2020-09-17
</publication_date>

<priority_date>
2019-03-13
</priority_date>

<ipc_classes>
G06T15/50,G06T19/00,G06T7/11,G06T7/30
</ipc_classes>

<assignee>
MEMENTO LTD
</assignee>

<inventors>
TRUONG, Michael Vi Nguyen
</inventors>

<docdb_family_id>
66380358
</docdb_family_id>

<title>
IMAGE PROCESSING METHOD AND SYSTEM
</title>

<abstract>
A method of combining image data to generate a virtual reality representation of an event, the method comprising; receiving a first set of one or more images captured at the location of the event; generating the virtual reality representation in dependence on a first representation that has been generated in dependence on said first set of one or more images; receiving a second set of one or more images captured at the location of the event by one or more attendees of the event; and including the second set of one or more images in the virtual reality representation.
</abstract>

<claims>
1. A method comprising:
receiving a first set of one or more images captured at the location of an event;
generating a virtual reality representation in dependence on a first representation that has been generated in dependence on said first set of one or more images;
receiving a second set of one or more images captured at the location of the event by one or more attendees of the event;
processing the one or more images in the second set to separate regions at at least two different focal planes;
including the second set of one or more images in the virtual reality representation based at least on one of the regions.
2. The method of claim 1, wherein the first representation is a panoramic image or a 3D model.
3. The method of claim 1 or claim 2, where the processing of the one or more images in the second set to separate the regions comprises processing of the one or more images by a neutral network to segment the regions.
4. The method of claims 1 to 3, wherein the first set of one or more images has been captured before the event takes place.
5. The method of any one of claims 1 to 4, wherein the regions correspond to the foreground and background of the one or more images in the second set,
wherein the including of the or each image of second set is performed based on the background or foreground of the or each image.
6. The method of claim 5, wherein the generation of the first representation is further dependent on the region corresponding to the background of one or each image of the second set.
7. The method of claim 5 or claim 6, further comprising processing the background of the one or more images in the second set to minimize the difference in colour balance between the background of the one or more images in the second set and the first
representation.
8. The method of any one of claims 1 to 7, wherein the regions correspond to separate regions of the foreground at different focal planes, wherein the inclusion of the or each image of the second set is performed based on one of the separate regions of the foreground of the or each image.
9. The method of any one of claims 1 to 4, wherein the regions correspond to static and dynamic regions of the or each image, wherein the inclusion of the or each image of the second set is performed based on one of the separate regions corresponding to the static region of the or each image.
10. The method of any one of claims 1 to 9, wherein the first representation is represented by a cube map; and
the inclusion of the second set of one or more images in the virtual reality
representation comprises:
comparing the one or more images in the second set to each face of the cube map and assigning each of the one or more images in the second set to a face of the cube map based on the comparing; and
combining each of the one or more images in the second set with the region of the first representation corresponding to the face of the cube map to which each of the one or more images in the second set has been assigned.
11. The method of any one of claims 1 to 10, further comprising receiving contextual data associated with the one or more images of the second set; wherein the inclusion of the second set of one or more images is determined based on the contextual data.
12. The method of any claim 11, wherein the contextual data includes sensor data captured by the device used to capture each image.
13. The method of claim 12, wherein the sensor data includes the location of the device used to capture each of the one or more images at the time when the image was captured.
14. The method of claims 12 and 13, wherein the sensor data includes the orientation of the device used to capture each of the one or more images at the time when the image was captured.
15. The method of any one of claims 12 to 14, wherein the sensor data includes the time at which each of the one or more images was captured.
16. The method of any one of claims 1 to 15, further comprising:
receiving a third set of one or more images captured at the location of the event wherein the third set of one of more images has been captured at a different position within the same location to the first set of one or more images;
wherein the generation of the virtual reality representation of the event is further in dependence on a second representation that has been generated in dependence on said third set of one or more images.
17. The method of claim 16, further comprising, when the first representation and the second representation are both a panoramic image, obtaining a 3D model of the location of the event based on the first representation and the second representation;
wherein the virtual reality representation is generated based on the obtained 3D model of the location of the event.
18. The method of any one of claims 1 to 17, further comprising: generating a 3D model of an object in dependence on at least one of the second set of one or more images; and including the 3D model of the object in the VR representation.
19. The method of any one of the preceding claims, further comprising:
using a trained neural network to segment at least one further region in the first representation and in the one or more of the images of the second set, wherein the further regions are associated with a respective object, and to semantically label the further region in association with the respective object;
matching the labels associated with objects in the or each second images with labels associated with objects in the one or images of the second set,
wherein the including the or each second image into the virtual reality representation is in dependence on a result of the matching.
20. A system configured to produce a virtual reality representation of an event, the system comprising a remote processor, wherein the remote processor is configured to perform the method of any one of claims 1 to 19.
21. The system of claim 20, further comprising a network of mobile devices wherein each mobile device is configured to capture images; wherein the network of mobile devices is networked with the remote processor.
</claims>
</document>
