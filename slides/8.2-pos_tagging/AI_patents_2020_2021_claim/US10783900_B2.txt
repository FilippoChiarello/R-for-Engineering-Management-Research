<document>

<filing_date>
2015-09-08
</filing_date>

<publication_date>
2020-09-22
</publication_date>

<priority_date>
2014-10-03
</priority_date>

<ipc_classes>
G06N3/04,G10L15/02,G10L15/16,G10L15/26,G10L15/28,G10L25/30
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
SAINATH, TARA N.
SAK, HASIM
SENIOR, ANDREW W.
VINYALS, ORIOL
</inventors>

<docdb_family_id>
55633221
</docdb_family_id>

<title>
Convolutional, long short-term memory, fully connected deep neural networks
</title>

<abstract>
Methods, systems, and apparatus, including computer programs encoded on computer storage media, for identifying the language of a spoken utterance. One of the methods includes receiving input features of an utterance; and processing the input features using an acoustic model that comprises one or more convolutional neural network (CNN) layers, one or more long short-term memory network (LSTM) layers, and one or more fully connected neural network layers to generate a transcription for the utterance.
</abstract>

<claims>
1. A method comprising: receiving input audio features of an utterance, wherein the input audio features include information about an acoustic signal, and wherein the input audio features comprise respective audio segment features for each of a plurality of audio segments of the utterance; and processing the input audio features using an acoustic model to generate a transcription for the utterance, wherein the processing comprises: for each of the audio segments of the utterance: generating first features for the audio segment of the utterance by processing the audio segment features for the audio segment of the utterance using one or more convolutional neural network (CNN) layers to reduce spectral variation in the audio segment features; generating second features for the audio segment of the utterance by processing both the audio segment features for the audio segment of the utterance and the first features using one or more long short-term memory network (LSTM) layers to reduce temporal variations in the second features, wherein a first layer of the one or more LSTM layers is configured to receive, as input, the audio segment features for the audio segment and the first features generated for the audio segment; and generating third features for the audio segment of the utterance by processing the second features using one or more fully connected neural network layers to transform the second features into a space for classification; and determining the transcription for the utterance based on the third features for the plurality of audio segments of the utterance.
2. The method of claim 1, wherein processing the first features using the one or more LSTM layers to generate the second features comprises: processing the first features using a linear layer to generate reduced features having a reduced dimension from a dimension of the first features; and processing the reduced features using the one or more LSTM layers to generate the second features.
3. The method of claim 1, further comprising: generating, based on the input audio features, short-term features having a first number of contextual frames, wherein features generated using the one or more CNN layers include long-term features having a second number of contextual frames that are more than the first number of contextual frames of the short-term features; and providing the short-term features and the long-term features as input to the one or more LSTM layers.
4. The method of claim 1, wherein the one or more CNN layers, the one or more LSTM layers, and the one or more fully connected neural network layers have been jointly trained to determine trained values of parameters of the one or more CNN layers, the one or more LSTM layers, and the one or more fully connected neural network layers.
5. The method of claim 1, wherein the input audio features include log-mel features having multiple dimensions.
6. The method of claim 1, wherein the input audio features include one or more contextual frames indicating a temporal context of an acoustic signal.
7. The method of claim 1, wherein processing the input audio features using the acoustic model to generate the transcription for the utterance further comprises determining likelihoods of a particular acoustic state given the input audio features.
8. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to perform operations comprising: receiving input audio features of an utterance, wherein the input audio features include information about an acoustic signal, and wherein the input audio features comprise respective audio segment features for each of a plurality of audio segments of the utterance; and processing the input audio features using an acoustic model to generate a transcription for the utterance, wherein the processing comprises: for each of the audio segments of the utterance: generating first features for the audio segment of the utterance by processing the audio segment features for the audio segment of the utterance using one or more convolutional neural network (CNN) layers to reduce spectral variation in the audio segment features; generating second features for the audio segment of the utterance by processing both the audio segment features for the audio segment of the utterance and the first features using one or more long short-term memory network (LSTM) layers to reduce temporal variations in the second features, wherein a first layer of the one or more LSTM layers is configured to receive, as input, the audio segment features for the audio segment and the first features generated for the audio segment; and generating third features for the audio segment of the utterance by processing the second features using one or more fully connected neural network layers to transform the second features into a space for classification; and determining the transcription for the utterance based on the third features for the plurality of audio segments of the utterance.
9. The system of claim 8, wherein processing the first features using the one or more LSTM layers to generate the second features comprises: processing the first features using a linear layer to generate reduced features having a reduced dimension from a dimension of the first features; and processing the reduced features using the one or more LSTM layers to generate the second features.
10. The system of claim 8, wherein the operations further comprise: generating, based on the input audio features, short-term features having a first number of contextual frames, wherein features generated using the one or more CNN layers include long-term features having a second number of contextual frames that are more than the first number of contextual frames of the short-term features; and providing the short-term features and the long-term features as input to the one or more LSTM layers.
11. The system of claim 8, wherein the one or more CNN layers, the one or more LSTM layers, and the one or more fully connected neural network layers have been jointly trained to determine trained values of parameters of the one or more CNN layers, the one or more LSTM layers, and the one or more fully connected neural network layers.
12. The system of claim 8, wherein processing the input audio features using the acoustic model to generate the transcription for the utterance further comprises determining likelihoods of a particular acoustic state given the input audio features.
13. A computer program product encoded on one or more non-transitory computer storage media, the computer program product comprising instructions that when executed by one or more computers cause the one or more computers to perform operations comprising: receiving input audio features of an utterance, wherein the input audio features include information about an acoustic signal, and wherein the input audio features comprise respective audio segment features for each of a plurality of audio segments of the utterance; and processing the input audio features using an acoustic model to generate a transcription for the utterance, wherein the processing comprises: for each of the audio segments of the utterance: generating first features for the audio segment of the utterance by processing the audio segment features for the audio segment of the utterance using one or more convolutional neural network (CNN) layers to reduce spectral variation in the audio segment features; generating second features for the audio segment of the utterance by processing both the audio segment features for the audio segment the first features using one or more long short-term memory network (LSTM) layers to reduce temporal variations in the second features, wherein a first layer of the one or more LSTM layers is configured to receive, as input, the audio segment features for the audio segment and the first features generated for the audio segment; and generating third features for the audio segment of the utterance by processing the second features using one or more fully connected neural network layers to transform the second features into a space for classification; and determining the transcription for the utterance based on the third features for the plurality of audio segments of the utterance.
14. The computer program product of claim 13, wherein processing the first features using the one or more LSTM layers to generate the second features comprises: processing the first features using a linear layer to generate reduced features having a reduced dimension from a dimension of the first features; and processing the reduced features using the one or more LSTM layers to generate the second features.
15. The computer program product of claim 13, wherein the operations further comprise: generating, based on the input audio features, short-term features having a first number of contextual frames, wherein features generated using the one or more CNN layers include long-term features having a second number of contextual frames that are more than the first number of contextual frames of the short-term features; and providing the short-term features and the long-term features as input to the one or more LSTM layers.
16. The computer program product of claim 13, wherein the one or more CNN layers, the one or more LSTM layers, and the one or more fully connected neural network layers have been jointly trained to determine trained values of parameters of the one or more CNN layers, the one or more LSTM layers, and the one or more fully connected neural network layers.
17. The computer program product of claim 13, wherein processing the input audio features using the acoustic model to generate the transcription for the utterance further comprises determining likelihoods of a particular acoustic state given the input audio features.
</claims>
</document>
