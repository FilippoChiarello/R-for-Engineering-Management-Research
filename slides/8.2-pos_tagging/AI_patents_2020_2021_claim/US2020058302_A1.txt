<document>

<filing_date>
2019-04-24
</filing_date>

<publication_date>
2020-02-20
</publication_date>

<priority_date>
2018-04-26
</priority_date>

<ipc_classes>
G06K9/00,G10L15/18,G10L15/22,G10L15/25
</ipc_classes>

<assignee>
BOE TECHNOLOGY GROUP COMPANY
</assignee>

<inventors>
WU, NAIFU
MA, XITONG
FENG, SHA
KOU, LIXIN
</inventors>

<docdb_family_id>
63609654
</docdb_family_id>

<title>
LIP-LANGUAGE IDENTIFICATION METHOD AND APPARATUS, AND AUGMENTED REALITY DEVICE AND STORAGE MEDIUM
</title>

<abstract>
A lip-language identification method and an apparatus thereof, an augmented reality device and a storage medium. The lip-language identification method includes: acquiring a sequence of face images for an object to be identified; performing lip-language identification based on a sequence of face images so as to determine semantic information of speech content of the object to be identified corresponding to lip actions in a face image; and outputting the semantic information.
</abstract>

<claims>
1. A lip-language identification method, comprising: acquiring a sequence of face images for an object to be identified; performing lip-language identification based on the sequence of face images, so as to determine semantic information of speech content of the object to be identified corresponding to lip actions in the face images; and outputting the semantic information.
2. The lip-language identification method according to claim 1, wherein the performing lip-language identification based on the sequence of face images, so as to determine the semantic information of the speech content of the object to be identified corresponding to the lip actions in the face image, comprises: sending the sequence of face images to a server, and performing, by the server, the lip-language identification so as to determine the semantic information of the speech content of the object to be identified corresponding to the lip actions in the face image.
3. The lip-language identification method according to claim 2, further comprising: receiving semantic information sent by the server, in prior to the outputting the semantic information.
4. The lip-language identification method according to claim 1, wherein the semantic information is semantic text information and/or semantic audio information.
5. The lip-language identification method according to claim 4, wherein outputting the semantic information comprises: displaying the semantic text information within a visual field of a user wearing an augmented reality device; or playing the semantic audio information, according to an output mode instruction.
6. The lip-language identification method according to claim 1, wherein acquiring the sequence of face images for the object to be identified, comprises: acquiring a sequence of images including the object to be identified; positioning the object to be identified and acquiring the azimuth of the object to be identified; and determining a position of a face region of the object to be identified in each frame of image in the sequence of images according to the positioned azimuth of the object to be identified; and generating the sequence of face images by cropping an image of the face region of the object to be identified from each frame of the images.
7. The lip-language identification method according to claim 6, wherein positioning the azimuth of the object to be identified, comprises: positioning the azimuth of the object to be identified according to a voice signal emitted when the object to be identified is speaking.
8. The lip-language identification method according to claim 2, further comprising saving the sequence of face images, after acquiring the sequence of face images for the object to be identified.
9. The lip-language identification method according to claim 8, wherein sending the sequence of face images to the server comprises: sending the saved sequence of face images to the server upon receiving a sending instruction.
10. A lip-language identification apparatus, comprising: a face image sequence acquiring unit, configured to acquire a sequence of face images for an object to be identified; a sending unit, configured to send the sequence of face images to a server, wherein the server determines semantic information corresponding to lip actions in the face images by performing lip-language identification; and a receiving unit, configured to receive semantic information from the server.
11. The lip-language identification apparatus according to claim 10, further comprising: an output unit, configured to output semantic information.
12. The lip-language identification apparatus according to claim 11, wherein the output unit comprises: an output mode instruction generation subunit, configured to generate a display mode instruction, wherein the output mode instruction includes a display mode instruction and an audio mode instruction.
13. The lip-language identification apparatus according to claim 12, wherein the semantic information is semantic text information and/or semantic audio information, and the output unit further comprises: a display subunit, configured to display the semantic text information within a visual field of a user wearing an augmented reality device upon receiving the display mode instruction; and a play subunit, configured to play the semantic audio information upon receiving the audio mode instruction.
14. The lip-language identification apparatus according to claim 10, wherein the face image sequence acquiring unit comprises: an image sequence acquiring subunit, configured to acquire a sequence of images for the object to be identified; a positioning subunit, configured to position an azimuth of the object to be identified; and a face image sequence generation subunit, configured to determine a position of a face region of the object to be identified in each frame of image in the sequence of images according to the positioned azimuth of the object to be identified; and crop an image of the face region of the object to be identified from the each frame image so as to generate the sequence of face images.
15. A lip-language identification apparatus, comprising: a processor; and a machine-readable storage medium, storing instructions that are executed by the processor for performing the lip-language identification method according to claim 1.
16. An augmented reality device, comprising the lip-language identification apparatus according to claim 10.
17. The augmented reality device according to claim 16, further comprising a camera device, a display device or a play device; wherein the camera device is configured to capture an image of the object to be identified; the display device is configured to display semantic information; and the play device is configured to play the semantic information.
18. A lip-language identification method, comprising: receiving a sequence of face images for an object to be identified sent by an augmented reality device; determining semantic information of speech content of the object to be identified corresponding to lip actions in the face images, by performing lip-language identification based on the sequence of face images; and sending the semantic information to the augmented reality device.
19. A storage medium that stores non-transitorily computer readable instructions that, when executed by a computer, the computer may execute instructions for the lip-language identification method according to claim 1.
20. A storage medium that stores non-transitorily computer readable instructions that, when executed by a computer, the computer may execute instructions for the lip-language identification method according to the lip-language identification method according to claim 18.
</claims>
</document>
