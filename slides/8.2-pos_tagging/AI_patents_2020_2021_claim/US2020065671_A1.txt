<document>

<filing_date>
2019-08-23
</filing_date>

<publication_date>
2020-02-27
</publication_date>

<priority_date>
2018-08-23
</priority_date>

<ipc_classes>
G06N3/08
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
ABRAHAM, ARUN
ALLUR, SHARAN KUMAR
MALA, VENKAPPA
SAHNI, MANAS
</inventors>

<docdb_family_id>
69590218
</docdb_family_id>

<title>
ELECTRONIC DEVICE AND OPERATING METHOD THEREOF OF PROCESSING NEURAL NETWORK MODEL BY USING PLURALITY OF PROCESSORS
</title>

<abstract>
A method of processing a neural network model by using a plurality of processors includes allocating at least one slice to each layer from among a plurality of layers included in the neural network model, allocating each layer from among the plurality of layers to the plurality of processors based on respective processing times of the plurality of processors for processing each of the at least one slice, and processing the neural network model by using the plurality of processors based on a result of the allocation.
</abstract>

<claims>
1. A method of processing a neural network model using a plurality of processors, the method comprising: allocating at least one slice to each layer from among a plurality of layers included in the neural network model; allocating each layer from among the plurality of layers to a processor from among the plurality of processors based on respective processing times of the plurality of processors for processing the at least one slice allocated to each layer; and processing the neural network model by using the plurality of processors based on a result of the allocating, wherein the processing times comprise a switching time elapsed for each processor from among the plurality of processors to receive data for processing a current slice from a previous processor from among the plurality of processors processing a previous slice.
2. The method of claim 1, wherein, as at least one layer from among the plurality of layers is determined as a slice point, each layer from among the plurality of layers is allocated to the at least one slice, and wherein the slice point is determined based on at least one of whether each layer of the plurality of layers is a branching point of the plurality of layers, whether each layer of the plurality of layers is a point at which the plurality of layers is combined, whether each layer of the plurality of layers comprises a task able to be processed by a same processor, and whether each layer of the plurality of layers comprises a task that needs high accuracy.
3. The method of claim 1, wherein each layer from among the plurality of layers is allocated to the processor from among the plurality of processors based on a path corresponding to a smallest sum of the processing times from among a plurality of paths generated as a plurality of nodes indicating combinations of different slices from among the at least one slice and different processors from among the plurality of processors are connected according to an order in which the at least one slice is arranged.
4. The method of claim 1, wherein a plurality of pieces of input data to be input to a layer of the plurality of layers are sequentially processed for each channel from among a plurality of channels of a same number as a number of the plurality of pieces of input data, and, wherein for the plurality of channels of the layer, as much memory size as needed to process a task in a first channel from among the plurality of channels is allocated.
5. The method of claim 1, further comprising: identifying at least one layer included in at least one slice allocated to a first processor of the plurality of processors; identifying at least one blob indicating at least one of data input to the at least one layer, data output from the at least one layer, and data temporarily stored in the at least one layer; and allocating memory to store data of the at least one blob.
6. The method of claim 5, wherein the allocating comprises: determining an order for processing the at least one layer; and allocating memory for a current blob by determining whether a period for using a previous blob is terminated before data of the current blob is generated based on the order.
7. The method of claim 5, wherein the allocating comprises determining a size of the memory based on a largest data size from among data sizes of the at least one blob to which a same memory is allocated.
8. An electronic device for processing a neural network model, the electronic device comprising: a memory configured to store the neural network model; at least one processor configured to allocate at least one slice to each layer from among a plurality of layers included in the neural network model, allocate each layer from among the plurality of layers to a processor from among a plurality of processors based on respective processing times of the plurality of processors for processing the at least one slice allocated to each layer, process the neural network model based on a result of the allocation; and an outputter configured to output a result of processing the neural network model, wherein the processing times comprises a switching time elapsed for each processor from among the plurality of processors to receive data for processing a current slice from a previous processor from among the plurality of processors processing a previous slice.
9. The electronic device of claim 8, wherein each layer from among the plurality of layers is allocated to the at least one slice by determining at least one layer of the plurality of layers as a slice point, and wherein the slice point is determined based on at least one of whether each layer of the plurality of layers is a branching point of the plurality of layers, whether each layer of the plurality of layers is a point at which the plurality of layers are combined, whether each layer of the plurality of layers comprises a task able to be processed by a same processor, and whether each layer of the plurality of layers comprises a task that needs high accuracy.
10. The electronic device of claim 8, wherein each layer from among the plurality of layers is allocated to the processor from among the plurality of processors based on a path corresponding to a smallest sum of the processing times from among a plurality of paths generated as a plurality of nodes indicating combinations of different slices from among the at least one slice and different processors from among the plurality of processors are connected according to an order in which the at least one slice is arranged.
11. The electronic device of claim 8, wherein a plurality of pieces of input data to be input to a layer of the plurality of layers are sequentially processed for each channel from among a plurality of channels of a same number as a number of the plurality of pieces of input data, and, wherein for the plurality of channels of the layer, as much memory size as needed to process a task in a first channel from among the plurality of channels is allocated.
12. The electronic device of claim 8, wherein the at least one processor identify at least one layer included in at least one slice allocated to a first processor of the plurality of processors, identify at least one blob indicating at least one of data input to the at least one layer, data output from the at least one layer, and data temporarily stored in the at least one layer, and allocate memory to store data of the at least one blob.
13. The electronic device of claim 12, wherein the at least one processor determine an order for processing the at least one layer, and allocate memory for a current blob by determining whether a period for using a previous blob is terminated before data of the current blob is generated based on the order.
14. The electronic device of claim 12, wherein the at least one processor determine a size of the memory to store data of the at least one blob based on a largest data size from among data sizes of the at least one blob to which a same memory is allocated.
15. A non-transitory computer-readable recording medium having recorded thereon a program for implementing the method of claim 1.
</claims>
</document>
