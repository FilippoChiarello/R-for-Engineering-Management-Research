<document>

<filing_date>
2018-03-01
</filing_date>

<publication_date>
2020-11-10
</publication_date>

<priority_date>
2018-03-01
</priority_date>

<ipc_classes>
B41B27/08,G06F17/27,G06F17/28,G06F40/284,G06F40/47,G06F40/49,G06N3/04,G10L15/00,G10L15/06,G10L15/18,G10L15/183
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
SUZUKI MASAYUKI
ITOH, NOBUYASU
KURATA, GAKUTO
</inventors>

<docdb_family_id>
67767707
</docdb_family_id>

<title>
Use of small unit language model for training large unit language models
</title>

<abstract>
A computer-implemented method, computer program product, and apparatus are provided. The method includes generating a plurality of sequences of small unit tokens from a first language model that is trained with a small unit corpus including the small unit tokens, the small unit corpus having been derived by tokenization with a small unit. The method further includes tokenizing the plurality of sequences of small unit tokens by a large unit that is larger than the small unit, to create a derived large unit corpus including derived large unit tokens.
</abstract>

<claims>
1. A computer program product including one or more computer readable storage mediums collectively storing program instructions that are executable by a processor or programmable circuitry to cause the processor or programmable circuitry to perform operations comprising: generating a plurality of sequences of small unit tokens from a first language model that is trained with a small unit corpus including the small unit tokens, the small unit corpus having been derived by tokenization with a small unit; tokenizing the plurality of sequences of small unit tokens by a large unit that is larger than the small unit, to create a derived large unit corpus including derived large unit tokens, the large unit corpus being an in-domain corpus; re-tokenizing the derived large unit corpus by the small unit to re-create the small unit corpus including the small unit tokens, the large unit corpus including the derived large unit tokens and having been derived by tokenization with the large unit; and dynamically training a plurality of word language models using combined word sequences associated with an out-of-domain native text corpus and the in-domain corpus, the large unit tokens being utilized in the out-of-domain corpus during the training and being derived by joining a randomly determined number of sequential small unit tokens in the plurality of sequences of small unit tokens, the plurality of word language models trained using the small unit corpus and the large unit corpus being interpolated to generate an interpolated language model.
2. The computer program product of claim 1, wherein the operations further comprise: training the first language model with the small unit corpus.
3. The computer program product of claim 2, wherein the first language model is a recurrent neural network.
4. The computer program product of claim 1, wherein the operations further comprise: training a second language model with the derived large unit corpus.
5. The computer program product of claim 4, wherein the second language model is an n-gram language model.
6. The computer program product of claim 4, wherein the operations further comprise: preparing an original vocabulary from the large unit corpus; and adding at least a part of the derived large unit tokens in the derived large unit corpus to the original vocabulary to create an edited vocabulary.
7. The computer program product of claim 6, wherein the adding at least a part of the derived large unit tokens in the derived large unit corpus, includes: selecting derived large unit tokens that appear in a native text corpus, from among the derived large unit tokens in the derived large unit corpus; and adding the selected derived large unit tokens to the original vocabulary to create the edited vocabulary.
8. The computer program product of claim 7, wherein the selecting derived large unit tokens that appear in a native text corpus, from among the derived large unit tokens in the derived large unit corpus, includes: selecting derived large unit tokens that appear in a native text corpus more than a predetermined frequency or amount, from among the derived large unit tokens in the derived large unit corpus.
9. The computer program product of claim 8, wherein the operations further comprise: training a third language model with the large unit corpus by using the edited vocabulary; and interpolating the second language model and the third language model to create an interpolated language model.
10. The computer program product of claim 8, wherein the operations further comprise: training a third language model with the large unit corpus by using the edited vocabulary; training a fourth language model with the native text corpus by using the edited vocabulary; and interpolating the second language model, the third language model, and the fourth language model to create an interpolated language model.
11. A computer-implemented method, comprising: generating a plurality of sequences of small unit tokens from a first language model that is trained with a small unit corpus including the small unit tokens, the small unit corpus having been derived by tokenization with a small unit; tokenizing the plurality of sequences of small unit tokens by a large unit that is larger than the small unit, to create a derived large unit corpus including derived large unit tokens, the large unit corpus being an in-domain corpus; re-tokenizing the derived large unit corpus by the small unit to re-create the small unit corpus including the small unit tokens, the large unit corpus including the derived large unit tokens and having been derived by tokenization with the large unit; and dynamically training a plurality of word language models using combined word sequences associated with an out-of-domain native text corpus and the in-domain corpus, the large unit tokens being utilized in the out-of-domain corpus during the training and being derived by joining a randomly determined number of sequential small unit tokens in the plurality of sequences of small unit tokens, the plurality of word language models trained using the small unit corpus and the large unit corpus being interpolated to generate an interpolated language model.
12. The computer-implemented method of claim 11, further comprising: training the first language model with the small unit corpus.
13. The computer-implemented method of claim 12, wherein the first language model is a recurrent neural network.
14. The computer-implemented method of claim 12, further comprising: re-tokenizing a large unit corpus by the small unit to re-create the small unit corpus including the small unit tokens, the large unit corpus including the large unit tokens and having been derived by tokenization with the large unit.
15. The computer-implemented method of claim 14, further comprising: training a second language model with the derived large unit corpus.
16. An apparatus comprising: a processor or a programmable circuitry; and one or more computer readable mediums collectively including instructions that, when executed by the processor or the programmable circuitry, cause the processor or the programmable circuitry to perform operations comprising: generating a plurality of sequences of small unit tokens from a first language model that is trained with a small unit corpus including the small unit tokens, the small unit corpus having been derived by tokenization with a small unit; tokenizing the plurality of sequences of small unit tokens by a large unit that is larger than the small unit, to create a derived large unit corpus including derived large unit tokens, the large unit corpus being an in-domain corpus; re-tokenizing the derived large unit corpus by the small unit to re-create the small unit corpus including the small unit tokens, the large unit corpus including the derived large unit tokens and having been derived by tokenization with the large unit; and dynamically training a plurality of word language models using combined word sequences associated with an out-of-domain native text corpus and the in-domain corpus, the large unit tokens being utilized in the out-of-domain corpus during the training and being derived by joining a randomly determined number of sequential small unit tokens in the plurality of sequences of small unit tokens, the plurality of word language models trained using the small unit corpus and the large unit corpus being interpolated to generate an interpolated language model.
17. The apparatus of claim 16, wherein the operations further comprise: training the first language model with the small unit corpus.
18. The apparatus of claim 17, wherein the first language model is a recurrent neural network.
19. The apparatus of claim 17, wherein the operations further comprise: re-tokenizing a large unit corpus by the small unit to re-create the small unit corpus including the small unit tokens, the large unit corpus including the large unit tokens and having been derived by tokenization with the large unit.
</claims>
</document>
