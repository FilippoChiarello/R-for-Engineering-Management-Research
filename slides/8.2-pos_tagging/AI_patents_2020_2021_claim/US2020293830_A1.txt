<document>

<filing_date>
2020-05-29
</filing_date>

<publication_date>
2020-09-17
</publication_date>

<priority_date>
2018-04-03
</priority_date>

<ipc_classes>
G06K9/62,G06N3/08
</ipc_classes>

<assignee>
ALIBABA GROUP
</assignee>

<inventors>
ZHANG, HAITAO
LIU, YONGCHAO
GUO, YUFENG
</inventors>

<docdb_family_id>
64402822
</docdb_family_id>

<title>
ARTICLE DAMAGE DETECTION
</title>

<abstract>
The present specification provides methods, apparatuses, and devices for detecting damages to an article. In one aspect, the method includes: obtaining at least two images that are time sequentially related and show the article at different angles; providing the at least two images as input to a detection model in time order, wherein the detection model comprises a first sub-model and a second sub-model that have been jointly trained on training samples associated with labels indicating respective article damage degrees; processing the at least two images using the first sub-model to determine a feature processing result based on respective features identified from each image; processing the feature processing result using the second sub-model to perform time series analysis on the feature processing result to determine a damage detection result; and obtaining, as output from the detection model, the damage detection result.
</abstract>

<claims>
1. A method for evaluating damages to an article, comprising: obtaining at least two images that are time sequentially related and show the article at different angles; providing the at least two images as input to a detection model in time order, wherein the detection model comprises a first sub-model and a second sub-model that have been jointly trained on training samples associated with labels indicating respective article damage degrees; processing the at least two images using the first sub-model to determine a feature processing result based on respective features identified from each image; processing the feature processing result using the second sub-model to perform time series analysis on the feature processing result to determine a damage detection result; and obtaining, as output from the detection model, the damage detection result.
2. The method of claim 1, wherein: the first sub-model is a deep convolutional neural network; and the second sub-model is a long short-term memory (LSTM) network.
3. The method of claim 2, wherein the second sub-model is an attention-based LSTM network.
4. The method of claim 1, wherein the at least two images that are time sequentially related and show the article at different angles comprise at least one of: photos of the article in motion captured by a stationary camera, videos of the article in motion recorded by a stationary camera, photos of the article in a stationary state captured by a moving camera, or videos of the article in a stationary state recorded by a moving camera.
5. The method of claim 1, wherein the damage detection result comprises a classification result of each of one or more types of damage.
6. The method of claim 1, wherein processing the at least two images using the first sub-model to determine the feature processing result comprises, for each image: performing feature extraction, damage discovery, and feature fusion to determine the feature processing result.
7. The method of claim 1, wherein obtaining the at least two images comprises: moving the article in accordance with a first set of movement instructions; or moving a camera in accordance with a second set of movement instructions.
8. The method of claim 1, wherein obtaining the at least two images further comprises: illuminating the article in accordance with a set of illumination instructions.
9. A non-transitory, computer-readable medium storing one or more instructions executable by a computer system to perform one or more operations for evaluating damages to an article, wherein the operations comprise: obtaining at least two images that are time sequentially related and show the article at different angles; providing the at least two images as input to a detection model in time order, wherein the detection model comprises a first sub-model and a second sub-model that have been jointly trained on training samples associated with labels indicating respective article damage degrees; processing the at least two images using the first sub-model to determine a feature processing result based on respective features identified from each image; processing the feature processing result using the second sub-model to perform time series analysis on the feature processing result to determine a damage detection result; and obtaining, as output from the detection model, the damage detection result.
10. The non-transitory, computer-readable medium of claim 9, wherein: the first sub-model is a deep convolutional neural network; and the second sub-model is a long short-term memory (LSTM) network.
11. The non-transitory, computer-readable medium of claim 10, wherein the second sub-model is an attention-based LSTM network.
12. The non-transitory, computer-readable medium of claim 9, wherein the at least two images that are time sequentially related and show the article at different angles comprise at least one of: photos of the article in motion captured by a stationary camera, videos of the article in motion recorded by a stationary camera, photos of the article in a stationary state captured by a moving camera, or videos of the article in a stationary state recorded by a moving camera.
13. The non-transitory, computer-readable medium of claim 9, wherein the damage detection result comprises a classification result of each of one or more types of damage.
14. The non-transitory, computer-readable medium of claim 9, wherein processing the at least two images using the first sub-model to determine the feature processing result comprises, for each image: performing feature extraction, damage discovery, and feature fusion to determine the feature processing result.
15. A computer-implemented system, comprising: one or more computers; and one or more computer memory devices interoperably coupled with the one or more computers and having tangible, non-transitory, machine-readable media storing one or more instructions that, when executed by the one or more computers, perform one or more operations for evaluating damages to an article, wherein the operations comprise: obtaining at least two images that are time sequentially related and show the article at different angles; providing the at least two images as input to a detection model in time order, wherein the detection model comprises a first sub-model and a second sub-model that have been jointly trained on training samples associated with labels indicating respective article damage degrees; processing the at least two images using the first sub-model to determine a feature processing result based on respective features identified from each image; processing the feature processing result using the second sub-model to perform time series analysis on the feature processing result to determine a damage detection result; and obtaining, as output from the detection model, the damage detection result.
16. The computer-implemented system of claim 15, wherein: the first sub-model is a deep convolutional neural network; and the second sub-model is a long short-term memory (LSTM) network.
17. The computer-implemented system of claim 16, wherein the second sub-model is an attention-based LSTM network.
18. The computer-implemented system of claim 15, wherein the at least two images that are time sequentially related and show the article at different angles comprise at least one of: photos of the article in motion captured by a stationary camera, videos of the article in motion recorded by a stationary camera, photos of the article in a stationary state captured by a moving camera, or videos of the article in a stationary state recorded by a moving camera.
19. The computer-implemented system of claim 15, wherein the damage detection result comprises a classification result of each of one or more types of damage.
20. The computer-implemented system of claim 15, wherein processing the at least two images using the first sub-model to determine the feature processing result comprises, for each image: performing feature extraction, damage discovery, and feature fusion to determine the feature processing result.
</claims>
</document>
