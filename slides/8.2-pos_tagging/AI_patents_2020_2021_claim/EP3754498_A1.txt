<document>

<filing_date>
2020-03-18
</filing_date>

<publication_date>
2020-12-23
</publication_date>

<priority_date>
2019-06-21
</priority_date>

<ipc_classes>
G06F9/50
</ipc_classes>

<assignee>
INTEL CORPORATION
</assignee>

<inventors>
ADILETTA, MATTHEW
GUIM BERNAT, FRANCESC
SOOD, KAPIL
DUBAL, Scott P.
MCDONNELL, Niall
CONNOR, Partick
KONDAPALLI, Ranghu
Herdrich, Andrew
BACHMUTSKY, Alexander
HEARN, James
</inventors>

<docdb_family_id>
68161614
</docdb_family_id>

<title>
ARCHITECTURE FOR OFFLOAD OF LINKED WORK ASSIGNMENTS
</title>

<abstract>
Examples are described herein that can be used to offload a sequence of work events to one or more accelerators to a work scheduler. An application can issue a universal work descriptor to a work scheduler. The universal work descriptor can specify a policy for scheduling and execution of one or more work events. The universal work descriptor can refer to one or more work events for execution. The work scheduler can, in some cases, perform translation of the universal work descriptor or a work event descriptor for compatibility and execution by an accelerator. The application can receive notice of completion of the sequence of work from the work scheduler or an accelerator.
</abstract>

<claims>
1. A work scheduler apparatus comprising: an input interface to receive a combined work descriptor, the combined work descriptor associated with at least one processing operation, the at least one processing operation to be managed by the work scheduler apparatus; an ingress queue to receive a work request based on the combined work descriptor for performance by an accelerator; an egress queue to store a work request assigned to a target accelerator; a scheduler to assign a work request in an ingress queue to an egress queue, wherein a work request includes a reference to another work request; and logic to provide an identifier of a result data to a requesting entity that requested operations based on the combined work descriptor, wherein performance and availability of data between work requests occur independent from oversight by the requesting entity.
2. The work scheduler apparatus of claim 1, wherein the combined work descriptor is to refer to a first work request, the first work request to include a reference to a second work request to be performed by a target accelerator, and the work scheduler comprising a translator to translate a first work request to a format accepted by a target accelerator.
3. The work scheduler apparatus of one of the previous claims, wherein the combined work descriptor is to refer to a first work request and the first work request is in a format accepted by a target accelerator.
4. The work scheduler apparatus of one of the previous claims, wherein the work scheduler is to enqueue a work request to an egress queue to assign to a next accelerator after completion of a work request.
5. The work scheduler apparatus of one of the previous claims, wherein the scheduler is to:
assign a work request from an ingress queue to an egress queue based on quality of service (QoS) associated with the assigned work request.
6. The work scheduler apparatus of one of the previous claims, wherein after selection of an egress queue by the scheduler and based on a target accelerator sharing virtual memory space with the entity that requested operations, the work scheduler is to receive a pointer to data from the entity that requested operations and perform pointer translation.
7. The work scheduler apparatus of one of the previous claims, wherein the work request comprises a request to process data, decrypt data, encrypt data, store data, transfer data, parse data, copy data, perform an inference using data, or transform data.
8. The work scheduler apparatus of any of claims 1-7, comprising at least two accelerators, an accelerator comprising one or more of: field programmable gate arrays (FPGAs), graphics processor units (GPUs), artificial intelligence (AI) inference engines, image recognition, object detection, speech recognition, memory, storage, central processing units (CPUs), software executed by a hardware device, or network interface.
9. A computer-implemented method comprising: receiving a combined work descriptor that identifies at least one work descriptor for performance by an accelerator and the combined work descriptor specifies a policy for managing work associated with the combined work descriptor; allocating a work descriptor associated with the combined work descriptor to an egress queue based on a scheduling policy specified by the combined work descriptor; receiving a queue entry in an ingress queue that identifies a next operation for an accelerator; and providing a result from processing based on the combined work descriptor.
10. The method of claim 9, wherein allocating a work descriptor associated with the combined work descriptor to an egress queue based on a scheduling policy specified by the combined work descriptor comprises assigning a work request from an ingress queue to an egress queue based on quality of service (QoS) associated with the work request.
11. The method of claim 9 or 10, wherein allocating a work descriptor associated with the combined work descriptor to an egress queue based on a scheduling policy specified by the combined work descriptor comprises providing load balancing of work requests in an ingress queue to an accelerator to distribute work requests to different accelerators that perform a function specified in the distributed work requests.
12. The method of any of claims 9-11, wherein the combined work descriptor refers to a first work request, the first work request to include a reference to a second work request to be performed by a target accelerator and comprising translating the first work request to a format accepted by the target accelerator.
13. A system comprising: a core; a memory; a work scheduler; at least one accelerator; and an interconnect to communicatively couple the core, the memory, the work scheduler, and the at least one accelerator, wherein: the core is to execute an application that is to request performance of a sequence of work based on a combined work descriptor and provide the combined work descriptor to the work scheduler via the interconnect, the work scheduler comprises a scheduler logic, ingress queues, egress queues, and a command translator, the work scheduler is to access a work descriptor from the memory based on content of the combined work descriptor and allocate the work descriptor to an ingress queue for execution by an accelerator, the scheduler logic is to determine an egress queue and position in an egress queue for the work descriptor based in part on a configuration, the ingress queue is to receive another work descriptor after execution by the accelerator, and the work scheduler is to indicate data is available from the sequence of work to the application.
14. The system of claim 13, wherein the combined work descriptor is to refer to a first work request, the first work request to include a reference to a second work request to be performed by a target accelerator, and the command translator to translate a first work request to a format accepted by the target accelerator.
15. The system of any of claims 13 or 14, wherein the work request comprises a request to process data, decrypt data, encrypt data, store data, transfer data, parse data, copy data, perform an inference using data, or transform data.
</claims>
</document>
