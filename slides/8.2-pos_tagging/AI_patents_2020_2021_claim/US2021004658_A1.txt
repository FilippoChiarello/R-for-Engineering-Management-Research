<document>

<filing_date>
2020-09-21
</filing_date>

<publication_date>
2021-01-07
</publication_date>

<priority_date>
2016-03-31
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
SolidRun Ltd.
</assignee>

<inventors>
KHOURY, RABEEH
OMARY, KOSSAY
ZIV, ATAI
LEVY, Avi
</inventors>

<docdb_family_id>
74065744
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR PROVISIONING OF ARTIFICIAL INTELLIGENCE ACCELERATOR (AIA) RESOURCES
</title>

<abstract>
A system and method for provisioning of artificial intelligence accelerator (AIA) resources. The method includes receiving a request for an NPU allocation from a client device; determining an available NPU based on a scanning of a network to discover NPU resources; and allocating the available NPU to the client device.
</abstract>

<claims>
1. A neural processing unit (NPU), comprising: a network interface controller connected to at least one client device over a network fabric, wherein the network interface controller is configured to receive instructions from the at least one client device; and an artificial intelligence accelerator (AIA) connected to the network interface controller, wherein the AIA is configured to receive instructions from the network interface controller and execute the received instructions.
2. The NPU of claim 1, wherein the NPU is configured to: connect with an orchestrator over the network fabric.
3. The NPU of claim 2, wherein the orchestrator is operative to allocate the NPU to the one or more client devices.
4. The NPU of claim 3, wherein allocation of the NPU is based on at least one of: a specific priority, and a billing table.
5. The NPU of claim 1, wherein the AIA is configured to host one or more trained neural network models.
6. The NPU of claim 5, wherein the trained neural network models are pre-trained neural network models.
7. An orchestrator for provisioning of artificial intelligence accelerator (AIA) resources, comprising: a processing circuitry; and a memory, the memory containing instructions that, when executed by the processing circuitry, configure the orchestrator to: receive a request for an NPU allocation from a client device; determine an available NPU based on a scanning of a network to discover NPU resources; and allocate the available NPU to the client device.
8. The orchestrator of claim 7, wherein the orchestrator is further configured to receive instructions from at least a client device.
9. The orchestrator of claim 8, wherein, in response to instructions, the orchestrator is further configured to: determine whether to execute at least one of: tasks, and threads; and generate a computing plan.
10. The orchestrator of claim 9, wherein the computing plan further comprises at least one of: instructions specifying storage and use of generated outputs, instructions specifying an NPU for execution of tasks, and instructions specifying an NPU for execution of threads.
11. The orchestrator of claim 10, wherein the orchestrator is further configured to: transmit, to the NPU, one or more components of the computing plan.
12. A method for provisioning of artificial intelligence accelerator (AIA) resources, comprising: receiving a request for an NPU allocation from a client device; determining an available NPU based on a scanning of a network to discover NPU resources; and allocating the available NPU to the client device.
13. The method of claim 12, wherein determining an available NPU further comprises: receiving, via one or more ports, at least an NPU availability signal.
14. The method of claim 12, wherein allocating the available NPU to the client device further comprises: allocating the available NPU based on at least one of: a specific priority and a billing table.
15. The method of claim 12, wherein allocating the available NPU to the client device further comprises: allocating the available NPU to the client device for direct use, wherein direct use includes allocating the available NPU not through the orchestrator.
16. The method of claim 12, wherein allocating the available NPU to the client device further comprises: allocating at least a first group of NPUs based on at least a first tier; and allocating at least a second group of NPUs based on at least a second tier.
17. The method of claim 12, further comprising: determining that the client device has completed use of the available NPU.
18. The method of claim 17, further comprising: deallocating the available NPU, upon determining that the client device completed use of the available NPU.
19. The method of claim 16, wherein determining the available NPU further comprises: determining a tier associated with the client device; and allocating an NPU from a tier which matches the determined client device tier.
20. The method of claim 12, further comprising: allocating a plurality of NPUs to the client device, such that an output of a first NPU is an input for a second NPU.
</claims>
</document>
