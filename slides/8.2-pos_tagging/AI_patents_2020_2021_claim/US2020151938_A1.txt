<document>

<filing_date>
2018-11-08
</filing_date>

<publication_date>
2020-05-14
</publication_date>

<priority_date>
2018-11-08
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,G06T15/02,G06T15/80
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
SHECHTMAN, ELYA
FANG, CHEN
HERTZMANN, AARON
LI, YIJUN
</inventors>

<docdb_family_id>
70551910
</docdb_family_id>

<title>
Generating stylized-stroke images from source images utilizing style-transfer-neural networks with non-photorealistic-rendering
</title>

<abstract>
This disclosure relates to methods, non-transitory computer readable media, and systems that integrate (or embed) a non-photorealistic rendering ("NPR") generator with a style-transfer-neural network to generate stylized images that both correspond to a source image and resemble a stroke style. By integrating an NPR generator with a style-transfer-neural network, the disclosed methods, non-transitory computer readable media, and systems can accurately capture a stroke style resembling one or both of stylized edges or stylized shadings. When training such a style-transfer-neural network, the integrated NPR generator can enable the disclosed methods, non-transitory computer readable media, and systems to use real-stroke drawings (instead of conventional paired-ground-truth drawings) for training the network to accurately portray a stroke style. In some implementations, the disclosed methods, non-transitory computer readable media, and systems can either train or apply a style-transfer-neural network that captures a variety of stroke styles, such as different edge-stroke styles or shading-stroke styles.
</abstract>

<claims>
We claim:
1. A non-transitory computer readable medium storing instructions thereon that, when executed by at least one processor, cause a computer system to: generate a simplified image of a source image utilizing a non-photorealistic-rendering ("NPR") generator corresponding to a style-transfer-neural network, the source image exhibiting a stroke style; extract a feature map from the simplified image utilizing an encoder of the style-transfer-neural network; and generate a stylized-stroke image corresponding to the source image and exhibiting the stroke style by decoding the feature map utilizing a decoder of the style-transfer-neural network.
2. The non-transitory computer readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computer system to: receive an indication of a user selection of a stroke-style setting corresponding to the stroke style from among a first stroke-style setting and a second stroke-style setting; and based on receiving the indication of the user selection of the stroke-style setting, generate the stylized-stroke image exhibiting the stroke style.
3. The non-transitory computer readable medium of claim 1, wherein the stoke style comprises one of pencil, woodcut, ink, crayon, or charcoal.
4. The non-transitory computer readable medium of claim 1, wherein the instructions, when executed by the at least one processor, cause the computer system to: receive the indication of the user selection of the stroke-style setting corresponding to the stroke style by receiving an indication of a user selection of a shading-style setting corresponding to a shading-stroke style from among a first shading-style setting and a second shading-style setting; and generate the stylized-stroke image exhibiting the stroke style by generating a stylized-shading image comprising shading exhibiting the shading-stroke style based on receiving the indication of the user selection of the shading-style setting.
5. The non-transitory computer readable medium of claim 4, wherein the shading-stroke style comprise one of lines, crossed, smudge, or stippling.
6. The non-transitory computer readable medium of claim 1, wherein: the style-transfer-neural network comprises an edge-style-transfer-neural network trained to generate stylized-edge images exhibiting an edge-stroke style; the stroke style comprises the edge-stroke style; and the instructions, when executed by the at least one processor, cause the computer system to generate the stylized-stroke image by generating a stylized-edge image corresponding to the source image utilizing a decoder of the edge-style-transfer-neural network, the stylized-edge image comprising edges exhibiting the edge-stroke style.
7. The non-transitory computer readable medium of claim 1, wherein: the style-transfer-neural network comprises a shading-style-transfer-neural network trained to generate stylized-shading images exhibiting a shading-stroke style; the stroke style comprises the shading-stroke style; and the instructions, when executed by the at least one processor, cause the computer system to generate the stylized-stroke image by generating a stylized-shading image corresponding to the source image utilizing a decoder of the shading-style-transfer-neural network, the stylized-shading image comprising shading exhibiting the shading-stroke style.
8. The non-transitory computer readable medium of claim 7, further comprising instructions that, when executed by the at least one processor, cause the computer system to: generate an additional simplified image of the source image utilizing an additional NPR generator corresponding to an edge-style-transfer-neural network; based on the additional simplified image, generate a stylized-edge image corresponding to the source image and comprising edges exhibiting an edge-stroke style by utilizing an edge-style-transfer-neural network; generate a fusion map for synthesizing stroke styles from the stylized-edge image and the stylized-shading image utilizing a style-fusion-neural network; and based on the fusion map, generate a style-fusion image comprising the edges exhibiting the edge-stroke style and the shading exhibiting the shading-stroke style.
9. The non-transitory computer readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computer system to generate the simplified image of the source image utilizing the NPR generator by: generating the simplified image comprising edge depictions from the source image utilizing an extended difference-of-gaussians operator; or generating the simplified image comprising a contrast abstraction from the source image utilizing an objective-abstraction function.
10. The non-transitory computer readable medium of claim 1, wherein the source image comprises a natural photograph and the stylized-stroke image exhibits the stroke style both locally and holistically.
11. The non-transitory computer readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computer system to train the style-transfer-neural network by: generating a simplified-training image of a stroke-training image exhibiting the stroke style utilizing the NPR generator of the style-transfer-neural network; extracting a training-feature map from the simplified-training image utilizing the encoder of the style-transfer-neural network; based on the training-feature map, generating a stylized-stroke-sample image exhibiting the stroke style utilizing a decoder of the style-transfer-neural network; determining an adversarial loss using an adversarial-loss function based on a discriminator-neural network comparing the stylized-stroke-sample image and a real-stroke drawing; determining a reconstruction loss from a reconstruction-loss function based on a comparison of the stylized-stroke-sample image and the stroke-training image; and adjusting network parameters of the style-transfer-neural network based on the determined adversarial loss and the determined reconstruction loss.
12. The non-transitory computer readable medium of claim 11, wherein: the style-transfer-neural network comprises an edge-style-transfer-neural network, the stroke style comprises an edge-stroke style, and the stroke-training image comprises an edge-stroke-training image; and the instructions, when executed by the at least one processor, cause the computer system to: generate the simplified-training image of the stroke-training image by generating the simplified-training image comprising edge depictions from the edge-stroke-training image utilizing an NPR generator corresponding to the edge-style-transfer-neural network; and generate the stylized-stroke-sample image by generating a stylized-edge-sample image exhibiting the edge-stroke style utilizing a decoder of the edge-style-transfer-neural network.
13. The non-transitory computer readable medium of claim 11, wherein: the style-transfer-neural network comprises a shading-style-transfer-neural network, the stroke style comprises a shading-stroke style, and the stroke-training image comprises a shading-stroke-training image; and the instructions, when executed by the at least one processor, cause the computer system to: generate the simplified-training image of the stroke-training image by generating the simplified-training image comprising a contrast abstraction from the shading-stroke-training image utilizing an NPR generator corresponding to the shading-style-transfer-neural network; and generate the stylized-stroke-sample image by generating a stylized-shading-sample image exhibiting the shading-stroke style utilizing a decoder of the shading-style-transfer-neural network.
14. A system comprising: at least one processor; at least one non-transitory computer readable medium comprising a first non-photorealistic-rendering ("NPR") generator, an edge-style-transfer-neural network, a second NPR generator, a shading-style-transfer-neural network, and a style-fusion-neural network; and instructions that, when executed by at least one processor, cause the system to: generate a stylized-edge image corresponding to a source image and comprising edges exhibiting an edge-stroke style by utilizing the first NPR generator to generate a first simplified image and providing the first simplified image to the edge-style-transfer-neural network; generate a stylized-shading image corresponding to the source image and comprising shading exhibiting a shading-stroke style by utilizing the second NPR generator to generate a second simplified image and providing the second simplified image to the shading-style-transfer-neural network; generate a fusion map for synthesizing stroke styles from the stylized-edge image and the stylized-shading image utilizing the style-fusion-neural network; and based on the fusion map, generate a style-fusion image comprising the edges exhibiting the edge-stroke style and the shading exhibiting the shading-stroke style.
15. The system of claim 14, further comprising instructions that, when executed by the at least one processor, cause the system to: receive an indication of a user selection of an edge-style setting corresponding to the edge-stroke style from among a first edge-style setting and a second edge-style setting; and based on receiving the indication of the user selection of the edge-style setting, generate the stylized-edge image comprising edges exhibiting the edge-stroke style.
16. The system of claim 14, further comprising instructions that, when executed by the at least one processor, cause the system to: receive an indication of a user selection of a shading-style setting corresponding to the shading-stroke style from among a first shading-style setting and a second shading-style setting; and based on receiving the indication of the user selection of the shading-style setting, generate the stylized-shading image comprising shading exhibiting the shading-stroke style.
17. The system of claim 14, wherein the edge-style-transfer-neural network comprises an encoder and a decoder, the system further comprising instructions that, when executed by the at least one processor, cause the system to generate the stylized-edge image by: utilizing the first NPR generator to generate the first simplified image comprising edge depictions from the source image; extracting a feature map from the first simplified image utilizing the encoder of the edge-style-transfer-neural network; and generating the stylized-edge image corresponding to the source image and comprising edges exhibiting the edge-stroke style by decoding the feature map utilizing the decoder of the edge-style-transfer-neural network.
18. The system of claim 14, wherein the shading-style-transfer-neural network comprises an encoder and a decoder, the system further comprising instructions that, when executed by the at least one processor, cause the system to generate the stylized-shading image by: utilizing the second NPR generator to generate the second simplified image comprising a contrast abstraction from the source image; extracting a feature map from the second simplified image utilizing the encoder of the shading-style-transfer-neural network; and generating the stylized-shading image corresponding to the source image and comprising shading exhibiting the shading-stroke style by decoding the feature map utilizing the decoder of the edge-style-transfer-neural network.
19. The system of claim 14, wherein the style-fusion-neural network comprises a first encoder, a second encoder, and a decoder, the system further comprising instructions that, when executed by the at least one processor, cause the system to generate the fusion map for synthesizing the stroke styles from the stylized-edge image and the stylized-shading image by: extracting a first feature map from the stylized-edge image utilizing the first encoder of the style-fusion-neural network; extracting a second feature map from the stylized-shading image utilizing the second encoder of the style-fusion-neural network; concatenating the first feature map and the second feature map to generate a concatenated feature map; and based on the concatenated feature map, generating the fusion map for synthesizing the stroke styles from the stylized-edge image and the stylized-shading image utilizing the decoder of the style-fusion-neural network.
20. A computer-implemented method for training and applying multi-branch-style-transfer networks with embedded non-photorealistic-rendering ("NPR") generators to generate style-fusion images comprising: performing a step for training a multi-branch-style-transfer network to generate style-fusion-sample images exhibiting an edge-stroke style and a shading-stroke style; and performing a step for applying the multi-branch-style-transfer network to generate a style-fusion image exhibiting the edge-stroke style and the shading-stroke style based on a source image.
</claims>
</document>
