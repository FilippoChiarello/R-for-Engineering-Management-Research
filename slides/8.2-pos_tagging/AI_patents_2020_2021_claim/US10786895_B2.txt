<document>

<filing_date>
2017-12-05
</filing_date>

<publication_date>
2020-09-29
</publication_date>

<priority_date>
2016-12-22
</priority_date>

<ipc_classes>
B25J11/00,B25J9/00,B25J9/16,G06K9/00,G10L15/22,G10L25/63
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
SEOUL NATIONAL UNIVERSITY
</assignee>

<inventors>
KIM, EUN SOL
KIM, JI SEOB
KWAK, DONG HYUN
LEE, SO HEE
ON, KYOUNG WOON
ROH, KYUNG SHIK
YOON, SUK JUNE
ZHANG, BYOUNG TAK
</inventors>

<docdb_family_id>
62625524
</docdb_family_id>

<title>
Operation method for activation of home robot device and home robot device supporting the same
</title>

<abstract>
A home robot device includes a memory, a movement module, and a processor. The processor is configured to execute a motion based on specified motion execution information stored in the memory, obtain feedback information of a user, generate modified motion execution information by modifying at least a portion of the specified motion execution information based on the feedback information of the user, where the modified motion execution information includes a movement value of at least one joint unit of the home robot device or at least one support linked to the at least one joint unit selected from a probability model of the specified motion execution information, and execute a motion of the home robot device based on the modified motion execution information.
</abstract>

<claims>
1. A home robot device comprising: a memory; a movement module; and a processor electrically connected with the memory and the movement module, wherein the processor is configured to: using the movement module, execute a motion based on specified motion execution information stored in the memory; obtain feedback information of a user; generate a reward value from the feedback information, wherein the reward value corresponds to a point in the motion; generate modified motion execution information by modifying at least a portion of the specified motion execution information based on the feedback information of the user, wherein the modified motion execution information includes a movement value of at least one joint unit of the home robot device corresponding to the point in the motion or at least one support linked to the at least one joint unit, wherein the movement value is selected from a probability model of the specified motion execution information, and wherein the movement value is proportional to the reward value; and execute a motion of the home robot device based on the modified motion execution information.
2. The home robot device of claim 1, wherein the processor is further configured to: obtain external motion information from a motion information providing source; extract reference information included in the external motion information, wherein the reference information includes information on a number of at least one joint of the motion information providing source, a position of the at least one joint, a movement direction of the at least one joint, or a movement angle of the at least one joint; and generate the specified motion execution information based on contraction-mapping the extracted reference information to the at least one joint unit of the home robot device.
3. The home robot device of claim 2, wherein the processor is further configured to; classify the at least one joint in the extracted reference information according to one or more parts of the motion information providing source; and map the at least one joint in the extracted reference information to corresponding joint units in the home robot device by corresponding the one or more parts of the motion information providing source to one or more parts of the home robot device associated with the joint units.
4. The home robot device of claim 1, wherein the processor is further configured to: obtain external motion information from a motion information providing source; extract reference information included in the external motion information, wherein the reference information includes information on a number of at least one joint of the motion information providing source, a position of the at least one joint, a movement direction of the at least one joint, or a movement angle of the at least one joint; and identically map the movement direction, the movement angle, or a movement speed of the at least one joint in the reference information to the at least one joint unit of the home robot device.
5. The home robot device of claim 2, further comprising at least one of: a camera configured to obtain an image of the motion information providing source to acquire the external motion information, and a communication circuit configured to receive the external motion information from an external electronic device.
6. The home robot device of claim 1, further comprising: a camera, wherein the processor is further configured to: extract coordinates of a face feature point of the user by analyzing an image obtained using the camera; and determine emotion information by comparing the coordinates of the face feature point with coordinate data in an emotion table previously stored in the memory.
7. The home robot device of claim 6, wherein the processor is further configured to: compare the emotion information of the user with a preference table stored in the memory to determine a user preference score value; or determine the user preference score value based on preference distribution information previously stored in the memory.
8. The home robot device of claim 7, wherein the processor is further configured to modify at least a portion of the specified motion execution information to increase the user preference score value.
9. The home robot device of claim 8, wherein the processor is further configured to: modify the specified motion execution information by a specified number of times or more; modify the specified motion execution information until the user preference score value is a specified value or more; or modify the specified motion execution information until an inflection point, at which the user preference score value changes from positive to negative.
10. The home robot device of claim 1, further comprising: at least one of a camera and a microphone, wherein the processor is further configured to: extract a facial feature vector from an image obtained by the camera or a voice feature vector from a voice signal of the user collected by the microphone; and obtain emotion information of the user by comparing the facial feature vector or the voice feature vector with data in an emotion table previously stored in the memory.
11. An operation method for a home robot device, the operation method comprising: executing a motion of the home robot device based on specified motion execution information stored in a memory; obtaining feedback information of a user; generating a reward value from the feedback information, wherein the reward value corresponds to a point in the motion; generating modified motion execution information by modifying at least a portion of the specified motion execution information based on the feedback information of the user, wherein the modified motion execution information includes a movement value of at least one joint unit of the home robot device corresponding to the point in the motion or at least one support linked to the at least one joint unit, wherein the movement value is selected from a probability model of the specified motion execution information, and wherein the movement value is proportional to the reward value; and executing a motion of the home robot device based on the modified motion execution information.
12. The operation method of claim 11, further comprising: generating the specified motion execution information, wherein the generating of the specified motion execution information includes: obtaining external motion information from a motion information providing source; extracting reference information included in the external motion information, wherein the reference information includes information on a number of at least one joint of the motion information providing source, a position of the at least one joint, a movement direction of the at least one joint, or a movement angle of the at least one joint; and generating the specified motion execution information based on contraction-mapping the extracted reference information to the at least one joint unit of the home robot device.
13. The operation method of claim 12, wherein the obtaining of the external motion information includes at least one of: obtaining an image of the motion information providing source using a camera; and obtaining the external motion information from an external electronic device using a communication channel established between the home robot device and the external electronic device.
14. The operation method of claim 12, wherein the generating of the specified motion execution information further includes: classifying the at least one joint in the extracted reference information according to one or more parts of the motion information providing source; and mapping the at least one joint in the extracted reference information to corresponding joint units in the home robot device by corresponding the one or more parts of the motion information providing source to one or more parts of the home robot device associated with the joint units.
15. The operation method of claim 11, further comprising: generating the specified motion execution information, wherein the generating of the specified motion execution information includes: obtaining external motion information from a motion information providing source; extracting reference information included in the external motion information, wherein the reference information includes information on a number of at least one joint of the motion information providing source, a position of the at least one joint, a movement direction of the at least one joint, or a movement angle of the at least one joint; and identically map the movement direction, the movement angle, or a movement speed of the at least one joint in the reference information to the at least one joint unit of the home robot device.
16. The operation method of claim 11, wherein the obtaining of the feedback information of the user includes: activating a camera; extracting coordinates of a face feature point of the user by analyzing an image obtained using the camera; and determining emotion information of the user by comparing the coordinates of the face feature point with coordinate data in a previously stored emotion table.
17. The operation method of claim 16, wherein the obtaining of the feedback information of the user further includes: comparing the emotion information of the user with a previously stored preference table to determine a user preference score value; or determine the user preference score value based on previously stored preference distribution information.
18. The operation method of claim 17, wherein the generating of the modified motion execution information further includes: modifying at least a portion of the specified motion execution information to increase the user preference score value.
19. The operation method of claim 18, wherein the generating of the modified motion execution information further includes one of: modifying the specified motion execution information by a specified number of times or more; modifying the specified motion execution information until the user preference score value is a specified value or more; or modifying the specified motion execution information until an inflection point, at which the user preference score value changes from positive to negative.
20. The operation method of claim 11, wherein the obtaining of the feedback information of the user includes: activating at least one of a camera and a microphone; extracting a facial feature vector from an image obtained by the camera or a voice feature vector from a voice signal of the user collected by the microphone; and obtaining emotion information of the user by comparing the facial feature vector or the voice feature vector with data in a previously stored emotion table.
</claims>
</document>
