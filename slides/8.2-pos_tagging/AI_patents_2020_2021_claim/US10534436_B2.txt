<document>

<filing_date>
2015-01-30
</filing_date>

<publication_date>
2020-01-14
</publication_date>

<priority_date>
2015-01-30
</priority_date>

<ipc_classes>
G06F3/00,G06F3/01,G06F3/03,G06F3/042,G06F3/0485,G06F3/0488
</ipc_classes>

<assignee>
SOFTKINETIC SOFTWARE
SONY DEPTHSENSING SOLUTIONS
</assignee>

<inventors>
THOLLOT, JULIEN
KAMOVICH, ALIAKSANDR
GUIGUES, LAURENT
</inventors>

<docdb_family_id>
52469001
</docdb_family_id>

<title>
Multi-modal gesture based interactive system and method using one single sensing system
</title>

<abstract>
Described herein is a method and a system for providing efficient and complementary natural multi-modal gesture based interaction with a computerized system which displays visual feedback information on a graphical user interface on an interaction surface. The interaction surface is within the frustum of an imaging device comprising a single sensing system. The system uses the single sensing system for detecting both touch gesture interactions with the interaction surface (120) and three-dimensional touch-less gesture interactions in areas or volumes above the interaction surface performed by hands of a user. Both types of interaction are associated contextually with an interaction command controlling the computerized system when the gesture has been detected. The system comprises preferably a projection system for displaying the graphical user interface and visual feedback on the interaction surface, the projection system being locatable on the same side or on the opposite side of the interaction surface to the sensing system.
</abstract>

<claims>
1. A method for multi-modal touch and touch-less interaction with a computerized system in which said multi-modal touch and touch-less interaction is performed using data information from a single sensing system, the single sensing system being a three-dimensional imaging device including a camera, the method comprising: a) detecting and tracking, using the camera of the three-dimensional imaging device, at least one portion of at least one object within a frustum of the three-dimensional imaging device; b) initiating the interaction by determining, using the camera of the three-dimensional imaging device, if said at least one portion of said at least one object being tracked is performing a first predetermined touch gesture on a first predetermined interactive area on a first portion of an interaction surface and/or a first predetermined touch-less gesture in a first predetermined interactive volume on a vector axis normal to and not including a second predetermined interactive area on a second portion of the interaction surface; and c) interacting with the computerized system by detecting and recognizing, using the camera of the three-dimensional imaging device, one or more gestures performed by said at least one portion of said at least one object within the frustum of the three-dimensional imaging device, the detected and recognized one or more gestures comprising a second predetermined touch gesture on a third predetermined interactive area on a third portion of the interaction surface and/or a second predetermined touch-less gesture in the first predetermined interactive volume on the vector axis normal to the second predetermined interactive area on the second portion of the interaction surface or in a second predetermined interactive volume on a vector axis normal to and not including a fourth predetermined interactive area on a fourth portion of interaction surface.
2. The method according to claim 1, wherein detecting the performance of a touch gesture on the first predetermined interactive area of the interaction surface corresponds to detecting when said at least one portion of said at least one object being tracked is positioned in three-dimensional space at a same location in space as the first predetermined interactive area on the interaction surface.
3. The method according to claim 1, wherein detecting if the predetermined touch gesture has been performed is determined when a distance from the interaction surface of said at least one portion of said at least one object being tracked, in three-dimensional space, is below a threshold value.
4. The method according to claim 1, wherein determining a multi-touch gesture is performed when positions of at least two portions of said at least one object being tracked in space reach at least the first predetermined interactive area belonging to the interaction surface.
5. The method according to claim 1, further comprising determining touch gesture interaction controls as a function of successive positions and duration of a touch gesture and/or a multi-touch gesture.
6. The method according to claim 1, further comprising determining touch-less gesture interaction controls as a function of the touch-less gesture performed by said at least one portion of said at least one object being tracked.
7. The method according to claim 1, wherein (c) further comprises ending the interaction when a predetermined event is triggered, said predetermined event comprising at least one of: the elapsing of a predetermined time period, the recognition of a predetermined touch gesture on the interaction surface, the recognition of a predetermined touch-less gesture in the frustum of the three-dimensional imaging device, and an exit of said at least one object from a predetermined volume in three-dimensional space.
8. The method according to claim 7, wherein (b) comprises using a first detected and tracked portion of said at least one object, and (c) comprises using a second detected and tracked portion of said at least one object.
9. The method according to claim 7, wherein (b) and (c) are sequentially controlled using a single detected and tracked portion of one object.
10. The method according to claim 1, further comprising displaying visual feedback on a graphical user interface on at least the first and/or second portion of the interaction surface, the visual feedback being relative to at least one of: the position of said at least one portion of said at least one object and the recognized gestures of said at least one portion of said at least one object being tracked.
11. The method according to claim 10, wherein interaction with the graphical user interface displayed on the interaction surface further comprises: d) determining a position in space and a topology of the interaction surface using the three-dimensional imaging device; e) determining a set of interactive areas on the interaction surface as a function of the topology and size of the interaction surface; and f) associating with each interactive area in the interaction surface at least one touch gesture interaction control.
12. The method according to claim 11, further comprising: g) associating with the set of interactive areas, at least one interactive volume, each interactive volume being located above said interactive area along a normal vector to that interactive area; and h) associating each interactive volume with a predetermined touch-less gesture interaction control.
13. The method according to claim 11, wherein the interaction surface comprises at least one of: a portion of a body of a user, a desk table, a wall, an infrared translucent surface, and/or an object, the graphical user interface being projected onto the interaction surface.
14. A system for interacting with a graphical user interface, the system comprising: a display system configured to display the graphical user interface onto an interaction surface; a three-dimensional imaging device comprising a camera and configured to track, using the camera, at least one portion of at least one hand of a user within a frustum of the three-dimensional imaging device; and a computer system configured to control the display system and the three-dimensional imaging device and to determine one or more gesture based interaction controls using data output from the three-dimensional imaging device, wherein the interaction surface is within at least a portion of the frustum of the three-dimensional imaging device and is substantially aligned therewith, wherein the computer system is configured to: initiate an interaction by determining, using the camera of the three-dimensional imaging device, if said at least one portion of said at least one hand being tracked is performing: a first predetermined touch gesture on a first predetermined interactive area in a first portion of the interaction surface or a first predetermined touch-less gesture in a first predetermined interactive volume on a vector axis normal to and not including a second predetermined interactive area in a second portion of the interaction surface; and detect and recognize, using the camera of the three-dimensional imaging device, one or more gestures performed by said at least one portion of said at least one hand being tracked subsequent to said initiation of said interaction, wherein detecting and recognizing one or more gestures comprises detecting and recognizing a second predetermined touch gesture on a third predetermined interactive area on a third portion of the interaction surface and/or a second predetermined touch-less gesture in the first predetermined interactive volume or in a second predetermined interactive volume on a vector axis normal to and not including a fourth predetermined interactive area on a fourth portion of interaction surface.
15. The system according to claim 14, wherein the display system comprises a projector located on a same side of the interaction surface onto which the graphical user interface is displayed as the three-dimensional imaging device.
16. The system according to claim 14, wherein the three-dimensional imaging system and a projector are located on opposite sides of the interaction surface onto which the graphical user interface is displayed, the interaction surface being operable for diffusing radiation having a wavelength in a range which substantially corresponds to a visible part of the electromagnetic spectrum and for transmitting radiation having a wavelength in a range which substantially corresponds to an infrared part of the electromagnetic spectrum with limited diffusion, a transmission coefficient of the interaction surface being higher than 50% and a limited diffusion of the interaction surface being lower than 20 degrees.
17. A system comprising a three-dimensional imaging device including a camera and a non-transitory computer medium configured for storing executable instructions, which when executed on a computerized system, cause the computerized system to perform a method comprising: a) detecting and tracking, using the camera of the three-dimensional imaging device, at least one portion of at least one object within a frustum of the three-dimensional imaging device; b) initiating the interaction by determining, using the camera of the three-dimensional imaging device, if said at least one portion of said at least one object being tracked is performing at least one of: a first predetermined touch gesture on a first predetermined interactive area on a first portion of an interaction surface and a first predetermined touch-less gesture in a first predetermined interactive volume on a vector axis normal to and not including a second predetermined interactive area on a second portion of the interaction surface; and c) interacting with the computerized system by detecting and recognizing, using the camera of the three-dimensional imaging device, one or more gestures performed by said at least one portion of said at least one object within the frustum of the three-dimensional imaging device, the detected and recognized one or more gestures comprising a second predetermined touch gesture on a third predetermined interactive area on a third portion of the interaction surface and/or a second predetermined touch-less gesture in the first predetermined interactive volume on the vector axis normal to the second predetermined interactive area on the second portion of the interaction surface or in a second predetermined interactive volume on a vector axis normal to and not including a fourth predetermined interactive area on a fourth portion of interaction surface.
18. The method according to claim 1, wherein the first portion of the interaction surface and the second portion of the interaction surface are different portions of the interaction surface.
19. The system according to claim 14, wherein the first portion of the interaction surface and the second portion of the interaction surface are different portions of the interaction surface.
20. The system according to claim 17, wherein the first portion of the interaction surface and the second portion of the interaction surface are different portions of the interaction surface.
</claims>
</document>
