<document>

<filing_date>
2019-08-08
</filing_date>

<publication_date>
2020-03-12
</publication_date>

<priority_date>
2018-09-04
</priority_date>

<ipc_classes>
G06F16/34
</ipc_classes>

<assignee>
SIEMENS
</assignee>

<inventors>
KROMPASS, DENIS
KUMAR KARN, SANJEEV
WALTINGER, ULLI
</inventors>

<docdb_family_id>
63491494
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR NATURAL LANGUAGE PROCESSING
</title>

<abstract>
A natural language processing system configured for receiving an input sequence of input tokens representing a first sequence of words in a natural language of a first text and generating an output sequence of output tokens representing a second sequence of words in a natural language of a second text. The natural language processing system has at least one sequence-to-sequence (seq2seq) model and a policy gradient generator. The seq2seq model comprises an encoder, an attention module and a decoder. The encoder and the decoder each comprise a forward recurrent neural network RNN and a backward RNN, and the attention module is configured for applying weights to the encoded representations.
</abstract>

<claims>
Patent Claims
1. A natural language processing system (1) configured for receiving an input sequence of input tokens (xl, x2, ... xN) representing a first sequence of words in a natural language of a first text and generating an output sequence of output tokens (yl, y2, ..., yN) representing a second sequence of words in a natural language of a second text and having at least one sequence-to-sequence, seq2seq, model (10) comprising :
an encoder (30) comprising a forward recurrent neural network RNN and a backward RNN configured for encoding each input token (xl, x2, ... xN) in the input sequence into a respective encoded representation (cl, c2, ... cN) of each input token
(xl, x2, ... xN) ;
an attention module (20) configured for applying weights (wl, w2, ... wN) to the encoded representations (cl, c2, ... cN) ; and a decoder (40) comprising a forward RNN and a backward RNN configured for decoding the weighted encoded representations (cl, c2, ... cN) into the output tokens (yl, y2, ..., yN) of the output sequence;
the natural language processing system (1) further having a policy gradient generator (320) configured for training the weights (wl, w2, ... wN) for adaption to one corpus for each seq2seq model (10) .
2. The natural language processing system (1) of claim 1, wherein the policy gradient generator (320) is a selfcritical policy generator.
3. The natural language processing system (1) of claim 1 or
2, wherein the policy gradient generator (320) is configured for training the weights (wl, w2, ... wN) by means of a
REINFORCE algorithm.
4. The natural language processing system (1) of any one of claims 1 to 3, wherein the policy gradient generator (320) is given by the equation: log( Ge(xt))
where x represents the output token (yl, y2, ...yN) , R repre sents a cumulative reward (340) and b represents a baseline, respectively for a state S of a sequence of length T.
5. The natural language processing system (1) of any one of claims 1 to 4, wherein a recall-oriented-understudy-forgisting-evaluation-, ROUGE-, recall is used as a reward (340) in training the weights (wl, w2, ... wN) by the policy gradient generator (320) .
6. The natural language processing system (300) of any one of claims 1 to 5, wherein the second text is a ShortText comprising one of several linguistic styles and the policy gradient generator (320) is configured for adaption to the one of several linguistic styles based on the corpus for the respective seq2seq model (10) .
7. The natural language processing system (1) of any one of claims 1 to 5, wherein the second text is a teaser comprising one of several linguistic styles and the policy gradient generator (320) is configured for adaption to the one of the several linguistic styles based on the corpus for the respective seq2seq model (10) .
8. The natural language processing system (1) of claim 7, wherein the teaser is a fragment-based teaser or a quotebased teaser.
9. The natural language processing system (1) of any of claims 1 to 8, wherein the policy gradient generator (320) is configured for utilising an algorithm for generating the corpus of a dataset comprising examples of a linguistic style of the second text and using the respective corpus having the recognised linguistic style for training the weights (wl, w2,
... wN) .
10. A computer-implemented method for processing natural language, by receiving an input sequence of input tokens (xl, x2, ... xN) representing a first sequence of words in a natural language of a first text and generating an output sequence of output tokens (yl, y2, ... yN) representing a second sequence of words of a second text, comprising the steps:
encoding (SI) each input token (xl, x2, ... xN) in the input sequence into a respective encoded representation (cl, c2, ... cN) of each input token (xl, x2, ... xN) by an encoder (30) of at least one sequence-to-sequence, seq2seq, model (10) comprising a forward recurrent neural network, RNN, and a backward RNN;
applying (S2 ) weights (wl, w2, ... wN) to the encoded representations (cl, c2, ... cN) by an attention module (20) of the at least one seq2seq model (10);
decoding (S3) the weighted encoded representations (cl, c2, ... cN) into the output tokens (yl, y2, ..., yN) of the output sequence by a decoder (40) of the at least one seq2seq model (10) comprising a forward RNN and a backward RNN; and
training (S4 ) the weights (wl, w2, ... wN) for adaption to one corpus for each seq2seq model (10) by a policy gradient generator (320) .
11. The method of claim 10, wherein training (S4 ) the weights (wl, w2, ... wN) is implemented by means of a REINFORCE algorithm .
12. The method of claim 10 or 11, further comprising using a recall-oriented understudy-for-gisting-evaluation-, ROUGE-, recall as a reward (340) in training (S4) the weights (wl, w2, ... wN) .
13. The method of any one of claims 10 to 12, further comprising utilising an algorithm for generating the corpus of a dataset comprising examples of a linguistic style of the second text and using the respective corpus having the recognised linguistic style for training the weights (wl, w2, ... wN) by the policy gradient generator (320) .
14. A non-transitory computer-readable data storage medium (500) comprising executable program code (550) configured to, when executed, perform the method according to any one of claims 1 to 13.
</claims>
</document>
