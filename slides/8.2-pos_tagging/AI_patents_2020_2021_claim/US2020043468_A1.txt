<document>

<filing_date>
2018-07-31
</filing_date>

<publication_date>
2020-02-06
</publication_date>

<priority_date>
2018-07-31
</priority_date>

<ipc_classes>
G06F16/68,G06N20/00,G10L15/00,G10L15/06,G10L15/16,G10L15/26,G10L25/03
</ipc_classes>

<assignee>
NUANCE COMMUNICATIONS
</assignee>

<inventors>
SUN, YANG
VOZILA, PAUL JOSEPH
WILLETT, DANIEL
ZHAN, PUMING
</inventors>

<docdb_family_id>
69228987
</docdb_family_id>

<title>
System and method for performing automatic speech recognition system parameter adjustment via machine learning
</title>

<abstract>
A system, method and computer-readable storage device provides an improved speech processing approach in which hyper parameters used for speech recognition are modified dynamically or in batch mode rather than fixed statically. The method includes estimating, via a model trained on audio data and/or metadata, a set of parameters useful for performing automatic speech recognition, receiving speech at an automatic speech recognition system, applying, by the automatic speech recognition system, the set of parameters to processing the speech to yield text and outputting the text from the automatic speech recognition system.
</abstract>

<claims>
1. A method comprising: applying a first set of parameters for recognizing first speech received at an automatic speech recognition system to yield a first text, wherein the first set of parameters comprises one or more of a word insertion penalty, a silence prior, a word penalty and a beam pruning width; estimating, via a model trained on at least metadata to prepare the model to output parameters associated with different acoustic environments, a second set of parameters useful for performing automatic speech recognition, wherein the estimating is performed dynamically during speech recognition of the first speech by the automatic speech recognition system; receiving second speech at the automatic speech recognition system; applying, by the automatic speech recognition system, the second set of parameters to processing the second speech to yield second text; and outputting the second text from the automatic speech recognition system.
2. The method of claim 1, wherein the parameters in the second set of parameters comprise hyper parameters.
3. The method of claim 1, wherein the second set of parameters comprises one or more of a language model scale, an acoustic model scale, a duration model scale, other search pruning control parameters, and a language model interpolation vector.
4. The method of claim 1, wherein the model utilizes one or more of a signal-to-noise ratio estimate, reverberation time estimate, a short-term window frequency analysis, a mel-scale frequency cepstral analysis, time-domain signal audio signal directly, and the metadata to estimate the second set of parameters.
5. The method of claim 4, wherein the metadata comprises one or more of an applicationId, a speakerId, a deviceId, a channelId, a date/time, a geographic location, an application context, or a dialogue state.
6. The method of claim 1, wherein the model comprises one of a feedforward neural network, unidirectional or bidirectional recurrent neural network, a convolutional neural network or a support vector machine model.
7. The method of claim 1, wherein the metadata is incorporated into the model via one-hot encoding or embedding.
8. (canceled)
9. The method of claim 1, wherein estimating and applying the second set of parameters to processing the second speech to yield the second text is performed in a batch mode with parameters estimated based on full utterances.
10. The method of claim 9, wherein the applying of the second set of parameters to processing the second speech to yield the second text is performed in either a delayed decoding pass by the automatic speech recognition system, or in a rescoring that rescores result options from a first speech recognition pass given estimated hyper parameters.
11. The method of claim 9, wherein estimated hyper parameters estimated on one utterance are only applied in decoding of a respectively next utterance.
12. The method of claim 1, wherein estimating the second set of parameters useful for performing automatic speech recognition yields (1) the second set of parameters directly as target layer outputs or (2) the second set of parameters as a predefined parameter configuration chosen from a group of predefined parameter configurations.
13. An automatic speech recognition system comprising: a processor; and a non-transitory computer-readable storage medium having instructions stored which, when executed by the processor, cause the processor to perform operations comprising: applying a first set of parameters for recognizing first speech to yield a first text, wherein the first set of parameters comprises one or more of a word insertion penalty, a silence prior, a word penalty and a beam pruning width; estimating, via a model trained on at least metadata to prepare the model to output parameters associated with different acoustic environments, a second set of parameters useful for performing automatic speech recognition, wherein the estimating is performed dynamically during speech recognition of the first speech; receiving second speech at the automatic speech recognition system; applying the second set of parameters to processing the second speech to yield second text; and outputting the second text from the automatic speech recognition system.
14. The automatic speech recognition system of claim 13, wherein the parameters in the second set of parameters comprise hyper parameters.
15. The automatic speech recognition system of claim 13, the second set of parameters comprises one or more of a language model scale, an acoustic model scale, a duration model scale, other search pruning control parameters, and a language model interpolation vector.
16. The automatic speech recognition system of claim 13, wherein the model comprises one of a feedforward neural network, unidirectional or bidirectional recurrent neural network, a convolutional neural network, sequence model, transformer model, or a support vector machine model.
17. The automatic speech recognition system of claim 13, wherein the automatic speech recognition system utilizes one or more of audio-signal derived features and metadata features to estimate the second set of parameters, wherein the audio-signal derived features comprise one or more of a signal-to-noise ratio estimate, a reverberation time estimate, a short-term window frequency analysis, a mel-scale frequency cepstral analysis, a time-domain audio signal directly and the metadata features.
18. The automatic speech recognition system of claim 17, wherein the metadata features comprise one or more of an applicationId, a speakerId, a deviceId, a channelId, a date/time, a geographic location, an application context and a dialogue state.
19. The automatic speech recognition system of claim 13, wherein applying the second set of parameters to processing the speech to yield the second text is performed dynamically during automatic speech recognition.
20. A computer-readable storage device having instructions stored which, when executed by an automatic speech recognition system, cause the automatic speech recognition system to perform operations comprising: applying a first set of parameters for recognizing first speech received to yield a first text, wherein the first set of parameters comprises one or more of a word insertion penalty, a silence prior, a word penalty and a beam pruning width; estimating, via a model trained on at least metadata to prepare the model to output hyper parameters associated with different acoustic environments, a second set of parameters useful for performing automatic speech recognition, wherein the estimating is performed dynamically during speech recognition of the first speech; receiving second speech at the automatic speech recognition system; applying the second set of parameters to processing the second speech to yield second text; and outputting the second text from the automatic speech recognition system.
</claims>
</document>
