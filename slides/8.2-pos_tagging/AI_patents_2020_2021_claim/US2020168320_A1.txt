<document>

<filing_date>
2020-01-09
</filing_date>

<publication_date>
2020-05-28
</publication_date>

<priority_date>
2018-11-25
</priority_date>

<ipc_classes>
G06K9/00,G06N3/08,G16H30/20
</ipc_classes>

<assignee>
AIVITAE
</assignee>

<inventors>
HU, BOB SUEH-CHIEN
</inventors>

<docdb_family_id>
70770916
</docdb_family_id>

<title>
Methods and systems for autonomous control of imaging devices
</title>

<abstract>
Methods and systems are described for autonomous control of imaging devices. In particular, the methods and system described herein may account for the differences in normalization of training data and/or test data. The methods and systems may process images through an additional customization layer, which itself may comprise an artificial neural network. The additional customization layer is trained to normalize data for specific applications and/or differences between subsets of data.
</abstract>

<claims>
1. A system for autonomous control of magnetic resonance imaging devices, the system comprising: memory configured to store a plurality of data subsets; and control circuitry configured to: receive a first image from a first data subset of a plurality of data subsets; generate a first pixel array based on the first image; label the first pixel array with a known object; train an artificial neural network to detect the known object based on the labeled first pixel array; receive a second image corresponding to a second data subset of the plurality of data subsets; generate a second pixel array based on the second image; determine a first customization layer for the trained artificial neural network based on a comparison of the first data subset and the second data subset; process the second pixel array through the trained artificial neural network and the first customization layer to identify the second pixel array as corresponding to the known object; receive an output from the trained artificial neural network and the first customization layer; and automatically adjust an imaging device based on the output.
2. A method for autonomous control of imaging devices, the method comprising: receiving, using control circuitry, a first image from a first data subset of a plurality of data subsets; generating, using the control circuitry, a first pixel array based on the first image; labeling, using the control circuitry, the first pixel array with a known object; training, using the control circuitry, an artificial neural network to detect the known object based on the labeled first pixel array; receiving, using the control circuitry, a second image corresponding to a second data subset of the plurality of data subsets; generating, using the control circuitry, a second pixel array based on the second image; determining, using the control circuitry, a first customization layer for the trained artificial neural network based on a comparison of the first data subset and the second data subset; processing, using the control circuitry, the second pixel array through the trained artificial neural network and the first customization layer to identify the second pixel array as corresponding to the known object; and receiving, using the control circuitry, an output from the trained artificial neural network and the first customization layer.
3. The method of claim 2, further comprising: receiving a third image corresponding to a third data subset of the plurality of data subsets; generating a third pixel array based on the third image; determining a second customization layer for the trained artificial neural network based on a comparison of the first data subset and the third data subset; and processing the third pixel array through the trained artificial neural network and the second customization layer.
4. The method of claim 2, wherein processing, using the control circuitry, the second pixel array through the trained artificial neural network and the first customization layer, further comprises: inputting the second pixel array into the trained artificial neural network; receiving a preliminary output from the trained artificial neural network; and inputting the preliminary output from the trained artificial neural network into the first customization layer.
5. The method of claim 2, wherein the comparison of the first data subset and the second data subset indicates a difference in tilt of images from the first data subset and the second data subset, and wherein the output adjusts an imaging device based on the difference.
6. The method of claim 2, wherein the comparison of the first data subset and the second data subset indicates non-linear adjustments in visual characteristics of images from the first data subset and the second data subset, and wherein the output comprises a non-linear adjustment to a visual characteristic.
7. The method of claim 2, wherein processing, using the control circuitry, the second pixel array through the trained artificial neural network and the first customization layer, further comprises: inputting the second pixel array into the first customization layer; receiving a preliminary output from the first customization layer; and inputting the preliminary output from the first customization layer into the trained artificial neural network.
8. The method of claim 7, wherein the first customization layer comprises a generative neural network and the trained artificial neural network is a discriminative neural network.
9. The method of claim 7, further comprising: determining a portion of the known object that is obscured in the second image; and generating a version of the second image where the portion is not obscured.
10. The method of claim 7, wherein the first customization layer comprises a geometric neural network and the trained artificial neural network is a convolutional neural network.
11. The method of claim 7, further comprising: determining a three-dimensional model of the known object; and labeling a feature of the known object in the second image based on the three-dimensional model.
12. A non-transitory, machine-readable medium storing instructions for autonomous control of imaging devices that, when executed by a data processing apparatus, cause the data processing apparatus to perform operations comprising: receiving a first image from a first data subset of a plurality of data subsets; generating a first pixel array based on the first image; labeling the first pixel array with a known object; training an artificial neural network to detect the known object based on the labeled first pixel array; receiving a second image corresponding to a second data subset of the plurality of data subsets; generating a second pixel array based on the second image; determining a first customization layer for the trained artificial neural network based on a comparison of the first data subset and the second data subset; processing the second pixel array through the trained artificial neural network and the first customization layer to identify the second pixel array as corresponding to the known object; and receiving an output from the trained artificial neural network and the first customization layer.
13. The non-transitory, machine-readable medium of claim 12, further comprising instructions that cause the data processing apparatus to perform operations comprising: receiving a third image corresponding to a third data subset of the plurality of data subsets; generating a third pixel array based on the third image; determining a second customization layer for the trained artificial neural network based on a comparison of the first data subset and the third data subset; and processing the third pixel array through the trained artificial neural network and the second customization layer.
14. The non-transitory, machine-readable medium of claim 12, further comprising instructions that cause the data processing apparatus to perform operations comprising: inputting the second pixel array into the trained artificial neural network; receiving a preliminary output from the trained artificial neural network; and inputting the preliminary output from the trained artificial neural network into the first customization layer.
15. The non-transitory, machine-readable medium of claim 12, wherein the comparison of the first data subset and the second data subset indicates a difference in tilt of images from the first data subset and the second data subset, and wherein the output adjusts an imaging device based on the difference.
16. The non-transitory, machine-readable medium of claim 12, wherein the comparison of the first data subset and the second data subset indicates non-linear adjustments in visual characteristics of images from the first data subset and the second data subset, and wherein the output comprises a non-linear adjustment to a visual characteristic.
17. The non-transitory, machine-readable medium of claim 12, wherein processing the second pixel array through the trained artificial neural network and the customization layer, further comprises: inputting the second pixel array into the first customization layer; receiving a preliminary output from the first customization layer; and inputting the preliminary output from the first customization layer into the trained artificial neural network.
18. The non-transitory, machine-readable medium of claim 17, wherein the first customization layer comprises a generative neural network and the trained artificial neural network is a discriminative neural network.
19. The non-transitory, machine-readable medium of claim 17, further comprising instructions that cause the data processing apparatus to perform operations comprising: determining a portion of the known object that is obscured in the second image; and generating a version of the second image where the portion is not obscured.
20. The non-transitory, machine-readable medium of claim 17, wherein the first customization layer comprises a graph neural network and the trained artificial neural network is a convolutional neural network.
21. The non-transitory, machine-readable medium of claim 17, further comprising instructions that cause the data processing apparatus to perform operations comprising: determining a three-dimensional model of the known object; and labeling a feature of the known object in the second image based on the three-dimensional model.
</claims>
</document>
