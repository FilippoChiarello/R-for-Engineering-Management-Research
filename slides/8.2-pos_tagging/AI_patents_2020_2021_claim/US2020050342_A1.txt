<document>

<filing_date>
2018-08-07
</filing_date>

<publication_date>
2020-02-13
</publication_date>

<priority_date>
2018-08-07
</priority_date>

<ipc_classes>
G06F17/16,G06F3/0481,G06F3/0484,G06T19/00
</ipc_classes>

<assignee>
LEE, WEN-CHIEH GEOFFREY
</assignee>

<inventors>
LEE, WEN-CHIEH GEOFFREY
</inventors>

<docdb_family_id>
69407187
</docdb_family_id>

<title>
Pervasive 3D Graphical User Interface
</title>

<abstract>
A three-dimensional graphical user interface (3D GUI) configured to be used by a computer, a display system, an electronic system, or an electro-mechanical system. The 3D GUI provides an enhanced user-engaging experience while enabling a user to manipulate the motion of an object of arbitrary size and a multiplicity of independent degrees of freedom, using sufficient degrees of freedom to represent the motion. During operation, the 3D GUI fetches positional data in absolute address mode and translational and rotational motion vector data from a unique 3D navigation device that uses color as a means of motion detection. The 3D navigation device transforms color index data to the 3D GUI through a matrix representation of the six degrees of freedom (DOF) of motion of an object.
</abstract>

<claims>
1. A system comprising: a main memory and at least one processor coupled to the main memory in a computer, a display system, an electronic system, or an electro-mechanical system, configured to present on a display device a three-dimensional graphical user interface (3D GUI); wherein said 3D GUI is configured to allow maneuvering an object in a 3D space represented by said 3D GUI, by a motion of at least three independent degrees of freedom, said motion being characterized by either linear or non-linear motion vectors, or both; and wherein said linear and non-linear motion vectors represent translational and rotational motion respectively and are generated by a single gestural motion of a navigational device on a reference surface without applying the input of other motion detection devices.
2. The system of claim 1, wherein the accuracy of a position of said object remains constant during said maneuvering, said accuracy not being affected by accumulated errors of positional accuracy after said object has been maneuvered by a navigational device that measures the absolute position from a reference surface.
3. The system of claim 1, wherein a 3D space is represented by said GUI, wherein said 3D space contains a predetermined vector field that is configured to combine with the motion vector provided by said navigational device to constitute a new motion vector for said object, wherein the magnitude and directionality of said new motion vector are different from those of ones not generated by the action of said navigational device.
4. The system of claim 1, wherein the 3D space represented by said GUI contains a plurality of two dimensional planes, each of which denotes a unique application program or feature.
5. The system of claim 4 wherein activating or de-activating said two dimensional planes requires the provision of a non-linear motion vector for an object such as a cursor.
6. The system of claim 1, wherein the essential property of an object, such as a cursor or a physical object, is denoted by a matrix of data instead of a single value defining a mathematical point.
7. The system of claim 1 further comprising a layered configuration of separate software modules that are loaded, partially or in whole, into said main memory and are separately dedicated to providing perspectives and robotic kinematics, wherein said software modules can act separately or in a collaborative manner.
8. The system of claim 1, wherein said 3D GUI is configured to allow a user to interact with an object displayed therein or with a physical object controlled by said computer, electronic system, or electro-mechanical system using an operating system loaded into said memory, by means of linear and/or nonlinear motion vectors, wherein said linear and/or nonlinear motion vector can be imparted to produce the motion of a graphical entity that is as small as a pixel or a minimal graphical unit of said GUI, a voxel.
9. A system comprising: a memory and at least one processor coupled to the memory in a computer, electronic system, or electro-mechanical system that is configured to enhance a user's experience of engaging with a 3D scenery presented on a 3D graphical rendering device and comprising a plurality of 3D vector graphics; wherein said system is configured to adjust a perspective angle of a 2.5D coordinate system embedded in said 3D graphical rendering device, wherein said perspective angle is an angle of intersection between X and Z axes in said 2.5D coordinate system, or an angle of intersection between X and Y axes in said 2.5D coordinate system; said system is configured to enable an operator to visually classify a plurality of graphical vectors, wherein said graphical vectors are entities to construct said 3D vector graphics, and/or motion vectors into different classes, wherein each said class denotes a unique set of features associated with said vector graphics, whose characteristics are different from those of the others.
10. The system of claim 9, configured so that said visually classifying reaches an optimal condition at a specific perspective angle which is reached by an initial 3D position, followed by series of translational motion and rotational motion vectors of a world space camera.
11. The system of claim 1, wherein said system is configured to engage interactively with the user through manipulation of 3D graphical scenery, wherein graphical objects presented in said 3D graphical scenery are denoted by a plurality of nodes, wherein said nodes include vertexes, characterized by a 3D address having X, Y and Z components and 3D motion vectors in six degrees of freedom having ΔX, ΔY, and ΔZ, and α, β, and γ components, wherein said plurality of nodes are classified into multiple classes based on interactions between a cursor and said plurality of nodes.
12. A computer-implemented method of engaging a user with a plurality of objects represented in a three dimensional graphical user interface (3D GUI) shown on a display, comprising the steps of: providing a a 3D navigational device communicating with said 3D GUI which is moved along a reference surface, or by similar use of a navigational device of equivalent functionality communicating with said 3D GUI, along a two dimensional (2D) vector that lies on a surface tinted by tri-coloration by a chemical, physical, mechanical, or electromagnetic process; transforming said 2D vector into a three dimensional vector, or one that has more than two dimensional degrees of freedom, said transformation being based on the hue value of an area of said tinted surface whose image is captured by a color-sensitive image sensor in said 3D navigational device or said navigational device.
13. A system comprising: a main memory and at least one processor coupled to the main memory in a computer, a display system, an electronic system, or an electro-mechanical system, configured to present on a display device a three-dimensional graphical user interface (3D GUI); wherein said system is configured to control a robot to which it is attached; a navigational device, configured to control the position or motion of the robot in real time, wherein said position and motion of said robot has more than three degrees of freedom, all controllable, in-situ, by the navigational device, and wherein said more than three degrees of freedom are configured to be monitored and their motion to be modified by said three-dimensional graphical user interface (3D GUI).
14. The system of claim 13, wherein said robot is a structure of arbitrary formation or an articulated structure.
15. The system of claim 14, wherein said structure of arbitrary formation is a cartoon character or other graphical structure.
16. The system of claim 13 wherein said robot is a movable device external to said 3D GUI.
17. The system of claim 13, wherein said system is configured so that if said robot is invisible it is controllable by a user, functioning as a perspective camera looking into a scene rendered by said 3D GUI from a specific position or perspective angle designated by said robot.
18. The system of claim 13, wherein said system is configured so that if said robot is invisible its motion is controllable by a user, whereby said robot functions as a pen that performs a signature signing process, a utility that is used in a graphical sketching process, a cartoon in a motion picture, an action of a role model in a video game, an information extracting process from a 3D image diagnosing process, which can be deemed as the representative of an operator's biological finger, hand, wrist, elbow, arm, legs, head, or torso.
19. A computer system comprising: a display; a memory; a system of one or more processors, coupled to said memory, including one or more display servers; a three-dimensional graphical user interface (3D GUI) projected on said display comprising a layered configuration of software modules stored in said memory and configured to interact with said processors and to be executed by said processors, whereupon said 3D GUI is configured to establish regions, denoted windows, on said display wherein graphical objects can be displayed in terms of a three-dimensional coordinate system, said graphical objects being represented by absolute positions in said three-dimensional coordinates and said graphical objects being maneuverable while observing rules of translation and rotation of three-dimensional kinematics for objects having at least three independent degrees of freedom and without loss of positional accuracy resulting from said maneuvering; and a 3D navigational device configured to operationally interact with said 3D GUI through an application interface (API) and to provide said absolute three-dimensional positions for said graphical objects to be represented in said 3D GUI and, by causing said software modules stored in said memory to be executed, to cause said graphical objects to be moved on said display between said absolute positions and with at least three independent degrees of freedom while obeying the rules of translation and rotation of three-dimensional kinematics.
20. The computer system of claim 19 wherein said software modules include processor-readable, executable instructions to generate robotic actions, to create graphical actions obeying laws of three-dimensional kinematics and perspective.
21. The computer system of claim 19 wherein said software modules are further operationally managed by a display server whereby functions generated by said modules are properly displayed in said windows.
22. The computer system of claim 19 wherein said software modules include processing modules that separately control cursor maneuver, perspective manipulation, robotics, and machine learning processes.
23. A computer-implemented method for providing simultaneous engagement of a three-dimensional graphical user interface (3D GUI) with a plurality of 3D objects, said method initiated by a predefined movement of a 3D navigational device for activating a status of a feature used for selecting/deselecting an event, wherein said predefined movement comprises the following steps: causing a 3D cursor in said 3D GUI to move in a two-dimensional plane in a corresponding manner by placing a finger on the top surface of said 3D navigational device, while a user is moving said 3D navigational device over a two-dimensional reference surface, whose tilting condition is controlled by the position of said finger with respect to the surface of said 3D navigational device; then using a straight line linking first and second terminals as a diagonal line, while a user depresses a button or said feature at a beginning of a movement, at the first terminal, of said 3D navigational device over said two dimensional reference surface, and holding said button/feature until said cursor has moved along said two dimensional plane over a distance and reached the second terminal, wherein said 3D GUI constructs a volume in a 3D space, allowing for a user to select/deselect a plurality of 3D objects enclosed by said volume.
24. A computer-implemented method for controlling/adjusting the kinematics of a robot in an in-situ manner, comprising the steps of: multiplying a plurality of matrices, each of which has a dimension equal to or higher than three; using a maneuvering device that is configured to provide translational motion vectors and rotational vectors simultaneously, both in 3D format, to change a position/motion of a selected joint or movable part of said robot while other non-selected joints or parts undergo predefined actions with no variations.
25. A computer-implemented method for creating a digital impressionistic graphical rendering effect in a computer, electronic control system, or electro-mechanical system, comprising the steps of: creating, in a three-dimensional graphical user interface (3D GUI), a three-dimensional graphical object having a volume as one of its fundamental properties, wherein said graphical object is constructed by a plurality of connected vertexes, wherein said plurality of vertexes defines an external faceted shape of said graphical object; creating a feature vector space in a memory of said computer, electronic control system, or electro-mechanical system as a representation of said three-dimensional graphical object wherein each vertex is associated with a multidimensional feature vector and wherein each vertex has a vector normal to said external facet it defines; and when, in the memory of said computer, electronic control system, or electro-mechanical system, said 3D GUI separates a plurality of said multidimensional feature vectors by the distinct margins among one another (i.e., classifying), said normal vectors of said facets are aligned to several directions, causing said vertexes and said facets defined by said vertexes to be classified in a way that is perceivable to and recognizable by the viewer.
26. The computer-implemented method for creating a graphical rendering effect of a computer, electronic control system, or electro-mechanical system of claim 25, wherein said method uses a 3D navigational device that is controllably moving along a tinted 2D reference surface, accessing a chosen vertex and, by touching a surface element of said 3D navigational device, altering intensities of a system of illumination within said 3D navigational device thereby changing a direction of said normal vector of said accessed vertex and altering a visual characteristic of said graphical object thereby.
27. A computer-implemented method for three dimensional (3D) graphical rendering, comprising the steps of: rendering a plurality of three dimensional graphical vectors referenced to at least one vanishing point(s); wherein a position of said vanishing point(s) can be manipulated by an artificial intelligence technique; dividing into one or more classes said plurality of three dimensional graphical vectors tracked by said method, each of which forms a margin with one another that is configured to be recognized by a user or said method; wherein when said margin reaches different values, said method generates graphical rendering effects, or supports levels of interaction between a user and said method.
28. The computer-implemented method of claim 27 wherein said artificial intelligence technique comprises a support vector machine (SVM) or a neural network.
29. The computer-implemented method of claim 27 wherein said artificial intelligence technique comprises a manual method controllable by a user.
30. A non-transitory computer readable storage medium having instructions that when executed by a processor causes said processor to present on a display device a three-dimensional graphical user interface (3D GUI), and to perform operations comprising; allow a maneuvering of an object in a 3D space to be represented by said 3D GUI, by a motion of at least three independent degrees of freedom, said motion being characterized by either linear or non-linear motion vectors, or both; and wherein said linear and non-linear motion vectors represent translational and rotational motion respectively and are generated by a single gestural motion of a navigational device on a reference surface without applying the input of other motion detection devices.
</claims>
</document>
