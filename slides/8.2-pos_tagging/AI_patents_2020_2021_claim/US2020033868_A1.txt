<document>

<filing_date>
2018-07-27
</filing_date>

<publication_date>
2020-01-30
</publication_date>

<priority_date>
2018-07-27
</priority_date>

<ipc_classes>
B60W50/00,B60W50/08,B60W50/10,B60W60/00,G05D1/00,G05D1/02,G06N3/08
</ipc_classes>

<assignee>
GM GLOBAL TECHNOLOGY OPERATIONS
</assignee>

<inventors>
MUDALIGE, UPALI P.
PALANISAMY, PRAVEEN
</inventors>

<docdb_family_id>
69149015
</docdb_family_id>

<title>
Systems, methods and controllers for an autonomous vehicle that implement autonomous driver agents and driving policy learners for generating and improving policies based on collective driving experiences of the autonomous driver agents
</title>

<abstract>
Systems and methods are provided autonomous driving policy generation. The system can include a set of autonomous driver agents, and a driving policy generation module that includes a set of driving policy learner modules for generating and improving policies based on the collective experiences collected by the driver agents. The driver agents can collect driving experiences to create a knowledge base. The driving policy learner modules can process the collective driving experiences to extract driving policies. The driver agents can be trained via the driving policy learner modules in a parallel and distributed manner to find novel and efficient driving policies and behaviors faster and more efficiently. Parallel and distributed learning can enable accelerated training of multiple autonomous intelligent driver agents.
</abstract>

<claims>
1. A method, comprising: capturing, via one or more driver agents and one or more corresponding one or more driving environment processors, driving experiences during different driving scenarios in different driving environments, wherein each driving experience comprises data that represents a particular driving environment at a particular time; retrieving, via one or more driving policy learner modules of a driving policy generation module, at least some of the driving experiences; processing, at one or more driving policy learner modules, at least some of the driving experiences to learn and generate parameters that describe one or more policies, wherein each policy prescribes a distribution over a space of actions for any given state, and wherein each policy comprises a set of parameters that describe the policy and are processible by at least one of the driver agents to generate an action for controlling a vehicle; processing, at the one or more driver agents, received parameters for at least one candidate policy, and executing the at least one candidate policy to generate one or more actions that control the vehicle in a specific driving environment as observed by a corresponding driving environment processor; and processing, at a low-level controller, each action to generate control signals for controlling the vehicle when operating in that specific driving environment.
2. The method according to claim 1, wherein the data for each driving experience that represents a particular driving environment at a particular time, comprises: a state of the particular driving environment observed by a corresponding driving environment processor; an observation made using at least part of an observable state; an action generated by the driver agent; a reward comprising: a signal that signifies how desirable an action performed by the driver agent is at a given time under particular environment conditions, wherein the reward is automatically computed based on road rules and driving principles extracted from human driving data or defined using other appropriate methods based on traffic and the road rules; a goal to be achieved by the driver agent; instance information comprising: information that indicates impact or priority of the driving experience as determined by that driver agent at the time that particular driving experience was acquired; and other meta information about that particular driving experience; and a next state of the particular driving environment that results after the driver agent performs the action in the driving environment; and a next observation made using at least part of a next observable state.
3. The method according to claim 1, wherein processing, at the one or more driver agents, received parameters for at least one candidate policy, and executing the at least one candidate policy to generate one or more actions that control the vehicle in a specific driving environment as observed by a corresponding driving environment processor, comprises: processing, at each of the driving environment processors, sensor information from on-board sensors that describes a specific driving environment to generate a state of the specific driving environment; processing the state, at each of the one or more driver agents in accordance with a policy, to generate a corresponding action; and wherein processing, at a low-level controller, each action to generate control signals for controlling the vehicle to control the vehicle when operating in that specific driving environment, comprises: translating, at the low-level controller, each action to generate the control signals for controlling the vehicle to autonomously control the vehicle when operating in that state in that specific driving environment.
4. The method according to claim 1, wherein each of the driving policy learner modules comprises a Deep Reinforcement Learning (DRL) algorithm, and wherein processing, at one or more driving policy learner modules, at least some of the driving experiences, comprises: processing input information from at least some of the driving experiences, at each DRL algorithm, to learn and generate an output comprising: a set of parameters representing a policy that are developed through DRL, and wherein each policy is processible by at least one of the driver agents to generate an action for controlling the vehicle.
5. The method according to claim 4, wherein the output of the DRL algorithm comprises one or more of: (1) estimated values of state/action/advantage as determined by a state/action/advantage value function; and (2) a policy distribution.
6. The method according to claim 4, wherein each DRL algorithm comprises: a policy-gradient-based reinforcement learning algorithm; or a value-based reinforcement learning algorithm; or an actor-critic based reinforcement learning algorithm.
7. The method according to claim 4, wherein each of the driving policy learner modules further comprises a learning target module, and wherein processing, at one or more driving policy learner modules, at least some of the driving experiences, further comprises: processing, at each learning target module, trajectory steps of a driver agent within a driving environment to compute desired learning targets that are desired to be achieved, wherein each trajectory step comprises: a state, an observation, an action, a reward, a next-state and a next-observation, and wherein each learning target represents a result of an action that is desired for a given driving experience.
8. The method according to claim 7, wherein each of the learning targets comprises at least one of: a value target that comprises: an estimated value of a state/action/advantage to be achieved; and a policy objective to be achieved.
9. The method according to claim 7, wherein each DRL algorithm is configured to process data relating to driving experiences using stochastic gradient updates to train a neutral network comprising more than one layer of hidden units between its inputs and outputs, and wherein each of the driving policy learner modules further comprises a loss module and a gradient descent optimizer, wherein each loss module comprises: a loss function and an automatic differentiation module, and wherein processing, at one or more driving policy learner modules, at least some of the driving experiences, further comprises: processing, via each loss function, the learning targets output by the corresponding learning target module and the output of the corresponding DRL algorithm to compute an overall output loss; and processing, at each automatic differentiation module, the overall output loss to generate gradient data for each parameter; and processing, at each gradient descent optimizer, the gradient data for each parameter to compute updated parameters representing a policy, wherein the gradient data represents gradients of each neuron in each neural network used by each DRL algorithm, and wherein the gradients quantitatively define how much of a contribution each neuron made which resulted in the loss due to output of that neural network.
10. The method according to claim 1, wherein each policy specifies a set of parameters that when executed by a particular driver agent define behaviors to be enacted by the vehicle by controlling actuators of the vehicle to operate in response to a given set of sensor inputs.
11. A system, comprising: a driver agent module comprising: one or more driving environment processors each being configured to: observe a driving environment; and one or more driver agents each corresponding to one of the driving environment processors, and each being configured to: execute a policy that controls a vehicle in a specific driving environment as observed by a corresponding driving environment processor for that driver agent module; and capture driving experiences during different driving scenarios in different driving environments, wherein each driving experience comprises data that represents a particular driving environment at a particular time; a driving policy generation module comprising: one or more driving policy learner modules each being configured to: retrieve at least some of the driving experiences process at least some of the driving experiences to learn and generate parameters that describe one or more policies, wherein each policy prescribes a distribution over a space of actions for any given state, and wherein each policy comprises a set of parameters that describe the policy and are processible by at least one of the driver agents to generate an action for controlling the vehicle; and a low-level controller configured to process each action to generate control signals for controlling the vehicle when operating in that specific driving environment.
12. The system according to claim 11, wherein the data for each driving experience that represents a particular driving environment at a particular time, comprises: a state of the particular driving environment observed by a corresponding driving environment processor; an observation made using at least part of an observable state; an action generated by the driver agent; a reward comprising: a signal that signifies how desirable an action performed by the driver agent is at a given time under particular environment conditions, wherein the reward is automatically computed based on road rules and driving principles extracted from human driving data or defined using other appropriate methods based on traffic and the road rules; a goal to be achieved by the driver agent; instance information comprising: information that indicates impact or priority of the driving experience as determined by that driver agent at the time that particular driving experience was acquired; and other meta information about that particular driving experience; and a next state of the particular driving environment that results after the driver agent performs the action in the driving environment; and a next observation made using at least part of a next observable state.
13. The system according to claim 11, wherein each of the driving environment processors is configured to process sensor information from on-board sensors that describes a specific driving environment to generate a state of the specific driving environment, and wherein each of the one or more driver agents is further configured to: process the state, in accordance with a policy, to generate a corresponding action, wherein each policy prescribes a distribution over a space of actions for any given state; and wherein the low-level controller is configured to translate each action to generate the control signals for controlling the vehicle to autonomously control the vehicle when operating in that state in that specific driving environment.
14. The system according to claim 11, wherein each of the driving policy learner modules comprises: a Deep Reinforcement Learning (DRL) algorithm that is configured to: process input information from at least some of the driving experiences to learn and generate an output comprising: a set of parameters representing a policy that are developed through DRL, and wherein each policy is processible by at least one of the driver agents to generate an action for controlling the vehicle.
15. The system according to claim 14, wherein the output of the DRL algorithm comprises one or more of: (1) estimated values of state/action/advantage as determined by a state/action/advantage value function; and (2) a policy distribution.
16. The system according to claim 14, wherein each DRL algorithm comprises: a policy-gradient-based reinforcement learning algorithm; or a value-based reinforcement learning algorithm; or an actor-critic based reinforcement learning algorithm.
17. The system according to claim 14, wherein each of the driving policy learner modules further comprises: a learning target module configured to process trajectory steps of a driver agent within a driving environment to compute desired learning targets that are desired to be achieved, wherein each trajectory step comprises: a state, an observation, an action, a reward, a next-state and a next-observation, and wherein each learning target represents a result of an action that is desired for a given driving experience.
18. The system according to claim 17, wherein each of the learning targets comprises at least one of: a value target that comprises: an estimated value of a state/action/advantage to be achieved; and a policy objective to be achieved.
19. The system according to claim 17, wherein each DRL algorithm is configured to process data relating to driving experiences using stochastic gradient updates to train a neutral network comprising more than one layer of hidden units between its inputs and outputs, and wherein each of the driving policy learner modules further comprises: a loss module, comprising: a loss function configured to process the learning targets output by the corresponding learning target module and the output of the corresponding DRL algorithm to compute an overall output loss; and an automatic differentiation module configured to process the overall output loss to generate gradient data for each parameter; and a gradient descent optimizer configured to process the gradient data for each parameter to compute updated parameters representing a policy, wherein the gradient data represents gradients of each neuron in each neural network used by each DRL algorithm, and wherein the gradients quantitatively define how much of a contribution each neuron made which resulted in the loss due to output of that neural network.
20. A system comprising: non-transitory memory comprising instructions; and one or more processors in communication with the memory, wherein the one or more processors execute the instructions to: capture, via one or more driver agents and one or more corresponding one or more driving environment processors, driving experiences during different driving scenarios in different driving environments, wherein each driving experience comprises data that represents a particular driving environment at a particular time; retrieve, via one or more driving policy learner modules of a driving policy generation module, at least some of the driving experiences; process, at one or more driving policy learner modules, at least some of the driving experiences to learn and generate parameters that describe one or more policies, wherein each policy prescribes a distribution over a space of actions for any given state, and wherein each policy comprises a set of parameters that describe the policy and are processible by at least one of the driver agents to generate an action for controlling a vehicle; process, at the one or more driver agents, received parameters for at least one candidate policy, and execute the at least one candidate policy to generate one or more actions that control the vehicle in a specific driving environment as observed by a corresponding driving environment processor; and process, at a low-level controller, each action to generate control signals for controlling the vehicle when operating in that specific driving environment.
</claims>
</document>
