<document>

<filing_date>
2019-10-29
</filing_date>

<publication_date>
2020-04-30
</publication_date>

<priority_date>
2018-10-29
</priority_date>

<ipc_classes>
G06F17/16,G06F21/32,G06K9/62,G06T7/70
</ipc_classes>

<assignee>
ONFIDO
</assignee>

<inventors>
CALI, JACQUES
MORTAZAVIAN, POURIA
</inventors>

<docdb_family_id>
64100588
</docdb_family_id>

<title>
INTERACTIVE USER AUTHENTICATION
</title>

<abstract>
A computer-implemented method, for verifying an electronic device user, comprising the steps of issuing at least one action instruction to an electronic device user using a notification mechanism of the electronic device; recording response data from a plurality of data acquisition systems of the electronic device, the response data pertaining to the user's response to the at least one action instruction; processing the response data to form classification scores; combining the classification scores to form a classification value; and verifying the user if the classification value satisfies a threshold, wherein each of the at least one action instruction comprises a liveness challenge.
</abstract>

<claims>
1. A computer-implemented method for verifying an electronic device user, the method comprising the steps of: issuing at least one action instruction to an electronic device user using a notification mechanism of the electronic device; recording response data from a plurality of data acquisition systems of the electronic device, the response data pertaining to the user's response to the at least one action instruction; processing the response data to form classification scores; combining the classification scores to form a classification value; and verifying the user if the classification value satisfies a threshold, wherein each of the at least one action instruction comprises a liveness challenge.
2. The method of claim 1, wherein at least one action instruction comprises an audio challenge and a motion challenge.
3. The method of any claim 1, wherein the plurality of data acquisition systems comprises a first data acquisition system and a second data acquisition system, wherein the first data acquisition system is different to the second data acquisition system.
4. The method of any claim 1, wherein the step of processing the response data to form classification scores comprises processing the response data to identify the likelihood of at least one characteristic pattern associated with an action instruction.
5. The method of claim 4, wherein processing the response data to identify the likelihood of at least one characteristic pattern associated with an action instruction comprises processing video data to assess the classification score of at least one characteristic motion associated with the action instruction.
6. The method of claim 5, wherein processing video data to assess the classification score of at least one characteristic motion associated with the action instructions comprises at least one of: performing visual speech recognition on the video data; and performing facial action recognition on the video data.
7. The method of claim 6, wherein performing facial action recognition on the video data comprises: performing a plurality of head pose estimations on the video data; processing the plurality of head pose estimations to form extracted pose information; and forming a facial action classification score using the extracted pose information.
8. The method of claim 7, wherein processing the plurality of head pose estimations to form extracted pose information comprises: extracting a series of angles from the plurality of head pose estimations; fitting a function to the series of angles; constructing a feature vector from parameters of the function, the fitting of the function, and the head pose estimations; and testing if the video data contains at least one characteristic motion with the feature vector.
9. The method of any claim 1, wherein the step of processing the response data to form classification scores comprises assessing a quality score for at least one data type.
10. The method of claim 9, wherein at least one data type comprises video data and, wherein assessing a quality score for video data takes into account at least one of: frame resolution; video frame rate; colour balance; contrast; illumination; blurriness; the presence of a face; and glare.
11. The method of claim 9, wherein at least one of the data type comprises audio data, and wherein assessing a quality score of audio data takes into account at least one of: sampling frequency; background noise; and the quality of the sound recording equipment.
12. The method of any claim 1, wherein combining the classification score to form a classification value comprises weighting each classification score by the quality assessment relating to the relevant data type.
13. The method of claim 12, wherein weighting each classification score by the quality assessment relating to the relevant data type comprises: weighting a classification score relating to video data with a quality score for video data; and weighting a classification score relating to audio data with a quality score for audio data.
14. A computer-readable medium comprising executable instructions for performing the method of claim 1.
15. A computer comprising a processor configured to execute executable code stored in memory, wherein the executable code comprises instructions for performing the method of claim 1.
</claims>
</document>
