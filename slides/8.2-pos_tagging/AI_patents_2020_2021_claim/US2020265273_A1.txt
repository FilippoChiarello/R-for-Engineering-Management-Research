<document>

<filing_date>
2020-02-14
</filing_date>

<publication_date>
2020-08-20
</publication_date>

<priority_date>
2019-02-15
</priority_date>

<ipc_classes>
G06K9/62,G06N20/10,G06N3/04,G06N3/08,G06T3/00,G06T7/00
</ipc_classes>

<assignee>
SURGICAL SAFETY TECHNOLOGIES
</assignee>

<inventors>
TAATI, BABAK
RUDZICZ, FRANK
GRANTCHAROV, TEODOR PANTCHEV
YANG, KEVIN LEE
WEI, HAIQI
ZHANG, YICHEN
</inventors>

<docdb_family_id>
72041984
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR ADVERSE EVENT DETECTION OR SEVERITY ESTIMATION FROM SURGICAL DATA
</title>

<abstract>
Embodiments described herein may provide devices, systems, methods, and/or computer readable medium for adverse event detection and severity estimation in surgical videos. The system can train multiple models for adverse detection and severity estimation. The system can load selected models for real-time adverse event detection and severity estimation.
</abstract>

<claims>
1. A system for automatically generating data structures adapted for storing classifications relating to an adverse event based on audio or video data, the classifications based at least on a plurality of classification tasks, the system comprising: a processor, operating in conjunction with computer memory, the processor configured to: receive a set of audio or video data; extract, using a feature extractor neural network, a vector of latent features from the set of audio or video data; provide, to each of a plurality of time-based classifiers, the vector of latent features from the convolutional network, each time-based classifier corresponding to a classification task of the plurality of classification tasks; train the feature extractor neural network on a training data set using a sigmoid binary cross-entropy loss; and train each time-based classifier of the plurality of time-based classifiers separately on each classification task of the plurality of classification tasks with a loss function that includes at least the binary cross-entropy loss.
2. The system of claim 1, wherein the set of audio or video data includes a set of video frames that have been stabilized to reduce camera motion through the use of bundled-camera path stabilization that reduces jitter and smooths camera paths so that the latent features are accumulated across a plurality of frames.
3. The system of claim 2, wherein stabilization includes warping images to align each frame's camera view based at least on homography.
4. The system of claim 1, wherein the feature extractor neural network is a three dimensional (3D) or two-dimensional (2D) convolutional network.
5. The system of claim 1, wherein the classification tasks include at least bleeding and thermal injury detection, and wherein the classification tasks are causally distinct and include distinguishing active injury events from prior injury artifacts.
6. The system of claim 1, wherein the loss function for each time-based classifier further includes focal loss.
7. The system of claim 1, wherein the loss function for each time-based classifier further includes uncertainty loss.
8. The system of claim 1, wherein the loss function for each time-based classifier further includes both focal and uncertainty loss.
9. The system of claim 1, wherein the processor is configured to receive a set of audio data, feature extractor neural network extracts the vector of latent features from a combination of the set of audio data and the set of video data.
10. The system of claim 9, wherein the training data set includes both training video data and training audio data.
11. A method for automatically generating data structures adapted for storing classifications relating to an adverse event based on audio or video data, the classifications based at least on a plurality of classification tasks, the method comprising: receiving a set of audio or video data; extracting, using a feature extractor neural network, a vector of latent features from the set of audio or video data; providing, to each of a plurality of time-based classifiers, the vector of latent features from the convolutional network, each time-based classifier corresponding to a classification task of the plurality of classification tasks; training the feature extractor neural network on a training data set using a sigmoid binary cross-entropy loss; and training each time-based classifier of the plurality of time-based classifiers separately on each classification task of the plurality of classification tasks with a loss function that includes at least the binary cross-entropy loss.
12. The method of claim 11, wherein the set of audio or video data includes a set of video frames that have been stabilized to reduce camera motion through the use of bundled-camera path stabilization that reduces jitter and smooths camera paths so that the latent features are accumulated across a plurality of frames.
13. The method of claim 12, wherein stabilization includes warping images to align each frame's camera view based at least on homography.
14. The method of claim 11, wherein the feature extractor neural network is a three dimensional (3D) or two-dimensional (2D) convolutional network.
15. The method of claim 11, wherein the classification tasks include at least bleeding and thermal injury detection, and wherein the classification tasks are causally distinct and include distinguishing active injury events from prior injury artifacts.
16. The method of claim 11, wherein the loss function for each time-based classifier further includes focal loss.
17. The method of claim 11, wherein the loss function for each time-based classifier further includes uncertainty loss.
18. The method of claim 11, wherein the loss function for each time-based classifier further includes both focal and uncertainty loss.
19. The method of claim 11, wherein the processor is configured to receive a set of audio data, feature extractor neural network extracts the vector of latent features from a combination of the set of audio data and the set of video data.
20. A non-transitory computer readable medium storing machine interpretable instructions, the machine interpretable instructions, which when executed by a processor, cause the processor to perform a method for automatically generating data structures adapted for storing classifications relating to an adverse event based on audio or video data, the classifications based at least on a plurality of classification tasks, the method comprising: receiving a set of audio or video data; extracting, using a feature extractor neural network, a vector of latent features from the set of audio or video data; providing, to each of a plurality of time-based classifiers, the vector of latent features from the convolutional network, each time-based classifier corresponding to a classification task of the plurality of classification tasks; training the feature extractor neural network on a training data set using a sigmoid binary cross-entropy loss; and training each time-based classifier of the plurality of time-based classifiers separately on each classification task of the plurality of classification tasks with a loss function that includes at least the binary cross-entropy loss.
</claims>
</document>
