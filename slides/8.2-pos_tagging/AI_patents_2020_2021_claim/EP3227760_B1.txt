<document>

<filing_date>
2015-11-23
</filing_date>

<publication_date>
2020-12-23
</publication_date>

<priority_date>
2014-12-03
</priority_date>

<ipc_classes>
G06F3/00,G06F3/01,G06F3/03
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
STACHNIAK, SZYMON
LEYVAND, TOMMER
SCHWESINGER, MARK
</inventors>

<docdb_family_id>
55022670
</docdb_family_id>

<title>
POINTER PROJECTION FOR NATURAL USER INPUT
</title>

<abstract>
A method to identify a targeted object based on eye tracking and gesture recognition. The method is enacted in a compute system controlled by a user and operatively coupled to a machine vision system. In this method, the compute system receives, from the machine vision system, video imaging a head and pointer of the user. Based on the video, the compute system computes a geometric line of sight of the user, which is partly occluded by the pointer. Then, with reference to position data for one or more objects, the compute system identifies the targeted object, situated along the geometric line of sight.
</abstract>

<claims>
1. A method for use in a computer system (16) that is operatively coupled to a machine vision system (22), comprising: Receiving (106), from the machine vision system, video imaging a head and pointer (80) of a user (12) in the same field of view; Computing (108), based on the video, a geometric line of sight (L) of the user partly occluded by the pointer; Identifying (110), with reference to position data for one or more real-world objects (84), a targeted real-world object situated along the geometric line of sight; processing (120) the video to identify an incremental forward movement of the pointer along the geometric line of sight; executing a process associated with the targeted real-world object in response to the incremental forward movement, wherein executing a process comprises: retrieving an image of the one or more real-world objects from an auxiliary camera (86); and inserting the image into a document open on the compute system (16); wherein computing the geometric line of sight includes computing spatial coordinates of the pointer from the video, and wherein the geometric line of sight is a straight line passing through the pointer and through the eye of the user.
2. The method of claim 1, further comprising presenting the one or more real-world objects on a display (14) operatively coupled to the compute system.
3. The method of claim 1, wherein computing the geometric line of sight includes computing spatial coordinates of the head from the video.
4. The method of claim 1, wherein computing the geometric line of sight includes computing spatial coordinates of an eye or pupil (64) of the user from the video.
5. The method of claim 1, wherein the pointer includes a finger of the user.
6. The method of claim 1, wherein the machine vision system includes a depth camera (26), and wherein the video includes a series of time-resolved depth images from the depth camera.
7. The method of claim 1, wherein the machine vision system includes a flat-image camera (28), and wherein the video includes a series of time-resolved images from the flat-image camera.
8. The method of claim 1, wherein the machine vision system includes a depth (26) and a flat-image camera (28), and wherein said cameras have parallel optical axes oriented in the same direction.
9. The method of claim 1, further comprising representing the targeted real-world object visually as a selected object in response to a selection command spoken by the user.
10. The method of claim 1, further comprising accumulating the position data by assigning spatial coordinates to each of the one or more real-world objects, wherein the spatial coordinates define a position of each real-world object within a field of view of the machine vision system
11. The method of claim 10, wherein accumulating the position data includes receiving user input defining the spatial coordinates.
12. A compute system (16) that is operatively couplable to a machine vision system (22), a display (14), and an auxiliary camera (86);
wherein the compute system (16) comprises: a logic machine and a data-storage machine, the data-storage machine storing instructions that, when executed by the logic machine, cause the compute system (16), when coupled to the machine vision system (22), the display (14), and the auxiliary camera (86), to: receive (140) from the machine vision system, via a hardware interface, video imaging a head and pointer (80) of a user (12); compute (108A), based on the video, an initial line of sight of the user partly occluded by the pointer; identify (110), with reference to position data for one or more registered real-world objects (84), a targeted real-world object situated along the initial line of sight; retrieve an image of the targeted real-world object (84) from the auxiliary camera (86); compute, (108B) based on the video, a subsequent line of sight (L) of the user partly occluded by the pointer; cause (136) the display (136) to form, along the subsequent line of sight, the image to represent the targeted real-world object, wherein computing each line of sight includes computing spatial coordinates of the pointer from the video, and wherein each line of sight is a straight line passing through the pointer and through the eye of the user.
</claims>
</document>
