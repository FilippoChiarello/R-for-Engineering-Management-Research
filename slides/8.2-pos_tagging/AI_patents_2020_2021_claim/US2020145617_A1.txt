<document>

<filing_date>
2020-01-02
</filing_date>

<publication_date>
2020-05-07
</publication_date>

<priority_date>
2018-10-01
</priority_date>

<ipc_classes>
G06K9/00,G06K9/62,G06K9/72,G10L15/18,H04N5/44,H04N7/14,H04N7/15
</ipc_classes>

<assignee>
AT&T INTELLECTUAL PROPERTY I (AMERICAN TELEPHONE AND TELEGRAPH COMPANY INTELLECTUAL PROPERTY I)
</assignee>

<inventors>
LIU ZHU
RENGER, BERNARD S.
XU, TAN
ZAVESKY, ERIC
</inventors>

<docdb_family_id>
69230353
</docdb_family_id>

<title>
METHOD AND APPARATUS FOR CONTEXTUAL INCLUSION OF OBJECTS IN A CONFERENCE
</title>

<abstract>
Aspects of the subject disclosure may include, for example, an assessment of a context associated with a conference, an identification of an object associated with the conference in accordance with the context, and a presentation of the object as part of the conference. The conference may include a videoconference and the object may include a physical object, a virtual object, or a combination thereof. Other embodiments are disclosed.
</abstract>

<claims>
1. A device, comprising: a processing system including a processor; and a memory that stores executable instructions that, when executed by the processing system, facilitate performance of operations, the operations comprising: identifying a first virtual object associated with a videoconference based on an assessed first context of the videoconference, wherein the first virtual object is a non-physical object; responsive to determining that a representation of the first virtual object should not be included as part of the videoconference, causing the representation of the first virtual object to be excluded from a presentation associated with the videoconference; and responsive to determining that the representation of the first virtual object should be included as part of the videoconference, causing the representation of the first virtual object to be included in the presentation.
2. The device of claim 1, wherein responsive to the determining that the representation of the first virtual object should not be included as part of the videoconference: assessing a second context associated with the videoconference; identifying a second virtual object associated with a subject matter of the videoconference based on the assessing of the second context; and responsive to determining that a representation of the second virtual objected should be included as part of the videoconference, causing the representation of the second virtual object to be included in the presentation.
3. The device of claim 1, wherein responsive to the determining that the representation of the first virtual object should be included as part of the videoconference: applying an enhancement to the first virtual object resulting in a first enhanced virtual object.
4. The device of claim 3, wherein the operations further comprise: transmitting the first enhanced virtual object to a second device to cause the second device to include the first enhanced virtual object as the representation of the first virtual object in the presentation at the second device.
5. The device of claim 1, wherein the operations further comprise: performing a visual analysis of image data to recognize a first object associated with the first virtual object.
6. The device of claim 5, wherein the operations further comprise: performing audio processing on audio data to recognize the first object.
7. The device of claim 6, wherein the audio processing comprises natural language processing.
8. The device of claim 1, wherein the identifying of the first virtual object comprises providing the first virtual object as a recommendation, and wherein the operations further comprise: receiving user feedback in response to the recommendation, wherein the determining that the representation of the first virtual object should not be included as part of the videoconference and the determining that the representation of the first virtual object should be included as part of the videoconference are based on the user feedback.
9. The device of claim 8, wherein the operations further comprise: assessing a second context associated with a conference subsequent to receiving the user feedback; identifying the first virtual object, a second virtual object, or a combination thereof as being associated with a subject matter of the conference based on the second context and the user feedback; and causing the representation of the first virtual object, a representation of the second virtual object, or a combination thereof to be included in a presentation associated with the conference.
10. A machine-readable medium, comprising executable instructions that, when executed by a processing system including a processor, facilitate performance of operations, the operations comprising: identifying an object based on image data; determining a context associated with a conference; incorporating the object as part of a class of a plurality of objects; presenting a second object included in the class as part of the conference based on determining that the context calls for a second object included in the class; and presenting the object as part of the conference based on the context and responsive to the determining that the context calls for the second object.
11. The machine-readable medium of claim 10, wherein the operations further comprise: determining that a plurality of enhancements should be applied to the object based on a network parameter, wherein the plurality of enhancements includes changing a volume associated with the object and changing an appearance of the object, wherein the changing of the appearance of the object includes highlighting the object, changing a color of the object, changing a size of the object, changing an orientation of the object, or a combination thereof; and responsive to the determining that the plurality of enhancements should be applied to the object, applying the plurality of enhancements to the object as part of the presenting of the object as part of the conference.
12. The machine-readable medium of claim 10, wherein the object includes a virtual object, an augmented reality object, or a combination thereof, and wherein the object is presented as a selectable object as part of the conference.
13. The machine-readable medium of claim 10, wherein the presenting of the object as part of the conference comprises presenting the object in accordance with a first camera view at a first user device and presenting the object in accordance with a second camera view at a second user device, and wherein the second camera view is different from the first camera view.
14. The machine-readable medium of claim 10, wherein the presenting of the object as part of the conference is based on a user profile, a user device profile, an identification of a user camera, an identification of an environmental camera, first data provided by an environmental sensor, second data provided by a surveillance device, or a combination thereof.
15. The machine-readable medium of claim 10, wherein the operations further comprise: performing a visual analysis of the image data to detect the object, thereby resulting in a detected object; comparing the detected object to a plurality of objects included in a database, wherein the plurality of objects include the object; and selecting the object based on determining that a difference between the detected object and the object is less than a threshold.
16. The machine-readable medium of claim 10, wherein the operations further comprise: receiving audio data from at least one camera, a microphone, or a combination thereof; and processing the audio data, thereby resulting in processed audio data, wherein the identifying of the object is further based on the processed audio data.
17. A method, comprising: identifying, by a processing system including a processor, a cross reality object based on a image data, audio data, or a combination thereof; presenting, by the processing system, the cross reality object in a videoconference in accordance with a first camera view at a first user device; and presenting, by the processing system, the cross reality object in accordance with a second camera view at a second user device, wherein the second camera view is different from the first camera view.
18. The method of claim 17, wherein the presenting of the cross reality object in the videoconference is further in accordance with a context associated with the videoconference.
19. The method of claim 18, wherein the cross reality object comprises second image data, second audio data, or a combination thereof, wherein the cross reality object includes a selectable object, and wherein the method further comprises: receiving a selection of the cross reality object; assessing, by the processing system, a second context associated with the videoconference responsive to the selection of the cross reality object; identifying, by the processing system, a second cross reality object based on the second context; and presenting, by the processing system, the second cross reality object in the videoconference in accordance with the second context.
20. The method of claim 18, further comprising: assessing the context associated with the videoconference based on an analysis of a meeting notice associated with the videoconference, and wherein the meeting notice includes a subject line, a main body, at least one attachment, and an identification of a plurality of participants in the videoconference wherein the cross reality object includes a still image, an avatar, an emoticon, an emoji, or a combination thereof, that is representative of a first participant of the plurality of participants when a user device of the first participant is logging-in to the videoconference, and wherein the cross reality object includes a live video feed of the first participant when a connection is established with the user device and a server that is at least partially hosting the videoconference.
</claims>
</document>
