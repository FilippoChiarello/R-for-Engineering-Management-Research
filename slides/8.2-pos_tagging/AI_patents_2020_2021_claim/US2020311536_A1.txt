<document>

<filing_date>
2019-03-25
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2019-03-25
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
CHOI, JUNGWOOK
SRINIVASAN, VIJAYALAKSHMI
VENKATARAMANI, SWAGATH
</inventors>

<docdb_family_id>
72607580
</docdb_family_id>

<title>
DYNAMICALLY RESIZING MINIBATCH IN NEURAL NETWORK EXECUTION
</title>

<abstract>
A minibatch in a neural network execution may be dynamically resized based on on-chip memory. For example, a size of the minibatch is configured such that the minibatch fits within on-chip memory. The size of the minibatch may be resized for a sequence of layers in the neural network execution. A next layer's execution can commence responsive to the resized minibatch being completed in a previous layer without having to wait for all of the minibatch to be completed in the previous layer.
</abstract>

<claims>
1. A method comprising: dynamically resizing a minibatch in a neural network execution, wherein a size of the minibatch is configured such that the minibatch fits within on-chip memory.
2. The method of claim 1, wherein the size of the minibatch is resized to a maximum value that fits within the on-chip memory.
3. The method of claim 1, wherein the size of the minibatch is resized differently for a different layer in the neural network execution.
4. The method of claim 1, wherein the size of the minibatch is resized for a sequence of layers in the neural network execution, wherein a next layer's execution commences responsive to the resized minibatch being completed in a previous layer without having to wait for all of the minibatch to be completed in the previous layer.
5. The method of claim 4, wherein the next layer's execution accesses the resized minibatch residing on the on-chip memory.
6. The method of claim 4, wherein the previous layer and the next layer are layers of a sequence of layers, wherein processing of the previous layer and the next layer are repeated for minibatch size divided by the size of the resized minibatch times.
7. The method of claim 1, wherein the neural network is a deep neural network.
8. The method of claim 1, wherein the on-chip memory is a hardware accelerator's on-chip memory.
9. A system comprising: at least one hardware processor; a memory device coupled with the hardware processor; the at least one hardware processor operable to at least to dynamically resize a minibatch in a neural network execution, wherein a size of the minibatch is configured such that the minibatch fits within on-chip memory.
10. The system of claim 9, wherein the size of the minibatch is resized to a maximum value that fits within the on-chip memory.
11. The system of claim 9, wherein the size of the minibatch is resized differently for a different layer in the neural network execution.
12. The system of claim 9, wherein the size of the minibatch is resized for a sequence of layers in the neural network execution, wherein a next layer's execution commences responsive to the resized minibatch being completed in a previous layer without having to wait for all of the minibatch to be completed in the previous layer.
13. The system of claim 12, wherein the next layer's execution accesses the resized minibatch residing on the on-chip memory.
14. The system of claim 12, wherein the previous layer and the next layer are layers of a sequence of layers, wherein processing of the previous layer and the next layer are repeated for minibatch size divided by resized minibatch size times.
15. The system of claim 9, wherein the neural network is a deep neural network.
16. The system of claim 9, wherein the on-chip memory is a hardware accelerator's on-chip memory.
17. A computer program product comprising a computer readable storage medium having program instructions embodied therewith, the program instructions executable by a device to cause the device to: dynamically resize a minibatch in a neural network execution, wherein a size of the minibatch is configured such that the minibatch fits within on-chip memory.
18. The computer program product of claim 17, wherein the size of the minibatch is resized to a maximum value that fits within the on-chip memory.
19. The computer program product of claim 17, wherein the size of the minibatch is resized differently for a different layer in the neural network execution.
20. The computer program product of claim 17, wherein the size of the minibatch is resized for a sequence of layers in the neural network execution, wherein a next layer's execution commences responsive to the resized minibatch being completed in a previous layer without having to wait for all of the minibatch to be completed in the previous layer.
</claims>
</document>
