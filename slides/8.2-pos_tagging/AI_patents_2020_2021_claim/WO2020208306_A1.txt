<document>

<filing_date>
2020-04-08
</filing_date>

<publication_date>
2020-10-15
</publication_date>

<priority_date>
2019-04-11
</priority_date>

<ipc_classes>
G06F16/951,G06F16/953,G06F17/18
</ipc_classes>

<assignee>
VAEAENAENEN, MIKKO
</assignee>

<inventors>
VAEAENAENEN, MIKKO
</inventors>

<docdb_family_id>
72749097
</docdb_family_id>

<title>
INTELLIGENT SEARCH ENGINE
</title>

<abstract>
A search engine (200, 500, 800), method and a system for performing a search is provided. The search engine (200, 500, 800) is connected to at least one mobile device (210, 510, 810) and at least one web crawler (222, 522, 822). The web crawler (222, 522, 822) is configured to index documents and classify said documents. The search engine (200, 500, 800) receives a query from the mobile device (210, 510, 810) which is determined to be best answered by a crowd-sourced answer. The search engine (200, 500, 800) searches the documents and delivers at least one crowd-sourced answer (318, 618, 918). The search engine (200, 500, 800) displays the crowd-sourced answer (318, 618, 918) to a user.
</abstract>

<claims>
What is claimed is:
1. A search engine (200, 500, 800) connected to at least one mobile device (210, 510, 810) and at least one web crawler (222, 522, 822),
characterized in that,
-the web crawler (222, 522, 822) is configured to index documents and classify said documents,
-the determination to seek a crowdsourced answer is done for queries that are determined not to have a factual answer,
- the determination to seek a crowdsourced answer is not done for queries that are determined to have a factual answer,
-the factuality of the answer and/or the query is determined based on the dispersion of different answers to the query, so that a greater dispersion among different answers to the query is determined to imply non-factuality of the query and/or the answer, and a lesser dispersion of different answers to the query is determined to imply a greater factuality of the query and/or the answer,
-the search engine (200, 500, 800) receives a non-factual query from the mobile device (210, 510, 810) which is determined to be best answered by a crowd-sourced answer, -the search engine (200, 500, 800) searches the documents and delivers at least one crowd-sourced answer (318, 618, 918), and
-the crowd-sourced answer (318, 618, 918) is displayed to user so that a most popular crowd-sourced answer (318, 618, 918) is ranked first and displayed to the user first and/or a breakdown or selection of possible answers is shown to the user on the display.
2. A search engine (200, 500, 800) as claimed in claim 1, characterized in that, a most popular crowd-sourced answer is subjected to a veracity test, and if the veracity test is failed, the most popular search result passing the veracity test is ranked first.
3. A search engine (200, 500, 800) as claimed in claim 1, characterized in that, the web crawler (222, 522, 822) is configured to crawl and index any of the following individually or in a mix: text, voice, image and/or video.
4. A search engine (200, 500, 800) as claimed in claim 1, characterized in that, a crowdsourced answer may be sought to a query that is contextual, and/or context data required to answer the query is derived from the mobile device (210, 510, 810) of the user.
5. A search engine (200, 500, 800) as claimed in claim 4, characterized in that, a most popular crowdsourced answer is calculated by assigning different context weights to different results.
6. A search engine (200, 500, 800) as claimed in claim 1, characterized in that, the search engine (200, 500, 800) is trained with a training set of queries and a validation set of queries.
7. A search engine (200, 500, 800) as claimed in claim 1, characterized in that, the search engine (200, 500, 800) is trained with a training set of web crawler and/or index syntaxes and a validation set of web crawler and/or index syntaxes.
8. A method of performing a search by a search engine, the search engine (200, 500, 800) connected to at least one mobile device (210, 510, 810) and at least one web crawler (222, 522, 822), characterized in that,
-configuring the web crawler (222, 522, 822) to index documents and classify said documents,
-determining to seek a crowdsourced answer for queries that are determined not to have an unambiguous factual answer,
-determining to seek a crowd-sourced answer is not done for queries that are determined to have an unambiguous factual answer,
-the factuality of the answer and/or the query is determined based on the dispersion of different answers to the query, so that a greater dispersion among different answers to the query is determined to imply non-factuality of the query and/or the answer, and a lesser dispersion of different answers to the query is determined to imply a greater factuality of the query and/or the answer,
-receiving a query from the mobile device (210, 510, 810) which is determined to be best answered by a crowdsourced answer,
-searching said documents and delivering at least one crowd-sourced answer (318, 618, 918), and
-displaying the crowd-sourced answer (318, 618, 918) to a user so that the most popular crowd-sourced answer (318, 618, 918) is ranked first and displayed to the user first and/or displaying a breakdown or selection of possible answers to the user on the display.
9. The method as claimed in claim 8, characterized in that, subjecting the most popular crowd-sourced answer to a veracity test, and if the veracity test is failed, the most popular search result passing the veracity test is ranked first.
10. The method as claimed in claim 8, characterized in that, the web crawler (222, 522, 822) crawls and indexes any of the following individually or in a mix: text, voice, image and/or video.
11. The method as claimed in claim 8, characterized in that, seeking a crowdsourced answer to a query that is contextual, and/or context data required to answer the query is derived from the mobile device (210, 510, 810) of the user.
12. The method as claimed in claim 11, characterized in that, calculating the most popular crowd-sourced answer by assigning different context weights to different results.
13. The method as claimed in claim 8, characterized in that, training the search engine (200, 500, 800) with a training set of queries and a validation set of queries.
14. The method as claimed in claim 8, characterized in that, training the search engine (200, 500, 800) with a training set of web crawler and/or index syntaxes and a validation set of web crawler and/or index syntaxes.
15. A system for performing a search through a search engine, the search engine (200, 500, 800) connected to at least one mobile device (210, 510, 810) and at least one web crawler (222, 522, 822), characterized in that,
-a configuration module (212, 512) of the search engine (200, 500, 800) configures the web crawler (222, 522, 822) to index documents and classify said documents,
-the AI module (216, 516, 816) is configured to seek a crowd-sourced answer for queries that are determined not to have an unambiguous factual answer,
-the AI module (216, 516, 816) is configured to not seek a crowd-sourced answer for queries that are determined to have an unambiguous factual answer,
-the factuality of the answer and/or the query is determined based on the dispersion of different answers to the query, so that a greater dispersion among different answers to the query is determined to imply non-factuality of the query and/or the answer, and a lesser dispersion of different answers to the query is determined to imply a greater factuality of the query and/or the answer,
-a receiving module (214, 514) of the search engine (200, 500, 800) is configured to receive a query from the mobile device (210, 510, 810) which is determined to be best answered by a crowd-sourced answer,
-an Artificial Intelligence (AI) module (216, 516, 816) of the search engine (200, 500, 800) is configured to search the documents and deliver at least one crowd-sourced answer (318, 618, 918), and
-a display module (218, 518) of the search engine (200, 500, 800) is configured to display the crowd-sourced answer (318, 618, 918) to a user so that the ranking module (858) ranks first the most popular crowd-sourced answer ad this answer is displayed first and/or and/or a breakdown or selection of possible answers is shown to the user on the display.
16. The system as claimed in claim 15, characterized in that, a veracity module (532) configured to subject the most popular crowd-sourced answer to a veracity test, and if the veracity test fails, a most popular search result that passes the veracity test is ranked first.
17. The system as claimed in claim 15, characterized in that, the web crawler (222, 522, 822) is configured to crawl and index any of the following individually or in a mix: text, voice, image and/or video.
18. The system as claimed in claim 15, characterized in that, a determination module (852) configured to seek a crowd-sourced answer to a query that is contextual and/or a context module derives context data required to answer the query from the mobile device (210, 510, 810) of the user.
19. The system as claimed in claim 18, characterized in that, a calculation module (854) is configured to calculate the most popular crowd-sourced answer by assigning different context weights to different results.
20. The system as claimed in claim 15, characterized in that, a training module (856) is configured to train the search engine (200, 500, 800) with a training set of queries and a validation set of queries.
21. The system as claimed in claim 15, characterized in that, the training module (856) is configured to train the search engine (200, 500, 800) with a training set of web crawler and/or index syntaxes and a validation set of web crawler and/or index syntaxes.
22. A search engine (200, 500, 800) as claimed in claim 5, characterized in that, the said context weights are user location dependent and/or user time dependent.
23. The method as claimed in claim 12, characterized in that, the said context weights are user location dependent and/or user time dependent.
24. The system as claimed in claim 19, characterized in that, the said context weights are user location dependent and/or user time dependent.
</claims>
</document>
