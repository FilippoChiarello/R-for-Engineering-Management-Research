<document>

<filing_date>
2019-08-09
</filing_date>

<publication_date>
2020-09-22
</publication_date>

<priority_date>
2018-08-10
</priority_date>

<ipc_classes>
G05D1/02,G06K9/00,G06T7/593,H04N13/239
</ipc_classes>

<assignee>
BUFFALO AUTOMATION GROUP
BUFFALO AUTOMATION GROUP
</assignee>

<inventors>
KHAKHARIA, MOHIT ARVIND
SURESH, THIRU VIKRAM
</inventors>

<docdb_family_id>
69405984
</docdb_family_id>

<title>
Deep learning and intelligent sensing system integration
</title>

<abstract>
Disclosed herein are systems, methods, and apparatuses for deep learning and intelligent sensing system integrations. A processor may be configured to receive a plurality of images from the sensor system, identify objects in the images in an offline mode, classify the objects in the images in the offline mode, generate heat maps in the offline mode, and send instructions regarding operation of the maritime vessel based on the objects that are identified. The visual sensor may be a stereoscopic camera. The processor may be further configured to perform stereoscopy. The instructions may include a speed or a heading of, for example, a maritime vessel.
</abstract>

<claims>
1. A system comprising: a sensor system disposed on a maritime vessel, the sensor system including a visual sensor; and a processor in electronic communication with the sensor system, wherein the processor is configured to: receive a plurality of images from the sensor system; identify objects in the images in an offline mode by assigning a label to each pixel in the images and segmenting two or more objects in the images; classify the objects in the images in the offline mode; generate a disparity map in the offline mode depicting a distance of matter in each pixel from the sensor system; and send instructions regarding operation of the maritime vessel based on the objects that are identified, and wherein the instructions include a speed or a heading.
2. The system of claim 1, wherein the visual sensor is a stereoscopic camera and wherein the processor is further configured to perform stereoscopy.
3. The system of claim 1, wherein the processor is further configured to estimate object depth by obtaining the object depth from the disparity map.
4. The system of claim 3, wherein the processor is further configured to infer the distance of the objects in an environment.
5. The system of claim 1, wherein the processor is further configured to determine route feasibility.
6. The system of claim 1, wherein the processor is further configured to generate a navigation policy.
7. The system of claim 1, wherein the objects includes a seashore, a watercraft, an iceberg, a static far object, a moving far object, or plain sea.
8. The system of claim 1, wherein the processor includes a convolutional neural network module.
9. The system of claim 8, wherein the processor is configured to use the convolutional neural network module to identify the objects in the images.
10. The system of claim 8, wherein the processor is configured to use the convolutional neural network module to classify the objects in the images.
11. The system of claim 1, further comprising an electronic storage device that includes a library of entries.
12. A method comprising: receiving, at a processor, a plurality of images from a sensor system disposed on a maritime vessel; identifying objects in the images using the processor in an offline mode by assigning a label to each pixel in the images and segmenting two or more objects in the images; classifying the objects in the images using the processor in the offline mode generating a disparity map in the offline mode depicting a distance of matter in each pixel from the sensor system; and sending instructions regarding operation of the maritime vessel, using the processor, based on the objects that are identified, and wherein the instructions include a speed or a heading.
13. The method of claim 12, further comprising performing stereoscopy using the processor.
14. The method of claim 12, further comprising determining an object depth estimate using the processor, wherein the object depth is determined by obtaining the object depth from the disparity map.
15. The method of claim 14, further comprising inferring the distance of the objects in an environment using the processor.
16. The method of claim 12, further comprising determining route feasibility using the processor.
17. The method of claim 12, further comprising determining a navigation policy using the processor.
18. The method of claim 12, wherein the objects includes a seashore, a watercraft, an iceberg, a static far object, a moving far object, or plain sea.
19. The method of claim 12, wherein the identifying and the classifying includes using a convolutional neural network.
20. A non-transitory computer readable medium storing a program configured to instruct a processor to execute the method of claim 12.
</claims>
</document>
