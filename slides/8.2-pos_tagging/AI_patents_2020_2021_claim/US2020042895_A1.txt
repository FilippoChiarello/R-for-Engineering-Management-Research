<document>

<filing_date>
2018-02-08
</filing_date>

<publication_date>
2020-02-06
</publication_date>

<priority_date>
2017-02-21
</priority_date>

<ipc_classes>
G06F15/173,G06N20/00
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
JOUPPI, NORMAN PAUL
THORSON, GREGORY MICHAEL
CHAO, CLIFFORD HSIANG
ROUNE, BJARKE HAMMERSHOLT
MCLAREN, IAN MORAY
</inventors>

<docdb_family_id>
61563107
</docdb_family_id>

<title>
PARALLEL PROCESSING OF REDUCTION AND BROADCAST OPERATIONS ON LARGE DATASETS OF NON-SCALAR DATA
</title>

<abstract>
Methods, systems, and apparatus, including instructions encoded on storage media, for performing reduction of gradient vectors and similarly structured data that are generated in parallel, for example, on nodes organized in a mesh or torus topology defined by connections in at least two dimension between the nodes. The methods provide parallel computation and communication between nodes in the topology.
</abstract>

<claims>
1. 1-23. (canceled)
24. A method comprising: training a machine learning model on batches of training data by processing each batch on a corresponding computational node of a plurality of nodes that are interconnected according to a network topology that comprises rows of nodes and columns of nodes, wherein each node is in exactly one of the rows and one of the columns, wherein one node in each row of nodes is designated as a starting node of the row and one node in each row of nodes is designated as a finishing node of the row, wherein each row has an even number of nodes, wherein each node in each row other than the starting and finishing nodes has a respective preceding adjacent node in the row and a respective next adjacent node in the row, wherein the finishing node in each row is the next adjacent node for both nodes adjacent to the finishing node in the row, wherein training the machine learning model comprises executing code by each node to cause the node to process a respective batch of training data through a respective replica of the machine learning model to obtain respective replica gradient data; generating final gradient data from respective replica gradient data of each node of the plurality of nodes, comprising: for each row of the network topology: sending, by a respective starting node for the row, (i) a first half of replica gradient data obtained by the respective starting node to a first adjacent node in the row connected to the starting node, and (ii) the remainder of the replica gradient data obtained by the starting node to a second adjacent node in the row connected to the starting node, combining, by each node in the row other than the starting node, gradient data from the preceding adjacent node of the node with replica gradient data obtained by the node, sending, by each node other than the finishing node, the respective combined gradient data on the node to a next adjacent node of the node, and determining, by the finishing node of the row, that the finishing node has combined the gradient data received from its two adjacent nodes in the row, and in response, broadcasting, by the finishing node, the respective intermediate gradient data to each other node in the row, and then computing, by each column of the network topology and from the respective intermediate gradient data of each node in the column, the final gradient data; and broadcasting the final gradient data to each node in the plurality of nodes.
25. The method of claim 24, wherein one node in each column of nodes is designated as a starting node of the column and one node in each column of nodes is designated as a finishing node of the column, wherein each column has an even number of nodes, wherein each node in each column other than the starting and finishing nodes has a respective preceding adjacent node in the column and a respective next adjacent node in the column, wherein the finishing node in each column is the next adjacent node for both nodes adjacent to the finishing node in the column, and wherein computing, by each column of the network topology and from the respective intermediate gradient data of each node in the column, the final gradient data comprises: sending, by a starting node for the column, (i) a first half of intermediate gradient data obtained by the respective starting node to a first adjacent node in the column connected to the starting node, and (ii) the remainder of the intermediate gradient data obtained by the starting node to a second adjacent node in the column connected to the starting node, combining, by each node in the column other than the starting node, gradient data from the preceding adjacent node of the node with intermediate gradient data obtained by the node, sending, by each node other than the finishing node, the respective combined intermediate data on the node to a next adjacent node of the node; and wherein broadcasting the final gradient data to each node in the plurality of nodes comprises determining, by the finishing node of each column, that the finishing node has combined the gradient data received from its two adjacent nodes in the column, and in response, broadcasting, by the finishing node, the combined intermediate gradient data to each other node in the column, wherein the combined intermediate gradient data is the final gradient data.
26. The method of claim 24, wherein respective replica gradient data for each node comprises a respective gradient vector comprising a plurality of elements, and wherein combining, by each node in the row other than the starting node, the gradient data from the preceding adjacent node of the node with the replica gradient data obtained by the node comprises performing an element-wise average of elements of a gradient vector of the gradient data from the preceding adjacent node of the node with elements of a gradient vector of the replica gradient data obtained by the node.
27. The method of claim 24, wherein sending, by the starting node for the row, (i) a first half of the replica gradient data obtained by the respective starting node to the first adjacent node in the row connected to the starting node, and (ii) the remainder of the replica gradient data obtained by the starting node to the second adjacent node in the row connected to the starting node comprises: streaming, by the starting node for the row, the replica gradient data to the first adjacent node and the second adjacent node; and wherein sending, by each node other than the finishing node, the respective combined gradient data on the node to the next adjacent node of the node comprises streaming, by each node, the respective combined gradient data on the node to the next adjacent node of the node.
28. The method of claim 27, wherein combining, by each node in the row other than the starting node, the gradient data from the preceding adjacent node of the node with the replica gradient data obtained by the node comprises combining the gradient data from the preceding adjacent node of the node as the replica gradient data is streamed into the node.
29. The method of claim 24, wherein the plurality of nodes are implemented on a single module.
30. The method of claim 29, wherein the single module is an integrated circuit.
31. The method of claim 24, wherein each node comprises one or more processing units and one or more non-transitory computer-readable storage media.
32. The method of claim 24, wherein the network topology is a three-dimensional torus topology.
33. A system comprising: a plurality of nodes implemented on one or more processing units and one or more storage devices, wherein the plurality of nodes are interconnected according to a network topology that comprises rows of nodes and columns of nodes, wherein the rows comprise wrap-around links, wherein each node is in exactly one of the rows and one of the columns, wherein one node in each row of nodes is designated as a starting node of the row and one node in each row of nodes is designated as a finishing node of the row, wherein each node in each row other than the starting and finishing nodes has a respective preceding adjacent node in the row and a respective next adjacent node in the row, wherein the finishing node in each row is the next adjacent node for both nodes adjacent to the finishing node in the row, and wherein the one or more storage devices store media encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising: training a machine learning model on batches of training data by processing each batch on a corresponding computational node of a plurality of nodes, comprising executing code by each node to cause the node to process a respective batch of training data through a respective replica of the machine learning model to obtain respective replica gradient data; generating final gradient data from respective replica gradient data of each node of the plurality of nodes, comprising: for each row of the network topology: sending, by a respective starting node for the row, (i) a first half of replica gradient data obtained by the respective starting node to a first adjacent node in the row connected to the starting node, and (ii) the remainder of the replica gradient data obtained by the starting node to a second adjacent node in the row connected to the starting node, combining, by each node in the row other than the starting node, gradient data from the preceding adjacent node of the node with replica gradient data obtained by the node, sending, by each node other than the finishing node, the respective combined gradient data on the node to a next adjacent node of the node, and determining, by the finishing node of the row, that the finishing node has combined the gradient data received from its two adjacent nodes in the row, and in response, broadcasting, by the finishing node, the respective intermediate gradient data to each other node in the row, and then computing, by each column of the network topology and from the respective intermediate gradient data of each node in the column, the final gradient data; and broadcasting the final gradient data to each node in the plurality of nodes.
34. The system of claim 33, wherein the columns comprise wrap-around links.
35. The system of claim 34, wherein one node in each column of nodes is designated as a starting node of the column and one node in each column of nodes is designated as a finishing node of the column, wherein each column has an even number of nodes, wherein each node in each column other than the starting and finishing nodes has a respective preceding adjacent node in the column and a respective next adjacent node in the column, wherein the finishing node in each column is the next adjacent node for both nodes adjacent to the finishing node in the column, and wherein computing, by each column of the network topology and from the respective intermediate gradient data of each node in the column, the final gradient data comprises: sending, by a starting node for the column, (i) a first half of intermediate gradient data obtained by the respective starting node to a first adjacent node in the column connected to the starting node, and (ii) the remainder of the intermediate gradient data obtained by the starting node to a second adjacent node in the column connected to the starting node, combining, by each node in the column other than the starting node, gradient data from the preceding adjacent node of the node with intermediate gradient data obtained by the node, sending, by each node other than the finishing node, the respective combined intermediate data on the node to a next adjacent node of the node; and wherein broadcasting the final gradient data to each node in the plurality of nodes comprises determining, by the finishing node of each column, that the finishing node has combined the gradient data received from its two adjacent nodes in the column, and in response, broadcasting, by the finishing node, the combined intermediate gradient data to each other node in the column, wherein the combined intermediate gradient data is the final gradient data.
36. The system of claim 33, wherein the rows and columns of nodes are rows and columns of a layer of a plurality of layers interconnected according to the network topology.
37. The system of claim 33, wherein the network topology is a three-dimensional torus topology.
38. The system of claim 33, wherein the batches of training data are first batches of training data, and wherein the operations further comprising after broadcasting the final gradient data to each node in the plurality of nodes: updating, by each node, model parameter values for the machine learning model using the final gradient data, and training the machine learning model on second batches of training data.
39. The system of claim 33, wherein the plurality of nodes are implemented on a single module.
40. The system of claim 39, wherein the single module is an integrated circuit.
41. The system of claim 33, wherein sending, by the starting node for the row, (i) a first half of the replica gradient data obtained by the respective starting node to the first adjacent node in the row connected to the starting node, and (ii) the remainder of the replica gradient data obtained by the starting node to the second adjacent node in the row connected to the starting node comprises: streaming, by the starting node for the row, the replica gradient data to the first adjacent node and the second adjacent node; and wherein sending, by each node other than the finishing node, the respective combined gradient data on the node to the next adjacent node of the node comprises streaming, by each node, the respective combined gradient data on the node to the next adjacent node of the node.
42. The system of claim 41, wherein combining, by each node in the row other than the starting node, the gradient data from the preceding adjacent node of the node with the replica gradient data obtained by the node comprises combining the gradient data from the preceding adjacent node of the node as the replica gradient data is streamed into the node.
43. One or more computer-readable storage media encoded with instructions that are executable by one or more processing units implementing a plurality of nodes, wherein the plurality of nodes are interconnected according to a network topology that comprises rows of nodes and columns of nodes, wherein the rows comprise wrap-around links, wherein each node is in exactly one of the rows and one of the columns, wherein one node in each row of nodes is designated as a starting node of the row and one node in each row of nodes is designated as a finishing node of the row, wherein each node in each row other than the starting and finishing nodes has a respective preceding adjacent node in the row and a respective next adjacent node in the row, wherein the finishing node in each row is the next adjacent node for both nodes adjacent to the finishing node in the row, and wherein the instructions, when executed by one or more processing units, cause the one or more processing units to perform operations comprising: training a machine learning model on batches of training data by processing each batch on a corresponding computational node of a plurality of nodes, comprising executing code by each node to cause the node to process a respective batch of training data through a respective replica of the machine learning model to obtain respective replica gradient data; generating final gradient data from respective replica gradient data of each node of the plurality of nodes, comprising: for each row of the network topology: sending, by a respective starting node for the row, (i) a first half of replica gradient data obtained by the respective starting node to a first adjacent node in the row connected to the starting node, and (ii) the remainder of the replica gradient data obtained by the starting node to a second adjacent node in the row connected to the starting node, combining, by each node in the row other than the starting node, gradient data from the preceding adjacent node of the node with replica gradient data obtained by the node, sending, by each node other than the finishing node, the respective combined gradient data on the node to a next adjacent node of the node, and determining, by the finishing node of the row, that the finishing node has combined the gradient data received from its two adjacent nodes in the row, and in response, broadcasting, by the finishing node, the respective intermediate gradient data to each other node in the row, and then computing, by each column of the network topology and from the respective intermediate gradient data of each node in the column, the final gradient data; and broadcasting the final gradient data to each node in the plurality of nodes.
</claims>
</document>
