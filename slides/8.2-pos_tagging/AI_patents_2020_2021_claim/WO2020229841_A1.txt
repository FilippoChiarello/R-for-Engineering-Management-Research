<document>

<filing_date>
2020-05-15
</filing_date>

<publication_date>
2020-11-19
</publication_date>

<priority_date>
2019-05-16
</priority_date>

<ipc_classes>
G06Q10/06
</ipc_classes>

<assignee>
ROBORACE LIMITED
</assignee>

<inventors>
BALCOMBE, BRYN
SOKOLOV, Mikhail
</inventors>

<docdb_family_id>
67384659
</docdb_family_id>

<title>
A METAVERSE DATA FUSION SYSTEM
</title>

<abstract>
A real-world vehicle includes multiple data sources that generate sensor data that is spatially- mapped to a real-world region; a data fusion system is configured to fuse or integrate (i) the spatially-mapped sensor data with (ii) virtual data, that has been generated outside of the vehicle or generated independently of the operation of the vehicle, and is spatially-mapped to a virtual world. This enables a fusion of the real and virtual worlds which enables a self-driving car to interact not only with the physical world but also to virtual objects introduced into the path of the car (e.g. by a test or development engineer) to test how well the car and its autonomous driving systems cope with the virtual object.
</abstract>

<claims>
1. A data fusion system for use in a real-world vehicle, in which the vehicle includes multiple data sources that generate sensor data that is spatially-mapped to a real-world region; and in which the data fusion system is configured to fuse or integrate (i) the spatially-mapped sensor data with (ii) virtual data that has been generated outside of the vehicle or, whether inside or outside of the vehicle, has been generated independently of the vehicle or the operation of the vehicle, and is also spatially-mapped to a virtual world.
Data fusion
2. The data fusion system of Claim 1 in which there are data sources that generate control data and in which the data fusion system is further configured to fuse or integrate the control data, as well as the sensor data, with the virtual data.
3. The data fusion system of any preceding Claim in which fused or integrated (i) sensor data and/or control data and (ii) the virtual data is supplied to a real-world vehicle control system that controls the vehicle in dependence on that fused or integrated data input.
4. The data fusion system of any preceding Claim in which the vehicle is configured to respond autonomously to the fused or integrated (i) sensor data and/or control data and (ii) the virtual data.
5. The data fusion system of any preceding Claim in which data generated by the vehicle control system is fused or integrated with (i) the sensor data and/or control data and (ii) the virtual data.
6. The data fusion system of any preceding Claim in which data fusion or integration takes places with near zero latency.
7. The data fusion system of any preceding Claim in which data handling components ("data infusers") perform the function of any of: (i) handling the virtual data; (ii) passing that virtual data into vehicle sub-systems that handle the sensor data and/or control data so that the virtual data can be fused, merged or integrated with the sensor data and/or control data.
World model
8. The data fusion system of any preceding Claim which fuses or integrates into a single world model the (i) sensor data and/or control data and (ii) the virtual data.
9. The data fusion system of preceding Claim 8 in which the single world model is a fused spatially-mapped world that is a single unified representation of a global state that reconciles any differences in (i) the sensor data and/or control data and (ii) the virtual data.
10. The data fusion system of preceding Claim 8 or 9 which uses a world model that is generated from (i) a real-world source or sources, including a spatially mapped real-world region and (ii) a virtual world source or sources, including a spatially mapped virtual-world region that corresponds to the real-world region.
11. The data fusion system of preceding Claim 8 - 10 in which the world model is resident or stored in memory that is (i) wholly in the vehicle or (ii) is distributed between in-vehicle memory and memory external to the vehicle, or (iii) is wholly outside of the vehicle.
12. The data fusion system of any preceding Claim 8 - 11 in which the world model comprises one or more of the following: objects, conditions and events; where objects specify spatially-mapped elements or things in the real and virtual worlds; conditions characterise the ambient environment in spatially-mapped regions of the real and virtual worlds; and events specify how objects behave or react in defined circumstances.
13. The data fusion system of any preceding Claim 8 - 12 which predicts the next most probable state of an object in the world model.
14. The data fusion system of any preceding Claim 8 - 13 in which the next most probable state of an object in the world model is predicted using one or more of the following techniques: dead reckoning, methods of mathematical extrapolation, Kalman filtering, deep learning inference and specific problem-solving methods like Pacejka models for vehicle tyre dynamics or SLAM for localisation in unknown environments.
15. The data fusion system of any preceding Claim 8 - 14 in which the data fusion system performs real-time data processing and computation of the next most probable state, but only for those objects that are momentarily involved in actions that modify or form a local world model.
Virtual world
16. The data fusion system of any preceding Claim in which the spatially-mapped virtual data is generated within a spatially-mapped virtual world.
17. The data fusion system of preceding Claim 16 in which the virtual world is created in a system that is external to the vehicle, is controlled independently of the vehicle and is not generated by the vehicle or any sensor or control systems in the vehicle.
18. The data fusion system of preceding Claim 16 or 17 in which the virtual world resides wholly externally to the vehicle and shares the same spatial mapping or otherwise corresponds to the world model that is resident or stored in memory that is (i) wholly in the vehicle or (ii) is distributed between in-vehicle memory and memory external to the vehicle, or (iii) is wholly outside of the vehicle.
19. The data fusion system of preceding Claim 16 - 18 in which the virtual data includes data that mirrors, spatially matches or spatially relates at least in part to the world in which the vehicle moves or operates.
20. The data fusion system of preceding Claim 16 - 19 in which the virtual data includes one or more of events, conditions or objects which present, or provide data to be fused with data from, some or all of the in-vehicle sensors so that the in-vehicle sensors react as though they are actual real-world events, conditions or objects.
21. The data fusion system of preceding Claim 16 - 20 in which the virtual data includes one or more of events, conditions or objects which present to a real-world vehicle control system as though they are actual events, conditions or objects detected by some or all of the invehicle sensors.
22. The data fusion system of preceding Claim 16 - 21 in which the virtual data includes one or more of events, conditions or objects which are added in order to test how effectively the real-world vehicle control or planning and control system reacts to the events, conditions or objects.
23. The data fusion system of preceding Claim 16 - 22 in which the virtual data includes objects which the vehicle has to avoid, such as virtual people, cones, barriers, signage, buildings, or other vehicles.
24. The data fusion system of preceding Claim 16 - 23 in which the virtual data includes objects and/or conditions which the vehicle has to react to, such as rain, fog, ice, uneven road surfaces.
25. The data fusion system of preceding Claim 16 - 24 in which the virtual data includes objects or loots which the vehicle has to pass through, such as route paths, intersections, entrances and exits.
26. The data fusion system of preceding Claim 16 - 25 in which the virtual data includes objects or loots which the vehicle has to pass through in order to earn points in a race, game or competition.
27. The data fusion system of preceding Claim 16 - 26 in which the virtual data includes objects or loots which the vehicle has to pass through in order to earn points in a race, game or competition and these are positioned close to virtual or real objects which the vehicle has to avoid, such as virtual people, barriers, signage, or other vehicles.
28. The data fusion system of preceding Claim 16 - 27 in which the virtual data includes objects or loots and/or conditions to form part of a media entertainment, such as eSports streaming, television, games, film.
29. The data fusion system of preceding Claim 16 - 28 in which the virtual data includes one or more of objects and/or conditions to form part of a vehicle testing or development program.
Real-world
30. The data fusion system of any preceding Claim which processes data that includes any of the following: the real-world locations of other vehicles, robots, drones and people, the local topography, the route or road the vehicle is travelling along, any other the status of traffic lights, the time of day, weather conditions, type of road, weather, location of parking bays, and garages.
Agents
31. The data fusion system of any preceding Claim which uses agents that are responsible for tracking objects or events or condition added or injected into the world model.
32. The data fusion system of preceding Claim 31 where agents have their own local world model that tracks the objects, events or conditions relevant to the state and behaviour of each agent.
33. The data fusion system of preceding Claim 31 or 32 where agents share their state and behaviour with other agents.
34. The data fusion system of preceding Claim 31 - 33 where the agents are responsible for tracking objects, events and condition added or injected into the world model.
35. The data fusion system of preceding Claim 31 - 34 where the agents are responsible for handling errors.
36. The data fusion system of preceding Claim 31 - 35 where a single agent corresponds to or represents a single virtual vehicle.
37. The data fusion system of preceding Claim 31 - 36 where a world model comprises a multi-agent system including multiple virtual vehicles.
Data Distribution Framework
38. The data fusion system of any preceding Claim that uses a decentralised, data centric architecture, such as an OMGDDS framework, to handle or transfer one or more of the sensor data, control data and the virtual data.
39. The data fusion system of preceding Claim 38 where tunnelling DDS data packets are tunnelled through non-IP networks including, but not limited to, industrial M2M (machine-tomachine) protocols, V2X (vehicle-to-everything), CAN, FlexRay and others.
40. The data fusion system of preceding Claim 38 or 39 where a data distribution framework provides a connectivity method with a boosted stack of protocols for Vehicle-toEverything (V2X) communication that extends the capabilities and performance of existing V2X systems with one or more of the following features: Ability to broadcast messages as frequent as every 10 milliseconds; extended message format enabling signalling via V2X radio transparently to regular V2X systems and not affecting their work; DDS tunnelling over IEEE 802. l ip and 3GPP C-V2X; Universal Over-The-Top (OTT) data transmission via V2X radio for any UDP and TCP connectivity in a transparent way to regular V2X systems without affecting their work.
Data Infusion Framework
41. The data fusion system of any preceding Claim that uses an extensible toolkit of reusable software and hardware components designed to provide a standard way to build and deploy real-time data infusers for various control and sensor systems allowing infusion of artificial virtual data into normal data.
42. The data fusion system of preceding Claim 41 that provides data infusion logic for OMG Data Distribution Service.
43. The data fusion system of any preceding Claim 41 - 42 that provides data infusion logic for Vehicle-to-Everything (V2X) communication.
44. The data fusion system of any preceding Claim 41 - 43 that provides data infusion logic for automotive"Universal Measurement and Calibration Protocol" (e.g. ASAM MCD-1 XCP) connecting measurement and calibration systems to vehicle ECUs.
Data Infusers
45. The data fusion system of any preceding Claim which includes data infusers, which are plug-in components for ingesting data that represents any of the following virtual data: virtual objects, conditions or events.
46. The data fusion system of preceding Claim 45 in which data infusers supply or provide virtual data to be fused with real-world sensor and/or control data.
47. The data fusion system of preceding Claim 45 - 46 in which the data infusers provide data to a real-world vehicle control system that processes (i) the virtual data, or (ii) the fused or integrated virtual and sensor and/or control data, as real data or equivalent to real-world data.
48. The data fusion system of preceding Claim 45 - 47 in which the data infusers maintain their data sampling rates and resolution independently of one another.
49. The data fusion system of preceding Claim 45 - 48 which the data infusers maintain coherency of computation in self-organized mesh networks of data infuser components.
50. The data fusion system of preceding Claim 45 - 49 which the data infusers for processing sensor data are specifically designed for various types of sensors used in robotics and automotive including, but not limited to, radars, LIDARs, ultrasound, computer vision and stereo vision cameras.
51. The data fusion system of preceding Claim 50 in which the sensor data includes: imagebased sensor signals, including any sensors that output 2D serial images; sensor signals based on point-clouds, including data from LIDARs, computer vision systems and stereo-cameras; sensor signals based on serial data, including ultrasonic sensors, radar, temperature, and velocity sensors.
Representation Framework
52. The data fusion system of any preceding Claim which includes a representation framework, which is an extensible toolkit of reusable software integration adaptors providing an immersive representation of the virtual world to end-users via user interfaces, and/or interactive platforms and/or devices.
53. The data fusion system of preceding Claim 52 in which the representation framework is capable of integration with user interfaces including, but not limited to, singleor multi screen video displays, mobile terminals and remote controllers, VR/AR headsets, user motion trackers, direct manipulation and tangible interfaces.
54. The data fusion system of preceding Claim 52 - 53 in which the representation framework includes a software integration toolkit having a multi-layered structure of world model representations, where various properties of objects have affinities to specific representation layers and each of these layers can be assigned to a specific representation method, which is served by specific user interface components and respective devices.
55. The data fusion system of preceding Claim 54 in which a basic representation layer is a set of video-streams transmitted from cameras installed on a real-world vehicle racetrack and giving various points of view, and on top of this basic layer there are one or more representation layers or overlays visualising virtual objects for various media channels, and these virtual overlays are be applied to the underlying video streams using appropriate tools, devices and user-interfaces, so that a blended scene results that combines real and virtual objects.
Vehicle control
56. The data fusion system of any preceding Claim configured to work with a vehicle that includes a real-world Automated Driving System (ADS) planning and control system ("ADS Planning and Control layer") that controls or actuates systems in the vehicle, such as steering, brakes, accelerometer and that real-world planning and control system takes inputs from the data fusion system.
57. The data fusion system of preceding Claim 56 where the vehicle includes an ADS that generates a local world model that processes real-world data, and the ADS provides input data to the data fusion system, which in turn provides input data to a real-world planning and control system.
58. The data fusion system of preceding Claim 56 or 57 where a local world model in an ADS sends data to, and an ADS Planning and Control layer receives data from, an external world model or virtual world.
59. The data fusion system of preceding Claim 56 - 58 where the local world model in the ADS sends data to and the ADS Planning and Control layer receives data from, a world model that is an embedded portion or sub-system of the ADS.
60. The data fusion system of preceding Claim 56 -59 where the local world model in the ADS sends data to, and the ADS Planning and Control layer receives data from, both an external world model and also a world model that is an embedded portion or sub-system of the ADS
61. The data fusion system of preceding Claim 56 - 60 where the world model enables the injection of any of the following; virtual objects, virtual paths, virtual routes, into the ADS which the ADS then includes in its control and planning operations.
62. The data fusion system of preceding Claim 56 - 61 where the local world model sends data over an OMG DDS databus or similar real-time communication middleware.
63. The data fusion system of preceding Claim 56 - 62 where the output of the world model matches the expected inputs of the ADS Planning and Control normally received from the local world model and in this mode the ADS Planning and Control has no indication whether an object is real or virtual.
64. The data fusion system of preceding Claim 57 - 63 where the output of the world model matches the expected inputs of the ADS Planning and Control normally received from the local world model with additional flags that indicate whether an object is real or virtual and in this mode the ADS Planning and Control system is adapted to take advantage of this additional object information.
Vehicle
65. The data fusion system of any preceding Claim configured to work with a vehicle that is a car, plane, land vehicle, delivery vehicle, bus, sea vehicle, drone, robot, or other selfpropelled device.
66. The data fusion system of preceding Claim 65 where the vehicle is an autonomous car, plane, land vehicle, delivery vehicle, bus, sea vehicle, drone, robot, or other self-propelled device.
67. The data fusion system of preceding Claim 65 - 66 where the vehicle is a racing vehicle.
68. The data fusion system of preceding Claim 65 - 67 where the vehicle is one of several mechanically similar racing vehicles with each have different control systems or software sub systems for those control systems, and the different vehicles compete to react in an optimal manner to the same new virtual data supplied to each of them.
69. The data fusion system of preceding Claim 65 - 68 where the vehicle is an autonomous car, plane, vehicle, drone, robot, or other self-propelled device configured to film or record other vehicles that are racing.
70. The data fusion system of preceding Claim 65 - 69 where the vehicle is driven or piloted by a human and a display in the vehicle shows some or all of the virtual world to that human driver or pilot.
Audience experience
71. The data fusion system of any preceding Claim configured to enable a spectator, viewer, participant or controller of an event featuring the vehicle(s) to view, on a display, both the real-world vehicle and anything generated in the virtual world, such as objects or conditions which the vehicle interacts with.
72. The data fusion system of any preceding Claim configured to enable a spectator, viewer, participant or controller of an event featuring the vehicle(s) to view both the real-world vehicle and, on a display, such as an augmented reality headset or glasses, anything generated in the virtual world, such as objects or conditions which the vehicle interacts with.
73. The data fusion system of any preceding Claim 71 or 72 in which the spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to navigate through the fused real and virtual worlds to alter their view of that fused world.
74. The data fusion system of any preceding Claim 71 - 73 in which the spectator, viewer, participant or controller is able to navigate through the fused real and virtual worlds to alter the view of that fused world that they are viewing, filming or recording or streaming.
75. The data fusion system of any preceding Claim 71 - 74 in which the spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to add or control in the virtual world any one or more of the following: (a) objects which are added in order to test how effectively the real-world control system reacts to the objects; (b) objects which the vehicle has to avoid, such as virtual people, barriers, signage, or other vehicles.
76. The data fusion system of any preceding Claim 71 - 75 in which the spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to add or control in the virtual world objects or loots which the vehicle has to pass through, such as route paths, entrances and exits.
77. The data fusion system of any preceding Claim 71 - 76 in which the spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to add or control in the virtual world objects or loots which the vehicle has to pass through in order to earn points in a race, game or competition.
78. The data fusion system of any preceding Claim 71 - 77 in which the spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to add or control in the virtual world objects or loots which the vehicle has to pass through in order to earn points in a race, game or competition and these are positioned close to virtual or real objects which the vehicle has to avoid, such as virtual people, barriers, signage, or other vehicles.
79. A vehicle that includes a data fusion system as defined in Claim 1- 78.
80. A method of developing, improving or testing a vehicle, in which the vehicle includes a data fusion system as defined in Claim 1- 78 and virtual objects, events or conditions are added to the virtual world processed by the data fusion system to test how the vehicle responds to those virtual objects, events or conditions.
81. A vehicle that has been developed, improved or tested using the method defined in Claim in Claim 1- 78.
Game or entertainment system
82. A game or entertainment system, the system generating images that display or otherwise feature a vehicle that includes a data fusion system as defined in Claim 1 - 78 above or a vehicle as defined in Claim 79 or 81.
83. The game or entertainment system of Claim 82 in which an AV or human-driven realworld vehicle, or AI-assisted human-driven real-world vehicle, races in a real-world driving region; and there is (i) a virtual-world representation of that real-world driving region, and (ii) a virtual vehicle racing against the real-world vehicle, and in which the real-world vehicle reacts to the virtual vehicle as though the virtual vehicle is present in the real-world and the virtual vehicle reacts to the real-world vehicle as though the real-world vehicle is present in the virtual-world.
84. The game or entertainment system of Claim 82 - 83 in which there is a real-world, full size vehicle in a real-world driving region and also a virtual-world representation of that realworld driving region, and in which the real-world vehicle reacts to control inputs from a user in a simulator or wearing an AR or VR headset.
85. The game or entertainment system of Claim 82 - 84 in which self-driving cars compete against virtual cars controlled by eSports stars safely located inside driver-in-the-loop simulators.
86. The game or entertainment system of Claim 82 - 85 in which human drivers with augmented reality displays compete against virtual vehicles controlled by eSports stars safely located inside driver-in-the-loop simulators.
87. The game or entertainment system of Claim 82 - 86 in which eSports drivers in simulators directly control physical cars at various levels of control abstraction; operational, tactical and strategic depending upon communication latencies.
88. The game or entertainment system of Claim 82 - 86 in which several mechanically similar racing vehicles with each have different control systems or software sub-systems for those control systems compete against one another to react in an optimal manner to the same new virtual data supplied to each of them.
89. The game or entertainment system of Claim 88 in which the virtual data includes one or more of events, conditions or objects which present, or provide data to be fused with data from, some or all of the in-vehicle sensors so that the in-vehicle sensors react as though they are actual real-world events, conditions or objects.
90. The game or entertainment system of Claim 88 - 89 in which the virtual data includes one or more of events, conditions or objects which present to a real-world vehicle control system as though they are actual events, conditions or objects detected by some or all of the invehicle sensors.
91. The game or entertainment system of Claim 88 - 90 in which the virtual data includes one or more of events, conditions or objects which are added in order to test how effectively the real-world vehicle control system reacts to the events, conditions or objects.
92. The game or entertainment system of Claim 88 - 91 in which the virtual data includes objects which the vehicle has to avoid, such as virtual people, cones, barriers, signage, buildings, or other vehicles.
93. The game or entertainment system of Claim 88 - 92 in which the virtual data includes objects and/or conditions which the vehicle has to react to, such as rain, fog, ice, uneven road surfaces.
94. The game or entertainment system of Claim 88 -93 in which the virtual data includes objects or loots which the vehicle has to pass through, such as route paths, intersections, entrances and exits.
95. The game or entertainment system of Claim 88 - 94 in which the virtual data includes objects or loots which the vehicle has to pass through in order to earn points in a race, game or competition.
96. The game or entertainment system of Claim 88 - 95 in which the virtual data includes objects or loots which the vehicle has to pass through in order to earn points in a race, game or competition and these are positioned close to virtual or real objects which the vehicle has to avoid, such as virtual people, barriers, signage, or other vehicles.
97. The game or entertainment system of Claim 88 - 96 in which the virtual data includes objects and/or conditions to form part of a media entertainment, such as eSports streaming, television, games, film.
98. The game or entertainment system of Claim 88 - 97 in which a spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to view, on a display, both the real-world vehicle and any objects generated in the virtual world, such as objects or conditions which the vehicle interacts with.
99. The game or entertainment system of Claim 88 - 98 in which a spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to view both the real-world vehicle and, on a display, such as an augmented reality headset or glasses, any objects generated in the virtual world, such as objects or conditions which the vehicle interacts with.
100. The game or entertainment system of Claim 88 - 99 in which a spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to navigate through the fused real and virtual worlds to alter their view of that fused world.
101. The game or entertainment system of Claim 88 - 100 in which a spectator, viewer, participant or controller is able to navigate through the fused real and virtual worlds to alter the view of that fused world that they are viewing, filming or recording or streaming.
102. The game or entertainment system of Claim 88 - 101 in which spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to add or control in the virtual world any one or more of the following: (a) objects which are added in order to test how effectively the real-world control system reacts to the objects; (b) objects which the vehicle has to avoid, such as virtual people, barriers, signage, or other vehicles.
103. The game or entertainment system of Claim 88 - 102 in which a spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to add or control in the virtual world objects which the vehicle has to pass through, such as route paths, entrances and exits.
104. The game or entertainment system of Claim 88 - 103 in which a spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to add or control in the virtual world objects or loots which the vehicle has to pass through in order to earn points in a race, game or competition.
105. The game or entertainment system of Claim 88 - 104 in which a spectator, viewer, participant or controller of an event featuring the vehicle(s) is able to add or control in the virtual world objects or loots which the vehicle has to pass through in order to earn points in a race, game or competition and these are positioned close to virtual or real objects which the vehicle has to avoid, such as virtual people, barriers, signage, or other vehicles.
</claims>
</document>
