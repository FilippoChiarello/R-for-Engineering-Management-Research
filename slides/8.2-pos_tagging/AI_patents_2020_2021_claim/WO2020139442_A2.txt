<document>

<filing_date>
2019-10-09
</filing_date>

<publication_date>
2020-07-02
</publication_date>

<priority_date>
2018-10-10
</priority_date>

<ipc_classes>
G01S15/04,G01S15/88,G01S15/89
</ipc_classes>

<assignee>
FARSOUNDER
</assignee>

<inventors>
BERARD, AUSTIN
HENLEY, HEATH
LAPISKY, EVAN
ZIMMERMAN, MATTHEW
</inventors>

<docdb_family_id>
71129655
</docdb_family_id>

<title>
THREE-DIMENSIONAL FORWARD-LOOKING SONAR TARGET RECOGNITION WITH MACHINE LEARNING
</title>

<abstract>
Machine learning algorithms can interpret three-dimensional sonar data to provide more precise and accurate determination of seafloor depths and in-water target detection and classification. The models apply architectures for interpreting volumetric data to three-dimensional forward-looking sonar data. A baseline set of training data is generated using traditional image and signal processing techniques, and used to train and evaluate a machine learning model, which is further improved by additional inputs to improve both seafloor and in-water target detection.
</abstract>

<claims>
What is claimed is:
1. A method for detecting and classifying underwater features, the method comprising:
obtaining three-dimensional forward-looking sonar (3D-FLS) data;
providing the 3D-FLS data as input to a machine learning algorithm; and
using the algorithm to detect a feature in the 3D-FLS data and classify the feature as seafloor or an in-water target.
2. The method of claim 1, wherein the 3D-FLS data comprises a point cloud of backscatter strength data.
3. The method of claim 1, wherein the 3D-FLS data comprises metadata comprising one or more of: sensor acceleration readings, gyroscope readings, sensor roll orientation, sensor pitch orientation, sensor heave, sensor heading, sensor course, sensor latitude, sensor longitude, water temperature, water salinity, and sound speed profile.
4. The method of claim 1, wherein the 3D-FLS data are obtained from a 3D-FLS system configured to be mounted on the hull of a boat.
5. The method of claim 4, wherein the algorithm is run on a processor operably connected to the 3D-FLS system.
6. The method of claim 1, wherein the algorithm is a convolutional neural network.
7. The method of claim 1, wherein the algorithm has been trained on labeled 3D-FLS training data.
8. The method of claim 7, further comprising: obtaining data from a secondary source; and
improving the labeled 3D-FLS training data with the secondary source data.
9. The method of claim 8, wherein the secondary source data comprises one or more of:
manually labeled volumetric backscatter strength data;
bathymetric survey data from a reference source;
multibeam echosounder data (MBES);
single beam echosounder data (SBES);
3D-FLS data obtained from the same sonar system at a different angle or time;
nautical chart data;
radar data; and
automatic identification system (AIS) data.
10. The method of claim 9, wherein the MBES or SBES data are obtained from the same vessel as the 3D-FLS data.
11. The method of claim 8, wherein the secondary source data comprise information about position, speed, and heading.
12. The method of claim 1, further comprising sub-classifying the in-water targets as one or more of: wakes; buoys; fish; boats; and engine noise.
13. The method of claim 1, wherein the algorithm generates an output comprising a classification for each point in the 3D-FLS data, the classification representing a likelihood that the point corresponds to (i) seafloor, (ii) an in-water target, or (iii) background.
14. A system for real-time detection and classification of underwater features, the system comprising:
a three-dimensional forward-looking sonar (3D-FLS) device configured to insonify a region ahead of a vessel and collect 3D-FLS data; and a processor operably coupled to the 3D-FLS device, the processor configured to run a machine learning algorithm on the 3D-FLS data to detect and classify features in the 3D-FLS data, the features comprising seafloor and inwater targets.
15. The system of claim 14, wherein the 3D-FLS device is configured to be mounted on the hull of a boat.
16. The system of claim 14, wherein the 3D-FLS data comprise sonar return signals forming a volumetric point cloud of backscatter strength data.
17. The system of claim 14, wherein the 3D-FLS data comprise metadata comprising one or more of: sensor acceleration readings, gyroscope readings, sensor roll orientation, sensor pitch orientation, sensor heave, sensor heading, sensor course, sensor latitude, sensor longitude, water temperature, water salinity, and sound speed profile.
18. The system of claim 14, wherein the algorithm is a convolutional neural network.
19. The system of claim 14, wherein the algorithm has been trained on labeled 3D-FLS training data.
20. The system of claim 19, wherein the processor is configured to improve the training data with manually labeled volumetric backscatter strength data.
21. The system of claim 19, wherein the algorithm has been further trained on reference seafloor data.
22. The system of claim 21, further comprising a downward-looking sonar operably connected to the processor and configured to obtain the reference seafloor data.
23. The system of claim 22, wherein the downward-looking sonar comprises an echosounder
24. The system of claim 19, wherein the algorithm has been further trained on 3D-FLS data collected from the 3D-FLS device at a different angle or time.
25. The system of claim 14, wherein the processor is further configured to sub-classify the in water targets as one or more of: wakes; buoys; fish; boats; and engine noise.
26. The system of claim 14, wherein the algorithm generates an output comprising a classification for each point in the 3D-FLS data, the classification representing a likelihood that the point corresponds to (i) seafloor, (ii) an in-water target, or (iii) background.
27. The system of claim 14, further comprising a display for displaying the features with labels indicating their classifications or classification likelihoods.
</claims>
</document>
