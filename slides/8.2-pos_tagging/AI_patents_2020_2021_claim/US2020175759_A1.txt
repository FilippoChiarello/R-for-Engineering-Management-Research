<document>

<filing_date>
2018-11-29
</filing_date>

<publication_date>
2020-06-04
</publication_date>

<priority_date>
2018-11-29
</priority_date>

<ipc_classes>
G06F3/01,G06K9/62,G06N20/00,G06T15/20,G06T17/00,G06T19/00
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
INNAMORATI, CARLO
KAUFMAN, DANIEL
MITRA, NILOY
RUSSELL, BRYAN
</inventors>

<docdb_family_id>
70849233
</docdb_family_id>

<title>
Synthetic data generation for training a machine learning model for dynamic object compositing in scenes
</title>

<abstract>
This application relates generally to augmenting images and videos with dynamic object compositing, and more specifically, to generating synthetic training data to train a machine learning model to automatically augment an image or video with a dynamic object. The synthetic training data may contain multiple data points from thousands of simulated dynamic object movements within a virtual environment. Based on the synthetic training data, the machine learning model may determine the movement of a new dynamic object within new virtual environment.
</abstract>

<claims>
1. A non-transitory computer-readable storage medium having stored thereon instructions for causing at least one computer system to generate synthetic training data for dynamic object compositing, the instructions comprising: receiving a first model comprising a representation of a first environment and a first viewpoint associated with the first model; simulating within the first model and from the first viewpoint, a first dynamic object moving away from a virtual camera at a first initial velocity to generate first synthetic training data comprising a sequence of video frames depicting a simulation of the dynamic object interacting in a virtual environment; training, based on the first synthetic training data, a first machine learning model; generating, based on the first machine learning model, a second initial velocity, and a second viewpoint associated with an image in a second environment, movement data of a second dynamic object within the second environment; and superimposing, based on the movement data of the second dynamic object, the second dynamic object into the image.
2. The computer-readable storage medium of claim 1, the instructions further comprising: rendering, from the first viewpoint, a depth map of the first model; rendering, from the first viewpoint, a surface normal map of the first model; and receiving, a second viewpoint associated with the image, wherein the second viewpoint is a viewpoint associated with a second virtual camera.
3. The computer-readable storage medium of claim 1, the instructions further comprising: receiving a third viewpoint associated with the first model, wherein the third and first viewpoints are different; and simulating within the first model, from the third viewpoint, the first dynamic object moving to generate second synthetic training data.
4. The computer-readable storage medium of claim 1, the instructions further comprising: receiving a third model comprising a representation of a third environment; receiving a fourth viewpoint associated with the third model; and simulating within the third model, from the fourth viewpoint, the first dynamic object moving to generate third synthetic training data.
5. The computer-readable storage medium of claim 1, wherein the first environment is an indoor environment and the second environment is a different indoor environment.
6. The computer-readable storage medium of claim 1, wherein the first model is a three-dimensional model.
7. The computer-readable storage medium of claim 1, wherein the movement data indicates the first dynamic object interacting with one or more scene objects within the first model.
8. The computer-readable storage medium of claim 8, the instructions further comprising: receiving user gesture data associated with a first human gesture; and determining, based on the user gesture data, the second initial velocity.
9. A computer-implemented method for generating synthetic training data for dynamic object compositing, the method comprising: receiving a first model comprising a representation of a first environment and a first viewpoint associated with the first model; and simulating within the first model and from the first viewpoint, a first dynamic object moving away from a virtual camera at a first initial velocity to generate first synthetic training data comprising a sequence of video frames depicting a simulation of the dynamic object interacting in a virtual environment.
10. The computer-implemented method of claim 9, further comprising: training, based on the first synthetic training data, a first machine learning model; receiving an image comprising a representation of a second environment, wherein the second environment is different from the first environment; receiving, a second viewpoint associated with the image; receiving a second dynamic object data associated with a second dynamic object; generating, based on the first machine learning model, a second initial velocity, and the second viewpoint, movement data of the second dynamic object within the image; and superimposing, based on the movement data of the second dynamic object, the second dynamic object into the image.
11. The computer-implemented method of claim 10, further comprising: receiving a third viewpoint associated with the first model, wherein the third and first viewpoints are different; and simulating within the first model, from the third viewpoint, the first dynamic object moving to generate second synthetic training data.
12. The computer-implemented method of claim 10, further comprising: receiving a third model comprising a representation of a third environment; receiving a fourth viewpoint associated with the third model; and simulating within the third model, from the fourth viewpoint, the first dynamic object moving to generate third synthetic training data.
13. The computer-implemented method of claim 10, wherein the first environment is an indoor environment and the second environment is a different indoor environment.
14. The computer-implemented method of claim 10, wherein the first model is a three-dimensional model.
15. A system for generating synthetic training data for dynamic object compositing, comprising: one or more processors; and a memory coupled with the one or more processors, the memory configured to store instructions that when executed by the one or more processors cause the one or more processors to: receive a first model comprising a representation of a first environment; receive a first viewpoint associated with the first model; receive a first dynamic object data associated with a first dynamic object; simulate within the first model, from the first viewpoint, movement of the first dynamic object to generate first synthetic training data comprising a sequence of video frames depicting a simulation of the dynamic object interacting in the first environment; train, based on the first synthetic training data, a first machine learning model; receive an image comprising a representation of a second environment, wherein the second environment is different from the first environment; receive, a second viewpoint associated with the image; receive a second dynamic object data associated with a second dynamic object; generate, based on the first machine learning model, a second initial velocity, and the second viewpoint, movement data of the second dynamic object within the image; and superimpose, based on the movement data of the second dynamic object, the second dynamic object into the image.
16. The system of claim 15, wherein the instructions that when executed by the one or more processors further cause the one or more processors to: receive a third viewpoint associated with the first model, wherein the third and first viewpoints are different; and simulate within the first model, from the third viewpoint, the first dynamic object moving to generate second synthetic training data.
17. The system of claim 15, wherein the instructions that when executed by the one or more processors further cause the one or more processors to: receive a third model comprising a representation of a third environment; receive a fourth viewpoint associated with the third model; and simulate within the third model, from the fourth viewpoint, the first dynamic object moving to generate third synthetic training data.
18. The system of claim 15, wherein the first environment is an indoor environment and the second environment is a different indoor environment.
19. The system of claim 15, wherein the first model is a three-dimensional model.
20. The system of claim 15, wherein the movement data indicates the first dynamic object interacting with one or more scene objects within the first model.
</claims>
</document>
