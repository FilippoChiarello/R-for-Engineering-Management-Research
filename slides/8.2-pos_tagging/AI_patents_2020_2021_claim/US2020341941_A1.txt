<document>

<filing_date>
2019-04-26
</filing_date>

<publication_date>
2020-10-29
</publication_date>

<priority_date>
2019-04-26
</priority_date>

<ipc_classes>
G06F13/42,G06F15/78,G06F3/06,G06F9/54,G06N20/00,H04L12/66
</ipc_classes>

<assignee>
XILINX
</assignee>

<inventors>
DASTIDAR, JAIDEEP
MITTAL, MILLIND
</inventors>

<docdb_family_id>
70476541
</docdb_family_id>

<title>
MACHINE LEARNING MODEL UPDATES TO ML ACCELERATORS
</title>

<abstract>
Examples herein describe a peripheral I/O device with a hybrid gateway that permits the device to have both I/O and coherent domains. As a result, the compute resources in the coherent domain of the peripheral I/O device can communicate with the host in a similar manner as CPU-to-CPU communication in the host. The dual domains in the peripheral I/O device can be leveraged for machine learning (ML) applications. While an I/O device can be used as an ML accelerator, these accelerators previously only used an I/O domain. In the embodiments herein, compute resources can be split between the I/O domain and the coherent domain where a ML engine is in the I/O domain and a ML model is in the coherent domain. An advantage of doing so is that the ML model can be coherently updated using a reference ML model stored in the host.
</abstract>

<claims>
1. A peripheral I/O device, comprising: a hybrid gateway configured to communicatively couple the peripheral I/O device to a host; I/O logic comprising a machine learning (ML) engine assigned to an I/O domain; and coherent logic comprising a ML model assigned to a coherent domain, wherein the ML model shares the coherent domain with compute resources in the host.
2. The peripheral I/O device of claim 1, wherein the hybrid gateway is configured to use a coherent interconnect protocol to extend the coherent domain of the host into the peripheral I/O device.
3. The peripheral I/O device of claim 2, wherein the hybrid gateway comprises an update agent configured to use a cache-coherent shared-memory multiprocessor paradigm to update the ML model in response to changes made in a reference ML model stored in memory associated with the host.
4. The peripheral I/O device of claim 3, wherein using the cache-coherent shared-memory multiprocessor paradigm to update the ML model results in only a portion of the ML model being updated when the reference ML model is updated.
5. The peripheral I/O device of claim 1, further comprising: a NoC coupled to the I/O logic and the coherent logic, wherein at least one of the NoC, programmable logic (PL)-to-PL messages, and wire signaling is configured to permit parameters associated with a layer in the ML model to be transferred from the coherent logic to the I/O logic.
6. The peripheral I/O device of claim 5, wherein the ML engine is configured to process a ML data set received from the host using the parameters in the ML model.
7. The peripheral I/O device of claim 1, further comprising: a programmable logic (PL) array, wherein a first plurality of PL blocks in the PL array are part of the I/O logic and are assigned to the I/O domain and a second plurality of PL blocks in the PL array are part of the coherent logic and are assigned to the coherent domain.
8. The peripheral I/O device of claim 7, further comprising: a plurality of memory blocks, wherein a first subset of the plurality of memory blocks are part of the I/O logic and are assigned to the I/O domain and a second subset of the plurality of memory blocks are part of the coherent logic and are assigned to the coherent domain, wherein the first subset of the plurality of memory blocks can communicate with the first plurality of PL blocks but not directly communicate with the second plurality of PL blocks and the second subset of the plurality of memory blocks can communicate with the second plurality of PL blocks but not directly communicate with the first plurality of PL blocks.
9. A computing system, comprising: a host comprising: a memory storing a reference ML model, and a plurality of CPUs forming, along with the memory, a coherent domain; and a peripheral I/O device, comprising: I/O logic comprising a ML engine assigned to an I/O domain, and coherent logic comprising a ML model assigned to the coherent domain along with the memory and the plurality of CPUs in the host.
10. The computing system of claim 9, wherein the peripheral I/O device is configured to use a coherent interconnect protocol to extend the coherent domain into the peripheral I/O device.
11. The computing system of claim 10, wherein the peripheral I/O device comprises an update agent configured to use a cache-coherent shared-memory multiprocessor paradigm to update the ML model in response to changes made in the reference ML model stored in the memory.
12. The computing system of claim 11, wherein using the cache-coherent shared-memory multiprocessor paradigm to update the ML model results in only a portion of the ML model being updated when the reference ML model is updated.
13. The computing system of claim 12, wherein the host comprises multiple reference ML models stored in the memory, and wherein the peripheral I/O device comprises: multiple ML models in the coherent domain that correspond to the multiple reference ML models, and multiple ML engines assigned to the I/O domain, wherein the multiple ML engines are configured to execute independently of each other.
14. The computing system of claim 9, wherein the peripheral I/O device comprises: a programmable logic (PL) array, wherein a first plurality of PL blocks in the PL array are part of the I/O logic and are assigned to the I/O domain and a second plurality of PL blocks in the PL array are part of the coherent logic and are assigned to the coherent domain.
15. The computing system of claim 14, wherein the peripheral I/O device comprises: a plurality of memory blocks, wherein a first subset of the plurality of memory blocks are part of the I/O logic and are assigned to the I/O domain and a second subset of the plurality of memory blocks are part of the coherent logic and are assigned to the coherent domain, wherein the first subset of the plurality of memory blocks can communicate with the first plurality of PL blocks but not directly communicate with the second plurality of PL blocks and the second subset of the plurality of memory blocks can communicate with the second plurality of PL blocks but not directly communicate with the first plurality of PL blocks.
16. A method, comprising: updating a subportion of a reference ML model in memory associated with a host; updating a subset of a cached ML model in coherent logic associated with a peripheral I/O device coupled to the host, wherein the memory of the host and the coherent logic of the peripheral I/O device are in a same coherent domain; retrieving the updated subset of the cached ML model from the coherent domain; and processing a ML data set according to parameters in the retrieved subset of the cached ML model using an ML engine, wherein the ML engine is in I/O logic in the peripheral I/O device assigned to an I/O domain.
17. The method of claim 16, wherein updating the subset of the cached ML model is performed using a coherent interconnect protocol that extends the coherent domain of the host into the peripheral I/O device.
18. The method of claim 16, wherein updating the subset of the cached ML model is performed using a cache-coherent shared-memory multiprocessor paradigm in response to changes made in the reference ML model.
19. The method of claim 18, wherein using the cache-coherent shared-memory multiprocessor paradigm to update the cached ML model results in only a portion of the cached ML model being updated when the reference ML model is updated.
20. The method of claim 16, retrieving the updated subset of the cached ML model from the coherent domain is performed using at least one of a NoC, PL-to-PL messages, and wire signaling that communicatively couples the coherent logic assigned to the coherent domain to the I/O logic assigned to the I/O domain.
</claims>
</document>
