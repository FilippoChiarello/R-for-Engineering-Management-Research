<document>

<filing_date>
2019-02-14
</filing_date>

<publication_date>
2020-12-31
</publication_date>

<priority_date>
2018-02-16
</priority_date>

<ipc_classes>
G06F40/42,G06N3/04,G06N3/08,G10L13/02,G10L25/30
</ipc_classes>

<assignee>
DOLBY LABORATORIES LICENSING CORPORATION
</assignee>

<inventors>
HORGAN, Michael Getty
MORALES, Jaime H.
KUMAR, Vivek
VASCO, Cristina Michel
ZHOU, Cong
</inventors>

<docdb_family_id>
66102176
</docdb_family_id>

<title>
SPEECH STYLE TRANSFER
</title>

<abstract>
Computer-implemented methods for speech synthesis are provided. A speech synthesizer may be trained to generate synthesized audio data that corresponds to words uttered by a source speaker according to speech characteristics of a target speaker. The speech synthesizer may be trained by time-stamped phoneme sequences, pitch contour data and speaker identification data. The speech synthesizer may include a voice modeling neural network and a conditioning neural network.
</abstract>

<claims>
1. A computer-implemented audio processing method, comprising: training a speech synthesizer, wherein the training comprises: (a) receiving, by a content extraction process implemented via a control system comprising one or more processors and one or more non-transitory storage media, first audio data corresponding to first speech of a first person; (b) producing, by the content extraction process, a first time-stamped phoneme sequence and first pitch contour data corresponding to the first speech; (c) receiving, by a first neural network implemented via the control system, the first time-stamped phoneme sequence and the first pitch contour data; (d) producing, by the first neural network, first neural network output corresponding to the first time-stamped phoneme sequence and the first pitch contour data, the first neural network output comprising a plurality of frame sizes; (e) receiving, by a second neural network implemented via the control system, the first neural network output, the second neural network comprising a hierarchy of modules, each module operating at a different temporal resolution, wherein the first neural network has produced the first neural network output such that each of the plurality of frame sizes of the first neural network output corresponds to a temporal resolution of a module of the second neural network; (f) generating, by the second neural network, first predicted audio signals; (g) comparing, via the control system, the first predicted audio signals to first test data, the test data being audio data corresponding to speech of the first person; (h) determining, via the control system, a loss function value for the first predicted audio signals; and (i) repeating (a) through (h) until a difference between a current loss function value for the first predicted audio signals and a prior loss function value for the first predicted audio signals is less than or equal to a predetermined value, wherein repeating (f) comprises changing a physical state of at least one non-transitory storage medium location corresponding with at least one weight of the second neural network.
2. The audio processing method of claim 1, wherein (a) further comprises receiving first time-stamped text corresponding to the first speech of the first person.
3. The audio processing method of claim 1, wherein (a) further comprises receiving first identification data corresponding to the first person.
4. The audio processing method of claim 3, further comprising controlling the speech synthesizer for speech generation, wherein the speech generation comprises: (j) receiving, by the content extraction process, second audio data corresponding to second speech of a second person, second time-stamped text corresponding to the second speech and first identification data corresponding to speech of the first person; (k) producing, by the content extraction process, a second time-stamped phoneme sequence and second pitch contour data corresponding to the second speech; (l) receiving, by the first neural network, the second time-stamped phoneme sequence and the second pitch contour data of (k); (m) producing, by the first neural network, first neural network output corresponding to the second time-stamped phoneme sequence and the second pitch contour data of (k); (n) receiving, by the second neural network, the first neural network output of (m) and the first identification data; and (o) generating, by the second neural network, synthesized audio data corresponding to the first neural network output of (m) and the first identification data.
5. The audio processing method of claim 4, wherein the synthesized audio data corresponds to words uttered by the second person according to speech characteristics of the first person.
6. The audio processing method of claim 5, wherein the training involves receiving the first audio data in a first language and wherein the synthesized audio data corresponds to words uttered by the second person in a second language.
7. The audio processing method of claim 4, further comprising causing one or more transducers to reproduce the synthesized audio data.
8. The audio processing method of claim 4, wherein the training further comprises: receiving, by a third neural network, the first audio data; and training the third neural network to determine first speech characteristics corresponding to speech of the first person and to output encoded audio data.
9. The audio processing method of claim 8, wherein the training further comprises training a fourth neural network to determine whether the encoded audio data corresponds to speech of the first person.
10. The audio processing method of claim 9, wherein the speech generation further comprises: receiving, by the third neural network, the second audio data; generating, by the third neural network, second encoded audio data corresponding to the second audio data; receiving, by the fourth neural network, the second encoded audio data; generating modified second encoded audio data via an iterative process until the fourth neural network determines that the modified second encoded audio data corresponds to speech of the first person and, after the fourth neural network determines that the modified second encoded audio data corresponds to speech of the first person, providing the modified second encoded audio data to the second neural network.
11. The audio processing method of claim 1, wherein repeating (a) through (h) involves training at least one of the first neural network or the second neural network via backward propagation based on a current loss function value.
12. The audio processing method of claim 1, wherein the first neural network comprises a bi-directional recurrent neural network.
13. A speech synthesizing apparatus, comprising: an interface system; and a control system comprising one or more processors and one or more non-transitory storage media operatively coupled to the one or more processors, the control system configured for implementing a speech synthesizer, the speech synthesizer including a content extractor, a first neural network and a second neural network, the first neural network comprising a bi-directional recurrent neural network, the second neural network comprising a hierarchy of modules, each module operating at a different temporal resolution, the first neural network and the second neural network having been trained according to a process comprising: (a) receiving, via the interface system and by the content extractor, first audio data corresponding to first speech of a first person; (b) producing, by the content extractor, a first time-stamped phoneme sequence and first pitch contour data corresponding to the first speech; (c) receiving, by the first neural network, the first time-stamped phoneme sequence and the first pitch contour data; (d) producing, by the first neural network, first neural network output corresponding to the first time-stamped phoneme sequence and the first pitch contour data, the first neural network output comprising a plurality of frame sizes, each of the frame sizes corresponding with a temporal resolution of a module of the second neural network; (e) receiving, by the second neural network, the first neural network output; (f) generating, by the second neural network, first predicted audio signals; (g) comparing the first predicted audio signals to first test data, the test data being audio data corresponding to speech of the first person; (h) determining a loss function value for the first predicted audio signals; and (i) repeating (a) through (h) until a difference between a current loss function value for the first predicted audio signals and a prior loss function value for the first predicted audio signals is less than or equal to a predetermined value, wherein the control system is configured for controlling the speech synthesizer module for speech generation and wherein the speech generation comprises: (j) receiving, by the content extractor and via the interface system, second audio data corresponding to second speech of a second person, second time-stamped text corresponding to the second speech and first identification data corresponding to speech of the first person; (k) producing, by the content extractor, a second time-stamped phoneme sequence and second pitch contour data corresponding to the second speech; (l) receiving, by the first neural network, the second time-stamped phoneme sequence and the second pitch contour data of (k); (m) producing, by the first neural network, first neural network output corresponding to the second time-stamped phoneme sequence and the second pitch contour data of (k); (n) receiving, by the second neural network, the first neural network output of (m) and the first identification data; and (o) generating, by the second neural network, synthesized audio data corresponding to the first neural network output of (m) and the first identification data.
14. The speech synthesizing apparatus of claim 13, wherein the synthesized audio data corresponds to words uttered by the second person according to speech characteristics of the first person.
15. The speech synthesizing apparatus of claim 14, wherein the training involves receiving the first audio data in a first language and wherein the synthesized audio data corresponds to words uttered by the second person in a second language.
16. The speech synthesizing apparatus of claim 13, wherein the control system is configured for causing one or more transducers to reproduce the second synthesized audio data.
17. The speech synthesizing apparatus of claim 13, wherein generating the synthesized audio data comprises changing a physical state of at least one non-transitory storage medium location corresponding with at least one weight of the second neural network.
18. A speech synthesizing apparatus, comprising: an interface system; and a control system comprising one or more processors and one or more non-transitory storage media operatively coupled to the one or more processors, the control system configured for implementing a speech synthesizer, the speech synthesizer including a content extractor, a first neural network and a second neural network, the first neural network comprising a bi-directional recurrent neural network, the second neural network comprising a hierarchy of modules, each module operating at a different temporal resolution, the second neural network having been trained to generate first synthesized audio data corresponding to first speech of a first speaker by means of an audio processing method according to claim 1, wherein the control system is configured for controlling the speech synthesizer for: (a) receiving, by the content extractor and via the interface system, second audio data corresponding to second speech of a second person, second time-stamped text corresponding to the second speech and first identification data corresponding to speech of the first person; (b) producing, by the content extractor, a second time-stamped phoneme sequence and second pitch contour data corresponding to the second speech; (c) receiving, by the first neural network, the second time-stamped phoneme sequence and the second pitch contour data of (b); (d) producing, by the first neural network, first neural network output corresponding to the second time-stamped phoneme sequence and the second pitch contour data of (b), the first neural network output comprising a plurality of frame sizes, each of the frame sizes corresponding with a temporal resolution of a module of the second neural network; (e) receiving, by the second neural network, the first neural network output of (d) and the first identification data; and (f) generating, by the second neural network, second synthesized audio data corresponding to the first neural network output of (d) and the first identification data.
19. The speech synthesizing apparatus of claim 18, wherein the second synthesized audio data corresponds to words uttered by the second person according to speech characteristics of the first person.
20. The speech synthesizing apparatus of claim 19, wherein the training involves receiving the first audio data in a first language and wherein the second synthesized audio data corresponds to words uttered by the second person in a second language.
21. The speech synthesizing apparatus of claim 18, further comprising causing one or more transducers to reproduce the second synthesized audio data.
22. The speech synthesizing apparatus of claim 18, wherein generating the synthesized audio data comprises changing a physical state of at least one non-transitory storage medium location corresponding with at least one weight of the second neural network.
23. A speech synthesizing apparatus, comprising: an interface system; and a control system comprising one or more processors and one or more non-transitory storage media operatively coupled to the one or more processors, the control system configured for implementing a speech synthesizer, the speech synthesizer including a content extractor, a first neural network and a second neural network, the first neural network comprising a bi-directional recurrent neural network, the second neural network comprising a hierarchy of modules, each module operating at a different temporal resolution, the first neural network and the second neural network having been trained according to a process comprising: (a) receiving, via the interface system and by the content extractor, first audio data corresponding to first speech of a target speaker; (b) producing, by the content extractor, a first time-stamped phoneme sequence and first pitch contour data corresponding to the first speech; (c) receiving, by the first neural network, the first time-stamped phoneme sequence and the first pitch contour data; (d) producing, by the first neural network, first neural network output corresponding to the first time-stamped phoneme sequence and the first pitch contour data, the first neural network output comprising a plurality of frame sizes, each of the frame sizes corresponding with a temporal resolution of a module of the second neural network; (e) receiving, by the second neural network, the first neural network output; (f) generating, by the second neural network, first predicted audio signals; (g) comparing the first predicted audio signals to first test data; (h) determining a loss function value for the first predicted audio signals; and (i) repeating (a) through (h) until a difference between a current loss function value for the first predicted audio signals and a prior loss function value for the first predicted audio signals is less than or equal to a predetermined value, wherein the control system is configured for controlling the speech synthesizer module for speech generation and wherein the speech generation comprises: (j) receiving, by the content extractor and via the interface system, second audio data corresponding to second speech of a source speaker, second time-stamped text corresponding to the second speech and first identification data corresponding to speech of the target speaker; (k) producing, by the content extractor, a second time-stamped phoneme sequence and second pitch contour data corresponding to the second speech; (l) receiving, by the first neural network, the second time-stamped phoneme sequence and the second pitch contour data of (k); (m) producing, by the first neural network, first neural network output corresponding to the second time-stamped phoneme sequence and the second pitch contour data of (k); (n) receiving, by the second neural network, the first neural network output of (m) and the first identification data; and (o) generating, by the second neural network, synthesized audio data corresponding to the first neural network output of (m) and the first identification data.
24. The speech synthesizing apparatus of claim 23, wherein the synthesized audio data corresponds to words uttered by the source speaker according to speech characteristics of the target speaker.
25. The speech synthesizing apparatus of claim 23, wherein the target speaker and the source speaker are the same person at different ages.
26. The speech synthesizing apparatus of claim 23, wherein the first speech of the target speaker corresponds to speech of a person at a first age, or during a range of ages that includes the first age, and the second speech of the source speaker corresponds to speech of the person at a second age.
27. The speech synthesizing apparatus of claim 23, wherein the first age is a younger age than the second age.
</claims>
</document>
