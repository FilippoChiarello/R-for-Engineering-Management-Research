<document>

<filing_date>
2019-03-01
</filing_date>

<publication_date>
2020-09-03
</publication_date>

<priority_date>
2019-03-01
</priority_date>

<ipc_classes>
G06F17/16,G06F9/30,G06N3/063
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
FOWERS, JEREMY
LO, DANIEL
DANGWAL, DEEKSHA
</inventors>

<docdb_family_id>
69846568
</docdb_family_id>

<title>
DERIVING A CONCORDANT SOFTWARE NEURAL NETWORK LAYER FROM A QUANTIZED FIRMWARE NEURAL NETWORK LAYER
</title>

<abstract>
Systems and methods for deriving a concordant software neural network layer are provided. A method includes receiving first instructions configured to, using a neural network processor (NNP), process a first set of data corresponding to a neural network layer, where the NNP is configured to quantize the first set of the data to generate a set of quantized data and then perform matrix-vector multiply operations on the set of quantized data using a matrix-vector-multiplier incorporated within hardware associated with the NNP to generate a first set of results. The method further includes processing the first instructions to automatically generate second instructions configured for use with at least one processor, different from the NNP, such that the second instructions, when executed by the at least one processor to perform matrix multiply operations, generate a second set of results that are concordant with the first set of results.
</abstract>

<claims>
1. A method comprising: receiving first instructions configured to, using a neural network processor, process a first set of data corresponding to a neural network layer, wherein the neural network processor is configured to quantize the first set of the data to generate a set of quantized data and then perform matrix-vector multiply operations on the set of quantized data using a matrix-vector-multiplier incorporated within hardware associated with the neural network processor to generate a first set of results; and processing the first instructions to automatically generate second instructions configured for use with at least one processor, different from the neural network processor, such that the second instructions, when executed by the at least one processor to perform matrix multiply operations corresponding to the neural network layer, generate a second set of results that are concordant with the first set of results.
2. The method of claim 1, wherein the processing the first instructions to automatically generate second instructions further comprises extracting information concerning dependencies between the matrix-vector multiply operations and operations selected from among a softmax operation, a ReLU operation, or an addition operation.
3. The method of claim 1, wherein the first set of data is represented in a first precision format having a first precision and the set of quantized data is represented in a second precision format having a second precision lower than the first precision.
4. The method of claim 3, wherein the first precision format comprises floating point format, and wherein the second precision format comprises a precision format selected from one of an integer format, a reduced floating point precision format, or a block floating point format.
5. The method of claim 1, wherein the set of quantized data comprises a set of quantized training data for use with operations associated with the second instructions.
6. The method of claim 1, wherein the first set of data is organized in an N by N matrix form, and wherein N is an integer greater than 1 and N is a native dimension associated with the matrix-vector-multiplier, and wherein the processing the first instructions to automatically generate the second instructions comprises transforming the first set of data from the N by N matrix form to another form suitable for use with the second instructions.
7. The method of claim 1, wherein the processing the first instructions to automatically generate second instructions comprises transforming a form of the first set of data to another form suitable for use with the second instructions.
8. A system comprising: at least one processor; and a memory comprising: first instructions configured to, using a neural network processor having a matrix-vector-multiplier incorporated within hardware associated with the neural network processor and a multi-function unit incorporated within the hardware associated with the neural network processor, process a first set of data corresponding to a neural network layer, wherein the neural network processor is configured to quantize the first set of data to generate a first set of quantized data and then: (1) perform matrix operations on the first set of quantized data, using the matrix-vector-multiplier incorporated within the hardware associated with the neural network processor, to generate a first set of output data, (2) quantize the first set of output data to generate a first set of quantized output data, and (3) perform scalar operations, using the multi-function unit incorporated within the hardware associated with the neural network processor, on the first set of quantized output data to generate a second set of output data; and second instructions configured to process the first instructions to generate third instructions configured for use with the at least one processor, different from the neural network processor, wherein the third instructions comprise instructions for performing matrix multiply operations and instructions for performing scalar operations to process the neural network layer.
9. The system of claim 8, wherein the second instructions are further configured to extract information concerning dependencies between the matrix-vector multiply operations and operations selected from among a softmax operation, a ReLU operation, or an addition operation.
10. The system of claim 8, wherein the first set of data is represented in a first precision format having a first precision, and wherein each of the first set of quantized data and the first set of quantized output data is represented in a second precision format having a second precision lower than the first precision.
11. The system of claim 10, wherein the first precision format comprises floating point format, and wherein the second precision format comprises a precision format selected from one of an integer format, a reduced floating point precision format, or a block floating point format.
12. The system of claim 8, wherein the set of quantized data comprises a set of quantized training data for use with operations associated with the second instructions.
13. The system of claim 8, wherein the first set of data is organized in an N by N matrix form, and wherein N is an integer greater than 1 and N is a native dimension associated with the matrix-vector-multiplier, and wherein the second instructions further comprise instructions configured to transform the first set of data from the N by N matrix form to another form suitable for use with the third instructions.
14. The system of claim 8, wherein the second instructions further comprise instructions configured to transform a form of the first set of data to another form suitable for use with the third instructions.
15. A non-transitory computer-readable medium comprising code corresponding to a method, the method comprising: receiving first instructions configured to, using a neural network processor, process a first set of data corresponding to a neural network layer, wherein the neural network processor is configured to quantize the first set of the data to generate a set of quantized data and then perform matrix-vector multiply operations on the set of quantized data using a matrix-vector-multiplier incorporated within hardware associated with the neural network processor to generate a first set of results; and processing the first instructions to automatically generate second instructions configured for use with at least one processor, different from the neural network processor, such that the second instructions, when executed by the at least one processor to perform matrix multiply operations corresponding to the neural network layer, generate a second set of results that are concordant with the first set of results.
16. The non-transitory computer-readable medium of claim 15, wherein the processing the first instructions to automatically generate second instructions further comprises extracting information concerning dependencies between the matrix-vector multiply operations and operations selected from among a softmax operation, a ReLU operation, or an addition operation.
17. The non-transitory computer-readable medium of claim 15, wherein the first set of data is represented in a first precision format having a first precision and the set of quantized data is represented in a second precision format having a second precision lower than the first precision.
18. The non-transitory computer-readable medium of claim 17, wherein the first precision format comprises floating point format, and wherein the second precision format comprises a precision format selected from one of an integer format, a reduced floating point precision format, or a block floating point format.
19. The non-transitory computer-readable medium of claim 15, wherein the set of quantized data comprises a quantized set of training data for use with operations associated with the second instructions.
20. The non-transitory computer-readable medium of claim 15, wherein the first set of data is organized in an N by N matrix form, and wherein N is an integer greater than 1 and N is a native dimension associated with the matrix-vector-multiplier, and wherein the processing the first instructions to automatically generate the second instructions comprises transforming the first set of data from the N by N matrix form to another form suitable for use with the second instructions.
</claims>
</document>
