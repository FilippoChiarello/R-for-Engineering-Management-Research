<document>

<filing_date>
2020-05-28
</filing_date>

<publication_date>
2020-09-17
</publication_date>

<priority_date>
2018-01-24
</priority_date>

<ipc_classes>
G06F40/169,G06K9/00,G06K9/62,H04L29/06,H04N19/136,H04N19/46,H04N19/85,H04N21/234,H04N21/84
</ipc_classes>

<assignee>
TENCENT TECHNOLOGY (SHENZHEN) COMPANY
</assignee>

<inventors>
LIU WEI
MA, LIN
JIANG, Wen Hao
WANG, Jing Wen
</inventors>

<docdb_family_id>
67365558
</docdb_family_id>

<title>
VIDEO DESCRIPTION GENERATION METHOD AND APPARATUS, VIDEO PLAYING METHOD AND APPARATUS, AND STORAGE MEDIUM
</title>

<abstract>
The present disclosure discloses a video description generation method and apparatus, a video playing method and apparatus, and a computer-readable storage medium. The method includes: extracting video features, and obtaining a video feature sequence corresponding to video encoding moments in a video stream; encoding the video feature sequence by using a forward recurrent neural network and a backward recurrent neural network, to obtain a forward hidden state sequence and a backward hidden state sequence corresponding to each video encoding moment; and positioning, according to the forward hidden state sequence and the backward hidden state sequence, an event corresponding to each video encoding moment and an interval corresponding to the event at the video encoding moment, thereby predicting a video content description of the event. On the basis of distinguishing overlapping events, the interval corresponding to the event is introduced to predict and generate a word corresponding to the event at the video encoding moment, and events that overlap at the video encoding moment correspond to different intervals, so that the video content descriptions of events at this video encoding moment have a high degree of distinction. By analogy, events in the given video stream can be described more distinctively.
</abstract>

<claims>
1. A video description generation method, comprising: extracting video features, and obtaining a video feature sequence corresponding to video encoding moments in a video stream; encoding the video feature sequence by using a forward recurrent neural network and a backward recurrent neural network, to obtain a forward hidden state sequence and a backward hidden state sequence corresponding to each video encoding moment; positioning, according to the forward hidden state sequence and the backward hidden state sequence, an event corresponding to each video encoding moment and an interval corresponding to the event at the video encoding moment; predicting a video content description of the event according to the interval corresponding to the event at the video encoding moment, and generating a word corresponding to the event at the video encoding moment; and constructing a natural language description of the event in the video stream by using all words of the event that correspond to the video encoding moments.
2. The method according to claim 1, wherein the extracting video features, and obtaining a video feature sequence corresponding to video encoding moments in a video stream comprises: extracting a video feature of the video stream at each video encoding moment according to a specified time interval, the video features corresponding to the video encoding moments sequentially forming the video feature sequence.
3. The method according to claim 2, wherein the extracting a video feature of the video stream at each video encoding moment according to a specified time interval, the video features corresponding to the video encoding moments sequentially forming the video feature sequence comprises: performing, according to a specified time interval, video feature extraction on the video stream at each video encoding moment by using a sliding window, the video features corresponding to the video encoding moments sequentially forming the video feature sequence.
4. The method according to claim 1, wherein the positioning, according to the forward hidden state sequence and the backward hidden state sequence, an event corresponding to each video encoding moment and an interval corresponding to the event at the video encoding moment comprises: performing forward prediction and backward prediction by using the forward hidden state sequence and the backward hidden state sequence, to obtain forward candidate intervals and credibilities corresponding to the forward candidate intervals as well as backward candidate intervals and credibilities corresponding to the backward candidate intervals for each video encoding moment; and performing, for each video encoding moment, merge prediction on the forward candidate intervals and the backward candidate intervals according to the corresponding credibilities, a merge credibility obtained by the merge prediction positioning an event corresponding to the video encoding moment, and a forward candidate interval and a backward candidate interval that are predicted to be merged corresponding to the event forming an interval corresponding to the event at the video encoding moment.
5. The method according to claim 4, wherein the performing forward prediction and backward prediction by using the forward hidden state sequence and the backward hidden state sequence, to obtain forward candidate intervals and credibilities corresponding to the forward candidate intervals as well as backward candidate intervals and credibilities corresponding to the backward candidate intervals for each video encoding moment comprises: performing forward prediction and backward prediction on the forward hidden state sequence and the backward hidden state sequence by using a layer of fully connected network, the forward prediction being predicting, in the forward hidden state sequence by using a layer of fully connected network, several intervals forward for each forward hidden state at a corresponding video encoding moment, the intervals obtained through prediction being forward candidate intervals; mapping, for each forward candidate interval by performing a non-linear transformation operation, a forward hidden state to a space after passing through a neuron, and activating the forward hidden state after performing weighting and offset processing in the space, to obtain a credibility; and performing the operation on each backward candidate interval correspondingly, to obtain a corresponding credibility.
6. The method according to claim 1, wherein the predicting a video content description of the event according to the interval corresponding to the event at the video encoding moment, and generating a word corresponding to the event at the video encoding moment comprises: performing, in the interval corresponding to the event at the video encoding moment, event feature fusion detection for the event to obtain an event feature corresponding to the event at the video encoding moment; and decoding the video encoding moment by using the event feature corresponding to the event at the video encoding moment and context information mapped by the interval, to generate the word corresponding to the event at the video encoding moment.
7. The method according to claim 6, wherein before the decoding the video encoding moment by using the event feature corresponding to the event at the video encoding moment and context information mapped by the interval, to generate the word corresponding to the event at the video encoding moment, the predicting a video content description of the event according to the interval corresponding to the event at the video encoding moment, and generating a word corresponding to the event at the video encoding moment further comprises: obtaining, from the interval corresponding to the event at the video encoding moment, a front video encoding moment and a rear video encoding moment that are located at two ends of the interval; and forming the corresponding context information by using a forward hidden state corresponding to the front video encoding moment and a backward hidden state corresponding to the rear video encoding moment.
8. The method according to claim 6, wherein the performing, in the interval corresponding to the event at the video encoding moment, event feature fusion detection for the event to obtain an event feature corresponding to the event at the video encoding moment comprises: detecting visual features in the interval corresponding to the event at the video encoding moment; and integrating, under an attention mechanism, the visual features according to the context information mapped by the interval, to obtain the event feature corresponding to the event at the video encoding moment.
9. The method according to claim 8, wherein the integrating, under an attention mechanism, the visual features according to the context information mapped by the interval, to obtain the event feature corresponding to the event at the video encoding moment comprises: generating, relative to the event with the assistance of the context information mapped by the interval, state information of a word corresponding to a previous video encoding moment, and calculating a correlation between the state information and the visual features; calculating a weight of each visual feature of the event in the corresponding interval according to a correlation between the visual features and predicted video content of the event at the previous video encoding moment; and integrating the visual features of the event in the corresponding interval according to the corresponding weights, to generate the event feature corresponding to the event at the video encoding moment.
10. The method according to claim 6, wherein the decoding the video encoding moment by using the event feature corresponding to the event at the video encoding moment and context information mapped by the interval, to generate the word corresponding to the event at the video encoding moment comprises: mapping, to a same logical space, the event feature corresponding to the event at the video encoding moment and the context information mapped from the corresponding interval; performing a gating operation on the event feature and the context information that are mapped to the same logical space, to adaptively construct a feature inputted to a decoder by the event at the video encoding moment; and predicting and generating, by the decoder, the word corresponding to the event at the video encoding moment by using the feature inputted to the decoder by the event at the video encoding moment through the gating operation.
11. The method according to claim 10, wherein the performing a gating operation on the event feature and the context information that are mapped to the same logical space, to adaptively construct a feature inputted to a decoder by the event at the video encoding moment comprises: performing, in combination with a word generated from predicted video content of the event and state information at a previous video encoding moment, non-linear transformation on the event feature and the context information that are mapped to the same logical space, to generate a gated output value; and adaptively adjusting, by using the gated output value, the event feature and the context information that are mapped to the same logical space, a feature that is obtained after the adjustment of the event feature and the context information forming the feature inputted to the decoder by the event at the video encoding moment.
12. A video description generation apparatus, comprising: a feature extraction module, configured to extract video features, and obtain a video feature sequence corresponding to video encoding moments in a video stream; an encoding module, configured to encode the video feature sequence by using a forward recurrent neural network and a backward recurrent neural network, to obtain a forward hidden state sequence and a backward hidden state sequence corresponding to each video encoding moment; a positioning module, configured to position, according to the forward hidden state sequence and the backward hidden state sequence, an event corresponding to each video encoding moment and an interval corresponding to the event at the video encoding moment; a description prediction module, configured to predict a video content description of the event according to the interval corresponding to the event at the video encoding moment, and generate a word corresponding to the event at the video encoding moment; and a description construction module, configured to construct a natural language description of the event in the video stream by using all words of the event that correspond to the video encoding moments.
13. The apparatus according to claim 12, wherein the positioning module comprises: a bidirectional prediction unit, configured to perform forward prediction and backward prediction by using the forward hidden state sequence and the backward hidden state sequence, to obtain forward candidate intervals and credibilities corresponding to the forward candidate intervals as well as backward candidate intervals and credibilities corresponding to the backward candidate intervals for each video encoding moment; and a merge prediction unit, configured to perform, for each video encoding moment, merge prediction on the forward candidate intervals and the backward candidate intervals according to the corresponding credibilities, a merge credibility obtained by the merge prediction positioning an event corresponding to the video encoding moment, and a forward candidate interval and a backward candidate interval that are predicted to be merged corresponding to the event forming an interval corresponding to the event at the video encoding moment.
14. The apparatus according to claim 12, wherein the description prediction module comprises: a feature fusion unit, configured to perform, in the interval corresponding to the event at the video encoding moment, event feature fusion detection for the event to obtain an event feature corresponding to the event at the video encoding moment; and a decoding unit, configured to decode the video encoding moment by using the event feature corresponding to the event at the video encoding moment and context information mapped by the interval, to generate the word corresponding to the event at the video encoding moment.
15. The apparatus according to claim 14, wherein the feature fusion unit comprises: a feature detection subunit, configured to detect visual features in the interval corresponding to the event at the video encoding moment; and a feature integration subunit, configured to integrate, under an attention mechanism, the visual features according to the context information mapped by the interval, to obtain the event feature corresponding to the event at the video encoding moment.
16. A computer-readable storage medium, storing a computer program, the computer program, when executed by a processor, is configured to cause the processor to: obtain during playback of a video, a video feature sequence corresponding to each video encoding moment in video description generation, and determine a forward hidden state sequence and a backward hidden state sequence corresponding to each video encoding moment; position, according to the forward hidden state sequence and the backward hidden state sequence, an event corresponding to each video encoding moment and an interval corresponding to the event at the video encoding moment; predict a video content description of the event according to the interval corresponding to the event at the video encoding moment, and instantly obtain a natural language description of the event at the video encoding moment in a video stream; and display, according to the video encoding moment and a progress of the played video, the corresponding natural language description for the event that occurs in played video content.
17. The computer-readable storage medium according to claim 16, wherein in order to cause the processor to position, according to the forward hidden state sequence and the backward hidden state sequence, an event corresponding to each video encoding moment and an interval corresponding to the event at the video encoding moment, the computer program, when executed by the processor, is configured to cause the processor to: perform forward prediction and backward prediction by using the forward hidden state sequence and the backward hidden state sequence, to obtain forward candidate intervals and credibilities corresponding to the forward candidate intervals as well as backward candidate intervals and credibilities corresponding to the backward candidate intervals for each video encoding moment; and perform, for each video encoding moment, merge prediction on the forward candidate intervals and the backward candidate intervals according to the corresponding credibilities, a merge credibility obtained by the merge prediction positioning an event corresponding to the video encoding moment, and a forward candidate interval and a backward candidate interval that are predicted to be merged corresponding to the event forming an interval corresponding to the event at the video encoding moment.
18. The computer-readable storage medium according to claim 17, wherein in order to cause the processor to perform forward prediction and backward prediction by using the forward hidden state sequence and the backward hidden state sequence, to obtain forward candidate intervals and credibilities corresponding to the forward candidate intervals as well as backward candidate intervals and credibilities corresponding to the backward candidate intervals for each video encoding moment, the computer program, when executed by the processor, is configured to cause the processor to: perform forward prediction and backward prediction on the forward hidden state sequence and the backward hidden state sequence by using a layer of fully connected network, the forward prediction being predicting, in the forward hidden state sequence by using a layer of fully connected network, several intervals forward for each forward hidden state at a corresponding video encoding moment, the intervals obtained through prediction being forward candidate intervals; map, for each forward candidate interval by performing a non-linear transformation operation, a forward hidden state to a space after passing through a neuron, and activate the forward hidden state after performing weighting and offset processing in the space, to obtain a credibility; and perform the operation on each backward candidate interval correspondingly, to obtain a corresponding credibility.
19. The computer-readable storage medium according to claim 16, wherein in order to cause the processor to predict a video content description of the event according to the interval corresponding to the event at the video encoding moment, the computer program, when executed by the processor, is configured to cause the processor to: perform, in the interval corresponding to the event at the video encoding moment, event feature fusion detection for the event to obtain an event feature corresponding to the event at the video encoding moment; and decode the video encoding moment by using the event feature corresponding to the event at the video encoding moment and context information mapped by the interval, to generate a word corresponding to the event at the video encoding moment.
20. The computer-readable storage medium according to claim 19, wherein before the processor decodes the video encoding moment by using the event feature corresponding to the event at the video encoding moment and context information mapped by the interval, to generate the word corresponding to the event at the video encoding moment, and in order cause the processor to predict a video content description of the event according to the interval corresponding to the event at the video encoding moment, and generate a word corresponding to the event at the video encoding moment, the computer program, when executed by the processor, is configured to cause the processor to: obtain, from the interval corresponding to the event at the video encoding moment, a front video encoding moment and a rear video encoding moment that are located at two ends of the interval; and form the corresponding context information by using a forward hidden state corresponding to the front video encoding moment and a backward hidden state corresponding to the rear video encoding moment.
</claims>
</document>
