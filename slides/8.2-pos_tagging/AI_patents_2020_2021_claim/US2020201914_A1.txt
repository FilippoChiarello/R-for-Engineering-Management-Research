<document>

<filing_date>
2020-03-02
</filing_date>

<publication_date>
2020-06-25
</publication_date>

<priority_date>
2017-04-19
</priority_date>

<ipc_classes>
G06F15/76,G06F16/9032,G06F40/42,G06N3/00,G06N3/04,G06N3/08,G06N7/00
</ipc_classes>

<assignee>
BROWN UNIVERSITY
</assignee>

<inventors>
WONG, LAWSON L.S.
KARAMCHETI, SIDDHARTH
GOPALAN, NAKUL
TELLEX, STEFANIE
ARUMUGAM, DILIP
</inventors>

<docdb_family_id>
63854624
</docdb_family_id>

<title>
INTERPRETING HUMAN-ROBOT INSTRUCTIONS
</title>

<abstract>
A system includes a robot having a module that includes a function for mapping natural language commands of varying complexities to reward functions at different levels of abstraction within a hierarchical planning framework, the function including using a deep neural network language model that learns how to map the natural language commands to reward functions at an appropriate level of the hierarchical planning framework.
</abstract>

<claims>
1. A system comprising: a robot comprising a module to interpret natural language commands, the module comprising a function for mapping natural language commands of varying complexities to reward functions at different levels of abstraction within a hierarchical planning framework, the function comprising: a deep neural network language model that learns how to map the natural language commands to reward functions at an appropriate level of the hierarchical planning framework.
2. The system of claim 1 wherein the Abstract Markov Decision Process comprises: a set of states that define an environment specified in an object-oriented fashion with object classes and attributes; a set of actions an agent can execute to transition between states or to invoke a lower-level AMDP subtask; a transition probability distribution over all possible next states given a current state and executed action; a numerical reward earned for a particular transition; a discount factor or effective time horizon under consideration; and a state projection function that maps lower-level states to higher-level (AMDP) states.
3. A system comprising: a robot comprising a module comprising a function for mapping natural language commands of varying complexities to reward functions at different levels of abstraction within a hierarchical planning framework, the function using a deep neural network language model that learns how to map the natural language commands to reward functions at an appropriate level of the hierarchical planning framework.
4. The system of claim 3 wherein the function further comprises an Abstract Markov Decision Process to represent a decision-making problem involving a hierarchy of the robot's states and actions.
5. The system of claim 4 wherein the Abstract Markov Decision Process comprises: a set of states that define an environment specified in an object-oriented fashion with object classes and attributes; a set of actions an agent can execute to transition between states or to invoke a lower-level AMDP subtask; a transition probability distribution over all possible next states given a current state and executed action; a numerical reward earned for a particular transition; a discount factor or effective time horizon under consideration; and a state projection function that maps lower-level states to higher-level (AMDP) states.
6. A method comprising: providing a mobile-manipulator robot; and providing a module that interprets and grounds natural language commands to a mobile-manipulator robot at multiple levels of abstraction, the module comprising a deep neural network language model that learns how to map the natural language commands to reward functions at an appropriate level of a hierarchical planning framework.
7. The method of claim 6 wherein the function further comprises an Abstract Markov Decision Process to represent a decision-making problem involving a hierarchy of the robot's states and actions.
8. The method of claim 7 wherein the Abstract Markov Decision Process comprises: a set of states that define an environment specified in an object-oriented fashion with object classes and attributes; a set of actions an agent can execute to transition between states or to invoke a lower-level AMDP subtask; a transition probability distribution over all possible next states given a current state and executed action; a numerical reward earned for a particular transition; a discount factor or effective time horizon under consideration; and a state projection function that maps lower-level states to higher-level (AMDP) states.
</claims>
</document>
