<document>

<filing_date>
2018-06-13
</filing_date>

<publication_date>
2020-07-21
</publication_date>

<priority_date>
2018-06-13
</priority_date>

<ipc_classes>
G06Q30/06,G10L13/04,G10L15/18,G10L15/22,G10L15/30
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
</assignee>

<inventors>
MELIS, ANTONIO
SERR, ROBERT WILLIAM
PULCIANI, ROBERT
POTHUKUCHI, MANIKYA PAVAN KIRAN
KAPILA, SAIPRASAD SATYA
RABUCHIN, STEVEN
</inventors>

<docdb_family_id>
71612000
</docdb_family_id>

<title>
Voice to voice natural language understanding processing
</title>

<abstract>
Techniques for providing a standardized voice user interface (VUI) that enables voice to voice natural language understanding (NLU) processing are described. The standardized VUI may be added to speechlets to enable a customer to interact with a business via server(s) using NLU processing. For example, a first user may initiate a first voice interaction using NLU processing with the server(s) and the system may initiate a second voice interaction using NLU processing between the server(s) and a second user. This enables a customer to initiate a transaction (e.g., request information, place an order, make a reservation, etc.) with the business using the speechlet. Thus, the business may use the speechlet to offer services without requiring additional infrastructure or complicated programming to implement.
</abstract>

<claims>
1. A computer-implemented method, the method comprising: receiving, from a first device associated with a user profile, first audio data corresponding to a first utterance; performing speech processing on the first audio data to determine first text data; determining that the first text data represents an order intended for a business; identifying business profile data corresponding to the business; determining, using the business profile data, that first data and second data are needed to complete the order; determining that a first portion of the first text data corresponds to the first data; determining second text data corresponding to a first request for the second data; performing text-to-speech (TTS) processing on the second text data to generate second audio data; sending, to the first device, the second audio data; receiving, from the first device, third audio data corresponding to a second utterance; performing speech processing on the third audio data to determine third text data; determining that a second portion of the third text data corresponds to the second data; generating fourth text data corresponding to the order, the fourth text data including at least: a second request for the order, the first portion of the first text data, and the second portion of the third text data; identifying a second device associated with the business profile data; performing TTS processing on the fourth text data to generate fourth audio data; sending the fourth audio data to the second device; receiving, from the second device, fifth audio data corresponding to a third utterance; performing speech processing on the fifth audio data to determine fifth text data; generating sixth text data using at least a portion of the fifth text data; performing TTS processing on the sixth text data to generate sixth audio data; and sending the sixth audio data to the first device.
2. The computer-implemented method of claim 1, wherein determining that the first portion of the first text data corresponds to the first data further comprises: determining that the first text data indicates a first item from a list; determining customization options available for the first item; selecting, using the user profile, a first option of the customization options; and determining a first portion of the first data, wherein the first portion of the first data indicates the first item and the first option.
3. The computer-implemented method of claim 1, further comprising, prior to receiving the first audio data: receiving a third request to generate the business profile data; sending an indication identifying a plurality of functions; receiving first input data selecting a first function; receiving second input data indicating first content associated with the first function; receiving third input data selecting a second function; receiving fourth input data indicating second content associated with the second function; and generating the business profile data including the first function, the first content, the second function, and the second content.
4. The computer-implemented method of claim 1, further comprising: initiate a communication session with the second device; sending, as part of the communication session, the fourth audio data to the second device; receiving, from the second device as part of the communication session, the fifth audio data; determining that the fifth text data represents confirmation of the order and indicates an amount of time before the order is ready; and generating the sixth text data indicating that the order was placed and the amount of time.
5. A computer-implemented method comprising: receiving, from a first device, first audio data corresponding to a first utterance; determining, based on the first audio data, that the first utterance corresponds to a transaction; determining first profile data associated with the transaction; determining, based on the first profile data, that first data is needed for the transaction; determining that the first audio data corresponds to at least a portion of the first data; identifying a second device associated with the first profile data; generating second audio data including first synthesized speech indicating at least some of the first data; sending the second audio data to the second device; receiving, from the second device, third audio data corresponding to a second utterance; performing speech processing on the third audio data to determine second data; generating, using at least a portion of the second data, fourth audio data including second synthesized speech; and sending the fourth audio data to the first device.
6. The computer-implemented method of claim 5, further comprising: performing speech processing on the first audio data to determine first text data; determining an intent associated with the first text data; determining, based on the intent, that the first text data corresponds to the transaction; identifying a first entity included in the first text data; determining, based on the first entity, the first profile data; determining, based on the first profile data, the first data; determining a second entity included in the first text data; and determining that the second entity corresponds to a first portion of the first data.
7. The computer-implemented method of claim 5, further comprising: performing speech processing on the first audio data to determine first text data; identifying a first entity included in the first text data; identifying a second entity included in the first text data; determining that the first entity corresponds to a first portion of the first data; determining that the second entity corresponds to a second portion of the first data; generating second text data indicating at least the first entity and the second entity; and generating the second audio data based on the second text data.
8. The computer-implemented method of claim 5, further comprising: identifying user profile data associated with the first device; determining that a portion of the user profile data is associated with the transaction; and determining a second portion of the first data based on the portion of the user profile data.
9. The computer-implemented method of claim 5, further comprising: determining an identification number associated with the first data; identifying a third device associated with the first profile data; sending the first data to the third device via a data network; and generating the second audio data, wherein the first synthesized speech indicates at least the identification number.
10. The computer-implemented method of claim 5, further comprising: initiating a communication session with the second device; determining first text data corresponding to at least some of the first data; performing text-to-speech (TTS) processing on the first text data to generate the second audio data; and sending, as part of the communication session, the second audio data to the second device.
11. The computer-implemented method of claim 5, further comprising: performing speech processing on the third audio data to determine the second data, the second data including first text data; determining an intent associated with the first text data; determining second text data based on the intent and the first text data; and performing text-to-speech (TTS) processing on the second text data to generate the fourth audio data.
12. The computer-implemented method of claim 5, further comprising: receiving, from the second device, fifth audio data corresponding to a third utterance; determining that the third utterance corresponds to a request for second data; generating sixth audio data including third synthesized speech requesting the second data; sending the sixth audio data to the first device; receiving, from the first device, seventh audio data; determining, based on the seventh audio data, the second data; generating eighth audio data corresponding to the second data; and sending the eighth audio data to the second device.
13. The computer-implemented method of claim 5, further comprising: determining second profile data associated with the transaction; identifying a third device associated with the second profile data; sending the second audio data to the third device; receiving, from the second device, fifth audio data corresponding to a third utterance; determining that the third utterance corresponds to third data; receiving, from the third device, sixth audio data corresponding to a fourth utterance; determining that the fourth utterance corresponds to fourth data; generating seventh audio data based on the third data and the fourth data; and sending the seventh audio data to the first device.
14. The computer-implemented method of claim 5, further comprising, prior to receiving the first audio data: receiving a request to generate the first profile data; sending an indication identifying a plurality of functions; receiving first input data selecting a first function; receiving second input data indicating first content associated with the first function; receiving third input data selecting a second function; receiving fourth input data indicating second content associated with the second function; and generating the first profile data including the first function, the first content, the second function, and the second content.
15. The computer-implemented method of claim 5, further comprising, prior to generating the second audio data: determining that the first audio data corresponds to a first portion of the first data; generating fifth audio data including third synthesized speech requesting a second portion of the first data; sending, to the first device, the fifth audio data; receiving, from the first device, sixth audio data corresponding to a third utterance; and determining, based on the sixth audio data, the second portion of the first data.
16. A system comprising: at least one processor; and memory including instructions operable to be executed by the at least one processor to cause the system to: receive, from a first device, first audio data corresponding to a first utterance; determine, based on the first audio data, that the first utterance corresponds to a transaction; determine first profile data associated with the transaction; determine, based on the first profile data, that first data is needed for the transaction; determine that the first audio data corresponds to at least a portion of the first data; identify a second device associated with the first profile data; generate second audio data including first synthesized speech indicating at least some of the first data; send the second audio data to the second device; receive, from the second device, third audio data corresponding to a second utterance; perform speech processing on the third audio data to determine second data; generate, using at least a portion of the second data, fourth audio data including second synthesized speech; and send the fourth audio data to the first device.
17. The system of claim 16, wherein the memory further comprises instructions that, when executed by the at least one processor, further cause the system to: perform speech processing on the first audio data to determine first text data; determine an intent associated with the first text data; determine, based on the intent, that the first text data corresponds to the transaction; identify a first entity included in the first text data; determine, based on the first entity, the first profile data; determine, based on the first profile data, the first data; determine a second entity included in the first text data; and determine that the second entity corresponds to a first portion of the first data.
18. The system of claim 16, wherein the memory further comprises instructions that, when executed by the at least one processor, further cause the system to: perform speech processing on the first audio data to determine first text data; identify a first entity included in the first text data; identify a second entity included in the first text data; determine that the first entity corresponds to a first portion of the first data; determine that the second entity corresponds to a second portion of the first data; generate second text data indicating at least the first entity and the second entity; and generate the second audio data based on the second text data.
19. The system of claim 16, wherein the memory further comprises instructions that, when executed by the at least one processor, further cause the system to: initiate a communication session with the second device; determine first text data corresponding to at least some of the first data; perform text-to-speech (TTS) processing on the first text data to generate the second audio data; and send, as part of the communication session, the second audio data to the second device.
20. The system of claim 16, wherein the memory further comprises instructions that, when executed by the at least one processor, further cause the system to: perform speech processing on the third audio data to determine the second data, the second data including first text data; determine an intent associated with the first text data; determine second text data based on the intent and the first text data; and perform text-to-speech (TTS) processing on the second text data to generate the fourth audio data.
21. The system of claim 16, wherein the memory further comprises instructions that, when executed by the at least one processor, further cause the system to: receive, from the second device, fifth audio data corresponding to a third utterance; determine that the third utterance corresponds to a request for second data; generate sixth audio data including third synthesized speech requesting the second data; send the sixth audio data to the first device; receive, from the first device, seventh audio data; determine, based on the seventh audio data, the second data; generate eighth audio data corresponding to the second data; and send the eighth audio data to the second device.
</claims>
</document>
