<document>

<filing_date>
2019-10-23
</filing_date>

<publication_date>
2020-04-29
</publication_date>

<priority_date>
2018-10-23
</priority_date>

<ipc_classes>
G10L15/22,G10L15/30
</ipc_classes>

<assignee>
CAPITAL ONE SERVICES
</assignee>

<inventors>
FARIVAR, REZA
GOODSITT, JEREMY EDWARD
ABDI TAGHI ABAD, FARDIN
WALTERS, AUSTIN GRANT
</inventors>

<docdb_family_id>
68342728
</docdb_family_id>

<title>
DYNAMIC VOCABULARY CUSTOMIZATION IN AUTOMATED VOICE SYSTEMS
</title>

<abstract>
Techniques to dynamically customize a menu system presented to a user by a voice interaction system are provided. Audio data from a user that includes the speech of a user can be received. Features can be extracted from the received audio data, including a vocabulary of the speech of the user. The extracted features can be compared to features associated with a plurality of user group models. A user group model to assign to the user from the plurality of user group models can be determined based on the comparison. The user group models can cluster users together based on estimated characteristics of the users and can specify customized menu systems for each different user group. Audio data can then be generated and provided to the user in response to the received audio data based on the determined user group model assigned to the user.
</abstract>

<claims>
1. An apparatus, comprising: a storage device; and logic, at least a portion of the logic implemented in circuitry coupled to the storage device, the logic to: receive audio data including speech of a user, the speech of the user relating to a user issue; extract one or more features from the received audio data, the one or more features including a vocabulary of the speech of the user; compare the one or more extracted features from the received audio data to features associated with a plurality of user group models; determine a user group model to assign to the user from the plurality of user group models based on the comparison of the one or more extracted features from the received audio data and the features associated with the plurality of user group models; generate audio data in response to the received audio data based on the determined user group model assigned to the user; and provide the generated responsive audio data to the user.
2. The apparatus of claim 1, the one or more extracted features including at least one of a speed of the speech of the user and phrases of the speech of the user.
3. The apparatus of claim 1, the one or more extracted features including at least one of a dialect, an accent, and a pronunciation of the speech of the user.
4. The apparatus of claim 1, the determined user group model associated with at least one of a perceived education level of the user, a perceived domain knowledge of the user, and a perceived language literacy of the user.
5. The apparatus of claim 1, the determined user group model to specify one or more menu options provided to the user in the generated responsive audio data.
6. The apparatus of claim 5, the determined user group model to specify an order the one or more menu options are provided to the user in the generated responsive audio data.
7. The apparatus of claim 5, the determined user group model to specify at least one of a dialect, an accent, and a tone of the one or more menu options provided to the user in the generated responsive audio data.
8. The apparatus of claim 5, the determined user group model to specify a vocabulary used to describe the one or more menu options provided to the user in the generated responsive audio data.
9. The apparatus of claim 1, the logic to generate visual data in response to the received audio data based on the determined interaction model and to provide the generated responsive visual data to the user.
10. A method, comprising: receiving audio data including speech of a user, the speech of the user relating to a user issue; extracting one or more features from the received audio data, the one or more features including a vocabulary of the speech of the user; comparing the one or more extracted features from the received audio data to features associated with a plurality of group user models; determining a group user model from the plurality of group user models to assign to the user based on the comparison of the one or more extracted features to the features associated with the plurality of group user models, each group user model in the plurality of group user models assigned to a plurality of users; generating audio data in response to the received audio data based on the determined group user model assigned to the user; and providing the generated responsive audio data to the user, the generated responsive audio data including a set of menu options provided to the user corresponding to the determined group user model, a vocabulary used to describe the menu options determined by the determined group user model.
11. The method of claim 10, the determined group user model to specify at least one of a dialect, a tone, and an accent of the generated responsive audio data.
12. The method of claim 10, further comprising extracting a pronunciation of the speech of the user.
13. The method of claim 10, the determined group user model associated with a language literacy of the user, a level of expertise of the user, and an education level of the user.
14. The method of claim 10, the determined group user model to specify an order of the set of menu options.
15. The method of claim 10, wherein the set of menu options and the vocabulary used to describe the menu options vary for each group user model.
16. At least one non-transitory computer-readable medium comprising a set of instructions that, in response to being executed on a computing device, cause the computing device to: receive audio data including speech of a user, the speech of the user relating to a user issue; extract one or more features from the received audio data, the one or more features including a vocabulary of the speech of the user; compare the one or more extracted features from the received audio data to features associated with a plurality of interaction models; determine an interaction model to assign to the user from the plurality of interaction models based on the comparison of the one or more extracted features from the received audio data and the features associated with the plurality of interaction models; generate audio data in response to the received audio data based on the determined interaction model; generate graphical data in response to the received audio data based on the determined interaction model; and provide the generated responsive audio data and the generated responsive graphical data to the user.
17. The at least one non-transitory computer-readable medium of claim 16, the determined interaction model associated with a perceived level of education of the user.
18. The at least one non-transitory computer-readable medium of claim 16, the one or more extracted features including a pronunciation of the vocabulary of the speech of the user.
19. The at least one non-transitory computer-readable medium of claim 16, the interaction model comprising a customized model unique to the user.
20. The at least one non-transitory computer-readable medium of claim 16, the interaction model comprising a group model assigned to a cluster of users.
</claims>
</document>
