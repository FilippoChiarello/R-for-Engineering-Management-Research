<document>

<filing_date>
2020-07-22
</filing_date>

<publication_date>
2021-01-28
</publication_date>

<priority_date>
2019-07-22
</priority_date>

<ipc_classes>
G06F40/35,G06F40/56,G06K9/62,G06N20/00,G06N3/04
</ipc_classes>

<assignee>
CAPITAL ONE SERVICES
</assignee>

<inventors>
MUELLER ERIK T.
OLABIYI, OLUWATOBI
Zhang, Rui
</inventors>

<docdb_family_id>
74187894
</docdb_family_id>

<title>
Multi-turn Dialogue Response Generation with Autoregressive Transformer Models
</title>

<abstract>
Machine classifiers in accordance with embodiments of the invention capture long-term temporal dependencies in the dialogue data better than the existing RNN-based architectures. Additionally, machine classifiers may model the joint distribution of the context and response as opposed to the conditional distribution of the response given the context as employed in sequence-to-sequence frameworks. Machine classifiers in accordance with embodiments further append random paddings before and/or after the input data to reduce the syntactic redundancy in the input data, thereby improving the performance of the machine classifiers for a variety of dialogue-related tasks. The random padding of the input data may further provide regularization during the training of the machine classifier and/or reduce exposure bias. In a variety of embodiments, the input data may be encoded based on subword tokenization.
</abstract>

<claims>
1. A computer-implemented method, comprising: initializing a model having a sequence to sequence network architecture, wherein the sequence to sequence network architecture comprises: an encoder; and a decoder; training the model based on a training set comprising a plurality of encoder sequences and a plurality of decoder sequences, wherein training the model comprises: generating an encoding of each encoder sequence and each decoder sequence in the training set; selecting a subset of the encodings; appending an informative padding to each of the selected subset of encodings; prepending a start of sequence token to each of the encodings of the encoder sequences; appending an end of sequence token to each of the encodings of the decoder sequences; and for each encoding of the encoder sequences: training the encoder using the encoding of the encoder sequence; and training the decoder using the encoding of the decoder sequence corresponding to the encoder sequence; and generating a prediction based on an input data set using the trained model.
2. The computer-implemented method of claim 1, wherein each encoding comprises an attention weight for each token in the encoding.
3. The computer-implemented method of claim 2, wherein training the encoder comprises updating the attention weight for at least one token in the encoding of the encoder sequence.
4. The computer-implemented method of claim 2, wherein training the decoder comprises updating the attention weight for at least one token in the encoding for the decoder sequence corresponding to the encoder sequence.
5. The computer-implemented method of claim 1, wherein an encoding of a sequence comprises a vector representation the sequence.
6. The computer-implemented method of claim 1, wherein generating the prediction comprises: generating an input encoding of the input data; generating an output sequence comprising a start of sequence token; completing the output sequence by: generating a next output sequence token by providing the input encoding to the trained model; appending the next output sequence token to the output sequence; iteratively generating next output sequence tokens by providing the input encoding to the trained model and appending each generated next output sequence token to the output sequence until the generated subsequent next output sequence token comprises an end of sequence token; and generating the prediction based on the output sequence.
7. The computer-implemented method of claim 1, wherein the encoder sequences comprise a set of dialog prompts.
8. The computer-implemented method of claim 7, wherein the decoder sequences comprise a set of dialog responses.
9. The computer-implemented method of claim 1, wherein: the training set comprises a vocabulary; and the encoding for a sequence comprises one hundred percent coverage for the vocabulary.
10. The computer-implemented method of claim 1, wherein the model is configured to generate predictions regarding multi-turn dialogs.
11. A device, comprising: a processor; and a memory in communication with the processor and storing instructions that, when read by the processor, cause the device to: initialize a model having a sequence to sequence network architecture, wherein the sequence to sequence network architecture comprises: an encoder; and a decoder; train the model based on a training set comprising a plurality of encoder sequences and a plurality of decoder sequences, wherein training the model comprises: generating an encoding of each encoder sequence and each decoder sequence in the training set, wherein an encoding comprises an attention weight; selecting a subset of the encodings; appending an informative padding to each of the selected subset of encodings; prepending a start of sequence token to each of the encodings of the encoder sequences; appending an end of sequence token to each of the encodings of the decoder sequences; and for each encoding of the encoder sequences: training the encoder using the encoding of the encoder sequence; updating the attention weight for the encoder sequence based on the training; and training the decoder using the encoding of the decoder sequence corresponding to the encoder sequence; and generate a prediction based on an input data set using the trained model.
12. The device of claim 11, wherein training the decoder comprises updating the attention weight for at least one token in the encoding for the decoder sequence corresponding to the encoder sequence.
13. The device of claim 11, wherein an encoding of a sequence comprises a vector representation the sequence.
14. The device of claim 11, wherein the instructions, when read by the processor, further cause the device to generate the prediction by causing the device to: generate an input encoding of the input data; generate an output sequence comprising a start of sequence token; complete the output sequence by: generating a next output sequence token by providing the input encoding to the trained model; appending the next output sequence token to the output sequence; iteratively generating next output sequence tokens by providing the input encoding to the trained model and appending each generated next output sequence token to the output sequence until the generated subsequent next output sequence token comprises an end of sequence token; and generate the prediction based on the output sequence.
15. The device of claim 11, wherein the encoder sequences comprise a set of dialog prompts.
16. The device of claim 15, wherein the decoder sequences comprise a set of dialog responses.
17. The device of claim 11, wherein: the training set comprises a vocabulary; and the encoding for a sequence comprises one hundred percent coverage for the vocabulary.
18. A computer-implemented method, comprising: initializing a model having a sequence to sequence network architecture, wherein the sequence to sequence network architecture comprises: an encoder; and a decoder; training the model based on a training set comprising a plurality of encoder sequences and a plurality of decoder sequences, wherein training the model comprises: generating an encoding of each encoder sequence and each decoder sequence in the training set, wherein an encoding comprises an attention weight; selecting a subset of the encodings; appending an informative padding to each of the selected subset of encodings; prepending a start of sequence token to each of the encodings of the encoder sequences; appending an end of sequence token to each of the encodings of the decoder sequences; and for each encoding of the encoder sequences: training the encoder using the encoding of the encoder sequence; updating the attention weight for the encoder sequence based on the training; and training the decoder using the encoding of the decoder sequence corresponding to the encoder sequence; obtaining input data; generating an input encoding of the input data; generating an output sequence comprising a start of sequence token; completing the output sequence by: generating a next output sequence token by providing the input encoding to the trained model; appending the next output sequence token to the output sequence; iteratively generating next output sequence tokens by providing the input encoding to the trained model and appending each generated next output sequence token to the output sequence until the generated subsequent next output sequence token comprises an end of sequence token; and generating a prediction based on the output sequence.
19. The computer-implemented method of claim 18, wherein the encoder sequences comprise a set of dialog prompts.
20. The computer-implemented method of claim 19, wherein the decoder sequences comprise a set of dialog responses.
</claims>
</document>
