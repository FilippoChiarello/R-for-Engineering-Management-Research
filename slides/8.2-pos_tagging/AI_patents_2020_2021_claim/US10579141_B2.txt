<document>

<filing_date>
2018-07-16
</filing_date>

<publication_date>
2020-03-03
</publication_date>

<priority_date>
2017-07-17
</priority_date>

<ipc_classes>
G02B27/00,G02B27/01,G06F3/01,G06T13/80,G09G5/37
</ipc_classes>

<assignee>
NORTH
</assignee>

<inventors>
ALEEM, IDRIS S.
BHARGAVA, MAYANK
JACOBS, DYLAN
</inventors>

<docdb_family_id>
64998882
</docdb_family_id>

<title>
Dynamic calibration methods for eye tracking systems of wearable heads-up displays
</title>

<abstract>
Systems, methods and articles that provide dynamic calibration of eye tracking systems for wearable heads-up displays (WHUDs). The eye tracking system may determine a user's gaze location on a display of the WHUD utilizing a calibration point model that includes a plurality of calibration points. During regular use of the WHUD by the user, the calibration point model may be dynamically updated based on the user's interaction with user interface (UI) elements presented on the display. The UI elements may be specifically designed (e.g., shaped, positioned, displaced) to provide in-use and on-going dynamic calibration of the eye tracking system, which in at least some implementations may be unnoticeable to the user.
</abstract>

<claims>
1. A method of operating a wearable heads-up display device (WHUD) comprising a display and a glint detection module, the method comprising: obtaining, by at least one processor, one or more calibration point models each comprising a plurality of calibration points, each calibration point comprising: a glint space point in a glint space captured by the glint detection module, the glint space point representative of a position of an eye of a user of the WHUD; and a display space point in a display space of the display, the display space point representative of a location on the display at which a gaze of the user is inferred to be resting when the glint space point is captured by the glint detection module; generating, by the at least one processor, a transform from the glint space to the display space for each of the one or more calibration point models; determining, by the at least one processor, a user gaze location in the display space using received glint information and the generated transform; from time-to-time during regular operation of the WHUD by the user, generating, by the at least one processor, at least one additional calibration point; adding, by the at least one processor, the additional calibration point to at least one of the calibration point models to generate one or more child calibration point models; generating, by the at least one processor, a transform for each of the one or more child calibration point models; and determining, by the at least one processor, the user gaze location in the display space using at least one glint space point received from the glint detection module and at least one transform of the one or more child calibration point models.
2. The method of claim 1 wherein generating at least one additional calibration point comprises generating an additional inferred calibration point comprising: a glint space point received from the glint detection module; and a display space point that corresponds to a location in the display space of a user interface (UI) element determined to be the user gaze location.
3. The method of claim 1 wherein generating at least one additional calibration point comprises generating an additional inferred calibration point for each of a plurality of user interface (UI) elements displayed on the display, each inferred calibration point comprising: a glint space point received from the glint detection module; and a display space point that corresponds to a location in the display space of one of the plurality of UI elements.
4. The method of claim 1 wherein generating at least one additional calibration point comprises generating at least one additional selected calibration point comprising: a glint space point received from the glint detection module; and a display space point that is a location of a user interface (UI) element on the display selected by the user during regular operation of the WHUD.
5. The method of claim 4 wherein determining the user gaze location in the display space using at least one glint space point received from the glint detection module and at least one transform comprises determining the user gaze location in the display space using at least one glint space point received from the glint detection module and the one or more child calibration point models that include the at least one additional selected calibration point.
6. The method of claim 1 wherein generating the transform comprises generating an affine transform from the glint space to the display space.
7. The method of claim 1 wherein generating the transform comprises solving a matrix utilizing at least one of a QR decomposition method or singular value decomposition method.
8. The method of claim 1, further comprising: from time-to-time during regular operation of the WHUD by the user, evicting at least one calibration point from a calibration point model.
9. The method of claim 8 wherein evicting at least one calibration point from the calibration point model comprises evicting an oldest calibration point from the calibration point model.
10. The method of claim 8 wherein evicting at least one calibration point from the calibration point model comprises evicting a calibration point based on at least one of the locations of calibration points in the calibration point model or the times at which the calibration points in the calibration point model were obtained.
11. The method of claim 1 wherein obtaining one or more calibration point models each comprising a plurality of calibration points comprises: populating, by the at least one processor, the display of the WHUD with a plurality of user interface (UI) elements; for each of the plurality of UI elements, receiving, by the at least one processor, a selection of the UI element by the user; receiving, by the at least one processor, a glint space point from the glint detection module obtained concurrently with the selection of the UI element by the user; and generating, by the at least one processor, a calibration point that comprises the received glint space point and a display space point representative of the location of the UI element on the display of the WHUD.
12. The method of claim 11 wherein populating the display of the WHUD with the plurality of UI elements comprises populating the display with the plurality of UI elements one at a time in a sequential order.
13. The method of claim 1 wherein obtaining one or more calibration point models each comprising a plurality of calibration points comprises: causing, by the at least one processor, four user interface (UI) elements to be sequentially displayed on the display, each of the four UI elements sequentially displayed in a different one of four corners of the display; and obtaining, by the at least one processor, four calibration points that each correspond to a respective one of the UI elements, each calibration point comprising a display point the display space and a glint space point in the glint space.
14. The method of claim 1 obtaining one or more calibration point models each comprising a plurality of calibration points comprises: causing, by at least one processor, a user interface (UI) element to move on the display of the WHUD according to a determined pattern; and generating, by the at least one processor, a plurality of calibration points as the UI element moves on the display, each calibration point comprises: a glint space point in the glint space captured by the glint detection module; and a display space point in the display space, the display space point representative of a location on the display of the moving UI element when the corresponding glint space point is captured by the glint detection module.
15. The method of claim 14 wherein causing the UI element to move on the display of the WHUD according to the determined pattern comprises causing the UI element to move on the display of the WHUD according to a rectangular-shaped pattern in a first direction and a second direction, the second direction opposite the first direction.
16. The method of claim 1, further comprising: receiving, by the at least one processor, at least one auxiliary sensor value from at least one auxiliary sensor during regular operation of the WHUD by the user; and optimizing, by the at least one processor, a transform of at least one calibration point model based at least in part on the received at least one auxiliary sensor value.
17. The method of claim 16 wherein receiving at least one auxiliary sensor value comprises obtaining at least one auxiliary sensor value from at least one of a proximity sensor, a gyroscope sensor or an accelerometer.
18. The method of claim 1, further comprising: receiving, by the at least one processor, a plurality of calibration points, each calibration point comprising: a glint space point; a display space point; and at least one auxiliary sensor value from at least one auxiliary sensor obtained concurrently with the glint space point and the display space point; and training a machine learning model utilizing the plurality of calibration points, or data derived therefrom, the trained machine learning model receives as inputs at least one current auxiliary sensor value and outputs at least one of a set of calibration points or transform parameters.
19. The method of claim 18, further comprising: optimizing, by the at least one processor, at least one transform utilizing the trained machine learning model.
20. The method of claim 18 wherein receiving a plurality of calibration points comprises receiving a plurality of calibration points from the WHUD and from a population of WHUDs operated by a population of users.
</claims>
</document>
