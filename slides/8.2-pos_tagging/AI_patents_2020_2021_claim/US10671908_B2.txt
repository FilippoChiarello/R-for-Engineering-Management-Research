<document>

<filing_date>
2017-04-14
</filing_date>

<publication_date>
2020-06-02
</publication_date>

<priority_date>
2016-11-23
</priority_date>

<ipc_classes>
G06F17/16,G06K9/62,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
SIMARD, PATRICE
</inventors>

<docdb_family_id>
62147063
</docdb_family_id>

<title>
Differential recurrent neural network
</title>

<abstract>
A differential recurrent neural network (RNN) is described that handles dependencies that go arbitrarily far in time by allowing the network system to store states using recurrent loops without adversely affecting training. The differential RNN includes a state component for storing states, and a trainable transition and differential non-linearity component which includes a neural network. The trainable transition and differential non-linearity component takes as input, an output of the previous stored states from the state component along with an input vector, and produces positive and negative contribution vectors which are employed to produce a state contribution vector. The state contribution vector is input into the state component to create a set of current states. In one implementation, the current states are simply output. In another implementation, the differential RNN includes a trainable OUT component which includes a neural network that performs post-processing on the current states before outputting them.
</abstract>

<claims>
Wherefore, what is claimed is:
1. A differential recurrent neural network (RNN), comprising: one or more computing devices, said computing devices being in communication with each other via a computer network whenever there is a plurality of computing devices, and a computer program having a plurality of sub-programs executable by said computing devices, wherein the sub-programs comprise, a state component sub-program for storing states, said state component sub-program comprising a state loop with an adder for each state being stored, wherein for each state being stored the state component sub-program modifies and stores a current state by adding the previous stored state to a corresponding element of a state contribution vector output by a trainable transition and differential non-linearity component sub-program using the associated state loop and adder each time an input vector is input into the differential RNN, and wherein during backpropagation, the state component sub-program accumulates gradients of a sequence used to train the differential RNN by adding them to previously stored gradient and storing the new gradient at each time step starting from the end of the sequence, said trainable transition and differential non-linearity component sub-program which comprises a neural network, and which takes as an input, an output of said previous stored states from the state component sub-program along with an input vector whenever an input vector is entered into the differential RNN, and which produces a positive contribution vector and a negative contribution vector each having elements each of which corresponds to a different element of the states being stored in the state component sub-program, and which employs the positive and negative contribution vectors to produce and output said state contribution vector that is input into the state component sub-program, wherein each element of the state contribution vector is computed as the difference of a function of a positive contribution value for a corresponding element in the positive contribution vector and the function of a negative contribution value for the corresponding element in the negative contribution vector, wherein said function is such that whenever the positive contribution vector equals the negative contribution vector, the state contribution vector represents the identity matrix, and wherein said function is such that whenever the positive contribution value for an element in the positive contribution vector is less than or equal to 0 and the negative contribution value for the corresponding element in the negative contribution vector is greater than or equal to 0, the corresponding state contribution vector element is 0, and an output of the differential RNN which outputs states.
2. The differential RNN of claim 1, wherein the trainable transition component sub-program comprises a neural network which is regularized to a linear function.
3. The differential RNN of claim 1, wherein the trainable transition component sub-program comprises a mirror deep neural network which is regularized to a linear function.
4. The differential RNN of claim 1, further comprising a trainable OUT component sub-program which comprises a neural network, and which takes as input said current states output from the state component sub-program, performs post-processing on the current states output from the state component sub-program to produce a set of post-processed states, and outputs the post-processed states from the output of the differential RNN.
5. The differential RNN of claim 4, wherein the trainable OUT component sub-program comprises a neural network which is regularized to a linear function.
6. The differential RNN of claim 4, wherein the trainable OUT component sub-program comprises a mirror deep neural network which is regularized to a linear function.
7. The differential RNN of claim 1, further comprising a gradient blocker component sub-program which allows the output of states from the state component sub-program to be input into the trainable transition component subprogram, along with an input whenever an input is entered into the differential RNN, but prevents a backpropagation signal from the trainable transition component sub-program generated during training of the differential RNN from being used by the state component sub-program.
8. The differential RNN of claim 1, further comprising a gradient normalizer component sub-program which divides, for each state being stored in the state component sub-program, a gradient being backpropagated via the state component sub-program during training of the differential RNN, by a normalization value computed based on a current time stamp associated with said training.
9. The differential RNN of claim 8, wherein the normalization value is the square root of the current time stamp.
10. The differential RNN of claim 8, wherein the normalization value is the current time stamp.
11. A computer-implemented process for training a differential recurrent neural network (RNN), comprising the actions of: using one or more computing devices to perform the following process actions, the computing devices being in communication with each other via a computer network whenever a plurality of computing devices is used: receiving a plurality of training sequence vectors, each comprising multiple groups of elements, each group of which corresponds to a different time step; for each training sequence vector received, (a) providing the elements of the training sequence vector corresponding to a current time step, which is initially the first time step in the sequence of time steps, to a trainable transition component of the differential RNN, said trainable transition component comprising a neural network, (b) providing a current version of a state vector stored by a state component of the differential RNN to the trainable transition component, said current version of the state vector having elements each of which corresponds to a different element of states being stored by the state component, (c) capturing the output of the trainable transition component which comprises a positive contribution vector and a negative contribution vector each having elements each of which corresponds to a different element of the states being stored by the state component, (d) providing the last-captured output of the trainable transition component to a differential non-linearity component of the differential RNN, (e) capturing the output of the differential non-linearity component which comprises a state contribution vector having elements each of which corresponds to a different element of the states being stored by the state component, (f) providing the last-captured state contribution vector to the state component which outputs a updated version of the state vector computed from the previous version of the state vector and the last-captured state contribution vector, (g) designating the output of the state component as a sequence output vector associated with the elements of the training sequence vector corresponding to a current time step, (h) determining if the elements of the training sequence vector corresponding to the current time step represent the elements of the last time step of the sequence of time steps, and if not incrementing the time step and repeating (a) through (h) until the elements of the training sequence vector corresponding to the current time step do represent the elements of the last time step of the sequence of time steps; and for each sequence output vector in reverse time step order, starting with the sequence output vector corresponding to the last time step of the sequence of time steps, (i) computing a cost function based on the similarity between the sequence output vector under consideration and the associated elements of the training sequence vector corresponding to the same time step, (j) computing a gradient vector using the last-computed cost function, wherein the gradient vector has elements each of which corresponds to a different one of the states being stored by the state component; (k) providing the last-computed gradient vector to an output side of the state component, said last-computed gradient vector being combined with a last previously-stored gradient vector to produce a current accumulated gradient vector, said current accumulated gradient vector then being stored by the state component, (l) providing a copy of the last-stored current accumulated gradient vector to an output side of the differential non-linearity component which in turn provides copies to each branch of an adder, wherein one copy is multiplied by the derivative of a first non-linearity function and the other copy is multiplied by a second non-linearity function, to produce a positive contribution gradient vector and a negative contribution gradient vector, (m) providing the positive and negative contribution gradient vectors to an output side of the trainable transition component, said positive and negative contribution gradient vectors being employed by the trainable transition component to modify a weigh matrix of the neural network (n) determining if the sequence output vector under consideration corresponds to the first time step of the sequence of time steps, and if not taking under consideration the sequence output vector corresponding to the time step immediately preceding that associated with the last-considered sequence output vector and repeating (i) through (n) until the last-considered sequence output vector corresponds to the first time step of the sequence of time steps.
12. The process of claim 11, wherein a residual gradient vector is output from an input side of the trainable transition component, said residual gradient vector being discarded and prevented from being input into the loop formed by the state component, the differential non-linearity component and the trainable transition component.
13. The process of claim 11, wherein each element of the state contribution vector output by the differential non-linearity component is computed as the difference of an activation function of a positive contribution value for a corresponding element in the positive contribution vector and the activation function of a negative contribution value for the corresponding element in the negative contribution vector, wherein said activation function is such that whenever the positive contribution vector equals the negative contribution vector, the state contribution vector represents the identity matrix, and wherein said activation function is such that whenever the positive contribution value for an element in the positive contribution vector is less than or equal to 0 and the negative contribution value for the corresponding element in the negative contribution vector is greater than or equal to 0, the corresponding state contribution vector element is 0.
14. The process of claim 13, wherein the derivative of a first non-linearity function and the derivative of second non-linearity function used to produce the positive contribution gradient vector and the negative contribution gradient vector, are both a derivative of a modified version of the activation function which is modified to ensure that the positive and negative contribution gradient vectors are not both zero.
15. The process of claim 11 wherein each element of the positive and negative contribution gradient vectors is regularized by the differential non-linearity component by adding a regularization term onto the value of the gradient element.
16. The process of claim 11, further comprising an action of, prior to combining the last-computed gradient vector with a last previously-stored gradient vector to produce and store a current accumulated gradient vector, dividing the last-computed gradient vector by a normalization value computed based on a current time step associated with the training.
17. A system for training a differential recurrent neural network (RNN), comprising: one or more computing devices, said computing devices being in communication with each other via a computer network whenever there is a plurality of computing devices, and a differential RNN training computer program having a plurality of sub-programs executed by said computing devices, wherein the sub-programs cause said computing devices to, receive a plurality of training sequence vectors, each comprising multiple groups of elements, each group of which corresponds to a different time step; for each training sequence vector received, (a) provide the elements of the training sequence vector corresponding to a current time step, which is initially the first time step in the sequence of time steps, to a trainable transition component of the differential RNN, said trainable transition component comprising a neural network, (b) provide a current version of a state vector stored by a state component of the differential RNN to the trainable transition component, said current version of the state vector having elements each of which corresponds to a different element of states being stored by the state component, (c) capture the output of the trainable transition component which comprises a positive contribution vector and a negative contribution vector each having elements each of which corresponds to a different element of the states being stored by the state component, (d) provide the last-captured output of the trainable transition component to a differential non-linearity component of the differential RNN, (e) capture the output of the differential non-linearity component which comprises a state contribution vector having elements each of which corresponds to a different element of the states being stored by the state component, (f) provide the last-captured state contribution vector to the state component which outputs a updated version of the state vector computed from the previous version of the state vector and the last-captured state contribution vector, (g) provide the updated version of the state vector to a trainable OUT component which comprises a neural network, and which performs post-processing on the updated version of the state vector and outputs a post-processed states vector, (h) designate the post-processed states vector as a sequence output vector associated with the elements of the training sequence vector corresponding to a current time step, (i) determine if the elements of the training sequence vector corresponding to the current time step represent the elements of the last time step of the sequence of time steps, and if not increment the time step and repeat (a) through (i) until the elements of the training sequence vector corresponding to the current time step do represent the elements of the last time step of the sequence of time steps; and for each sequence output vector in reverse time step order, starting with the sequence output vector corresponding to the last time step of the sequence of time steps, (j) compute a cost function based on the similarity between the sequence output vector under consideration and the associated elements of the training sequence vector corresponding to the same time step, (k) compute a gradient vector using the last-computed cost function, wherein the gradient vector has elements each of which corresponds to a different one of the states being stored by the state component; (l) provide the last-computed gradient vector to an output side of the trainable OUT component, said last-computed gradient vector being employed by the trainable OUT component to modify a weight matrix of its neural network, (m) provide the last-computed gradient vector to an output side of the state component, said last-computed gradient vector being combined with a last previously-stored gradient vector to produce a current accumulated gradient vector, said current accumulated gradient vector then being stored by the state component, (n) provide a copy of the last-stored current accumulated gradient vector to an output side of the differential non-linearity component which in turn provides copies to each branch of an adder, wherein one copy is multiplied by the derivative of a first non-linearity function and the other copy is multiplied by a second non-linearity function, to produce a positive contribution gradient vector and a negative contribution gradient vector, (o) provide the positive and negative contribution gradient vectors to an output side of the trainable transition component, said positive and negative contribution gradient vectors being employed by the trainable transition component to modify a weigh matrix of the neural network (p) determine if the sequence output vector under consideration corresponds to the first time step of the sequence of time steps, and if not take under consideration the sequence output vector corresponding to the time step immediately preceding that associated with the last-considered sequence output vector and repeat (j) through (p) until the last-considered sequence output vector corresponds to the first time step of the sequence of time steps.
18. The system of claim 17, wherein each element of the state contribution vector output by the differential non-linearity component is computed as the difference of an activation function of a positive contribution value for a corresponding element in the positive contribution vector and the activation function of a negative contribution value for the corresponding element in the negative contribution vector, wherein said activation function is such that whenever the positive contribution vector equals the negative contribution vector, the state contribution vector represents the identity matrix, and wherein said activation function is such that whenever the positive contribution value for an element in the positive contribution vector is less than or equal to 0 and the negative contribution value for the corresponding element in the negative contribution vector is greater than or equal to 0, the corresponding state contribution vector element is 0.
19. The system of claim 18, wherein the derivative of the first non-linearity function and the derivative of the second non-linearity function used to produce the positive contribution gradient vector and the negative contribution gradient vector, are both a derivative of a modified version of the activation function which is modified to ensure that the positive and negative contribution gradient vectors are not both zero.
20. The system of claim 17, further comprising an action of, prior to combining the last-computed gradient vector with a last previously-stored gradient vector to produce and store a current accumulated gradient vector, dividing the last-computed gradient vector by a normalization value computed based on a current time step associated with the training.
</claims>
</document>
