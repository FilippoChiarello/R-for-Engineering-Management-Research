<document>

<filing_date>
2020-02-20
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2019-03-26
</priority_date>

<ipc_classes>
G10L21/00,H04M3/00
</ipc_classes>

<assignee>
TENCENT AMERICA
</assignee>

<inventors>
WENG, CHAO
XU KUN
YU, CHENGZHU
YU, DONG
</inventors>

<docdb_family_id>
72607902
</docdb_family_id>

<title>
SEMEME PREDICTION METHOD, COMPUTER DEVICE, AND STORAGE MEDIUM BACKGROUND
</title>

<abstract>
Method and apparatus for automatically predicting lexical sememes using a lexical dictionary, comprising inputting a word, retrieving the word's semantic definition and sememes corresponding to the word from an online dictionary, setting each of the retrieved sememes as a candidate sememe, inputting the word's semantic definition and candidate sememe, and estimating the probability that the candidate sememe can be inferred from the word's semantic definition.
</abstract>

<claims>
1. A sememe prediction method performed by at least one computer processor, the method comprising:
inputting a word,
retrieving the word's semantic definition and sememes corresponding to the word from an online dictionary,
setting each of the retrieved sememes as a candidate sememe,
for each candidate sememe, inputting the word's semantic definition and candidate sememe, and
for each of the candidate sememes, estimating the probability that the candidate sememe can be inferred from the word's semantic definition.
2. The method of claim 1, wherein the method further comprises inputting the word's semantic definition and candidate sememe as a sequence of Chinese characters.
3. The method of claim 2, wherein the estimation probability further includes:
for each of the candidate sememes, performing segmentation over the word's semantic definition and candidate sememe so as to obtain a sequence of segmented words, and
constructing a vector for each character in the word's semantic definition and candidate sememe by concatenating characters and sememe embeddings and averaging embeddings of all candidate sememes.
4. The method of claim 3, wherein, the estimation probability further include performing the vector construction by having the character and sememe embeddings randomly initialized.
5. The method of claim 4, wherein the probability estimation further includes:
utilizing a bi-directional long short-term memory (Bi-LSTM) model to encode contextual embeddings for each of time-steps of the word's semantic definition and candidate sememe, and comparing each of the contextual embeddings of one sentence against all contextual embeddings of another sentence.
6. The method of claim 5, wherein the probability estimation further includes:
applying another bi-directional long short-term memory (Bi-LSTM) model to two sequences of matching vectors individually,
constructing a final fixed-length matching vector by concatenating vectors from a last time-step of the bi-directional long short-term memory (Bi-LSTM) models, and
employing a two-layer feed-forward neural network to consume the final fixed-length matching vector.
7. The method of claim 6, wherein the probability estimation further comprises applying a Softmax function.
8. The method of claim 1, wherein the probability estimation comprising training a model.
9. The method of claim 8, wherein the training includes employing at least one of random sampling, frequency based sampling, embedded based sampling, and confusion matrix sampling.
10. The method of claim 1, the method further comprising outputting the candidate sememe with the highest probability of being inferred from the word's semantic definition.
11. A device, comprising:
at least one memory configured to store program code;
at least one processor configured to read the program code and operate as instructed by the program code, the program code including:
first inputting code configured to cause said at least one processor to input a word, retrieving code configured to cause said at least one processor to retrieve the word's semantic definition and sememes corresponding to the word from an online dictionary,
setting code configured to cause said at least one processor to set each of the retrieved sememes as a candidate sememe,
second inputting code configured to cause said at least one processor to, for each candidate sememe, input the word's semantic definition and candidate sememe, and
probability estimation code configured to cause said at least one processor to, for each of the candidate sememes, estimate the probability that the candidate sememe can be inferred from the word's semantic definition.
12. The device of claim 11, wherein the second inputting code is further configured to cause said at least one processor to input the word's semantic definition and candidate sememe as a sequence of Chinese characters.
13. The device of claim 12, wherein the probability estimation code is further configured to cause said at least one processor to:
for each of the candidate sememes, perform segmentation over the word's semantic definition and candidate sememe so as to obtain a sequence of segmented words, and
construct a vector for each character in the word's semantic definition and candidate sememe by concatenating characters and sememe embeddings and averaging embeddings of all candidate sememes
14. The device of claim 13, wherein the probability estimation code is further configured to cause said at least one processor to construct the vector by having the character and sememe embeddings be randomly initialized.
15. The device of claim 14, wherein the probability estimation code is further configured to cause said at least one processor to:
utilize a bi-directional long short-term memory (Bi-LSTM) model to encode contextual embeddings for each of time-steps of the word's semantic definition and candidate sememe, and compare each of the contextual embeddings of one sentence against all contextual embeddings of another sentence.
16. The device of claim 15, wherein the probability estimation code is further configured to cause said at least one processor to:
apply another bi-directional long short-term memory (Bi-LSTM) model to two sequences of matching vectors individually,
construct a final fixed-length matching vector by concatenating vectors from a last timestep of the bi-directional long short-term memory (Bi-LSTM) models, and
employ a two-layer feed-forward neural network to consume the final fixed-length matching vector.
17. The device of claim 16, wherein the probability estimation code is further configured to cause said at least one processor to apply a Softmax function
18. The device of claim 11, wherein the probability estimation code is further configured to cause said at least one processor to train a model used to estimate the probability that the candidate sememe can be inferred from the word's semantic definition
19. The device of claim 11, wherein the probability estimation code is further configured to cause said at least one processor to employ at least one of random sampling, frequency based sampling, embedded based sampling, and confusion matrix sampling.
20. A non-transitory computer-readable medium storing instructions, the instructions comprising: one or more instructions that, when executed by one or more processors of a device, cause the one or more processors to:
input a word,
retrieve the word's semantic definition and sememes corresponding to the word from an online dictionary,
set each of the retrieved sememes as a candidate sememe,
for each candidate sememe, input the word's semantic definition and candidate sememe, and
for each of the candidate sememes, estimate the probability that the candidate sememe can be inferred from the word's semantic definition.
</claims>
</document>
