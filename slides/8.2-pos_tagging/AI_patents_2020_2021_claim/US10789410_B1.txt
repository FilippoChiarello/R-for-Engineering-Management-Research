<document>

<filing_date>
2017-06-26
</filing_date>

<publication_date>
2020-09-29
</publication_date>

<priority_date>
2017-06-26
</priority_date>

<ipc_classes>
G06F17/00,G06F40/10,G06F40/232,G06F40/268,G06F40/58
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
</assignee>

<inventors>
RAMASWAMY, SRIRAGHAVENDRA
Mahadeo Sah, Nilesh
Narayanan, Gururaj
Yeturu, Kalidas
</inventors>

<docdb_family_id>
72614962
</docdb_family_id>

<title>
Identification of source languages for terms
</title>

<abstract>
To prevent typos in electronic documents, quality checks are performed to identify portions of the document that have been misspelled. Major sources of false positives are words that have been transliterated from other languages into English. The quality control system described herein identifies transliterated words in electronic documents. The system may be trained to evaluate a set of words, and determine likelihoods that individual words in the set of words are transliterated (i.e., they are not native to the language and/or alphabet of the electronic document). In some embodiments, the model may be further configured to identify a language and/or alphabet that the individual word was transliterated from.
</abstract>

<claims>
1. A computing system for identifying transliterated words comprising: one or more processors; and memory storing computer-executable instructions executable by the one or more processors to perform operations comprising: training a classifier, wherein the training comprises: accessing a training corpus associated with a language, the training corpus comprising at least an electronic document associated with the language; determining features of the training corpus, wherein the features include one or more trigrams present in at least a first word from a first portion of text in the training corpus; determining characteristics of the features of the training corpus; and generating a feature model for the language based on the features of the training corpus, wherein the feature model comprises a vector embedding of trigram characteristics for the language and a random forest classifier; and applying the classifier to a second word from a second portion of text, wherein the applying comprises: determining one or more potential typographical errors in the second portion of text; performing a false positive search on the one or more potential typographical errors; determining that the one or more potential typographical errors include at least one false positive; determining one or more trigrams present in the second word in response to determining that the one or more potential typographical errors include the at least one false positive; determining, based at least in part on the feature model and the one or more trigrams present in the second word, a confidence score indicative of a likelihood that the second word is a transliterated word, wherein determining the confidence score comprises applying a negative weight to at least one trigram of the one or more trigrams present in the second word based at least in part on a negative correlation between the at least one trigram and the language; determining that the confidence score meets or exceeds a confidence score threshold; and determining, based on the confidence score meeting or exceeding the confidence score threshold, that the second word is transliterated.
2. The computing system as recited in claim 1, wherein the electronic document is a first electronic document, and the operations further comprise determining one or more potential typographical errors in a second electronic document, wherein the one or more potential typographical errors includes the second word.
3. The computing system as recited in claim 1, wherein the characteristics of the features include at least one of: a distribution of trigrams in the training corpus; one or more associations between the trigrams in the training corpus; or one or more trigram patterns occurring in the training corpus that are correlated with the language.
4. The computing system as recited in claim 1, wherein the language is a first language, and training the classifier further comprises: accessing a second training corpus associated with a second language, the second training corpus comprising at least a second electronic document associated with the second language; determining second features of the second training corpus, wherein the second features comprise one or more second trigrams present in a third word in the second training corpus; determining second characteristics of the second features of the second training corpus; and generate a second feature model for the second language based on the second features of the second training corpus.
5. The computing system as recited in claim 4, wherein the confidence score is a first confidence score, and the operations further comprise: determining, based on the second feature model, a second confidence score indicative of a second likelihood that the second word is a transliterated word of the second language, and determining, based on the first confidence score and the second confidence score, that the second word is a transliterated word of the first language.
6. The computing system as recited in claim 1, wherein the confidence score is one of multiple confidence scores, each of the multiple confidence scores including a value being indicative of a likelihood that the second word is a transliterated word.
7. A computer-implemented method comprising: training a classifier, wherein the training comprises: accessing a training corpus associated with a language, the training corpus comprising a first electronic document associated with the language; determining one or more features of the training corpus, wherein the one or more features comprise one or more n-grams present in a first portion of text in the training corpus; determining one or more characteristics of the one or more features of the training corpus; and generating a feature model for the language based at least in part on the one or more features of the training corpus, the feature model comprising a vector embedding of n-gram characteristics for the language and a random forest classifier; and applying the classifier to a second electronic document, wherein the applying comprises: determining one or more potential typographical errors in a second portion of text that is associated with the second electronic document; performing a false positive search on the one or more potential typographical errors; determining that the one or more potential typographical errors includes at least one false positive; determining one or more n-grams present in the second portion of text that is associated with the second electronic document in response to determining that the one or more potential typographical errors includes the at least one false positive; and determining, based at least in part on the feature model, a confidence score indicative of a likelihood that the second portion of text includes at least one transliterated word, wherein determining the confidence score comprises applying a negative weight to at least one n-gram of the one or more n-grams present in the second portion of text based at least in part on a negative correlation between the at least one n-gram and the language.
8. The computer-implemented method of claim 7, wherein the feature model further comprises an algorithmic model that probabilistically determines a native language of a portion of text based at least in part on n-grams present within the portion of text.
9. The computer-implemented method of claim 7, wherein determining the confidence score comprises: determining a vector representation of the one or more n-grams present in the second portion of text; determining one or more vector points for the one or more n-grams present in the second portion of text based at least in part on the vector representation; and determining the confidence score based at least in part on the one or more vector points.
10. The computer-implemented method of claim 7, wherein the second portion of text corresponds to a potential typographical error of the one or more potential typographical errors.
11. The computer-implemented method of claim 7, wherein the one or more characteristics of the one or more features include at least one of: a distribution of n-grams in the training corpus, one or more associations between n-grams in the training corpus, and one or more n-gram patterns occurring in the training corpus that are correlated with the language.
12. The computer-implemented method of claim 7, wherein the feature model identifies at least: the negative correlation between the at least one n-gram and the language, and a positive correlation between a second n-gram and the language.
13. The computer-implemented method of claim 7, wherein the language is a first language, the confidence score is a first confidence score indicative that the second portion of text includes a transliterated word of the first language, and the method further comprise: determining, based on a second feature model for a second language, a second confidence score indicative of a likelihood that the second portion of text includes a transliterated word of the second language, and determining, based on the first confidence score and the second confidence score, that the second portion of text includes a transliterated word of the first language.
14. A computing system comprising: one or more processors; and memory storing one or more computer-executable instructions that are executable by the one or more processors to perform operations comprising: training a first feature model to classify one or more first n-grams in a first language, the first feature model comprising a vector embedding of one or more first n-gram characteristics for the first language and a random forest classifier; training a second feature model to classify one or more second n-grams in a second language; determining one or more potential typographical errors in a portion of text; performing a false positive search on the one or more potential typographical errors; determining that at least one word of the one or more potential typographical errors qualifies as a false positive; determining one or more third n-grams present in the at least one word from the portion of text; determining, based at least in part on applying the first feature model to the at least one word, a first confidence score indicative of a first likelihood that the at least one word is a first transliterated word of the first language; determining, based at least in part on applying the second feature model to the at least one word, a second confidence score indicative of a second likelihood that the at least one word is a second transliterated word of the second language, wherein the second feature model applies a negative weight to at least one second n-gram of the one or more second n-grams based at least in part on a negative correlation between the at least one second n-gram and the second language; and determining, based at least in part on the first confidence score and the second confidence score, that the at least one word is the first transliterated word of the first language.
15. The computing system as recited in claim 14, wherein determining that the at least one word is the first transliterated word of the first language comprises comparing the first confidence score to a preset threshold value.
16. The computing system as recited in claim 14, wherein the training the first feature model comprises: accessing a training corpus associated with the first language, the training corpus comprising an electronic document associated with the first language; determining one or more features of the training corpus, wherein the one or more features comprise one or more fourth n-grams present in a first word in the training corpus; determining, based at least in part on the one or more features of the training corpus, one or more first characteristics of the one or more first n-grams in the first language; and generating the first feature model based at least in part on the one or more first characteristics.
17. The computing system as recited in claim 16, wherein the one or more first characteristics include at least one of: a distribution of n-grams in the training corpus; one or more associations between the n-grams in the training corpus; or one or more n-gram patterns occurring in the training corpus that are correlated with the first language.
18. The computing system as recited in claim 16, wherein determining the first confidence score comprises: determining a vector representation of the one or more third n-grams present in the at least one word, and determining the first confidence score based at least in part on the vector embedding and the vector representation.
19. The computing system as recited in claim 14, wherein the first feature model is an algorithmic model that probabilistically determines a native language of the at least one word based on the one or more third n-grams present within the at least one word.
20. The computing system as recited in claim 14, wherein the second feature model comprises an algorithmic model that probabilistically determines a native language of the portion of text based at least in part on n-grams present within the portion of text.
</claims>
</document>
