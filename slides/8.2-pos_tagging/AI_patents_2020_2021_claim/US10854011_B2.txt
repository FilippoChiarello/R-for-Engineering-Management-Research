<document>

<filing_date>
2019-04-09
</filing_date>

<publication_date>
2020-12-01
</publication_date>

<priority_date>
2018-04-09
</priority_date>

<ipc_classes>
G01S17/89,G01S17/93,G05D1/00,G06T19/00,G06T7/521
</ipc_classes>

<assignee>
DIRECT CURRENT CAPITAL
</assignee>

<inventors>
TAY, KAH SENG
SUN, QING
MARION, JAMES PATRICK
</inventors>

<docdb_family_id>
68096041
</docdb_family_id>

<title>
Method for rendering 2D and 3D data within a 3D virtual environment
</title>

<abstract>
One variation of a method includes: accessing a 2D color image recorded by a 2D color camera and a 3D point cloud recorded by a 3D depth sensor at approximately a first time, the 2D color camera and the 3D depth sensor defining intersecting fields of view and facing outwardly from an autonomous vehicle; detecting a cluster of points in the 3D point cloud representing a continuous surface approximating a plane; isolating a cluster of color pixels in the 2D color image depicting the continuous surface; projecting the cluster of color pixels onto the plane to define a set of synthetic 3D color points in the 3D point cloud, the cluster of points and the set of synthetic 3D color points representing the continuous surface; and rendering points in the 3D point cloud and the set of synthetic 3D color points on a display.
</abstract>

<claims>
We claim:
1. A method for augmenting 3D depth map data with 2D color image data comprising: accessing a 2D color image recorded at a first time via a 2D color camera arranged on a vehicle; accessing a 3D point cloud recorded at approximately the first time via a 3D depth sensor arranged on the vehicle, the 3D depth sensor and the 2D color camera defining intersecting fields of view and facing outwardly from the vehicle; detecting a cluster of points in the 3D point cloud approximating a plane, the cluster of points representing a continuous surface; identifying a cluster of color pixels in the 2D color image, the cluster of color pixels depicting the continuous surface; defining a set of synthetic 3D color points in the 3D point cloud by projecting the cluster of color pixels onto the 3D point cloud relative to the plane, the cluster of points and the set of synthetic 3D color points representing the continuous surface; and rendering points in the 3D point cloud and the set of synthetic 3D color points on a display.
2. The method of claim 1, further comprising: generating an augmented 3D point cloud by augmenting the 3D point cloud with the set of synthetic color points; passing the augmented 3D point cloud through a perception pipeline to determine characteristics of an object; based on the characteristics of the object, electing a next navigational action of the vehicle; and autonomously executing the next navigational action.
3. The method of claim 2, wherein the augmented 3D point cloud is passed through the perception pipeline in response to an initial confidence for the characteristics of the object as determined based on the 3D point cloud falling below a threshold confidence the characteristics of the object determined based on the augmented 3D point cloud having a revised confidence greater than the initial confidence.
4. The method of claim 1, further comprising: interpolating color values of points in the cluster of points representing the continuous surface in the 3D point cloud based on color values stored in the set of synthetic 3D color points.
5. The method of claim 1, further comprising: detecting a set of points in the 3D point cloud outside of the cluster of points; deriving a correspondence between the set of points in the 3D point cloud and a set of color pixels in the 2D color image; and interpolating color values of points in the set of points in the 3D point cloud based on color values stored in the set of color pixels, the cluster of points and the set of synthetic 3D color points being rendered in color on the display at a first point density, the set of points being rendered in color on the display at a second point density less than the first point density.
6. The method of claim 1, wherein a first dynamic object and a second dynamic object are detected within a sequence of 3D point clouds recorded via the 3D depth sensor prior to the first time, the first dynamic object detected in a field near the vehicle moving toward a path of the vehicle, and the second dynamic object detected in the field located remotely from the path of the vehicle, the continuous surface being on the first dynamic object, the cluster of points and the set of synthetic 3D color points depicting at least a portion of the first dynamic object, a second cluster of points depicting the second dynamic object exclusive of synthetic 3D color points derived from the 2D color image.
7. The method of claim 1, wherein a dynamic object and a static object are detected in a field near the vehicle within a sequence of 3D point clouds recorded via the 3D depth sensor prior to the first time, the dynamic object located proximal a path of the vehicle and, selected for selective augmentation with 2D color data, the 3D point cloud augmented with the set of synthetic 3D color points depicting the continuous surface on the dynamic object.
8. The method of claim 7, further: identifying a constellation of points depicting the dynamic object in the 3D point cloud; interpreting a set of surfaces on the dynamic object represented by the constellation of points; and selecting the cluster of points, in the constellation of points, that represent the continuous surface, the plane spanning a largest planar surface in the set of surfaces on the dynamic object.
9. The method of claim 7, further comprising: identifying a second cluster of points in the 3D point cloud representing the static object; aggregating groups of static points representing the static object in the sequence of 3D point clouds; and projecting the groups of static points into the 3D point cloud based on changes in position of the vehicle between the first time and times corresponding to the sequence of 3D point cloud.
10. One or more tangible non-transitory computer-readable storage media storing computer-executable instructions for performing a computer process on a computing system, the computer process comprising: obtaining a 2D color image of a scene around a vehicle; obtaining a 3D point cloud of the scene around the vehicle; detecting a cluster of points in the 3D point cloud approximating a plane, the cluster of points representing a continuous surface; identifying a cluster of color pixels in the 2D color image, the cluster of pixels depicting the continuous surface; defining a set of synthetic 3D color points in the 3D point cloud by projecting the cluster of color pixels onto the 3D point cloud relative to the plane; compiling the cluster of points and the set of synthetic 3D color points into a 3D frame; detecting characteristics of an object based on the 3D frame, the object including the continuous surface; and electing a next navigational action for execution by the vehicle based on the characteristics of the object.
11. The one or more tangible non-transitory computer-readable storage media of claim 10, the computer process further comprising: rendering the 3D frame for presentation.
12. The one or more tangible non-transitory computer-readable storage media of claim 10, the computer process further comprising: detecting a dynamic object in the scene around the vehicle, the dynamic object located proximal a path of the vehicle, the dynamic object including the continuous surface; detecting a static object in the scene around the vehicle, the static object located remotely from the path of the vehicle; selecting the dynamic object for selective augmentation with 2D color data; initializing the 3D frame with points from the 3D point cloud; and augmenting the 3D frame with the set of synthetic 3D color points depicting the continuous surface on the dynamic object.
13. The one or more tangible non-transitory computer-readable storage media of claim 10, wherein the vehicle is an autonomous vehicle and the next navigational action is elected for autonomous execution by the autonomous vehicle.
14. A system for augmenting 3D depth map data with 2D color image data comprising: a 3D depth sensor arranged on a vehicle, the 3D depth sensor capturing a 3D point cloud, a cluster of points in the 3D point cloud approximating a plane representing a continuous surface; a 2D color camera arranged on the vehicle, the 3D depth sensor and the 2D color camera defining intersecting fields of view of a scene around the vehicle, the 2D color camera capturing a 2D color image, a cluster of color pixels in the 2D color image depicting the continuous surface; and at least one processor defining a set of synthetic 3D color points in the 3D point cloud by projecting the cluster of color pixels onto the 3D point cloud relative to the plane, the cluster of points and the set of synthetic 3D color points representing the continuous surface.
15. The system of claim 14, wherein the cluster of points is detected in the 3D point cloud by detecting a set of clusters of points in the 3D point cloud, each cluster of points in the set of clusters of points representing a discrete surface in the scene around the vehicle, the cluster of points selected from the set of clusters of points in the 3D point cloud, the cluster of points representing the continuous surface located proximal a path of the vehicle.
16. The system of claim 12, wherein a second set of synthetic 3D color points are defined in the 3D point cloud, the second set of synthetic 3D color points representing a second continuous surface, the second continuous surface moving with the continuous surface.
17. The system of claim 13, wherein the continuous surface corresponds to a side of a vehicle proximal the path of the vehicle and the second continuous surface corresponds to a rear of the vehicle.
18. The system of claim 14, wherein the cluster of points is detected in the 3D point cloud by detecting a set of clusters of points in the 3D point cloud, each cluster of points in the set of clusters of points representing a discrete surface in the scene around the vehicle, the cluster of points selected from the set of clusters of points in the 3D point cloud, the cluster of points characterized by a density of points less than a threshold density.
19. The system of claim 18, wherein the threshold density is inversely proportional to a distance from the vehicle to the continuous surface.
20. The system of claim 14, wherein the continuous surface is defined by a roadside billboard and electronic content related to the roadside billboard is obtained based on iconography extracted from the cluster of color pixels, the electronic content being overlaid over the 3D point cloud and the set of synthetic 3D color points.
</claims>
</document>
