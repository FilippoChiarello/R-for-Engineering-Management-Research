<document>

<filing_date>
2018-09-28
</filing_date>

<publication_date>
2020-12-31
</publication_date>

<priority_date>
2017-10-09
</priority_date>

<ipc_classes>
G06F3/041,G06F3/16,G06K9/00,G06N20/00,G06T7/50,G10L25/63,H04N21/2187,H04N21/2668,H04N21/414,H04N21/442,H04N21/472
</ipc_classes>

<assignee>
ALIBABA GROUP
</assignee>

<inventors>
JIA, ZIJUN
</inventors>

<docdb_family_id>
66051089
</docdb_family_id>

<title>
METHODS AND DEVICES FOR ROBOTIC INTERACTIONS
</title>

<abstract>
Embodiments of the disclosure provide a robotic interaction method and device. The method includes: playing live streaming content selected by a user; obtaining emotion information of the user when the user is watching the live streaming content; sending the emotion information to a host corresponding to the live streaming content; and playing interactive content corresponding to the emotion information sent by the host. For example, when it is found that the user expresses an emotion of boredom, the live streaming content is adjusted to sing a song, do a dance, or play a game. Through this solution, live streaming content is provided to a user by live streaming, and an emotion of the user when the user is watching the live streaming content is perceived to interact with the user, to combine a live streaming technology with a perception technology, and according to the emotion of the user while watching the content, adjust the content watched by the user without delay, thereby achieving effective interaction between a content provider and a content viewer.
</abstract>

<claims>
1. 1-12. (canceled)
13. A method comprising: playing live streaming content selected by a user; obtaining emotion information of the user while the user is viewing the live streaming content; transmitting the emotion information to a host associated with the live streaming content; receiving interactive content from the host, the interactive content selected based on the emotion information; and playing back the interactive content.
14. The method of claim 13, the obtaining emotion information of the user comprising: recording an image of the user while the user is viewing the live streaming content; and performing expression recognition on the image of the user to obtain an expression reflecting an emotion of the user.
15. The method of claim 13, the obtaining emotion information of the user comprising: recording audio of the user while the user is viewing the live streaming content; and performing voice recognition on the audio to obtain a statement reflecting an emotion of the user.
16. The method of claim 13, further comprising controlling a feedback component of the robot to perform a corresponding interactive operation based on the emotion information.
17. The method of claim 13, further comprising: identifying whether the user is a child user prior to playing live streaming content; and displaying a live streaming content selection page corresponding to the child user, the live streaming content selection page allowing the child user to select the live streaming content prior to playing.
18. The method of claim 13, further comprising: recording perception data representing an interactive behavior of the user in response to a selection operation performed by the user on a friend from a viewer list; determining interaction control information associated with the perception data; and transmitting the interaction control information to a robot corresponding to the friend, the interaction control information causing the robot to perform a corresponding interactive operation.
19. The method of claim 18, the determining interaction control information associated with the perception data comprising: performing facial expression recognition on a color image; and determining an expression object corresponding to a recognized facial expression from a preset expression library; and using the expression object as the interaction control information.
20. The method of claim 18, the determining interaction control information associated with the perception data comprising: performing bone recognition on a depth image to obtain joint pose information of the user; determining robot joint pose information corresponding to the joint pose information of the user; and using the robot joint pose information as the interaction control information.
21. The method of claim 18, the determining interaction control information associated with the perception data comprising determining light control information corresponding to touch sensing information and using the touch sensing information as the interaction control information.
22. The method of claim 18, the perception data comprising interactive voice and the interaction control information comprising the interactive voice.
23. An apparatus comprising a processor; and a storage medium for tangibly storing thereon program logic for execution by the processor, the stored program logic comprising: logic, executed by the processor, for playing live streaming content selected by a user, logic, executed by the processor, for obtaining emotion information of the user while the user is viewing the live streaming content, logic, executed by the processor, for transmitting the emotion information to a host associated with the live streaming content, logic, executed by the processor, for receiving interactive content from the host, the interactive content selected based on the emotion information, and logic, executed by the processor, for playing back the interactive content.
24. The apparatus of claim 23, the logic for obtaining emotion information of the user comprising: logic, executed by the processor, for recording an image of the user while the user is viewing the live streaming content; and logic, executed by the processor, for performing expression recognition on the image of the user to obtain an expression reflecting an emotion of the user.
25. The apparatus of claim 23, the logic for obtaining emotion information of the user comprising: logic, executed by the processor, for recording audio of the user while the user is viewing the live streaming content; and logic, executed by the processor, for performing voice recognition on the audio to obtain a statement reflecting an emotion of the user.
26. The apparatus of claim 23, the stored program logic further comprising: logic, executed by the processor, for identifying whether the user is a child user prior to playing live streaming content; and logic, executed by the processor, for displaying a live streaming content selection page corresponding to the child user, the live streaming content selection page allowing the child user to select the live streaming content prior to playing.
27. The apparatus of claim 23, the stored program logic further comprising: logic, executed by the processor, for recording perception data representing an interactive behavior of the user in response to a selection operation performed by the user on a friend from a viewer list; logic, executed by the processor, for determining interaction control information associated with the perception data; and logic, executed by the processor, for transmitting the interaction control information to a robot corresponding to the friend, the interaction control information causing the robot to perform a corresponding interactive operation.
28. The apparatus of claim 27, the logic for determining interaction control information associated with the perception data comprising: logic, executed by the processor, for performing facial expression recognition on a color image; and logic, executed by the processor, for determining an expression object corresponding to a recognized facial expression from a preset expression library; and logic, executed by the processor, for using the expression object as the interaction control information.
29. The apparatus of claim 27, the logic for determining interaction control information associated with the perception data comprising: logic, executed by the processor, for performing bone recognition on a depth image to obtain joint pose information of the user; logic, executed by the processor, for determining robot joint pose information corresponding to the joint pose information of the user; and logic, executed by the processor, for using the robot joint pose information as the interaction control information.
30. The apparatus of claim 27, the logic for determining interaction control information associated with the perception data comprising logic, executed by the processor, for determining light control information corresponding to touch sensing information and using the touch sensing information as the interaction control information.
31. A non-transitory computer-readable storage medium for tangibly storing computer program instructions capable of being executed by a computer processor, the computer program instructions defining the steps of: playing live streaming content selected by a user; obtaining emotion information of the user while the user is viewing the live streaming content; transmitting the emotion information to a host associated with the live streaming content; receiving interactive content from the host, the interactive content selected based on the emotion information; and playing back the interactive content.
32. The computer-readable storage medium of claim 31, the computer program instructions further defining the steps of: recording perception data representing an interactive behavior of the user in response to a selection operation performed by the user on a friend from a viewer list; determining interaction control information associated with the perception data; and transmitting the interaction control information to a robot corresponding to the friend, the interaction control information causing the robot to perform a corresponding interactive operation.
</claims>
</document>
