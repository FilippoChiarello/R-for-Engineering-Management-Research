<document>

<filing_date>
2018-08-16
</filing_date>

<publication_date>
2020-07-08
</publication_date>

<priority_date>
2017-08-29
</priority_date>

<ipc_classes>
G06N3/04,G06T11/00
</ipc_classes>

<assignee>
BOE TECHNOLOGY GROUP COMPANY
</assignee>

<inventors>
NAVARRETE MICHELINI, PABLO
LIU, HANWEN
</inventors>

<docdb_family_id>
65503575
</docdb_family_id>

<title>
IMAGE PROCESSING METHOD, IMAGE PROCESSING APPARATUS, AND A NEURAL NETWORK TRAINING METHOD
</title>

<abstract>
A neural network training method, an image processing method, and an image processing apparatus for implementing image style transfer. The training method comprises: obtaining a first training input image and a second training input image (S10); inputting the first training input image to a neural network (S20); using the neural network to perform style transfer processing on the first training input image to obtain a training output image (S30); calculating a loss value of a parameter of the neural network by means of a loss function on the basis of the first training input image, the second training input image, and the training output image (S40); modifying the parameter of the neural network according to the loss value (S50); and when the loss value satisfies a preset condition, obtaining a trained neural network (S70), and when the loos value does not satisfy the preset condition, continuing to input the first training input image and the second training input image to repeat the training process above, wherein the loss function comprises a weight-to-bias ratio loss function.
</abstract>

<claims>
1. A training method of a neural network for implementing image style transfer, comprising: acquiring a first training input image and a second training input image; inputting the first training input image to the neural network; performing a style transfer process on the first training input image by using the neural network, so as to obtain a training output image; based on the first training input image, the second training input image and the training output image, calculating a loss value of parameters of the neural network through a loss function; and modifying the parameters of the neural network according to the loss value, in a case where the loss function satisfies a predetermined condition, obtaining a trained neural network, and in a case where the loss function does not satisfy the predetermined condition, continuing to input the first training input image and the second training input image so as to repeatedly perform the above training process, wherein the loss function comprises a weight-bias-ratio loss function.
2. The training method according to claim 1, wherein the neural network comprises a first convolutional neural network, the first convolutional neural network comprises a plurality of first convolutional kernels and a plurality of biases, and the parameters of the neural network comprise the plurality of first convolutional kernels and the plurality of biases,
the weight-bias-ratio loss function is expressed as: where LL1 represents the weight-bias-ratio loss function, W is an average value of absolute values of the plurality of first convolutional kernels, B is an average value of absolute values of the plurality of biases, and Îµ is a positive number.
3. The training method according to claim 2, wherein calculating the loss value of the parameters of the neural network through the loss function comprises: calculating a weight-bias-ratio loss value of the parameters of the neural network by the weight-bias-ratio loss function according to the plurality of first convolutional kernels and the plurality of biases, and the loss value comprises the weight-bias-ratio loss value.
4. The training method according to claim 3, wherein modifying the parameters of the neural network according to the loss value comprises:
adjusting a ratio between the plurality of first convolutional kernels and the plurality of biases according to the weight-bias-ratio loss value.
5. The training method according to any of claims 2 to 4, wherein the neural network further comprises a first conversion matrix, the first training input image has a first training input color channel, a second training input color channel and a third training input color channel,
inputting the first training input image to the neural network comprises: converting the first training input image into a first training intermediate image by the first conversion matrix; and inputting the first training intermediate image to the neural network; wherein converting the first training input image into the first training intermediate image by the first conversion matrix comprises:
converting, by the first conversion matrix, data information of the first training input color channel, data information of the second training input color channel and data information of the third training input color channel of the first training input image into data information of a first training luminance channel, data information of a first training color difference channel and data information of a second training color difference channel of the first training intermediate image.
6. The training method according to claim 5, wherein the first convolutional neural network comprises a first sub-network, a second sub-network and a third sub-network,
performing the style transfer process on the first training input image by using the neural network so as to obtain the training output image comprises: performing the style transfer process on the data information of the first training luminance channel of the first training intermediate image, the data information of the first training color difference channel of the first training intermediate image, and the data information of the second training color difference channel of the first training intermediate image by using the first sub-network, the second sub-network and the third sub-network respectively, so as to generate data information of a second training luminance channel of a second training intermediate image, data information of a third training color difference channel of the second training intermediate image, and data information of a fourth training color difference channel of the second training intermediate image; converting the second training intermediate image into the training output image, wherein the training output image is an image in a RGB format, and the second training intermediate image is an image in a YUV format.
7. The training method according to claim 6, wherein the neural network further comprises a second conversion matrix,
converting the second training intermediate image into the training output image comprises:
converting, by the second conversion matrix, the data information of the second training luminance channel of the second training intermediate image, the data information of the third training color difference channel of the second training intermediate image and the data information of the fourth training color difference channel of the second training intermediate image into data information of a first training output color channel of the training output image, data information of a second training output color channel of the training output image and data information of a third training output color channel of the training output image.
8. The training method according to claim 6 or 7, wherein the first sub-network comprises a first set of first convolutional layers, the second sub-network comprises a second set of first convolutional layers, and the third sub-network comprises a third set of first convolutional layers,
the second sub-network comprises a first standard up-sampling layer and a first standard down-sampling layer, and the third sub-network comprises a second standard up-sampling layer and a second standard down-sampling layer;
the first standard down-sampling layer is used to replace an initial first convolutional layer in the second set of first convolutional layers of the second sub-network, the first standard up-sampling layer is used to replace a last first convolutional layer in the second set of first convolutional layers of the second sub-network;
the second standard down-sampling layer is used to replace an initial first convolutional layer in the third set of first convolutional layers of the third sub-network, the second standard up-sampling layer is used to replace a last first convolutional layer in the third set of first convolutional layers of the third sub-network.
9. The training method according to any of claims 2 to 8, wherein the loss function further comprises a content loss function,
calculating the loss value of the parameters of the neural network through the loss function based on the first training input image, the second training input image and the training output image further comprises: extracting a first training input feature of the first training input image and extracting a first training output feature of the training output image by an analysis network; and calculating a content loss value of the parameters of the neural network by the content loss function, according to the first training input feature and the first training output feature, and the loss value comprises the content loss value.
10. The training method according to claim 9, wherein the loss function further comprises a style loss function,
calculating the loss value of the parameters of the neural network through the loss function based on the first training input image, the second training input image and the training output image further comprises: extracting a second training input feature of the second training input image and extracting a second training output feature of the training output image by the analysis network; and calculating a style loss value of the parameters of the neural network by the style loss function, according to the second training input feature and the second training output feature, and the loss value comprises the style loss value.
11. The training method according to claim 10, wherein both the first training input feature and the first training output feature are content features, and both the second training input feature and the second training output feature are style features.
12. The training method according to claim 10 or 11, wherein the analysis network comprises a second convolutional neural network, the second convolutional neural network comprises a plurality of second convolutional layers sequentially connected and a plurality of second pooling layers interposed between adjacent second convolutional layers, each of the plurality of second convolutional layers is used for extracting the first training input feature, the first training output feature, the second training output feature and/or the second training input feature,
a l-th second convolutional layer has Nl second convolutional kernels, the l-th second convolutional layer is used for generating and outputting Nl first training feature images of the first training input image, Nl second training feature images and Nl third training feature images of the training output image, and Nl fourth training feature images of the second training input image, and the Nl convolutional kernels are in one-to-one correspondence to the Nl first training feature images, to the Nl second training feature images, to the Nl third training feature images and to the Nl fourth training feature images, respectively,
the Nl first training feature images, the Nl second training feature images, the Nl third training feature images and the Nl fourth training feature images have same size.
13. The training method according to claim 12, wherein
a content loss function of the l-th second convolutional layer is expressed as: where Cl represents the content loss function, denotes a value of a j-th position in a first training feature image corresponding to an i-th second convolutional kernel of the l-th second convolutional layer, denotes a value of a j-th position in a third training feature image corresponding to the i-th second convolutional kernel of the l-th second convolutional layer, S1 is a constant,
a total content loss function is expressed as: where Lcontent represents the total content loss function, Q1 is a positive integer, and denotes a number of second convolutional layers for extracting and outputting the first training input feature and the first training output feature, and w1l, represents a weight of the C1;
a style loss function of the l-th second convolutional layer is expressed as: where El represents the style loss function, Ml represents a size of a fourth training feature image, denotes a value of a j-th position in a Gram matrix of a second training feature image corresponding to the i-th second convolutional kernel of the l-th second convolutional layer, denotes a value of a j-th position in a Gram matrix of a fourth training feature image corresponding to the i-th second convolutional kernel of the l-th second convolutional layer, S2 is a constant,
a total style loss function is expressed as: where Lstyle represents the total style loss function, Q2 is a positive integer, and denotes a number of second convolutional layers for extracting and outputting the second training input feature and the second training output feature, and w2l represents a weight of the E1.
14. An image processing method for implementing image style transfer based on a neural network, wherein the neural network is a neural network obtained by training according to the training method according to claim 1,
the image processing method comprises: acquiring a first image; inputting the first image to the neural network; and performing a style transfer process on the first image by using the neural network to generate a second image.
15. The image processing method according to claim 14, wherein the neural network further comprises a first conversion matrix, and the first image has a first input color channel, a second input color channel and a third input color channel,
inputting the first image to the neural network comprises: converting the first image into a first intermediate image by the first conversion matrix; and inputting the first intermediate image into the neural network; wherein converting the first image into the first intermediate image by the first conversion matrix comprises: converting, by the first conversion matrix, data information of the first input color channel of the first image, data information of the second input color channel of the first image and data information of the third input color channel of the first image into data information of a first luminance channel of the first intermediate image, data information of a first color difference channel of the first intermediate image and data information of a second color difference channel of the first intermediate image.
16. The image processing method according to claim 15, wherein the neural network further comprises a first convolutional neural network, the first convolutional neural network comprises a first sub-network, a second sub-network and a third sub-network,
performing a style transfer process on the first image by using the neural network to generate a second image comprises: performing the style transfer processing on the data information of the first luminance channel of the first intermediate image, the data information of the first color difference channel of the first intermediate image and the data information of the second color difference channel of the first intermediate image by using the first sub-network, the second sub-network and the third sub-network, respectively, so as to generate data information of a second luminance channel of a second intermediate image, data information of a third color difference channel of the second intermediate image and data information of a fourth color difference channel of the second intermediate image; converting the second intermediate image into the second image, wherein the second image is an image in a RGB format, and the second intermediate image is an image in a YUV format.
17. The image processing method according to claim 16, wherein the neural network further comprises a second conversion matrix,
converting the second intermediate image into the second image comprises:
converting, by the second conversion matrix, the data information of the second luminance channel of the second intermediate image, the data information of the third color difference channel of the second intermediate image and the data information of the fourth color difference channel of the second intermediate image into data information of a first output color channel of the second image, data information of a second output color channel of the second image and data information of a third output color channel of the second image.
18. An image processing device, comprising: an image acquisition module, configured to acquire a first image; an image processing module, comprising a neural network obtained by training according to the training method according to any of claims 1-13, the image processing module being used for performing a style transfer process on the first image by using the neural network to generate a second image.
19. The image processing device according to claim 18, further comprising a first conversion module,
wherein the first image has a first input color channel, a second input color channel and a third input color channel,
the first conversion module is used for converting data information of the first input color channel of the first image, data information of the second input color channel of the first image and data information of the third input color channel of the first image into data information of a first luminance channel of a first intermediate image, data information of a first color difference channel of the first intermediate image, and data information of a second color difference channel of the first intermediate image,
the neural network comprises a first sub-network, a second sub-network and a third sub-network, the first sub-network, the second sub-network and the third sub-network are respectively used for processing the data information of the first luminance channel of the first intermediate image, the data information of the first color difference channel of the first intermediate image, and the data information of the second color difference channel of the first intermediate image, so as to generate data information of a second luminance channel of a second intermediate image, data information of a third color difference channel of the second intermediate image, and data information of a fourth color difference channel of the second intermediate image.
20. The image processing device according to claim 19, further comprising a second conversion module,
wherein the second conversion module is used to convert the data information of the second luminance channel of the second intermediate image, the data information of the third color difference channel of the second intermediate image, and the data information of the fourth color difference channel of the second intermediate image into data information of a first output color channel of the second image, data information of a second output color channel of the second image and data information of a third output color channel of the second image.
21. The image processing device according to claim 19 or 20, wherein the first sub-network comprises a first set of first convolutional layers, the second sub-network comprises a second set of first convolutional layers, and the third sub-network comprises a third set of first convolutional layers;
the second sub-network comprises a first standard up-sampling layer and a first standard down-sampling layer, and the third sub-network comprises a second standard up-sampling layer and a second standard down-sampling layer;
the first standard down-sampling layer is used to replace an initial first convolutional layer in the second set of first convolutional layers of the second sub-network, the first standard up-sampling layer is used to replace a last first convolutional layer in the second set of first convolutional layers of the second sub-network;
the second standard down-sampling layer is used to replace an initial first convolutional layer in the third set of first convolutional layers of the third sub-network, the second standard up-sampling layer is used to replace a last first convolutional layer in the third set of first convolutional layers of the third sub-network.
22. An image processing device, comprising: a memory, for storing non-temporary computer-readable instructions; and a processor, for executing the computer-readable instructions, the training method according to any of claims 1 to 13 being performed while the computer-readable instructions are executed by the processor.
</claims>
</document>
