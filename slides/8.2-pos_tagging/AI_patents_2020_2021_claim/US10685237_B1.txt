<document>

<filing_date>
2020-02-18
</filing_date>

<publication_date>
2020-06-16
</publication_date>

<priority_date>
2019-10-25
</priority_date>

<ipc_classes>
G06K9/00,G06T7/215,G06T7/292,G08B13/14,G08B13/196,H04N7/18
</ipc_classes>

<assignee>
7-ELEVEN
</assignee>

<inventors>
KRISHNAMURTHY, SAILESH BHARATHWAAJ
MIRZA, SHAHMEER ALI
PAUL, DEEPANJAN
VAKACHARLA, SARATH
</inventors>

<docdb_family_id>
70223735
</docdb_family_id>

<title>
Action detection during image tracking
</title>

<abstract>
A system includes a sensor, a weight sensor, and a tracking subsystem. The tracking subsystem receives an image feed of top-view images generated by the sensor and weight measurements from the weight sensor. The tracking subsystem detects an event associated with an item being removed from a rack in which the weight sensor is installed. The tracking subsystem determines that a first person or a second person may be associated with the event. In response to determining that the first or second person may be associated with the event, buffer frames are stored of top-view images generated by the sensor during a time period associated with the event. The tracking subsystem then determines, using at least one of the stored buffer frames and a first action-detection algorithm, whether an action associated with the event was performed by the first person or the second person.
</abstract>

<claims>
1. A system, comprising: a sensor positioned above a rack in a space, the sensor configured to generate top-view images of at least a portion of a space comprising the rack; a plurality of weight sensors, each weight sensor associated with a corresponding item stored on a shelf of the rack; and a tracking subsystem coupled to the image sensor and the weight sensors, the tracking subsystem configured to: receive an image feed comprising frames of the top-view images generated by the sensor; receive weight measurements from the weight sensors; detect an event associated with one or both of a portion of a person entering a zone adjacent to the rack and a change of weight associated with a first item being removed from a first shelf associated with a first weight sensor; in response to detecting the event, determine that a first person or a second person may be associated with the detected event; in response to determining that the first or second person may be associated with the detected event, store buffer frames of top-view images generated by the sensor during a time period associated with the detected event; determine a region-of-interest of the top-view images of the stored frames; and determine, using the region-of-interest of at least one of the stored buffer frames and the first action-detection algorithm, whether an action associated with the detected event was performed by the first person or the second person; in response to determining the action associated with the event was performed by the first person, assign the action to the first person; and in response to determining the action associated with the event was performed by the second person, assign the action to the second person.
2. The system of claim 1, wherein the tracking subsystem is further configured to: determine whether results of the first action-detection algorithm satisfy criteria based at least in part on a number of iterations required to implement the first action-detection algorithm; and in response to determining the results of the first action-detection algorithm do not satisfy the criteria, determine, by applying a second action-detection algorithm to at least one of the buffer frames, whether the action associated with the detected event was performed by the first person or the second person, wherein the second action-detection algorithm is configured to detect the action using an artificial neural network.
3. The system of claim 2, wherein the stored buffer frames comprise three or fewer frames of top-view images associated with one or both of: the portion of the person entering the zone adjacent to the rack and the portion of the person exiting the zone adjacent to the rack.
4. The system of claim 3, wherein the tracking subsystem is further configured to determine a subset of the buffer frames to use with the first action-detection algorithm and a second subset of the buffer frames to use with the second action detection algorithm.
5. The system of claim 1, wherein the tracking subsystem is further configured to determine that the first person and the second person may be associated with the detected event based on a first relative orientation between the first person and the rack and a second relative orientation between the second person and the rack.
6. The system of claim 1, wherein: the detected action is associated with a person picking up the first item stored on the first shelf of the rack; and the tracking subsystem is further configured to: in response to determining the action was performed by the first person, assign the first item to the first person; and in response to determining the action was performed by the second person, assign the first item to the second person.
7. The system of claim 1, wherein: the first action-detection algorithm involves iterative dilation of a first contour associated with the first person and a second contour associated with the second contour; and the criteria comprise a requirement that the portion of the person entering the zone adjacent to the rack is associated with either the first person or the second person within a maximum number of iterative dilations of the first and second contours; and the tracking subsystem is further configured to, in response to determining the first person is associated with the portion of the person entering the zone adjacent to the rack within the maximum number of dilations, assign the action to the first person.
8. A method, comprising: receiving an image feed comprising frames of top-view images generated by a sensor, the sensor positioned above a rack in a space and configured to generate top-view images of at least a portion of a space comprising the rack; receiving weight measurements from a weight sensor associated with a corresponding item stored on a shelf of the rack; detecting an event associated with one or both of a portion of a person entering a zone adjacent to the rack and a change of weight associated with a first item being removed from a first shelf associated with the weight sensor; in response to detecting the event, determining that a first person or a second person may be associated with the detected event; in response to determining that the first or second person may be associated with the detected event, storing buffer frames of top-view images generated by the sensor during a time period associated with the detected event; following storing the buffer frames, determining a region-of-interest of the top-view images of the stored frames; and determining, using the region-of-interest of at least one of the stored buffer frames and the first action-detection algorithm, whether an action associated with the detected event was performed by the first person or the second person; in response to determining the action associated with the event was performed by the first person, assigning the action to the first person; and in response to determining the action associated with the event was performed by the second person, assigning the action to the second person.
9. The method of claim 8, further comprising: determining whether results of the first action-detection algorithm satisfy criteria based at least in part on a number of iterations required to implement the first action-detection algorithm; and in response to determining the results of the first action-detection algorithm do not satisfy the criteria, determining, by applying a second action-detection algorithm to at least one of the buffer frames, whether the action associated with the detected event was performed by the first person or the second person, wherein the second action-detection algorithm is configured to detect the action using an artificial neural network.
10. The method of claim 9, wherein the stored buffer frames comprise three or fewer frames of top-view images associated with one or both of: the portion of the person entering the zone adjacent to the rack and the portion of the person exiting the zone adjacent to the rack.
11. The method of claim 10, further comprising determining a subset of the buffer frames to use with the first action-detection algorithm and a second subset of the buffer frames to use with the second action detection algorithm.
12. The method of claim 8, further comprising determining that the first person and the second person may be associated with the detected event based on a first relative orientation between the first person and the rack and a second relative orientation between the second person and the rack.
13. The method of claim 8, wherein: the detected action is associated with a person picking up the first item stored on the first shelf of the rack; and the method further comprises: in response to determining the action was performed by the first person, assigning the first item to the first person; and in response to determining the action was performed by the second person, assigning the first item to the second person.
14. The method of claim 8, wherein: the first action-detection algorithm involves iterative dilation of a first contour associated with the first person and a second contour associated with the second contour; the criteria comprise a requirement that the portion of the person entering the zone adjacent to the rack is associated with either the first person or the second person within a maximum number of iterative dilations of the first and second contours; and the method further comprises, in response to determining the first person is associated with the portion of the person entering the zone adjacent to the rack within the maximum number of dilations, assigning the action to the first person.
15. A tracking subsystem coupled to an image sensor and a weight sensor, wherein the image sensor is positioned above a rack in a space and configured to generate top-view images of at least a portion of the space comprising the rack, wherein the weight sensor is configured to measure a change of weight when an item is removed from a shelf of the rack, the tracking subsystem configured to: receive an image feed comprising frames of the top-view images generated by the sensor; receive weight measurements from the weight sensor; detect an event associated with one or both of a portion of a person entering a zone adjacent to the rack and a change of weight associated with a first item being removed from a first shelf associated with the weight sensor; in response to detecting the event, determine that a first person or a second person may be associated with the detected event; in response to determining that the first or second person may be associated with the detected event, store buffer frames of top-view images generated by the sensor during a time period associated with the detected event; following storing the buffer frames, determine a region-of-interest of the top-view images of the stored frames; and determine, using the region-of-interest of at least one of the stored buffer frames and the first action-detection algorithm, whether an action associated with the detected event was performed by the first person or the second person; in response to determining the action associated with the event was performed by the first person, assign the action to the first person; and in response to determining the action associated with the event was performed by the second person, assign the action to the second person.
16. The tracking subsystem of claim 15, further configured to: determine whether results of the first action-detection algorithm satisfy criteria based at least in part on a number of iterations required to implement the first action-detection algorithm; and in response to determining the results of the first action-detection algorithm do not satisfy the criteria, determine, by applying a second action-detection algorithm to at least one of the buffer frames, whether the action associated with the detected event was performed by the first person or the second person, wherein the second action-detection algorithm is configured to detect the action using an artificial neural network.
17. The tracking subsystem of claim 15, wherein: the stored buffer frames comprise three or fewer frames of top-view images associated with one or both of: the portion of the person entering the zone adjacent to the rack and the portion of the person exiting the zone adjacent to the rack; and the tracking subsystem is further configured to determine a subset of the buffer frames to use with the first action-detection algorithm and a second subset of the buffer frames to use with the second action detection algorithm.
</claims>
</document>
