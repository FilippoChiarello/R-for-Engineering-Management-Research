<document>

<filing_date>
2018-03-23
</filing_date>

<publication_date>
2020-09-01
</publication_date>

<priority_date>
2017-05-16
</priority_date>

<ipc_classes>
G06F1/3231,G06F1/3234,G06F15/18,G06F15/76,G06F3/01,G06K9/00,G06K9/62,G06N20/00,G06N20/10,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
APPLE
</assignee>

<inventors>
HO, KELSEY, Y.
GERNOTH, THORSTEN
HAMSICI, ONUR C.
LIEBELT, JOERG A.
</inventors>

<docdb_family_id>
64271752
</docdb_family_id>

<title>
Attention detection
</title>

<abstract>
Detection of a user paying attention to a device may be used to enable or support biometric security (e.g., facial recognition) enabled features on the device. Images captured by a camera on the device may be used to determine if the user is paying attention to the device. Facial features of the user's face in the images may be assessed to determine if the user is paying attention to the device. Facial features may be assessed through comparison of feature vectors generated from the captured image feature to a set of known feature vectors. The known feature vectors for attention may be generated using a machine learning process.
</abstract>

<claims>
1. A method, comprising: capturing a plurality of reference images of a user using a camera located on a device, the device comprising a computer processor and a memory, wherein the captured reference images include the face of the user, the user being an authorized user of the device; generating a plurality of reference template images from the captured reference images; encoding a plurality of facial features of the user from the reference template images to generate reference feature vectors defining the facial features in a feature space, wherein at least one reference feature vector defines the user paying attention to the device, and wherein paying attention to the device includes the user looking at the device in a direction defined relative to the camera in the reference template images; selecting one or more classifiers based on the at least one reference feature vector that defines the user paying attention to the device, wherein the one or more classifiers are selected from a plurality of classifiers stored in the memory by: determining an area of the feature space associated with the at least one reference feature vector that defines the user paying attention to the device; and selecting the one or more classifiers by selecting classifiers with higher rankings in a ranking of the classifiers for the determined area of the feature space, wherein the ranking of the classifiers is based on confidence levels of the classifiers classifying the at least one reference feature vector that defines the user paying attention to the device in the determined area of the feature space, and wherein the ranking of the classifiers for different areas in the feature space has been determined by operating the plurality of classifiers of feature vectors generated from a plurality of training images that include users' faces paying attention to the device; capturing, using the camera, at least one additional image of the user, the additional captured image including the face of the user; encoding a plurality of facial features of the user from the additional captured image to generate feature vectors defining the facial features of the user from the additional captured image in the feature space; and determining whether the user in the additional captured image is paying attention to the device assessing the feature vectors for the additional captured image using the selected classifiers to correlate the feature vectors for the additional captured image to the at least one reference feature vector that defines the user paying attention to the device.
2. The method of claim 1, wherein the reference images of the user using the camera located on the device are captured during an enrollment protocol.
3. The method of claim 1, wherein selecting at least one classifier includes selecting a classifier with a highest ranking in the ranking of the classifiers for the determined area of the feature space.
4. The method of claim 1, wherein the classifier rankings are based on confidence levels of the classifiers in defining clusters of feature vectors in the different areas of the feature space associated with the feature vectors generated from the plurality of training images that include the users' faces paying attention to the device.
5. The method of claim 1, further comprising assessing an orientation of the device when the at least one additional image is captured, and rotating the at least one additional image to place a face of the user in the image in a selected orientation in response to the face of the user not being in the selected orientation.
6. The method of claim 1, further comprising controlling one or more actions of the device based on the determination whether the user is paying attention to the device.
7. The method of claim 1, wherein the reference template images are configured to be used to authenticate the user as the authorized user of the device.
8. A device, comprising: a computer processor; a memory; a camera; at least one illuminator providing infrared illumination; circuitry coupled to the camera and the illuminator, wherein the circuitry is configured to: capture a plurality of reference images of a user using the camera, wherein the captured reference images include the face of the user, the user being an authorized user of the device; generate a plurality of reference template images from the captured reference images; encode a plurality of facial features of the user from the reference template images to generate reference feature vectors defining the facial features in a feature space, wherein at least one reference feature vector defines the user paying attention to the device, and wherein paying attention to the device includes the user looking at the device in a direction defined relative to the camera in the reference template images; select one or more classifiers based on the at least one reference feature vector that defines the user paying attention to the device, wherein the one or more classifiers are selected from a plurality of classifiers stored in the memory by: determining an area of the feature space associated with the at least one reference feature vector that defines the user paying attention to the device; and selecting the one or more classifiers by selecting classifiers with higher rankings in a ranking of the classifiers for the determined area of the feature space, wherein the ranking of the classifiers is based on confidence levels of the classifiers classifying the at least one reference feature vector that defines the user paying attention to the device in the determined area of the feature space, and wherein the ranking of the classifiers for different areas in the feature space has been determined by operating the plurality of classifiers on feature vectors generated from a plurality of training images that include users' faces paying attention to the device; capture, using the camera, at least one additional image of the user, the additional captured image including the face of the user; encode a plurality of facial features of the user from the additional captured image to generate feature vectors defining the facial features of the user from the additional captured image in the feature space; and determine whether the user in the additional captured image is paying attention to the device by assessing the feature vectors for the additional captured image using the selected classifiers to correlate the feature vectors for the additional captured image to the at least one reference feature vector that defines the user paying attention to the device.
9. The device of claim 8, wherein the camera comprises an infrared sensor.
10. The device of claim 8, wherein the at least one illuminator comprises a flood infrared illuminator.
11. The device of claim 8, wherein the at least one illuminator comprises a speckle pattern illuminator, and wherein the circuitry is configured to: capture an image of the user while the user is being illuminated with speckle pattern illumination; and generate a depth image of the user from the captured image.
12. The device of claim 8, further comprising at least one sensor configured to assess an orientation of the device when the at least one additional image is captured.
13. The device of claim 8, wherein the circuitry is configured to rotate the at least one additional image to place a face of the user in the image in a selected orientation.
14. The device of claim 8, wherein the circuitry is configured to control one or more operations of the device based on the determination whether the user is paying attention to the device.
15. A method, comprising: providing a plurality of training images to a computer system, the computer system comprising a computer processor and a memory, wherein the provided training images include faces of reference users, and wherein the reference users' faces in the training images include a plurality of different facial features associated with a plurality of groups of people; encoding facial features of the reference users' faces in the provided training images to generate feature vectors defining the facial features in a feature space; separating the feature vectors in the feature space into a first set of feature vectors in the feature space and a second set of feature vectors in the feature space, the first set of feature vectors being feature vectors associated with the reference users' faces paying attention to the computer system and the second set of feature vectors being feature vectors associated with the reference users' faces not paying attention to the computer system; operating a plurality of classifiers on the first set of feature vectors in the feature space; ranking the classifiers in a plurality of areas in the feature space, wherein ranking a classifier in an area in the feature space comprises ranking the classifier based on a confidence level of the classifier in defining a grouping of feature vectors in the area of the feature space; providing the ranking of the classifiers to a device having a camera; selecting one or more classifiers for use in determining of attention to the device by a user of the device based on the ranking of the classifiers; capturing, using the camera, at least one image of a user, the captured image including a face of the user; generating one or more feature vectors from the captured image; and determining whether the user in the captured image is paying attention to the device by using the selected classifiers to classify the one or more feature vectors from the captured image.
16. The method of claim 15, wherein the computer system comprises machine learning circuitry.
</claims>
</document>
