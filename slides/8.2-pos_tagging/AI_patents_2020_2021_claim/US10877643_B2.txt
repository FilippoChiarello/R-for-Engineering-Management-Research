<document>

<filing_date>
2018-03-15
</filing_date>

<publication_date>
2020-12-29
</publication_date>

<priority_date>
2018-03-15
</priority_date>

<ipc_classes>
G06F3/01,G06F3/0346,G06F3/0481,G06F3/0482,G06F3/0484,G06F3/0488,G06K9/32,G06N20/00
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
WANTLAND, TIM
PEDERSEN, KYLE
</inventors>

<docdb_family_id>
63586894
</docdb_family_id>

<title>
Systems and methods to increase discoverability in user interfaces
</title>

<abstract>
The present disclosure provides systems and methods to improve discoverability of selectable user interface elements. In particular, the present disclosure provides computing devices that, in some implementations, intelligently identify user interface elements that are selectable (e.g., based on intelligent understanding of the user interface content) and visually modify one or more of such selectable user interface elements to indicate to the user that they are selectable. The visual modification can highlight or otherwise draw attention to the user interface element(s), thereby improving their discoverability.
</abstract>

<claims>
1. A computing device, comprising: one or more processors; a display; and one or more non-transitory computer-readable media that collectively store instructions that, when executed by the one or more processors, cause the computing device to perform operations, the operations comprising: providing a user interface that comprises a plurality of user interface elements for presentation on the display; continuously identifying one or more of the user interface elements that are selectable, as defined as belonging to a predetermined set of entities, by a user of the computing device using one or more machine-learned models to recognize selectable entities within the user interface; determining an object location of a user object relative to the display, wherein the user object comprises a finger or a stylus located adjacent to the display but not physically touching the display; determining whether the object location of the user object corresponds to one of the one or more user interface elements that are selectable by the user; and when the object location corresponds to a first user interface element of the one or more user interface elements that are selectable by the user, visually modifying at least the first user interface element of the one or more identified selectable user interface elements to have a modified appearance that indicates that the first user interface element is selectable by the user, without visually modifying other user interface elements that are not selectable by the user so as to visually distinguish the one or more user interface elements that are selectable by the user from the other user interface elements that are not selectable by the user, and wherein visually modifying at least the first user interface element comprises implementing a particular type of visual modification based on a content type associated with the first user interface element.
2. The computing device of claim 1, wherein the operations further comprise: determining an orientation of the computing device relative to a user object associated with a user of the computing device; wherein the operation of visually modifying the first user interface element to have the modified appearance comprises: determining a reflection location within the first user interface element based at least in part on the orientation of the computing device relative to the user object; and visually modifying the first user interface element to have a gleam appearance in which the first user interface element appears to reflect light at the reflection location within the first user interface element.
3. The computing device of claim 2, wherein the user object comprises a finger or a stylus located adjacent to the display but not physically touching the display.
4. The computing device of claim 2, wherein the computing device further comprises: a RADAR sensing system that senses an object location of the user object; wherein the operation of determining the orientation of the computing device relative to the user object comprises determining the orientation of the computing device relative to the object location sensed by the RADAR sensing system.
5. The computing device of claim 2 wherein: the user object comprises a face of the user; the computing device further comprises one or more cameras that capture imagery; the operations further comprise determining a face location of the face of the user based on the imagery captured by the one or more cameras; and the operation of determining the orientation of the computing device relative to the user object comprises determining the orientation of the computing device relative to the face location of the face of the user.
6. The computing device of claim 1, wherein: the computing device further comprises a gyroscope; the operations further comprise determining an orientation of the computing device based on data produced by the gyroscope; and the operation of visually modifying the first user interface element to have the modified appearance comprises: determining a reflection location within the first user interface element based at least in part on the orientation of the computing device; and visually modifying the first user interface element to have a gleam appearance in which the first user interface element appears at the reflection location.
7. The computing device of claim 3, further comprising, after visually modifying the first user interface element: receiving a user touch input from the user object that selects the first user interface element; and in response to the user touch input, providing a second user interface for presentation on the display, wherein the second user interface enables the user to one or more of search, store, or communicate information associated with the first user interface element.
8. A computer-implemented method, comprising: providing, by one or more computing devices, a user interface that comprises a plurality of user interface elements for presentation on a display; identifying, by the one or more computing devices, one or more of the user interface elements that are selectable by a user; continuously identifying, by the one or more computing devices, one or more of the user interface elements that are selectable by a user, as defined as belonging to a predetermined set of entities using one or more machine-learned models to recognize selectable entities within the user interface; determining, by the one or more computing devices, an object location of a user object relative to the display, wherein the user object comprises a finger or a stylus located adjacent to the display but not physically touching the display; determining, by the one or more computing devices, whether the object location of the user object corresponds to one of the one or more user interface elements that are selectable by the user; and when the object location corresponds to a first user interface element of the one or more selectable user interface elements, visually modifying, by the one or more computing devices, the first user interface element to visually highlight the first user interface element, without visually modifying other user interface elements that are not selectable by the user so as to visually distinguish the one or more user interface elements that are selectable by the user from the other user interface elements that are not selectable by the user, and wherein visually modifying at least the first user interface element comprises implementing a particular type of visual modification based on a content type associated with the first user interface element.
9. The computer-implemented method of claim 8, wherein determining, by the one or more computing devices, whether the object location of the user object corresponds to one of the one or more user interface elements that are selectable by the user comprises determining, by the one or more computing devices, whether the finger or the stylus is hovering over one of the one or more user interface elements that are selectable by the user.
10. The computer-implemented method of claim 8, wherein visually modifying, by the one or more computing devices, the first user interface element to visually highlight the first user interface element comprises temporarily visually modifying, by the one or more computing devices, the first user interface element to visually highlight the first user interface element.
11. The computer-implemented method of claim 8, wherein visually modifying, by the one or more computing devices, the first user interface element to visually highlight the first user interface element comprises visually modifying, by the one or more computing devices, the first user interface element according to an orientation of the display relative to the object location of the user object.
12. The computer-implemented method of claim 11, wherein visually modifying, by the one or more computing devices, the first user interface element according to the orientation of the display relative to the object location of the user object comprises: visually modifying, by the one or more computing devices, the first user interface element to have a gleam appearance in which the first user interface element appears to reflect light; wherein a reflection location at which the first user interface element appears to reflect light is based at least in part on the orientation of the display relative to the object location of the user object.
13. The computer-implemented method of claim 8, wherein visually modifying, by the one or more computing devices, the first user interface element to visually highlight the first user interface element comprises: determining, by the one or more computing devices, a first content type associated with the first user interface element; and visually modifying, by the one or more computing devices, the first user interface element according to a first modification type selected from a plurality of different available modification types that are respectively associated with a plurality of different content types.
14. The computer-implemented method of claim 8, wherein determining, by the one or more computing devices, the object location of the user object relative to the display comprises receiving, by the one or more computing devices, RADAR data from a RADAR sensing system that describes the object location of the user object relative to the display.
15. The computer-implemented method of claim 8, further comprising: receiving, by the one or more computing devices, a user touch input from the user object that selects the first user interface element; and in response to the user touch input, providing, by the one or more computing devices, a second user interface for presentation on the display, wherein the second user interface enables the user to store information associated with the first user interface element for later retrieval.
16. Apparatus configured: to provide a user interface that comprises a plurality of user interface elements for presentation on a display; to identify a subset of one or more of the user interface elements that are selectable by a user, wherein the subset of selectable user interface elements is identified based on contextual data associated with the user interface elements and using one or more machine-learned models to recognize selectable entities within the user interface; and to modify display of the subset of selectable user interface elements as a function of a location of a finger or stylus located adjacent to the display but not physically touching the display, without modifying display of other user interface elements that are not selectable by the user so as to visually distinguish the one or more user interface elements that are selectable by the user from the other user interface elements that are not selectable by the user, and wherein modifying display of the subset of selectable user interface elements comprises implementing a particular type of visual modification based on a content type associated with the subset of selectable user interface elements.
17. Apparatus as claimed in claim 16, wherein the apparatus is configured to display the one or more selectable user interface elements without modification in the presence of a first condition and to modify in real time display of selectable user interface elements as a function of the location of the finger or stylus located adjacent to the display but not physically touching the display in the absence of the first condition, the first condition being a neutral orientation of the apparatus.
18. Apparatus as claimed in claim 16, wherein the apparatus is configured to visually modify the subset of selectable user interface elements in a first way for textual user interface elements and in a second, different way for pictorial or graphical elements.
</claims>
</document>
