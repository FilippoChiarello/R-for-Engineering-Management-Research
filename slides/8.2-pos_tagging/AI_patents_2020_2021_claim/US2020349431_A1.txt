<document>

<filing_date>
2020-06-18
</filing_date>

<publication_date>
2020-11-05
</publication_date>

<priority_date>
2018-05-07
</priority_date>

<ipc_classes>
G06K9/62,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
SHANGHAI SENSETIME INTELLIGENT TECHNOLOGY COMPANY
</assignee>

<inventors>
XU CHAO
CHEN, Zitian
XIE, Shuqin
LU, Cewu
</inventors>

<docdb_family_id>
64026991
</docdb_family_id>

<title>
SYSTEM REINFORCEMENT LEARNING METHOD AND APPARATUS, AND COMPUTER STORAGE MEDIUM
</title>

<abstract>
A system reinforcement learning method includes: processing an input image based on a first network of a system to obtain a first result; inputting the first result to a second network of the system to obtain a second result; and obtaining a reinforcement operation based on the second result by means of a reinforcement network, and adjusting the first result based on the reinforcement operation to obtain a target result. According to the embodiments of the present disclosure, information is fed back from downstream to upstream by means of the reinforcement network, and an output result of the system is optimized.
</abstract>

<claims>
1. A system reinforcement learning method, comprising: processing an input image based on a first network of a system to obtain a first result; inputting the first result into a second network of the system to obtain a second result; and obtaining a reinforcement operation based on the second result by using a reinforcement network, adjusting the first result based on the reinforcement operation, and obtaining a target result based on the adjusted first result.
2. The method according to claim 1, wherein the obtaining the reinforcement operation based on the second result by using the reinforcement network, adjusting the first result based on the reinforcement operation, and obtaining the target result based on the adjusted first result comprises: obtaining the reinforcement operation by using the reinforcement network based on the second result outputted by the second network, and adjusting the first result based on the reinforcement operation to obtain a first intermediate result; inputting the first intermediate result into the second network, obtaining the second result based on the first intermediate result, and inputting the second result into the reinforcement network; and outputting the second result as the target result in response to a preset condition being met.
3. The method according to claim 2, wherein the reinforcement operation comprises at least one adjustment action; and the obtaining the reinforcement operation by using the reinforcement network based on the second result outputted by the second network, and adjusting the first result based on the reinforcement operation to obtain the first intermediate result comprises: obtaining at least one adjustment action probability based on the second result by using the reinforcement network, and determining the at least one adjustment action based on the at least one adjustment action probability; and adjusting the first result based on the at least one adjustment action to obtain the first intermediate result.
4. The method according to claim 2, wherein the adjusting the first result by using the reinforcement network based on the second result outputted by the second network to obtain the first intermediate result comprises: obtaining at least one adjustment action probability based on the second result and the first result by using the reinforcement network, and determining at least one adjustment action based on the at least one adjustment action probability; and adjusting the first result based on the at least one adjustment action to obtain the first intermediate result.
5. The method according to claim 4, wherein the obtaining the at least one adjustment action probability based on the second result and the first result by using the reinforcement network, and determining the at least one adjustment action based on the at least one adjustment action probability comprises: obtaining a connection result based on the second result and the first result; and obtaining the at least one adjustment action probability based on the connection result by using the reinforcement network, and determining the at least one adjustment action based on the adjustment action probability.
6. The method according to claim 1, further comprising: training the system based on a sample image, wherein the sample image comprises an annotated sample target result, wherein the training the system based on the sample image comprises: processing the sample image by using the first network of the system to obtain a first sample result; inputting the first sample result into the second network of the system to obtain a second sample result; obtaining a reinforcement operation based on the second sample result by using the reinforcement network, adjusting the first sample result based on the reinforcement operation, and obtaining a predicted target result based on the adjusted first sample result; and respectively adjusting parameters of the second network and the reinforcement network based on the predicted target result and the annotated sample target result.
7. The method according to claim 6, wherein the obtaining the reinforcement operation based on the second sample result by using the reinforcement network, adjusting the first sample result based on the reinforcement operation, and obtaining the predicted target result based on the adjusted first sample result comprises: obtaining the reinforcement operation based on the second sample result by means of the reinforcement network, and adjusting the first sample result based on the reinforcement operation to obtain a second intermediate sample result; obtaining an intermediate predicted result by using the second network based on the second intermediate sample result, and inputting the intermediate predicted result into the reinforcement network; and outputting the intermediate predicted result as the predicted target result in response to a preset condition being met, wherein the adjusting the parameters of the reinforcement network based on the predicted target result and the annotated sample target result comprises: determining a first loss based on the intermediate predicted result and the annotated sample target result; and adjusting the parameters of the reinforcement network based on at least one the first loss.
8. The method according to claim 7, wherein the adjusting the parameters of the reinforcement network based on the at least one first loss comprises: adjusting the parameters of the reinforcement network based on the at least one first loss according to an order in which the at least one first loss is obtained.
9. The method according to claim 7, wherein the determining the first loss based on the intermediate predicted result and the annotated sample target result comprises: determining a current intermediate reward based on the intermediate predicted result and the sample target result; determining a current predicted reward based on the intermediate predicted result by using a scoring network, wherein the scoring network and the reinforcement network are configured to share part of a network structure; determining a next intermediate predicted result based on the intermediate predicted result by using the reinforcement network and the second network, and determining a next predicted reward based on the next intermediate predicted result by using the scoring network; and determining the first loss based on the current intermediate reward, the current predicted reward, and the next predicted reward.
10. A system reinforcement learning apparatus, comprising: a processor; and a memory configured to store instructions executable by the processor, wherein the processor is configured to: process an input image to obtain a first result; process the first result to obtain a second result; obtain a reinforcement operation based on the second result; and adjust the first result based on the reinforcement operation and obtain a target result based on the adjusted first result.
11. The apparatus according to claim 10, wherein the processor is configured to: adjust the first result based on the second result to obtain a first intermediate result; and input the first intermediate result to obtain the second result based on the first intermediate result, input the second result into a reinforcement network, and output the second result as the target result in response to a preset condition being met.
12. The apparatus according to claim 11, wherein the processor is configured to: obtain at least one adjustment action probability based on the second result and determine at least one adjustment action based on the at least one adjustment action probability; and adjust the first result based on the at least one adjustment action to obtain the first intermediate result.
13. The apparatus according to claim 11, wherein the processor is configured to obtain at least one adjustment action probability based on the second result and the first result, and determine the at least one adjustment action based on the adjustment action probability; and adjust the first result based on the at least one adjustment action to obtain the first intermediate result.
14. The apparatus according to claim 13, wherein the processor is configured to: obtain a connection result based on the second result and the first result; and obtain the at least one adjustment action probability based on the connection result by using the reinforcement network, and determine the at least one adjustment action based on the at least one adjustment action probability.
15. The apparatus according to claim 10, wherein the processor is further configured to: train a system comprising a first network, a second network, and a reinforcement network based on a sample image, wherein the sample comprises an annotated sample target result; and input the sample image to obtain a predicted target result; and respectively adjust parameters of the second network and the reinforcement network based on the predicted target result and the annotated sample target result.
16. The apparatus according to claim 15, wherein the training module is configured to: input the sample image into the first network module, the second network module, the reinforcement network module, and the executor to obtain a predicted target result; and respectively adjust parameters of the second network and the reinforcement network based on the predicted target result and the annotated sample target result.
17. The apparatus according to claim 16, wherein the processor is configured to: obtain a reinforcement operation based on a second sample result; adjust a first sample result based on the reinforcement operation to obtain a second intermediate sample result; obtain an intermediate predicted result based on the second intermediate sample result, input the intermediate predicted result into the reinforcement network of the system, and output the intermediate predicted result as the predicted target result in response to a preset condition being met, wherein the operation of adjusting the parameters of the reinforcement network based on the predicted target result and the annotated sample target result comprises: determining a first loss based on the intermediate predicted result and the annotated sample target result; and adjusting the parameters of the reinforcement network based on at least one the first loss.
18. The apparatus according to claim 17, wherein the processor is configured to adjust the parameters of the reinforcement network based on the at least one first loss according to an order in which the at least one first loss is obtained.
19. The apparatus according to claim 17, wherein the processor is configured to: determine a current intermediate reward based on the intermediate predicted result and the annotated sample target result; determine a current predicted reward based on the intermediate predicted result by using a scoring network, wherein the scoring network and the reinforcement network are configured to share part of a network structure; determine a next intermediate predicted result based on the intermediate predicted result by using the reinforcement network and the second network; and determine a next predicted reward based on the next intermediate predicted result by means of the scoring network; and determine the first loss based on the current intermediate reward, the current predicted reward, and the next predicted reward.
20. A non-transitory computer storage medium, configured to store computer readable instructions, wherein the instructions, when being executed, implement a system reinforcement learning method, the method comprising: processing an input image based on a first network of a system to obtain a first result; inputting the first result into a second network of the system to obtain a second result; and obtaining a reinforcement operation based on the second result by using a reinforcement network, adjusting the first result based on the reinforcement operation, and obtaining a target result based on the adjusted first result.
</claims>
</document>
