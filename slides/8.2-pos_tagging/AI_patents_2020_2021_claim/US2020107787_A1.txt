<document>

<filing_date>
2019-09-30
</filing_date>

<publication_date>
2020-04-09
</publication_date>

<priority_date>
2018-10-09
</priority_date>

<ipc_classes>
A61B5/00,A61K9/00,G06K9/62,G16H30/40,G16H50/50
</ipc_classes>

<assignee>
LIGHT AI
</assignee>

<inventors>
WHITEHEAD, PETER
SARKARIA, SARBJIT
MALIAPEN, MAHENDRAN
GUPTA, UDIT
REBIFFE, STEVEN
</inventors>

<docdb_family_id>
70052773
</docdb_family_id>

<title>
Image Processing of Streptococcal Infection in Pharyngitis Subjects
</title>

<abstract>
A method for determining a disease state prediction, relating to a potential disease or medical condition of a subject, includes accessing a set of subject images, the subject images capturing a part of a subject's body, and accessing a set of clinical factors from the subject. The clinical factors are collected by a device or a medical practitioner substantially contemporaneously with the capture of the subject images. The subject images are inputted into an image model to generate disease metrics for disease prediction for the subject. The disease metrics generated by the image model and the clinical factors are inputted into a classifier to determine the disease state prediction, and the disease state prediction is returned.
</abstract>

<claims>
1. A method comprising: accessing a set of subject throat images from a subject capturing an inside of the subject's throat; accessing a set of clinical factors from the subject, the clinical factors collected by a device or a medical practitioner substantially contemporaneously with the capture of the subject throat images; inputting the subject throat images into an image model to generate a prediction regarding a pathogen presence prediction for the subject; inputting the pathogen presence prediction and the clinical factors into a classifier to determine a disease state prediction; and returning the disease state prediction.
2. The method of claim 1, wherein the image model comprises: a set of image parameter coefficients trained using a first set of training throat images and a first set of training labels, each corresponding to a first set of training subjects, the first set of training labels comprising: a viral label indicating a presence of a viral pathogen, a bacterial label indicating a presence of a bacterial pathogen, and a clear label indicating an absence of pathogens; and a function relating one of the throat images and the image parameter coefficients to the pathogen presence prediction.
3. The method of claim 1, wherein the classifier comprises: a set of classifier parameter coefficients trained using a set of training pathogen presence predictions, a set of training clinical factors, and a second set of training labels, each corresponding to a second set of training subjects, the second set of training labels comprising: a viral label indicating a presence of a viral pathogen, a bacterial label indicating a presence of a bacterial pathogen, and a clear subset indicating an absence of pathogens, the set of training pathogen presence predictions generated by inputting a second set of training throat images corresponding to the second set of subjects into the image model; the set of training clinical factors collected by a device or a medical practitioner substantially contemporaneously with the capture of the second set of training throat images, and a function relating the pathogen presence predictions, the clinical factors, and the classifier parameter coefficients to the disease state prediction.
4. The method of claim 1, wherein the set of training throat images were captured using the same image capture device used to capture the set of subject throat images.
5. The method of claim 4, wherein the set of training throat images and the set of subject throat images each comprise a plurality of throat images captured under ambient light conditions, a plurality of throat images captured under fluorescent light, and a plurality of throat images captured under white light illumination.
6. The method of claim 1, wherein the set of subject throat images is recorded with an image capture device comprising: a housing; a light emitter configured to emit excitation light at a wavelength selected to elicit auto-fluorescence of a pathogen; a light sensor configured to detect light emissions or an absence of light emissions resulting from the auto-fluorescence of the pathogen; and a display.
7. The method of claim 6, wherein the disease state prediction is displayed on the display of the image capture device.
8. The method of claim 1, wherein the set of subject throat images is recorded by a mobile phone device.
9. The method of claim 8, wherein the disease state prediction is displayed on the mobile phone device.
10. The method of claim 1, wherein the set of subject throat images comprises at least one blue throat image captured using a blue light emitter; and and at least one white throat image captured using a white light emitter.
11. The method of claim 1, wherein the subject throat images captures data regarding multiple wavelengths of light.
12. The method of claim 1, wherein at least one of the subject throat images captures infrared light image data.
13. The method of claim 1, wherein the set of subject throat images are pre-processed before being input into the image model, the pre-processing comprising at least one from the group consisting of: uniform aspect ratio correction, rescaling, normalization, object detection, segmentation, cropping, dimensionality reduction, dimensionality increment, brightness adjustment, image shifting, image flipping, zoom in or out, image rotation, image quality filtering, and image pixel correction.
14. The method of claim 1, wherein the image model is a convolutional neural network (CNN).
15. The method of claim 1, wherein the classifier is trained using one of: linear regression, logistic regression, multinomial regression, elastic net regression.
16. The method of claim 1, wherein the classifier is one of a random foreign classifier, a gradient boosted classifier, a support vector machine classifier, and a Na√Øve Bayes classifier.
17. The method of claim 1, wherein the pathogen presence prediction comprises at least one of: a probability of a presence of a viral pathogen, a probability of a presence of a bacterial pathogen, and a probability of an absence of a pathogen.
18. The method of claim 1, wherein the pathogen presence prediction comprises at least one of: a probability of a presence of exudate, a probability of a presence of petechiae, a probability of a presence of swollen tonsils, and a probability of a presence of a swollen uvula.
19. The method of claim 1, the disease state prediction comprises at least one of: a probability of viral pathogen infection, a probability of bacterial pathogen infection, and a probability of no pathogen infection.
20. The method of claim 1, wherein the set of clinical factors comprises at least one from the group consisting of: age, a presence or absence of swollen lymph nodes, subject temperature, a presence or absence of a fever, and a presence or absence of a cough.
21. The method of claim 1, wherein the set of clinical factors comprises at least one from the group consisting of: age, a presence or absence of swollen lymph nodes, subject temperature, a presence or absence of fever a presence or absence of a cough, a presence or absence of a runny nose, a presence or absence of a headache, a presence or absence of body aches, a presence or absence of vomiting, a presence or absence of diarrhea, a presence or absence of fatigue, a presence or absence of chills, and a duration of pharyngitis.
22. A computer system comprising a computer processor and a memory, the memory storing computer program instructions that when executed by the computer processor cause the processor to: access a set of subject throat images from a subject; access a set of clinical factors from the subject, the clinical factors collected by a device or a medical practitioner substantially contemporaneously with the capture of the subject throat images; input the subject throat images into an image model to generate a prediction regarding a pathogen presence prediction for the subject; input the pathogen presence prediction and the clinical factors into a classifier to determine a disease state prediction; and return the disease state prediction.
23. A non-transitory computer readable storage medium comprising computer program instructions that when executed by a computer processor cause the processor to: access a set of subject throat images from a subject; access a set of clinical factors from the subject, the clinical factors collected by a device or a medical practitioner substantially contemporaneously with the capture of the subject throat images; input the subject throat images into an image model to generate a prediction regarding a pathogen presence prediction for the subject; input the pathogen presence prediction and the clinical factors into a classifier to determine a disease state prediction; and return the disease state prediction.
24. A method comprising: accessing a set of subject images, the subject images capturing a part of a subject's body; accessing a set of clinical factors from the subject, the clinical factors collected by a device or a medical practitioner substantially contemporaneously with the capture of the subject images; inputting the subject images into an image model to generate disease metrics for disease prediction for the subject; inputting the disease metrics and the clinical factors into a classifier to determine a disease state prediction, the disease state prediction relating to a disease or medical condition; and returning the disease state prediction.
25. The method of claim 24, wherein the disease metrics comprise: feature metrics corresponding to identified features in the subject image, and infection metrics corresponding to a presence of a bacterial or viral infection in the part of the subject's body.
26. The method of claim 24, wherein the image model comprises: a set of image parameter coefficients trained using a first set of training subject images and a first set of training labels, each corresponding to a first set of training subjects, the first set of training labels comprising: a viral label indicating a presence of a viral pathogen, a bacterial label indicating a presence of a bacterial pathogen, and a clear label indicating an absence of pathogens; and a function relating one of the subject images and the image parameter coefficients to the disease metrics.
27. The method of claim 24, wherein the classifier comprises: a set of classifier parameter coefficients trained using a set of training disease metrics, a set of training clinical factors, and a second set of training labels, each corresponding to a second set of training subjects, the second set of training labels comprising: a viral label indicating a presence of a viral pathogen, a bacterial label indicating a presence of a bacterial pathogen, and a clear subset indicating an absence of pathogens, the set of training disease metrics generated by inputting a second set of training subject images corresponding to the second set of subjects into the image model; the set of training clinical factors collected by a device or a medical practitioner substantially contemporaneously with the capture of the second set of training subject images, and a function relating the disease metrics, the clinical factors, and the classifier parameter coefficients to the disease state prediction.
</claims>
</document>
