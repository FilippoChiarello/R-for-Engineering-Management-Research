<document>

<filing_date>
2019-06-18
</filing_date>

<publication_date>
2020-01-16
</publication_date>

<priority_date>
2017-04-28
</priority_date>

<ipc_classes>
G06F12/0862,G06F9/30,G06F9/38,G06F9/50,G06N3/04,G06N3/063,G06N3/08,G06T15/00
</ipc_classes>

<assignee>
INTEL CORPORATION
</assignee>

<inventors>
APPU, ABHISHEK R.
ASHBAUGH, BEN J.
CHEN FENG
GALOPPO VON BORRIES, NICOLAS C.
JAHAGIRDAR, SANJEEV
KOKER, ALTUG
RANGANATHAN VASANTH
RAY, JOYDEEP
SINHA, KAMAL
SRINIVASA, NARAYAN
SURTI, PRASOONKUMAR
VEMBU, BALAJI
</inventors>

<docdb_family_id>
61655692
</docdb_family_id>

<title>
Intelligent thread dispatch and vectorization of atomic operations
</title>

<abstract>
A mechanism is described for facilitating intelligent dispatching and vectorizing at autonomous machines. A method of embodiments, as described herein, includes detecting a plurality of threads corresponding to a plurality of workloads associated with tasks relating to a graphics processor. The method may further include determining a first set of threads of the plurality of threads that are similar to each other or have adjacent surfaces, and physically clustering the first set of threads close together using a first set of adjacent compute blocks.
</abstract>

<claims>
21. An apparatus comprising: one or more processors including a graphics processor; and a memory for storage of cache data for threads to be processed by the one or more processors; wherein the one or more processors are to: schedule a plurality of threads corresponding to a plurality of workloads for the one or more processors, load the plurality of threads for processing, prefetch data for one or more threads of the plurality of threads during loading of the one or more threads by the one or more processors, and store the prefetched data for the one or more threads in the memory.
22. The apparatus of claim 21, wherein the prefetched data includes one or more values that are currently constant as the one or more threads are loaded.
23. The apparatus of claim 21, wherein the one or more processors are further to generate information regarding data to be used by the one or more threads, wherein the prefetching of data is based at least in part on the information.
24. The apparatus of claim 23, wherein generating information regarding data to be used by the one or more threads includes obtaining addresses for a data block to be processed by the one or more threads.
25. The apparatus of claim 21, wherein the one or more threads are to be loaded into one or more shader cores.
26. The apparatus of claim 21, wherein the one or more threads are loaded in a streaming multiprocessor (SM) of a plurality of SMs of the graphics processor.
27. The apparatus of claim 21, wherein the memory is a register file.
28. The apparatus of claim 21, wherein the memory is a shared local memory.
29. A computing system comprising: one or more processors including a graphics processor, the graphics process including a plurality of streaming multiprocessors; and a shared local memory for storage of cache data for a thread group including a plurality of threads to be processed by the plurality of streaming multiprocessors; wherein the one or more processors are to: schedule the thread group for the plurality of streaming multiprocessors, load the threads of the thread group for processing, prefetch data for the thread group during loading of the thread group by the one or more processors, and store the prefetched data for the thread group in the shared local memory.
30. The computing system of claim 29, wherein the prefetched data includes one or more values that are currently constant as the threads of the thread group are loaded.
31. The computing system of claim 29, wherein the one or more processors are further to generate information regarding data to be used by the thread group, wherein the prefetching of data is based at least in part on the information.
32. The computing system of claim 31, wherein generating information regarding data to be used by the one or more threads includes obtaining addresses for a data block to be processed by the one or more threads.
33. The computing system of claim 29, wherein the threads of the thread group are to be loaded into one or more shader cores.
34. At least one non-transitory machine-readable medium comprising instructions that when executed by a computing device, cause the computing device to perform operations comprising: schedule a plurality of threads corresponding to a plurality of workloads for one or more processors including a graphics processor; load the plurality of threads for processing; prefetch data for one or more threads of the plurality of threads during loading of the one or more threads by the one or more processors; and store the prefetched data for the one or more threads in a memory.
35. The medium of claim 34, wherein the prefetched data includes one or more values that are currently constant as the one or more threads are loaded.
36. The medium of claim 35, wherein the operations further comprise: generating information regarding data to be used by the one or more threads, wherein the prefetching of data is based at least in part on the information.
37. The medium of claim 36, wherein generating information regarding data to be used by the one or more threads includes obtaining addresses for a data block to be processed by the one or more threads.
38. The medium of claim 37, wherein obtaining addresses for the data block includes parsing a header for the one or more threads.
39. The medium of claim 34, wherein the one or more threads are to be loaded into one or more shader cores.
40. The medium of claim 34, wherein the one or more threads are loaded in a streaming multiprocessor (SM) of a plurality of SMs of the graphics processor.
</claims>
</document>
