<document>

<filing_date>
2019-03-27
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2019-03-27
</priority_date>

<ipc_classes>
B60R11/04,G06N20/20,G06N3/04,G06N3/08,G06T11/00,G06T5/00,G06T5/50
</ipc_classes>

<assignee>
GM GLOBAL TECHNOLOGY OPERATIONS
</assignee>

<inventors>
ZENG, SHUQING
TONG, WEI
PENG, FARUI
Bian, Chengqi
</inventors>

<docdb_family_id>
72603650
</docdb_family_id>

<title>
Semantic preserved style transfer
</title>

<abstract>
A method for image style transfer using a Semantic Preserved Generative Adversarial Network (SPGAN) includes: receiving a source image; inputting the source image into the SPGAN; extracting a source-semantic feature data from the source image; generating, by the first decoder, a first synthetic image including the source semantic content of the source image in a target style of a target image using the source-semantic feature data extracted by the first encoder of the first generator network, wherein the first synthetic image includes first-synthetic feature data; determining a first encoder loss using the source-semantic feature data and the first-synthetic feature data; discriminating the first synthetic image against the target image to determine a GAN loss; determining a total loss as a function of the first encoder loss and the first GAN loss; and training the first generator network and the first discriminator network.
</abstract>

<claims>
1. A method for image style transfer using a Semantic Preserved Generative Adversarial Network (SPGAN), comprising: receiving, by a processor, a source image, wherein the source image was captured by a camera, the source image includes a source semantic content, and the source semantic content includes objects in the source image and an arrangement of the objects in the source image; inputting the source image into the SPGAN, wherein the SPGAN includes a first generator network including a first encoder and a first decoder, and a first discriminator network, wherein the SPGAN runs on the processor; extracting, by the first encoder of the first generator network, a source-semantic feature data from the source semantic content of the source image; generating, by the first decoder of the first generator network, a first synthetic image including the source semantic content of the source image in a target style of a target image using the source-semantic feature data extracted by the first encoder of the first generator network, wherein the target style is a spatially-average colors and a texture of the target image, and the first synthetic image includes first-synthetic feature data; determining, by the processor, a first encoder loss using the source-semantic feature data and the first-synthetic feature data; discriminating, using the first discriminator network, the first synthetic image generated by the first generator network against the target image to determine a GAN loss; determining a total loss as a function of the first encoder loss and the first GAN loss; and training, by the processor, the first generator network and the first discriminator network using the first encoder loss and the first GAN loss until the total loss is equal to or less than a predetermined loss threshold or reach the maximum number of training iterations in order to minimize image distortion during the image style transfer.
2. The method of claim 1, further comprising receiving, by the processor, the target image.
3. The method of claim 2, further comprising inputting the first synthetic image into a second generator network, wherein the second generator network includes a second encoder and a second decoder, and the first synthetic image has a first-synthetic semantic content, and the first-synthetic semantic content is objects in the first synthetic image and an arrangement of the objects in the first synthetic image.
4. The method of claim 3, further comprising extracting, by the second encoder of the second generator network, a first-synthetic feature data from a first-synthetic semantic content of the first synthetic image.
5. The method of claim 4, further comprising generating, by the second decoder of the second generator network, a second synthetic image including the first-synthetic semantic content of the first synthetic image in a source style of the source image using the first-synthetic feature data extracted by the second encoder of the second generator network, wherein the source style is a spatially-average colors and a texture of the source image, wherein the second synthetic image has a second-synthetic feature data.
6. The method of claim 5, further comprising determining, by the processor, a second encoder loss using the first-synthetic feature data and the second-synthetic feature data.
7. The method of claim 6, wherein the total loss is a function of the first encoder loss, the second encoder loss, and the first GAN loss and the second GAN loss.
8. The method of claim 7, further comprising determining a cycle loss using the second-synthetic feature data and a source image synthetic data.
9. The method of claim 8, wherein the total loss is a function of the first encoder loss, the second encoder loss, the first GAN loss, the second GAN loss and the cycle loss.
10. The method of claim 9, further comprising inputting the second synthetic image into the second generator network.
11. The method of claim 9, wherein the camera is part of a vehicle.
12. A system for image style transfer using Semantic Preserved Generative Adversarial Network (SPGAN), comprising: a plurality of sensors, wherein at least one of the sensors is a camera; a processor in communication with the plurality of sensors; wherein the processor is programmed: receive a source image, wherein the source image was captured by a camera, the source image includes a source semantic content, and the source semantic content is objects in the source image and an arrangement of the objects in the source image; input the source image into the SPGAN, wherein the SPGAN includes a first generator network including a first encoder and a first decoder, and a first discriminator network, wherein the SPGAN runs on the processor; extracting, by the first encoder of the first generator network, a source-semantic feature data from the source semantic content of the source image; generate, by the first decoder of the first generator network, a first synthetic image including the source semantic content of the source image in a target style of a target image using the source-semantic feature data extracted by the first encoder of the first generator network, wherein the target style is a spatially-average colors and a texture of the target image, and the first synthetic image includes first-synthetic feature data; determine a first encoder loss using the source-semantic feature data and the first-synthetic feature data; discriminate, using the first discriminator network, the first synthetic image generated by the first generator network against the target image to determine a GAN loss; determine a total loss as a function of the first encoder loss and the first GAN loss; and train the first generator network and the first discriminator network using the first encoder loss and the first GAN loss until the total loss is equal to or less than a predetermined loss threshold or reach the maximum number of training iterations in order to minimize image distortion during the image style transfer.
13. The system of claim 12, wherein the processor is programmed to receive the target image.
14. The system of claim 13, wherein the processor is programmed to input the first synthetic image into a second generator network, wherein the second generator network includes a second encoder and a second decoder, and the first synthetic image has a first-synthetic semantic content, and the first-synthetic semantic content is objects in the first synthetic image and an arrangement of the objects in the first synthetic image.
15. The system of claim 14, wherein the processor is programmed to extract, by the second encoder of the second generator network, a first-synthetic feature data from a first-synthetic semantic content of the first synthetic image.
16. The system of claim 15, wherein the processor is programmed to generate, by the second decoder of the second generator network, a second synthetic image including the first-synthetic semantic content of the first synthetic image in a source style of the source image using the first-synthetic feature data extracted by the second encoder of the second generator network, wherein the source style is a spatially-average colors and a texture of the source image, wherein the second synthetic image has a second-synthetic feature data.
17. The system of claim 16, wherein the processor is programmed to determine a second encoder loss using the first-synthetic feature data and the second-synthetic feature data.
18. The system of claim 17, wherein the total loss is a function of the first encoder loss, the second encoder loss, and the first GAN loss and the second GAN loss.
19. The system of claim 18, wherein the processor is programmed to determine a cycle loss using the second-synthetic feature data and source-semantic feature data.
20. The system of claim 19, wherein the total loss is a function of the first encoder loss, the second encoder loss, the first GAN loss, the second GAN loss, and the cycle loss, the processor is programmed to input the second synthetic image into the second generator network, and the camera is part of a vehicle.
</claims>
</document>
