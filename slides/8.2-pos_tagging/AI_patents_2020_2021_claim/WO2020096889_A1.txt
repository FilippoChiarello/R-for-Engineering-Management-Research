<document>

<filing_date>
2019-11-01
</filing_date>

<publication_date>
2020-05-14
</publication_date>

<priority_date>
2018-11-05
</priority_date>

<ipc_classes>
A61B1/00,A61B1/04,A61B90/70,B08B7/04
</ipc_classes>

<assignee>
MEDIVATORS
</assignee>

<inventors>
CHEONG, MER WIN
</inventors>

<docdb_family_id>
70611101
</docdb_family_id>

<title>
ASSESSING ENDOSCOPE CHANNEL DAMAGE USING ARTIFICIAL INTELLIGENCE VIDEO ANALYSIS
</title>

<abstract>
Embodiments of a system and method for training and using a model for assessing endoscope channel damage, based on artificial intelligence image and video analysis techniques, are generally described. In example embodiments, an analysis is performed by: receiving image data captured from a borescope advancing in a lumen of an endoscope; evaluating the image data with a trained artificial intelligence model to generate a predicted state of the lumen from the image data, with the model trained to generate the predicted state based on a feature identified from the image data; and outputting data that represents the predicted state of the lumen of the endoscope. Corresponding techniques for training the model are also performed by: receiving training image data captured from a lumen of an endoscope; performing feature extraction; training a model based on the identified features; and outputting the model for use in analyzing images.
</abstract>

<claims>
What is claimed is:
1. A method of artificial intelligence analysis for endoscope inspection, comprising: receiving image data captured from a borescope, the image data capturing at least one image of a lumen of an endoscope;
analyzing the image data with a trained artificial intelligence model to generate a predicted state of the lumen from the image data, the trained model configured to generate the predicted state based on a trained feature identified from the at least one image of the lumen; and
outputting data that represents the predicted state of the lumen of the endoscope.
2. The method of claim 1, further comprising:
performing feature extraction on the at least one image of the lumen to identify a subject feature;
wherein the subject feature is correlated to the trained feature based on a classification produced from the trained model, the trained model utilizing a trained machine learning algorithm.
3. The method of claim 2, wherein the feature extraction comprises extracting key points and feature descriptors from the at least one image of the lumen.
4. The method of claim 3, wherein the feature descriptors comprise point descriptors and patch descriptors, and wherein the feature descriptors are associated with respective classification words.
5. The method of claim 4, wherein the feature descriptors are used to populate a feature vector, and wherein respective features of the feature vector are clustered and analyzed using a bag of visual words model.
6. The method of claim 1, wherein the trained model is further configured to generate the predicted state based on a feature set including the trained feature, and wherein respective point descriptors or patch descriptors describe respective features of the feature set.
7. The method of claim 1, further comprising:
performing image pre-processing on the image data, before analyzing the image data with the trained model.
8. The method of claim 7, wherein the image pre-processing comprises at least one of: noise removal, an illumination change, geometric transformation, color mapping, resizing, cropping, or segmentation, to the at least one image of the lumen.
9. The method of claim 1, wherein the predicted state of the lumen comprises a damage state.
10. The method of claim 9, wherein the damage state is associated with at least one of: a discoloration, a foreign object, a residue, a scratch, a peeling surface, a buckling surface, or a perforation.
11. The method of claim 1, wherein the trained model is a machine learning classifier.
12. The method of claim 11, wherein the machine learning classifier is a support vector machine classifier, random forest classifier, gradient boosting classifier, or a contributionweighted combination of multiple classifiers.
13. The method of claim 11, wherein the machine learning classifier indicates a classification label associated with an integrity state or a state of damage.
14. The method of claim 1, wherein the trained model is a deep neural network, convolutional neural network, or recurrent neural network.
15. The method of claim 1, wherein the image data comprises a plurality of images extracted from a video, and wherein the analyzing of the image data with the trained artificial intelligence model is performed using multiple of the plurality of images extracted from the video.
16. The method of claim 1, wherein the image data comprises a sequence of images from a first position in the lumen to a second position in the lumen.
17. A computing device, comprising:
at least one processor; and
at least one memory device including instructions embodied thereon, wherein the instructions, when executed by the processor, cause the processor to perform artificial intelligence analysis operations for endoscope inspection, with the operations comprising: receiving image data captured from a borescope, the image data capturing at least one image of a lumen of an endoscope;
analyzing the image data with a trained artificial intelligence model to generate a predicted state of the lumen from the image data, the trained model configured to generate the predicted state based on a trained feature identified from the at least one image of the lumen; and
outputting data that represents the predicted state of the lumen of the endoscope.
18. The computing device of claim 17, the operations further comprising:
performing feature extraction on the at least one image of the lumen to identify a subject feature;
wherein the subject feature is correlated to the trained feature based on a classification produced from the trained model, the trained model utilizing a trained machine learning algorithm.
19. The computing device of claim 18, wherein the feature extraction comprises extracting key points and feature descriptors from the at least one image of the lumen.
20. The computing device of claim 19, wherein the feature descriptors comprise point descriptors and patch descriptors, and wherein the feature descriptors are associated with respective classification words.
21. The computing device of claim 20, wherein the feature descriptors are used to populate a feature vector, and wherein respective features of the feature vector are clustered and analyzed using a bag of visual words model.
22. The computing device of claim 17, wherein the trained model is further configured to generate the predicted state based on a feature set including the trained feature, and wherein respective point descriptors or patch descriptors describe respective features of the feature set.
23. The computing device of claim 17, the operations further comprising:
performing image pre-processing on the image data, before analyzing the image data with the trained model.
24. The computing device of claim 23, wherein the image pre-processing comprises at least one of: noise removal, an illumination change, geometric transformation, color mapping, resizing, cropping, or segmentation, to the at least one image of the lumen.
25. The computing device of claim 17, wherein the predicted state of the lumen comprises a damage state.
26. The computing device of claim 25, wherein the damage state is associated with at least one of: a discoloration, a foreign object, a residue, a scratch, a peeling surface, a buckling surface, or a perforation.
27. The computing device of claim 17, wherein the trained model is a machine learning classifier.
28. The computing device of claim 27, wherein the machine learning classifier is a support vector machine classifier, random forest classifier, or gradient boosting classifier.
29. The computing device of claim 27, wherein the machine learning classifier indicates a classification label associated with an integrity state or a state of damage.
30. The computing device of claim 17, wherein the trained model is a deep neural network, convolutional neural network, or recurrent neural network.
31. The computing device of claim 17, wherein the image data comprises a plurality of images extracted from a video, and wherein the analyzing of the image data with the trained artificial intelligence model is performed using multiple of the plurality of images extracted from the video.
32. The computing device of claim 17, wherein the image data comprises a sequence of images from a first position in the lumen to a second position in the lumen.
33. The computing device of claim 17, further comprising:
a user input device to receive input to select the image data and control the analyzing of the image data; and
a user output device to output a representation of data that represents the predicted state of the lumen of the endoscope.
34. A machine-readable storage medium including instructions, wherein the instructions, when executed by a processing circuitry of a computing system, cause the processing circuitry to perform operations of any of claims 1 to 33.
35. A system, comprising:
a borescope adapted to capture at least one digital image from a working channel of an endoscope; and
a visual inspection computing system, comprising a memory device and processing circuitry, the processing circuitry adapted to:
obtain the at least one digital image of the endoscope working channel;
analyze the at least one digital image with a trained artificial intelligence model to generate a predicted state of the working channel, wherein the trained model is configured to generate the predicted state based on a trained feature identified from the at least one digital image of the working channel; and
an output device adapted to provide a representation of data that represents the predicted state of the working channel of the endoscope.
36. The system of claim 35, further comprising:
a borescope movement device, adapted to advance the borescope within the working channel of the endoscope at a controlled rate to capture the at least one digital image of the endoscope working channel.
37. A method of training an artificial intelligence model for endoscope inspection, comprising:
receiving training data, the training data comprising image data capturing at least one image of a lumen of an endoscope, and respective labels associated with the at least one image;
performing feature extraction on the at least one image of the lumen to identify respective subject features;
training a model to associate the respective subject features with respective predicted states; and
outputting the model for use in analyzing subsequent images, to generate a predicted state of an image of a lumen from a subject endoscope.
38. The method of claim 37, wherein the feature extraction comprises extracting key points and feature descriptors from the at least one image of the lumen.
39. The method of claim 38, wherein the feature descriptors comprise point descriptors and patch descriptors, and wherein the feature descriptors are associated with respective words.
40. The method of claim 39, wherein the feature descriptors are used to populate a feature vector, and wherein respective features of the feature vector are clustered and analyzed using a bag of visual words model.
41. The method of claim 37, wherein the model is further trained to generate the predicted state based on a feature set including the subject features, and wherein respective point descriptors or patch descriptors describe respective features of the feature set.
42. The method of claim 37, further comprising:
performing image pre-processing on the image data, before performing the feature extraction.
43. The method of claim 42, wherein the image pre-processing comprises at least one of: noise removal, an illumination change, cropping, geometric transformation, color mapping, resizing, or segmentation, to the at least one image of the lumen.
44. The method of claim 37, wherein the predicted state of the lumen comprises a damage state.
45. The method of claim 44, wherein the damage state is associated with at least one of: a discoloration, a foreign object, a residue, a scratch, a peeling surface, a buckling surface, or a perforation.
46. The method of claim 37, wherein the model is trained as a machine learning classifier comprising a: support vector machine classifier, random forest classifier, or gradient boosting classifier.
47. The method of claim 46, wherein the machine learning classifier is adapted to provide a classification label associated with an integrity state or a state of damage.
48. The method of claim 37, wherein the image data comprises a plurality of images extracted from a video, and wherein the training of the model is performed using multiple of the plurality of images extracted from the video.
49. The method of claim 37, wherein the image data comprises a sequence of images from a first position in the lumen to a second position in the lumen.
50. A computing device, comprising:
at least one processor; and
at least one memory device including instructions embodied thereon, wherein the instructions, when executed by the processor, cause the processor to perform artificial intelligence analysis operations for training an artificial intelligence model for endoscope inspection, with the operations comprising:
receiving training data, the training data comprising image data capturing at least one image of a lumen of an endoscope, and respective labels associated with the at least one image;
performing feature extraction on the at least one image of the lumen to identify respective subject features;
training a model to associate the respective subject features with respective predicted states; and
outputting the model for use in analyzing subsequent images, to generate a predicted state of an image of a lumen from a subject endoscope.
51. The computing device of claim 50, wherein the feature extraction comprises extracting key points and feature descriptors from the at least one image of the lumen.
52. The computing device of claim 51, wherein the feature descriptors comprise point descriptors and patch descriptors, and wherein the feature descriptors are associated with respective words.
53. The computing device of claim 52, wherein the feature descriptors are used to populate a feature vector, and wherein respective features of the feature vector are clustered and analyzed using a bag of visual words model.
54. The computing device of claim 50, wherein the model is further trained to generate the predicted state based on a feature set including the subject features, and wherein respective point descriptors or patch descriptors describe respective features of the feature set.
55. The computing device of claim 50, the operations further comprising:
performing image pre-processing on the image data, before performing the feature extraction.
56. The computing device of claim 55, wherein the image pre-processing comprises at least one of: noise removal, an illumination change, cropping, geometric transformation, color mapping, resizing, or segmentation, to the at least one image of the lumen.
57. The computing device of claim 50, wherein the predicted state of the lumen comprises a damage state.
58. The computing device of claim 57, wherein the damage state is associated with at least one of: a discoloration, a foreign object, a residue, a scratch, a peeling surface, a buckling surface, or a perforation.
59. The computing device of claim 50, wherein the model is trained as a machine learning classifier comprising a: support vector machine classifier, random forest classifier, or gradient boosting classifier.
60. The computing device of claim 59, wherein the machine learning classifier is adapted to provide a classification label associated with an integrity state or a state of damage.
61. The computing device of claim 50, wherein the image data comprises a plurality of images extracted from a video, and wherein the training of the model is performed using multiple of the plurality of images extracted from the video.
62. The computing device of claim 50, wherein the image data comprises a sequence of images from a first position in the lumen to a second position in the lumen.
63. A machine-readable storage medium including instructions, wherein the instructions, when executed by a processing circuitry of a computing system, cause the processing circuitry to perform operations of any of claims 37 to 62.
</claims>
</document>
