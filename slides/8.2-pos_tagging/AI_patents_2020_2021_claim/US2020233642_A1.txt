<document>

<filing_date>
2020-04-06
</filing_date>

<publication_date>
2020-07-23
</publication_date>

<priority_date>
2018-06-05
</priority_date>

<ipc_classes>
G06F7/499
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
MUELLER, SILVIA, MELITTA
GOPALAKRISHNAN, KAILASH
AGRAWAL, ANKUR
LEE, DONGSOO
FLEISCHER, BRUCE
</inventors>

<docdb_family_id>
68692691
</docdb_family_id>

<title>
ENHANCED LOW PRECISION BINARY FLOATING-POINT FORMATTING
</title>

<abstract>
Techniques for operating on and calculating binary floating-point numbers using an enhanced floating-point number format are presented. The enhanced format can comprise a single sign bit, six bits for the exponent, and nine bits for the fraction. Using six bits for the exponent can provide an enhanced exponent range that facilitates desirably fast convergence of computing-intensive algorithms and low error rates for computing-intensive applications. The enhanced format can employ a specified definition for the lowest binade that enables the lowest binade to be used for zero and normal numbers; and a specified definition for the highest binade that enables it to be structured to have one data point used for a merged Not-a-Number (NaN)/infinity symbol and remaining data points used for finite numbers. The signs of zero and merged NaN/infinity can be "don't care" terms. The enhanced format employs only one rounding mode, which is for rounding toward nearest up.
</abstract>

<claims>
1. A system, comprising: a memory that stores computer-executable components; and a processor, operatively coupled to the memory, that executes computer-executable components, the computer-executable components comprising: a calculator component that facilitates operation on and calculation of binary floating-point numbers by the processor in accordance with a defined floating-point number format, in connection with execution of an application, wherein the defined floating-point number format utilizes greater than five bits in an exponent field.
2. The system of claim 1, wherein the computer-executable components further comprising: an enhanced format component that generates an enhanced floating-point number format employed by the processor and the calculator component to calculate the binary floating-point numbers.
3. The system of claim 1, wherein the floating-point number format utilizing six bits in the exponent field facilitates machine learning algorithms and deep learning training algorithms, and wherein the exponent field is adjacent a sign field comprising one bit of data representing a sign of the floating-point number.
4. The system of claim 3, wherein the calculator component generates an arbitrary value or symbol in the sign field of the enhanced floating-point number to reduce hardware complexity and based on a generation of a zero result for a binary floating-point number of the binary floating-point numbers.
5. The system of claim 1, wherein the system comprises at least one of an machine learning or deep learning training system.
6. The system of claim 1, wherein the system comprises a speech recognition or an image recognition system.
7. The system of claim 1, wherein the computer-executable components further comprise: an operation management component operatively coupled to the calculator component and the processor, wherein the operation management component: allocates a first portion of operations of the calculator component and associated data to a set of lower precision computation engines, wherein the associated data comprises data having a sensitivity to rounding errors less than a first defined threshold.
8. The system of claim 7, wherein the operation management component also allocates a second portion of the operations of the calculator component and second associated data to a set of higher precision computation engines, wherein the second associated data has a sensitivity to rounding errors greater than or equal to the first defined threshold.
9. The system of claim 7, wherein the set of lower precision computation engines comprises computation engines comprising 16-bit floating-point units.
10. The system of claim 8, wherein the set of higher precision computation engines comprises computation engines comprising 32-bit floating-point units or 64-bit floating-point units.
11. A computer-implemented method, comprising: generating, by a system operatively coupled to a processor, respective numerical fields in a defined floating-point number format, wherein the respective numerical fields comprise a sign field, an exponent field, and a mantissa field, wherein the defined floating-point number format utilizes greater than five bits in the exponent field; and calculating, by the system, binary floating-point numbers in accordance with the defined floating-point number format, in connection with execution of an application.
12. The computer-implemented method of claim 11, further comprising: generating, by the system, an enhanced floating-point number format to calculate the binary floating-point numbers.
13. The computer-implemented method of claim 11, wherein the floating-point number format utilizing six bits in the exponent field facilitates machine learning algorithms and deep learning training algorithms, and wherein the exponent field is adjacent a sign field comprising one bit of data representing a sign of the floating-point number.
14. The computer-implemented method of claim 11, wherein the defined floating-point number format utilizes a first binade to represent zero and normal numbers, wherein the first binade is associated with the exponent field having all zeros, and wherein a normal number of the normal numbers is a finite non-zero floating-point number with a magnitude greater than or equal to a minimum value that is determined as a function of a radix and a minimum exponent associated with the defined floating-point number format.
15. The computer-implemented method of claim 13, further comprising: generating, by the system, an arbitrary value or symbol in the sign field of the enhanced floating-point number to reduce hardware complexity and based on a generation of a zero result for a binary floating-point number of the binary floating-point numbers.
16. The computer-implemented method of claim 11, further comprising: allocating, by the system, a first portion of operations of the calculator component and associated data to a set of lower precision computation engines, wherein the associated data comprises data having a sensitivity to rounding errors less than a first defined threshold; and allocating, by the system, a second portion of the operations of the calculator component and second associated data to a set of higher precision computation engines, wherein the second associated data has a sensitivity to rounding errors greater than or equal to the first defined threshold.
17. The computer-implemented method of claim 16, wherein the set of lower precision computation engines comprises computation engines comprising 16-bit floating-point units, and wherein the set of higher precision computation engines comprises computation engines comprising 32-bit floating-point units or 64-bit floating-point units.
18. A computer program product that facilitates calculating floating-point numbers, the computer program product comprising a computer readable storage medium having program instructions embodied therewith, the program instructions are executable by a processor to cause the processor to: create respective fields in a defined floating-point number format, wherein the respective fields comprise a sign field, an exponent field, and a fraction field, wherein the defined floating-point number format contains six bits for the exponent in the exponent field; and calculate the floating-point numbers in accordance with the defined floating-point number format, in connection with utilization of an application.
19. The computer program product of claim 18, wherein the program instructions are further executable by the processor to cause the processor to: generate an enhanced floating-point number format to calculate the binary floating-point numbers, wherein the floating-point number format utilizes six bits in the exponent field facilitates machine learning algorithms and deep learning training algorithms, and wherein the exponent field is adjacent a sign field comprising one bit of data representing a sign of the floating-point number.
20. The computer program product of claim 19, wherein the program instructions are further executable by the processor to cause the processor to: generate an arbitrary value or symbol in the sign field of the enhanced floating-point number to reduce hardware complexity and based on a generation of a zero result for a binary floating-point number of the binary floating-point numbers.
</claims>
</document>
