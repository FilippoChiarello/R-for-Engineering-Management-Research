<document>

<filing_date>
2018-04-23
</filing_date>

<publication_date>
2020-12-15
</publication_date>

<priority_date>
2014-09-08
</priority_date>

<ipc_classes>
G06F16/783,G06F17/30,G06K9/00
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
IZO, TOMAS
MUSCETTOLA, NICOLA
SHETTY, SANKETH
VIJAYANARASIMHAN, SUDHEENDRA
TODERICI, GEORGE DAN
KHANDELWAL, NITIN
ABU-EL-HAIJA, SAMI
YANG, WEILONG
LE, PHUONG
GU, WEIHSIN
TSAI, MIN-HSUAN
VARADARAJAN, BALAKRISHNAN
NATSEV, APOSTOL
RICCO, SUSANNA
</inventors>

<docdb_family_id>
55437782
</docdb_family_id>

<title>
Selecting and presenting representative frames for video previews
</title>

<abstract>
A computer-implemented method for selecting representative frames for videos is provided. The method includes receiving a video and identifying a set of features for each of the frames of the video. The features including frame-based features and semantic features. The semantic features identifying likelihoods of semantic concepts being present as content in the frames of the video. A set of video segments for the video is subsequently generated. Each video segment includes a chronological subset of frames from the video and each frame is associated with at least one of the semantic features. The method generates a score for each frame of the subset of frames for each video segment based at least on the semantic features, and selecting a representative frame for each video segment based on the scores of the frames in the video segment. The representative frame represents and summarizes the video segment.
</abstract>

<claims>
1. A computer-implemented method for selecting representative frames for videos, the method comprising: receiving a video; identifying a plurality of semantic features for each of frames of the video by transmitting a designation of a frame to a plurality of semantic classifiers, receiving, from each of the plurality of semantic classifiers, a likelihood of a semantic concept being depicted in the frame of the video, and assigning a label corresponding to the semantic concept to the frame of the video based on the likelihood of the semantic concept being depicted in the frame of the video; selecting a plurality of representative frames of the video, wherein each representative frame is selected based on the assigned label; and generating a summarized video that combines at least a portion of the plurality of representative frames of the video.
2. The computer-implemented method of claim 1, wherein frame-based features are identified for each of the frames of the video, wherein the method further comprises analyzing the frame-based features to determine a set of shot boundaries within the video, and wherein a shot includes a set of sequential frames and a shot boundary indicates a frame between neighboring shots.
3. The computer-implemented method of claim 2, wherein determining the set of shot boundaries comprises applying a classifier to frames associated with the frame-based features to determine whether a frame is a shot boundary, wherein the classifier is trained using labeled shot boundaries as a positive feature set and frames near the shot boundaries as a hard-negative training set, and wherein the frame-based features comprise color differences with adjacent frames, motion features, audio volume, and audio speech detection.
4. The computer-implemented method of claim 2, wherein determining the set of shot boundaries comprises analyzing a coherence of the frame-based features, wherein the coherence measures similarity of frame-based features in a pre-determined temporal segment, and wherein the similarity provides a distance measure for segmenting the video.
5. The computer-implemented method of claim 2, wherein determining the set of shot boundaries comprises tracking the frame-based features across the series of frames of the video, wherein a frame is determined as a shot boundary when a change of frame-based features between the frame and neighboring frames is greater than a threshold.
6. The computer-implemented method of claim 1, further comprising: generating a plurality of video segments for the video, wherein each video segment includes a chronological subset of frames from the video, and wherein each frame is associated with at least one of the semantic features; and generating, for each video segment in the plurality of video segments, a score for each frame of the subset of frames of the video segment based at least on the semantic features, wherein each representative frame for each video segment in the plurality of video segments is selected based on the scores for the frames in the video segment, and wherein the representative frame represents and summarizes the video segment.
7. The computer-implemented method of claim 6, wherein the score comprises a semantic score that is generated by: identifying a plurality of semantic concepts for the video segment containing the frame by comparing each semantic feature generated for the chronological subset of frames included in the video segment to a threshold, each semantic concept of the plurality of semantic concepts having the corresponding semantic feature greater than the threshold; for each semantic concept of the plurality of semantic concepts, determining a frame-level score for each frame of the chronological subset of frames in the video segment by determining an amount the semantic concept being present in the frame compared to a reference value; and determining the semantic score for the frame by aggregating the frame-level scores of the frames in the segment.
8. The computer-implemented method of claim 1, wherein generating the score for the frame comprises combining semantic concepts and corresponding likelihood in the frame.
9. The computer-implemented method of claim 1, wherein generating the score for the frame comprises combining a semantic score and an aesthetic score by: calculating the semantic score based on the determined semantic features; calculating the aesthetic score using a set of quality measures; and combining the semantic score and the aesthetic score.
10. The computer-implemented method of claim 1, further comprising generating a segment table for the video, wherein the segment table stores the representative frames of the video and a plurality of semantic concepts associated with each of the representative frames.
11. A computer-implemented system for selecting representative frames for videos, the system comprising: a memory; and a hardware processor that, when executing computer-executable instructions stored in the memory, is configured to: receive a video; identify a plurality of semantic features for each of frames of the video by transmitting a designation of a frame to a plurality of semantic classifiers, receiving, from each of the plurality of semantic classifiers, a likelihood of a semantic concept being depicted in the frame of the video, and assigning a label corresponding to the semantic concept to the frame of the video based on the likelihood of the semantic concept being depicted in the frame of the video; select a plurality of representative frames of the video, wherein each representative frame is selected based on the assigned label; and generate a summarized video that combines at least a portion of the plurality of representative frames of the video.
12. The computer-implemented system of claim 11, wherein frame-based features are identified for each of the frames of the video, wherein the method further comprises analyzing the frame-based features to determine a set of shot boundaries within the video, and wherein a shot includes a set of sequential frames and a shot boundary indicates a frame between neighboring shots.
13. The computer-implemented system of claim 12, wherein determining the set of shot boundaries comprises applying a classifier to frames associated with the frame-based features to determine whether a frame is a shot boundary, wherein the classifier is trained using labeled shot boundaries as a positive feature set and frames near the shot boundaries as a hard-negative training set, and wherein the frame-based features comprise color differences with adjacent frames, motion features, audio volume, and audio speech detection.
14. The computer-implemented system of claim 12, wherein determining the set of shot boundaries comprises analyzing a coherence of the frame-based features, wherein the coherence measures similarity of frame-based features in a pre-determined temporal segment, and wherein the similarity provides a distance measure for segmenting the video.
15. The computer-implemented system of claim 12, wherein determining the set of shot boundaries comprises tracking the frame-based features across the series of frames of the video, wherein a frame is determined as a shot boundary when a change of frame-based features between the frame and neighboring frames is greater than a threshold.
16. The computer-implemented system of claim 11, wherein the hardware processor is further configured to: generate a plurality of video segments for the video, wherein each video segment includes a chronological subset of frames from the video, and wherein each frame is associated with at least one of the semantic features; and generate, for each video segment in the plurality of video segments, a score for each frame of the subset of frames of the video segment based at least on the semantic features, wherein each representative frame for each video segment in the plurality of video segments is selected based on the scores for the frames in the video segment, and wherein the representative frame represents and summarizes the video segment.
17. The computer-implemented system of claim 16, wherein the score comprises a semantic score that is generated by: identifying a plurality of semantic concepts for the video segment containing the frame by comparing each semantic feature generated for the chronological subset of frames included in the video segment to a threshold, each semantic concept of the plurality of semantic concepts having the corresponding semantic feature greater than the threshold; for each semantic concept of the plurality of semantic concepts, determining a frame-level score for each frame of the chronological subset of frames in the video segment by determining an amount the semantic concept being present in the frame compared to a reference value; and determining the semantic score for the frame by aggregating the frame-level scores of the frames in the segment.
18. The computer-implemented system of claim 11, wherein generating the score for the frame comprises combining semantic concepts and corresponding likelihood in the frame.
19. The computer-implemented system of claim 11, wherein generating the score for the frame comprises combining a semantic score and an aesthetic score by: calculating the semantic score based on the determined semantic features; calculating the aesthetic score using a set of quality measures; and combining the semantic score and the aesthetic score.
20. The computer-implemented system of claim 11, wherein the hardware processor is further configured to generate a segment table for the video, wherein the segment table stores the representative frames of the video and a plurality of semantic concepts associated with each of the representative frames.
21. A non-transitory computer-readable medium comprising computer-executable instructions that, when executed by a processor, cause the processor to perform a method for selecting representative frames for videos, the method comprising: receiving a video; identifying a plurality of semantic features for each of frames of the video by transmitting a designation of a frame to a plurality of semantic classifiers, receiving, from each of the plurality of semantic classifiers, a likelihood of a semantic concept being depicted in the frame of the video, and assigning a label corresponding to the semantic concept to the frame of the video based on the likelihood of the semantic concept being depicted in the frame of the video; selecting a plurality of representative frames of the video, wherein each representative frame is selected based on the assigned label; and generating a summarized video that combines at least a portion of the plurality of representative frames of the video.
</claims>
</document>
