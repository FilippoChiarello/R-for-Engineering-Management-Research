<document>

<filing_date>
2019-06-04
</filing_date>

<publication_date>
2020-12-10
</publication_date>

<priority_date>
2019-06-04
</priority_date>

<ipc_classes>
G06F16/71,G06F16/738,G06F16/783,G06K9/00
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
BAHL, PARAMVIR
ANANTHANARAYANAN, GANESH
COX, Landon
CROWN, Alexander
NOGHABI, Shadi
SHU, Yuanchao
</inventors>

<docdb_family_id>
70779855
</docdb_family_id>

<title>
CASCADED VIDEO ANALYTICS FOR EDGE COMPUTING
</title>

<abstract>
This document relates to performing live video stream analytics on edge devices. One example determines resources available to the system, and a video analytics configuration is selected that distributes work between edge devices and cloud devices in a cascading manner, where edge device processing is prioritized over cloud processing in order to conserve resources. This example can dynamically modify the allocation of processing depending on changing conditions, such as network availability.
</abstract>

<claims>
1. A system comprising: a processor; and a storage memory storing computer-readable instructions, which when executed by the processor, cause the processor to: receive a video query regarding a live video stream; determine resources available to the system and a defined threshold confidence value associated with the video query; select a configuration for processing the video query based at least on the determined resources; allocate processing between one or more cameras and one or more edge devices according to the selected configuration; and adjust the selected configuration to include processing among one or more cloud devices when processing results from the one or more cameras and the one or more edge devices do not meet the defined threshold confidence value.
2. The system of claim 1, wherein the selected configuration directs the one or more cameras or the one or more edge devices to extract video frames from the live video stream using a decoding module.
3. The system of claim 2, wherein the selected configuration directs the one or more cameras or the one or more edge devices to perform background subtraction on the extracted video frames.
4. The system of claim 3, wherein the background subtraction is performed on the extracted video frames to determine whether additional processing should be performed.
5. The system of claim 3, wherein the selected configuration directs the one or more cameras or the one or more edge devices to perform processing of the extracted video frames using a lightweight DNN model locally on the one or more cameras or the one or more edge devices.
6. The system of claim 5, wherein the selected configuration directs the one or more cloud devices to perform processing of the extracted video frames using a heavy DNN model when results from the lightweight DNN model do not meet the defined threshold confidence value.
7. The system of claim 6, wherein the lightweight DNN model comprises at least a first lightweight DNN model, and a second lightweight DNN model that requires additional computational resources than the first lightweight DNN model, but less computational resources than the heavy DNN model.
8. The system of claim 7, wherein the heavy DNN model comprises at least a first heavy DNN model, and a second heavy DNN model that requires additional computational resources than the first heavy DNN model.
9. The system of claim 6, wherein the computer-readable instructions, when executed by the processor, further cause the processor to: assign tags to objects discovered during processing of the extracted video frames; and store the tags in an index database for use in locating the objects in response to a query on a stored version of the live video stream.
10. The system of claim 1, wherein the computer-readable instructions, when executed by the processor, further cause the processor to: dynamically determine whether resources available to the system have changed; and when the resource availability has changed, modify the allocation of processing among the one or more cameras, the one or more edge devices, and the one or more cloud devices based at least on the resource availability having changed.
11. The system of claim 1, wherein determining resources available to the system further comprises determining whether network connectivity to the one or more cloud devices is available.
12. The system of claim 1, wherein the selected configuration is adjusted to an edge-only mode of processing by allocating all processing between the one or more cameras and the one or more edge devices when network connectivity to the one or more cloud devices is unavailable or bandwidth to the one or more cloud devices is insufficient.
13. A method comprising: allocating processing of input data between one or more edge devices and one or more cloud devices, the one or more edge devices using an edge processing model, and the one or more cloud devices using a cloud processing model different from the edge processing model; determining a current network capability between the one or more edge devices and one or more cloud devices; and shifting processing load of the input data to increase processing by the one or more edge devices using a moderate computationally-intensive algorithm upon determining that the current network capability between the one or more edge devices and the one or more cloud devices is unavailable.
14. The method of claim 13, further comprising allocating processing to one or more smart devices, the one or more smart devices performing processing that is computationally cheaper than the edge processing model used by the one or more edge devices.
15. The method of claim 13, further comprising dynamically shifting the processing load of the input data back to the one or more cloud devices upon determining that the current network capability between the one or more edge devices and the one or more cloud devices has been restored.
16. The method of claim 13, wherein the cloud processing model is a more computationally expensive model than the edge processing model.
17. A method comprising: receiving input video data from one or more cameras; accessing a database of a plurality of video processing configurations; evaluating the plurality of video processing configurations against resource availability across local devices and cloud devices; and selecting a configuration that allocates processing to the one or more cameras, one or more edge devices, and one or more cloud devices.
18. The method of claim 17, wherein the video processing configurations specify a frame resolution, frame rate, and a type of DNN model to be used in processing the input video data.
19. The method of claim 17, wherein the video processing configurations each have a resource cost, and a configuration is selected that achieves an optimal tradeoff between resource cost and average accuracy.
20. The method of claim 17, further comprising dynamically modifying the selected configuration upon determining that the resource availability has changed.
</claims>
</document>
