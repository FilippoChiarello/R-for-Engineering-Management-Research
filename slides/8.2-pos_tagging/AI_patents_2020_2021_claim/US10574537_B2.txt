<document>

<filing_date>
2019-04-05
</filing_date>

<publication_date>
2020-02-25
</publication_date>

<priority_date>
2018-07-03
</priority_date>

<ipc_classes>
G06N3/08,H04L12/24,H04L29/06
</ipc_classes>

<assignee>
UBITUS
</assignee>

<inventors>
TENG, AN-LUN
CHIANG, RONG
KUO, JUNG-CHANG
WEI, TZER-JEN
TSENG, YU-JU
</inventors>

<docdb_family_id>
66334032
</docdb_family_id>

<title>
Method for enhancing quality of media transmitted via network
</title>

<abstract>
A method for enhancing quality of media transmitted via network uses an AI enhancing model built-in the client device to enhance the quality of video streams received from network. The AI enhance module is pre-trained by using a neural network in the server to analyze differences between the decoded images and the raw images that are generated by the server. Wherein, the AI enhance module enhances decoded images by using algorithms which are defined by analyzing differences between the decoded images and the raw images that are generated by the server. Such that, the enhanced images are visually more similar to the raw images than the decoded images do.
</abstract>

<claims>
1. A method for enhancing quality of media transmitted via network, comprising: executing a first application in a server; said first application generating a plurality of raw images according to at least one command; said raw images being compressed by an encoder of the server to form a plurality of encoded images; executing a second application in a client device remote from the sever; the second application being relevant to and cooperative with the first application in such a manner that, the client device being operable by a user to generate said command; said client device transmitting said command to the server via a network, and retrieving said encoded images from the server via the network according to the command; said client device decoding said encoded images into a plurality of decoded images, and using an artificial intelligence ("AI") enhance module to enhance qualities of said decoded images in order to generate a plurality of enhanced images; wherein said AI enhance module processes said decoded images by using at least one algorithm which is defined by analyzing differences between said decoded images and said raw images in such a manner that, said enhanced images are visually more similar to the raw images than the decoded images do; and said client device outputting said enhanced images as displayed output images; wherein: said at least one algorithm of the AI enhance module of the client device comprises a plurality of weighted parameters; said weighted parameters are depending from the differences between said decoded images and said raw images; the raw images generated by the first application are divided into a plurality of scene-modes, each said scene-mode includes a plurality of said raw images; the weighted parameters are divided into a plurality of sets, each set includes a plurality of said weighted parameters and is corresponding to one of the scene-mode; in the step of said client device using said AI enhance module to enhance qualities of said decoded images in order to generate a plurality of enhanced images, said decoded images corresponding to raw images of different said scene-modes are processed by the same AI enhance module by using weighted parameters of different said sets that are corresponding to the scene-modes.
2. The method of claim 1, wherein, said at least one algorithm of the AI enhance module of the client device is defined by a training process of an artificial neural network module performed by the server; said weighted parameters are defined by said training process of said artificial neural network module performed by the server; said training process comprises: executing the first application in a training mode to generate a plurality of training raw images; encoding said training raw images into a plurality of training encoded images by using said encoder; decoding said training encoded images into a plurality of training decoded images by using a training decoder of the server; said artificial neural network module accepting said training decoded images and processing said training decoded images one by one by using at least one training algorithm in order to generate a plurality of training output images; said at least one training algorithm having a plurality of training weighted parameters; and using a compare module to compare the differences between the training output images and their corresponding training raw images one by one, so as to modify said training weighted parameters of said at least one training algorithm according to the differences between each said training output image and its corresponding training raw image; said training weighted parameters being modified to minimize the differences between the training output images and their corresponding training raw images; each time when the training weighted parameters being modified, the modified training weighted parameters being fed back to the at least one training algorithm for processing another said training decoded image in the step of said artificial neural network module accepting said training decoded images; wherein, after a predetermined amount of the training output images and their corresponding training raw images are compared, and a predetermined times of the training weighted parameters are modified, the training weighted parameters are applied to the at least one algorithm of said AI enhance module of the client device.
3. The method of claim 2, wherein, when the training decoded images and the training output images have the same color formats, said artificial neural network module is a residual network module, and in the step of said artificial neural network module accepting said training decoded images, each of the training output images is a sum of a corresponding training decoded image and an output of the residual network module for processing the corresponding training decoded image.
4. The method of claim 2, wherein, in the step of using a compare module to compare, the compare module employs a discriminator to compare the differences between the training output image and its corresponding training raw image for converging Generative Adversarial Networks (GAN) loss and modifying said training weighted parameters.
5. The method of claim 4, wherein the discriminator of the compare module is trained by the following: the training raw images comprise n channels, wherein n is an integer greater than two; the training decoded images comprise m channels, wherein m is an integer greater than two; in the step of said artificial neural network module accepting said training decoded images, said artificial neural network module processes said m channels of training decoded images to generate n channels of training output images; said n channels of training output images are summed with the m channels of training decoded images to generate m+n channels of simulated false samples; said n channels of training raw images are summed with the m channels of training decoded images to generate m+n channels of simulated true samples; in the step of using a compare module to compare, the m+n channels of simulated false samples and the m+n channels of simulated true samples are fed to the discriminator of the compare module for training an ability of the discriminator of the compare module to detect and recognize the simulated false samples and simulated true samples.
6. The method of claim 2, wherein: a color coding format of the training raw images is either RGB or YUV444, another color coding format of the training decoded images is YUV420; in the step of said artificial neural network module accepting said training decoded images, said artificial neural network module includes a first neural network and a second neural network; the second neural network is a Convolutional Neural Network (CNN); the first neural network accepts and processes the training decoded images and generates a plurality of first output X2 images that have the same coding format with the training raw images; the second neural network accepts and processes the first output X2 images and generates a plurality of second output images; the first output X2 images and the second output images are summed to form the training output images; in the step of using a compare module to compare, the compare module includes a first comparator and a second comparator; the first comparator compares the differences between the first output X2 images and their corresponding training raw images in order to train the first neural network; the second comparator compares the differences between the training output images and their corresponding training raw images in order to train the second neural network.
7. The method of claim 6, wherein, in the step of said artificial neural network module accepting said training decoded images, the first neural network accepts and processes the training decoded images of YUV420 color coding format by using the following: extracts Y-part data of the training decoded images, and the first neural network processes the Y-part data of the training decoded images in order to generate Y-part output data; extracts UV-part data of the training decoded images, and uses two-times amplified first neural network to process the UV-part data of the training decoded images in order to generate UV-part output data; sums the Y-part output data and the UV-part output data in order to generate the training output images.
8. The method of claim 6, wherein, in the step of said artificial neural network module accepting said training decoded images, the first neural network accepts and processes the training decoded images of YUV420 color coding format by using the following: the training decoded images comprise N channels, wherein N is an integer greater than 2; extracts Y-part data of the training decoded images; extracts UV-part data of the training decoded images, and uses two-times amplified first neural network to process the UV-part data of the training decoded images in order to generate UV-part output data with N−1 channels; concatenates the Y-part data and the UV-part output data in order to generate the training output images.
9. The method of claim 1, wherein said weighted parameters of said sets are all pre-stored in the client device, whenever the scene-mode changes, a different set of weighted parameters corresponding to said scene-mode will be applied to the AI enhance module for generating the enhanced images.
10. The method of claim 1, wherein said weighted parameters are stored in the server, whenever the scene-mode changes, a different set of weighted parameters corresponding to said scene-mode will be downloaded from the sever to the client device, and then applied to the AI enhance module for generating the enhanced images.
11. A method for enhancing quality of media, comprising: using a client device to decode a plurality of encoded images, said encoded images being generated by encoding a plurality of raw images; said client device decoding said encoded images into a plurality of decoded images, and using an artificial intelligence ("AI") enhance module to enhance qualities of said decoded images in order to generate a plurality of enhanced images; wherein said AI enhance module processes said decoded images by using at least one algorithm which is defined by analyzing differences between said decoded images and said raw images in such a manner that, said enhanced images are visually more similar to the raw images than the decoded images do; and said client device outputting said enhanced images as displayed output images; wherein: said at least one algorithm of the AI enhance module of the client device comprises a plurality of weighted parameters; said weighted parameters are depending from the differences between said decoded images and said raw images; the raw images are divided into a plurality of scene-modes, each said scene-mode includes a plurality of said raw images; the weighted parameters are divided into a plurality of sets, each set includes a plurality of said weighted parameters and is corresponding to one of the scene-mode; in the step of said client device using said AI enhance module to enhance qualities of said decoded images in order to generate a plurality of enhanced images, said decoded images corresponding to raw images of different said scene-modes are processed by the same AI enhance module by using weighted parameters of different said sets that are corresponding to the scene-modes.
12. The method of claim 11, wherein: said plurality of raw images are generated by a server remote from the client device; said raw images being compressed by an encoder of the server to form said plurality of encoded images; said at least one algorithm of the AI enhance module of the client device is defined by a training process of an artificial neural network module performed by the server; said weighted parameters are defined by said training process of said artificial neural network module performed by the server; said training process comprises: executing a first application in a training mode to generate a plurality of training raw images; encoding said training raw images into a plurality of training encoded images by using said encoder; decoding said training encoded images into a plurality of training decoded images by using a training decoder of the server; said artificial neural network module accepting said training decoded images and processing said training decoded images one by one by using at least one training algorithm in order to generate a plurality of training output images; said at least one training algorithm having a plurality of training weighted parameters; and using a compare module to compare the differences between the training output images and their corresponding training raw images one by one, so as to modify said training weighted parameters of said at least one training algorithm according to the differences between each said training output image and its corresponding training raw image; said training weighted parameters being modified to minimize the differences between the training output images and their corresponding training raw images; each time when the training weighted parameters being modified, the modified training weighted parameters being fed back to the at least one training algorithm for processing another said training decoded image in the step of said artificial neural network module accepting said training decoded images; wherein, after a predetermined amount of the training output images and their corresponding training raw images are compared, and a predetermined times of the training weighted parameters are modified, the training weighted parameters are applied to the at least one algorithm of said AI enhance module of the client device.
13. The method of claim 12, wherein, when the training decoded images and the training output images have the same color formats, said artificial neural network module is a residual network module, and in the step of said artificial neural network module accepting said training decoded images, each of the training output images is a sum of a corresponding training decoded image and an output of the residual network module for processing the corresponding training decoded image.
14. The method of claim 12, wherein, in the step of using a compare module to compare, the compare module employs a discriminator to compare the differences between the training output image and its corresponding training raw image for converging Generative Adversarial Networks (GAN) loss and modifying said training weighted parameters.
15. The method of claim 14, wherein the discriminator of the compare module is trained by the following: the training raw images comprise n channels, wherein n is an integer greater than two; the training decoded images comprise m channels, wherein m is an integer greater than two; in the step of said artificial neural network module accepting said training decoded images, said artificial neural network module processes said m channels of training decoded images to generate n channels of training output images; said n channels of training output images are summed with the m channels of training decoded images to generate m+n channels of simulated false samples; said n channels of training raw images are summed with the m channels of training decoded images to generate m+n channels of simulated true samples; in the step of using a compare module to compare, the m+n channels of simulated false samples and the m+n channels of simulated true samples are fed to the discriminator of the compare module for training an ability of the discriminator of the compare module to detect and recognize the simulated false samples and simulated true samples.
16. The method of claim 12, wherein: a color coding format of the training raw images is either RGB or YUV444, another color coding format of the training decoded images is YUV420; in the step of said artificial neural network module accepting said training decoded images, said artificial neural network module includes a first neural network and a second neural network; the second neural network is a Convolutional Neural Network (CNN); the first neural network accepts and processes the training decoded images and generates a plurality of first output X2 images that have the same coding format with the training raw images; the second neural network accepts and processes the first output X2 images and generates a plurality of second output images; the first output X2 images and the second output images are summed to form the training output images; in the step of using a compare module to compare, the compare module includes a first comparator and a second comparator; the first comparator compares the differences between the first output X2 images and their corresponding training raw images in order to train the first neural network; the second comparator compares the differences between the training output images and their corresponding training raw images in order to train the second neural network.
17. The method of claim 16, wherein, in the step of said artificial neural network module accepting said training decoded images, the first neural network accepts and processes the training decoded images of YUV420 color coding format by using the following: extracts Y-part data of the training decoded images, and the first neural network processes the Y-part data of the training decoded images in order to generate Y-part output data; extracts UV-part data of the training decoded images, and uses two-times amplified first neural network to process the UV-part data of the training decoded images in order to generate UV-part output data; sums the Y-part output data and the UV-part output data in order to generate the training output images.
18. The method of claim 16, wherein, in the step of said artificial neural network module accepting said training decoded images, the first neural network accepts and processes the training decoded images of YUV420 color coding format by using the following: the training decoded images comprise N channels, wherein N is an integer greater than 2; extracts Y-part data of the training decoded images; extracts UV-part data of the training decoded images, and uses two-times amplified first neural network to process the UV-part data of the training decoded images in order to generate UV-part output data with N−1 channels; concatenates the Y-part data and the UV-part output data in order to generate the training output images.
19. The method of claim 11, wherein said weighted parameters of said sets are all pre-stored in the client device, whenever the scene-mode changes, a different set of weighted parameters corresponding to said scene-mode will be applied to the AI enhance module for generating the enhanced images.
20. The method of claim 11, wherein said weighted parameters are stored in a server, whenever the scene-mode changes, a different set of weighted parameters corresponding to said scene-mode will be downloaded from the sever to the client device, and then applied to the AI enhance module for generating the enhanced images.
</claims>
</document>
