<document>

<filing_date>
2018-11-20
</filing_date>

<publication_date>
2020-11-03
</publication_date>

<priority_date>
2018-11-20
</priority_date>

<ipc_classes>
G06K9/00,G06T13/40,G10L25/57
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
FISER, JAKUB
HELLER, GEOFFREY
SIMONS, DAVID, P.
</inventors>

<docdb_family_id>
70728320
</docdb_family_id>

<title>
Automatic viseme detection for generating animatable puppet
</title>

<abstract>
Certain embodiments involve automatically detecting video frames that depict visemes and that are usable for generating an animatable puppet. For example, a computing device accesses video frames depicting a person performing gestures usable for generating a layered puppet, including a viseme gesture corresponding to a target sound or phoneme. The computing device determines that audio data including the target sound or phoneme aligns with a particular video frame from the video frames that depicts the person performing the viseme gesture. The computing device creates, from the video frames, a puppet animation of the gestures, including an animation of the viseme corresponding to the target sound or phoneme that is generated from the particular video frame. The computing device outputs the puppet animation to a presentation device.
</abstract>

<claims>
1. A method that includes one or more processing devices performing operations comprising: accessing video frames depicting a person performing gestures for generating a layered puppet, wherein the gestures performed in the video frames include a target viseme; identifying a video frame, in the video frames, that depicts the target viseme, wherein the target viseme corresponds to a target sound or phoneme, and wherein identifying the video frame comprises: accessing a reference audio dataset comprising reference sounds or phonemes, wherein the reference audio dataset is annotated to identify the reference sounds or phonemes; identifying, in the reference audio dataset, a reference audio portion corresponding to the target sound or phoneme that corresponds to the target viseme; comparing the reference audio portion to an input audio dataset corresponding to the video frames to identify a matching audio portion in the input audio dataset that matches the reference audio portion corresponding to the target sound or phoneme that corresponds to the target viseme; and identifying the video frame based on determining that the video frame has a video timestamp matching an audio timestamp of the matching audio portion that matches the reference audio portion that corresponds to the target sound or phoneme that corresponds to the target viseme; creating, from at least some of the video frames, a puppet animation of the gestures, wherein the puppet animation of the gestures includes a target puppet animation of the target viseme corresponding to the target sound or phoneme, and wherein the target puppet animation is generated from the video frame; and outputting, via a presentation device, the puppet animation.
2. The method of claim 1, wherein identifying the matching audio portion that matches the reference audio portion comprises applying a dynamic time warping operation to the input audio dataset and the reference audio dataset to determine a modification, the modification being based on the dynamic time warping operation and useable to modify a block of the input audio dataset or the reference audio dataset to align the block of the input audio dataset with the block of the reference audio dataset.
3. The method of claim 2, wherein identifying the matching audio portion that matches the reference audio portion further comprises: modifying the matching audio data according to the modification based on the dynamic time warping operation to align the matching audio portion to the reference audio portion; and matching the matching audio portion, as modified, with the reference audio portion.
4. The method of claim 1, wherein accessing the reference audio dataset comprises: applying a scoring operation that generates a plurality of suitability scores for a plurality of reference audio datasets, respectively, wherein the scoring operation generates a first suitability score for the reference audio dataset and a second suitability score for an additional reference audio dataset; and selecting the reference audio dataset from the plurality of reference audio datasets based on a comparison of the first suitability score to the second suitability score, wherein the comparison indicates that the reference audio dataset matches one or more attributes of the input audio dataset more closely than the additional reference audio dataset.
5. The method of claim 4, wherein the scoring operation comprises: computing, for the input audio dataset and the reference audio dataset, a first array of squash/stretch values by applying a dynamic time warping operation to the input audio dataset and the reference audio dataset; computing a first error value from the first array of squash/stretch values, wherein the first error value indicates a deviation of the first array of squash/stretch values from an array of baseline squash/stretch values for the dynamic time warping operation, wherein the first suitability score includes or is derived from the first error value; computing, for the input audio dataset and the additional reference audio dataset, a second array of squash/stretch values by applying the dynamic time warping operation to the input audio dataset and the additional reference audio dataset; and computing a second error value from the second array of squash/stretch values, wherein the second error value indicates a deviation of the second array of squash/stretch values from an array of baseline squash/stretch values for the dynamic time warping operation, wherein the second suitability score includes or is derived from the second error value.
6. The method of claim 5, wherein computing a particular error value of the first error value and the second error value from a particular array of the first array and the second array comprises: replacing a particular squash value in the particular array with a distance value that is a multiplicative inverse of the particular squash value; and computing, as an error value for the particular array, a root mean square error between an array of baseline distance values and the particular array in which the distance value has replaced the particular squash value.
7. The method of claim 4, wherein the scoring operation comprises comparing an input set of pitches from the input audio dataset and each of a plurality of reference sets of pitches from the plurality of reference audio datasets, respectively, wherein a particular suitability score indicates a similarity between the input set of pitches and a particular reference set of pitches.
8. The method of claim 1, the operations further comprising: identifying candidate video frames from the video frames; determining, from an image analysis of the candidate video frames, that a first candidate video frame has a first probability of depicting the person speaking the target sound or phoneme and that a second candidate video frame has a second probability of depicting the person speaking the target sound or phoneme; and selecting the first candidate video frame as the video frame based on the first probability being greater than the second probability, wherein the video frame is tagged based on the video frame being selected from the candidate video frames.
9. The method of claim 1, the operations further comprising: identifying, from a comparison of the input audio dataset with the reference audio dataset, a first set of timestamps at which the target sound or phoneme is present in the input audio dataset; identifying, from an image analysis of the video frames, a second set of timestamps at which the target sound or phoneme is present in the input audio dataset, wherein the second set of timestamps includes one or more timestamps absent from the first set of timestamps; and selecting the video frame based on a particular timestamp of the video frame being in the first set of timestamps and the second set of timestamps.
10. A system comprising: one or more processing devices; and a non-transitory computer-readable medium communicatively coupled to the one or more processing devices and storing instructions, wherein the one or more processing devices are configured to execute the instructions and thereby perform operations comprising: accessing video frames depicting a person performing gestures for generating a layered puppet, wherein the gestures performed in the video frames include a target viseme; identifying a video frame, in the video frames, that depicts the target viseme, wherein the target viseme corresponds to a target sound or phoneme, and wherein identifying the video frame comprises: accessing a reference audio dataset comprising reference sounds or phonemes, wherein the reference audio dataset is annotated to identify the reference sounds or phonemes; identifying, in the reference audio dataset, a reference audio portion corresponding to the target sound or phoneme that corresponds to the target viseme; comparing the reference audio portion to an input audio dataset corresponding to the video frames to identify a matching audio portion in the input audio dataset that matches the reference audio portion corresponding to the target sound or phoneme that corresponds to the target viseme; and identifying the video frame based on determining that the video frame has a video timestamp matching an audio timestamp of the matching audio portion that matches the reference audio portion that corresponds to the target sound or phoneme that corresponds to the target viseme; creating, from at least some of the video frames, a puppet animation of the gestures, wherein the puppet animation of the gestures includes a target puppet animation of the target viseme corresponding to the target sound or phoneme, and wherein the target puppet animation is generated from the video frame; and outputting, via a presentation device, the puppet animation.
11. The system of claim 10, wherein accessing the reference audio dataset comprises: applying a scoring operation that generates a plurality of suitability scores for a plurality of reference audio datasets, respectively, wherein the scoring operation generates a first suitability score for the reference audio dataset and a second suitability score for an additional reference audio dataset; and selecting the reference audio dataset from the plurality of reference audio datasets based on a comparison of the first suitability score to the second suitability score, wherein the comparison indicates that the reference audio dataset matches one or more attributes of the input audio dataset more closely than the additional reference audio dataset.
12. The system of claim 11, wherein the scoring operation comprises: computing, for the input audio dataset and the reference audio dataset, a first array of squash/stretch values by applying a dynamic time warping operation to the input audio dataset and the reference audio dataset; computing a first error value from the first array of squash/stretch values, wherein the first error value indicates a deviation of the first array of squash/stretch values from an array of baseline squash/stretch values for the dynamic time warping operation, wherein the first suitability score includes or is derived from the first error value; computing, for the input audio dataset and the additional reference audio dataset, a second array of squash/stretch values by applying the dynamic time warping operation to the input audio dataset and the additional reference audio dataset; and computing a second error value from the second array of squash/stretch values, wherein the second error value indicates a deviation of the second array of squash/stretch values from an array of baseline squash/stretch values for the dynamic time warping operation, wherein the second suitability score includes or is derived from the second error value, wherein computing a particular error value of the first error value and the second error value from a particular array of the first array and the second array comprises: replacing a particular squash value in the particular array with a distance value that is a multiplicative inverse of the particular squash value; and computing, as an error value for the particular array, a root mean square error between an array of baseline distance values and the particular array in which the distance value has replaced the particular squash value.
13. The system of claim 10, the operations further comprising: identifying candidate video frames from the video frames; determining, from an image analysis of the candidate video frames, that a first candidate video frame has a first probability of depicting the person speaking the target sound or phoneme and that a second candidate video frame has a second probability of depicting the person speaking the target sound or phoneme; and selecting the first candidate video frame as the video frame based on the first probability being greater than the second probability, wherein the video frame is tagged based on the video frame being selected from the candidate video frames.
14. The system of claim 10, the operations further comprising: identifying, from a comparison of the input audio dataset with the reference audio dataset, a first set of timestamps at which the target sound or phoneme is present in the input audio dataset; identifying, from an image analysis of the video frames, a second set of timestamps at which the target sound or phoneme is present in the input audio dataset, wherein the second set of timestamps includes one or more timestamps absent from the first set of timestamps; and selecting the video frame based on a particular timestamp of the video frame being in the first set of timestamps and the second set of timestamps.
15. The system of claim 10, wherein identifying the matching audio portion that matches the reference audio portion comprises: applying a dynamic time warping operation to the input audio dataset and the reference audio dataset to determine a modification, the modification being based on the dynamic time warping operation and useable to modify a block of the input audio dataset or the reference audio dataset to align the block of the input audio dataset with the block of the reference audio dataset; modifying the matching audio data according to the modification based on the dynamic time warping operation to align the matching audio portion to the reference audio portion; and matching the matching audio portion, as modified, with the reference audio portion.
16. A non-transitory computer-readable medium having program code stored thereon that, when executed by one or more processing devices, causes the one or more processing devices to perform operations comprising: accessing video frames depicting a person performing gestures for generating a layered puppet, wherein the gestures performed in the video frames include a target viseme; identifying a video frame, in the video frames, that depicts the target viseme, wherein the target viseme corresponds to a target sound or phoneme, and wherein identifying the video frame comprises: accessing a reference audio dataset comprising reference sounds or phonemes, wherein the reference audio dataset is annotated to identify the reference sounds or phonemes; identifying, in the reference audio dataset, a reference audio portion corresponding to the target sound or phoneme that corresponds to the target viseme; comparing the reference audio portion to an input audio dataset corresponding to the video frames to identify a matching audio portion in the input audio dataset that matches the reference audio portion corresponding to the target sound or phoneme that corresponds to the target viseme; and identifying the video frame based on determining that the video frame has a video timestamp matching an audio timestamp of the matching audio portion that matches the reference audio portion that corresponds to the target sound or phoneme that corresponds to the target viseme; creating, from at least some of the video frames, a puppet animation of the gestures, wherein the puppet animation of the gestures includes a target puppet animation of the target viseme corresponding to the target sound or phoneme, and wherein the target puppet animation is generated from the video frame; and outputting, via a presentation device, the puppet animation.
17. The non-transitory computer-readable medium of claim 16, wherein accessing the reference audio dataset comprises: applying a scoring operation that generates a plurality of suitability scores for a plurality of reference audio datasets, respectively, wherein the scoring operation generates a first suitability score for the reference audio dataset and a second suitability score for an additional reference audio dataset; and selecting the reference audio dataset from the plurality of reference audio datasets based on a comparison of the first suitability score to the second suitability score, wherein the comparison indicates that the reference audio dataset matches one or more attributes of the input audio dataset more closely than the additional reference audio dataset, wherein the scoring operation comprises: computing, for the input audio dataset and the reference audio dataset, a first array of squash/stretch values by applying a dynamic time warping operation to the input audio dataset and the reference audio dataset; computing a first error value from the first array of squash/stretch values, wherein the first error value indicates a deviation of the first array of squash/stretch values from an array of baseline squash/stretch values for the dynamic time warping operation, wherein the first suitability score includes or is derived from the first error value; computing, for the input audio dataset and the additional reference audio dataset, a second array of squash/stretch values by applying the dynamic time warping operation to the input audio dataset and the additional reference audio dataset; and computing a second error value from the second array of squash/stretch values, wherein the second error value indicates a deviation of the second array of squash/stretch values from an array of baseline squash/stretch values for the dynamic time warping operation, wherein the second suitability score includes or is derived from the second error value, wherein computing a particular error value of the first error value and the second error value from a particular array of the first array and the second array comprises: replacing a particular squash value in the particular array with a distance value that is a multiplicative inverse of the particular squash value; and computing, as an error value for the particular array, a root mean square error between an array of baseline distance values and the particular array in which the distance value has replaced the particular squash value.
18. The non-transitory computer-readable medium of claim 16, the operations further comprising: identifying candidate video frames from the video frames; determining, from an image analysis of the candidate video frames, that a first candidate video frame has a first probability of depicting the person speaking the target sound or phoneme and that a second candidate video frame has a second probability of depicting the person speaking the target sound or phoneme; and selecting the first candidate video frame as the video frame based on the first probability being greater than the second probability, wherein the video frame is tagged based on the video frame being selected from the candidate video frames.
19. The non-transitory computer-readable medium of claim 16, the operations further comprising: identifying, from a comparison of the input audio dataset with the reference audio dataset, a first set of timestamps at which the target sound or phoneme is present in the input audio dataset; identifying, from an image analysis of the video frames, a second set of timestamps at which the target sound or phoneme is present in the input audio dataset, wherein the second set of timestamps includes one or more timestamps absent from the first set of timestamps; and selecting the video frame based on a particular timestamp of the video frame being in the first set of timestamps and the second set of timestamps.
20. The non-transitory computer-readable medium of claim 16, wherein identifying the matching audio portion that matches the reference audio portion comprises: applying a dynamic time warping operation to the input audio dataset and the reference audio dataset to determine a modification, the modification being based on the dynamic time warping operation and useable to modify a block of the input audio dataset or the reference audio dataset to align the block of the input audio dataset with the block of the reference audio dataset; modifying the matching audio data according to the modification based on the time warping operation to align the matching audio portion to the reference audio portion; and matching the matching audio portion, as modified, with the reference audio portion.
</claims>
</document>
