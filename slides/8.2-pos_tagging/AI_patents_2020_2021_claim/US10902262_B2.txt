<document>

<filing_date>
2018-01-19
</filing_date>

<publication_date>
2021-01-26
</publication_date>

<priority_date>
2017-01-19
</priority_date>

<ipc_classes>
G06F11/30,G06F11/34,G06F3/14,G06K9/00,G06K9/32,G06K9/62,G06K9/66,G06N20/00,G06N3/08,G06N5/02,G06T11/00,G06T19/00,H04L29/06,H04L29/08
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
LI BO
GIBBS, SIMON, J.
LIOT, ANTHONY S.
YING, ZHIHAN
ANTOL, STANISLAW
BENDALE, ABHIJIT
JEON, WON J.
KANG, HYUN JAE
LUO, LU
KIM, JIHEE
MISTRY, PRANAV K.
</inventors>

<docdb_family_id>
62840668
</docdb_family_id>

<title>
Vision intelligence management for electronic devices
</title>

<abstract>
One embodiment provides a method comprising classifying one or more objects present in an input comprising visual data by executing a first set of models associated with a domain on the input. Each model corresponds to an object category. Each model is trained to generate a visual classifier result relating to a corresponding object category in the input with an associated confidence value indicative of accuracy of the visual classifier result. The method further comprises aggregating a first set of visual classifier results based on confidence value associated with each visual classifier result of each model of the first set of models. At least one other model is selectable for execution on the input based on the aggregated first set of visual classifier results for additional classification of the objects. One or more visual classifier results are returned to an application running on an electronic device for display.
</abstract>

<claims>
1. A method comprising: classifying one or more objects present in an input comprising visual data by executing a first set of models associated with a domain on the input, wherein each model of the first set of models corresponds to an object category, and each model is trained to generate a visual classifier result relating to a corresponding object category in the input with an associated confidence value indicative of accuracy of the visual classifier result; and aggregating a first set of visual classifier results based on confidence value associated with each visual classifier result of each model of the first set of models, wherein the one or more objects are further classified by selecting at least one other model for execution on the input based on the aggregated first set of visual classifier results and at least one similarity between a first set of objects recognized by the first set of models and at least one object recognized by the at least one other model, and one or more visual classifier results are returned to an application running on an electronic device for display.
2. The method of claim 1, further comprising: further classifying the one or more objects by selecting a second set of models to execute on the input based on the aggregated first set of visual classifier results and one or more similarities between the first set of objects recognized by the first set of models and a second set of objects recognized by the second set of models.
3. The method of claim 2, wherein the one or more similarities are indicative of a category ontology link between a first model of the first set of models and a second model of the second set of models, and the category ontology link represents that an object recognized by the first model has an object category that is the same as an object category corresponding to the second model based on ontology information relating to the first model and the second model.
4. The method of claim 2, wherein the one or more similarities are indicative of a category similarity link between a first model of the first set of models and a second model of the second set of models, and the category similarity link represents that an object recognized by the first model has an object category that is similar to an object category corresponding to the second model based on a similarity metric between the first model and the second model.
5. The method of claim 2, further comprising: determining an execution order of the second set of models based on one or more quality of service (QoS) preferences and model metadata information corresponding to the second set of models; and executing the second set of models in accordance with the execution order.
6. The method of claim 5, wherein the one or more QoS preferences comprises at least one of: a pre-determined threshold of engines that can be run simultaneously on the electronic device for execution of the second set of models, one or more resource requirements for execution of the second set of models, or information indicative of one or more models currently loaded into one or more engines.
7. The method of claim 5, wherein the model metadata information corresponding to the second set of models comprises at least one of: one or more model priorities for the second set of models, or one or more model residencies for the second set of models.
8. The method of claim 1, further comprising: selecting the domain based on the input and a domain selection method, wherein the selected domain is associated with at least one model that is selectable for execution on the input.
9. The method of claim 8, wherein the domain selection method is a hierarchy-based domain selection method, and selecting the domain comprises: selecting the domain based on one or more hierarchical relationships between multiple models organized hierarchically into a hierarchical structure based on hierarchical relationships between multiple object categories corresponding to the multiple models, the multiple models including the first set of models.
10. The method of claim 8, wherein the domain selection method is a feature vector-based domain selection method, and selecting the domain comprises: maintaining a first set of feature representations for a first set of domains, wherein each feature representation of the first set of representations corresponds to a domain of the first set of domains; and for each frame included in the input: extracting, from the frame, a feature representation for the frame; and selecting the domain from the first set of domains by comparing the feature representation for the frame against each feature representation of the first set of feature representations, the selected domain having a corresponding feature representation that is closest to the feature representation for the frame.
11. The method of claim 8, wherein the domain selection method is a temporal-based domain selection method, and selecting the domain comprises: maintaining a first set of feature representations for a first set of domains, wherein each feature representation of the first set of representations corresponds to a domain of the first set of domains; segmenting the input into multiple temporal windows, wherein each temporal window comprises a sequence of frames included in the input; and for each temporal window: extracting, from the temporal window, a feature representation for the temporal window; and selecting the domain from the first set of domains by comparing the feature representation for the temporal window against each feature representation of the first set of feature representations, the selected domain having a corresponding feature representation that is closest to the feature representation for the temporal window.
12. The method of claim 8, further comprising: selecting one or more models associated with the selected domain to activate for execution on the input based on at least one of: a pre-determined threshold of engines that can be run simultaneously on the electronic device, run-time resource requirements of all models associated with the selected domain, information indicative of one or more models that are currently active, information indicative of which of all models associated with the selected domain must remain active between execution passes, information indicative of which of all models associated with the selected domain must run on the electronic device, or information indicative of which of all models associated with the selected domain must run in a cloud service.
13. A system, comprising: at least one processor; and a non-transitory processor-readable memory device storing instructions that when executed by the at least one processor causes the at least one processor to perform operations including: classifying one or more objects present in an input comprising visual data by executing a first set of models associated with a domain on the input, wherein each model of the first set of models corresponds to an object category, and each model is trained to generate a visual classifier result relating to a corresponding object category in the input with an associated confidence value indicative of accuracy of the visual classifier result; and aggregating a first set of visual classifier results based on confidence value associated with each visual classifier result of each model of the first set of models, wherein the one or more objects are further classified by selecting at least one other model for execution on the input based on the aggregated first set of visual classifier results and at least one similarity between a first set of objects recognized by the first set of models and at least one object recognized by the at least one other model, and one or more visual classifier results are returned to an application running on an electronic device for display.
14. The system of claim 13, wherein the operations further comprise: further classifying the one or more objects by selecting a second set of models to execute on the input based on the aggregated first set of visual classifier results and one or more similarities between the first set of objects recognized by the first set of models and a second set of objects recognized by the second set of models; determining an execution order of the second set of models based on one or more quality of service (QoS) preferences and model metadata information corresponding to the second set of models; and executing the second set of models in accordance with the execution order.
15. The system of claim 13, wherein the operations further comprise: selecting the domain based on the input and a domain selection method, wherein the selected domain is associated with at least one model that is selectable for execution on the input.
16. The system of claim 15, wherein the domain selection method is a hierarchy-based domain selection method, and selecting the domain comprises: selecting the domain based on one or more hierarchical relationships between multiple models organized hierarchically into a hierarchical structure based on hierarchical relationships between multiple object categories corresponding to the multiple models, the multiple models including the first set of models.
17. The system of claim 15, wherein the domain selection method is a feature vector-based domain selection method, and selecting the domain comprises: maintaining a first set of feature representations for a first set of domains, wherein each feature representation of the first set of representations corresponds to a domain of the first set of domains; and for each frame included in the input: extracting, from the frame, a feature representation for the frame; and selecting the domain from the first set of domains by comparing the feature representation for the frame against each feature representation of the first set of feature representations, the selected domain having a corresponding feature representation that is closest to the feature representation for the frame.
18. The system of claim 15, wherein the domain selection method is a temporal-based domain selection method, and selecting the domain comprises: maintaining a first set of feature representations for a first set of domains, wherein each feature representation of the first set of representations corresponds to a domain of the first set of domains; segmenting the input into multiple temporal windows, wherein each temporal window comprises a sequence of frames included in the input; and for each temporal window: extracting, from the temporal window, a feature representation for the temporal window; and selecting the domain from the first set of domains by comparing the feature representation for the temporal window against each feature representation of the first set of feature representations, the selected domain having a corresponding feature representation that is closest to the feature representation for the temporal window.
19. The method of claim 15, wherein the operations further comprise: selecting one or more models associated with the selected domain to activate for execution on the input based on at least one of: a pre-determined threshold of engines that can be run simultaneously on the electronic device, run-time resource requirements of all models associated with the selected domain, information indicative of one or more models that are currently active, information indicative of which of all models associated with the selected domain must remain active between execution passes, information indicative of which of all models associated with the selected domain must run on the electronic device, or information indicative of which of all models associated with the selected domain must run in a cloud service.
20. A non-transitory computer readable storage medium including instructions to perform a method comprising: classifying one or more objects present in an input comprising visual data by executing a first set of models associated with a domain on the input, wherein each model of the first set of models corresponds to an object category, and each model is trained to generate a visual classifier result relating to a corresponding object category in the input with an associated confidence value indicative of accuracy of the visual classifier result; and aggregating a first set of visual classifier results based on confidence value associated with each visual classifier result of each model of the first set of models, wherein the one or more objects are further classified by selecting at least one other model for execution on the input based on the aggregated first set of visual classifier results and at least one similarity between a first set of objects recognized by the first set of models and at least one object recognized by the at least one other model, and one or more visual classifier results are returned to an application running on an electronic device for display.
</claims>
</document>
