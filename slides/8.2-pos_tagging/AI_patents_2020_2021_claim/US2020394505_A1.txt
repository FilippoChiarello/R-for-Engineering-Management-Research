<document>

<filing_date>
2020-06-02
</filing_date>

<publication_date>
2020-12-17
</publication_date>

<priority_date>
2019-06-13
</priority_date>

<ipc_classes>
G06F17/15,G06N3/04,G06N3/063
</ipc_classes>

<assignee>
CANON
</assignee>

<inventors>
Ariizumi, Masahiro
</inventors>

<docdb_family_id>
73735599
</docdb_family_id>

<title>
DATA PROCESSING APPARATUS AND CONTROL METHOD
</title>

<abstract>
There is provided with a data processing apparatus that carries out a computation corresponding to a neural network containing a plurality of layers. A processing unit includes a plurality of processors that, through pipeline processing, sequentially calculate data of each of blocks, each block corresponding to a part of a feature plane in one layer. A control unit determines a calculation order for the data of the blocks on the basis of structure information of the neural network, and sends a command that controls the calculation order to the plurality of processors.
</abstract>

<claims>
1. A data processing apparatus that carries out a computation corresponding to a neural network containing a plurality of layers, the apparatus comprising: a processing unit including a plurality of processors configured to, through pipeline processing, sequentially calculate data of each of blocks, each block corresponding to a part of a feature plane in one layer; and a control unit configured to determine a calculation order for the data of the blocks on the basis of structure information of the neural network, and to send a command that controls the calculation order to the plurality of processors.
2. The data processing apparatus according to claim 1, wherein the command includes information indicating the block for which the data is to be calculated.
3. The data processing apparatus according to claim 2, wherein the command further includes information specifying a processing parameter indicating a processing method used by the plurality of processors for the blocks.
4. The data processing apparatus according to claim 3, wherein the information specifying the processing parameter includes information specifying a layer to be processed in the neural network.
5. The data processing apparatus according to claim 1, wherein each of the plurality of processors has a buffer capable of holding two or more of the commands.
6. The data processing apparatus according to claim 1, wherein the control unit is further configured to send the command to each of the plurality of processors at once.
7. The data processing apparatus according to claim 1, wherein the control unit is further configured to send the command to at least one processor among the plurality of processors, asynchronously with respect to operations of the at least one processor.
8. The data processing apparatus according to claim 1, wherein the control unit is further configured to send the command to the plurality of processors over a bus.
9. The data processing apparatus according to claim 1, wherein each of the plurality of processors is configured to start processing of respective blocks asynchronously with respect to each other.
10. The data processing apparatus according to claim 1, further comprising: a buffer provided between a first processor and a second processor among the plurality of processors, a processing result from the first processor being transferred to the second processor, the buffer temporarily storing the processing result.
11. The data processing apparatus according to claim 1, wherein the neural network contains a plurality of process layers, and the block is one of the plurality of process layers.
12. The data processing apparatus according to claim 11, wherein the one of the plurality of process layers has a plurality of intermediate layers including at least a convolution layer.
13. The data processing apparatus according to claim 11, wherein the control unit is further configured to determine the calculation order through prioritizing data of a process layer closest to an output layer among processable data of the plurality of process layers.
14. The data processing apparatus according to claim 1, wherein the processing unit is a first processing unit, and the apparatus further comprises: a second processing unit including a plurality of processors configured to, through pipeline processing, sequentially calculate data of each of blocks, each block corresponding to a part of a feature plane in one layer, wherein the first processing unit is further configured to sequentially calculate data of a feature plane in a first partial structure of the neural network, and the second processing unit is further configured to sequentially calculate data of a feature plane in a second partial structure of the neural network, the second partial structure being different from the first partial structure.
15. The data processing apparatus according to claim 14, wherein a side output from the first partial structure is input to the second partial structure.
16. The data processing apparatus according to claim 1, wherein the processing unit is a first processing unit, and the apparatus further comprises: a second processing unit including a plurality of processors configured to, through pipeline processing, sequentially calculate data of each of blocks, each block corresponding to a part of a feature plane in one layer, wherein the control unit is further configured to switch whether or not to send the command to the second processing unit on the basis of structure information of the neural network.
17. A control method for carrying out a computation corresponding to a neural network containing a plurality of layers, the method comprising: performing pipeline processing to calculate data of each of blocks with a plurality of processors, each block corresponding to a part of a feature plane in one layer; determining a calculation order for the data of the blocks on the basis of structure information of the neural network; and sending a command that controls the calculation order to the plurality of processors.
</claims>
</document>
