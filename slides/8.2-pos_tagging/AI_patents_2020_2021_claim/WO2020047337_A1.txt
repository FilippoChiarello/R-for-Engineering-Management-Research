<document>

<filing_date>
2019-08-29
</filing_date>

<publication_date>
2020-03-05
</publication_date>

<priority_date>
2018-08-29
</priority_date>

<ipc_classes>
G06F9/50,G06F9/52,G06N3/063,G06N3/10
</ipc_classes>

<assignee>
QUALCOMM
</assignee>

<inventors>
HILL, REXFORD ALAN
VAIDHYANATHAN, NATARAJAN
VERRILLI, COLIN BEATON
</inventors>

<docdb_family_id>
69639951
</docdb_family_id>

<title>
METHOD, APPARATUS, AND SYSTEM FOR AN ARCHITECTURE FOR MACHINE LEARNING ACCELERATION
</title>

<abstract>
A method, apparatus, and system for an architecture for machine learning acceleration is presented. An apparatus includes a plurality of processing elements, each including a tightly-coupled memory, and a memory system coupled to the processing elements. A global synchronization manager is coupled to the plurality of the processing elements and to the memory system. The processing elements do not implement a coherency protocol with respect to the memory system. The processing elements implement direct memory access with respect to the memory system, and the global synchronization manager is configured to synchronize operations of the plurality of processing elements through the TCMs.
</abstract>

<claims>
1. An inference accelerator comprising:
a memory system;
a plurality of processing elements, each processing element:
having a tightly coupled memory (TCM);
coupled to the memory system; and
adapted to access the memory system; and
a global synchronization manager (GSM) module coupled to the plurality of processing elements and to the memory system, the GSM adapted to synchronize operations of the plurality of processing elements and memory system using corresponding synchronization modules of each of the plurality of processing elements.
2. The inference accelerator of claim 1, wherein the processing elements do not implement a coherency protocol with respect to the memory system.
3. The inference accelerator of claim 1, wherein each processing element further comprises:
a vector processor adapted to perform floating point operations;
a scalar processor; and
a matrix processor adapted to perform floating point operations.
4. The inference accelerator of claim 1, wherein:
the plurality of processing elements are interconnected by a first network configured to support multicast operations; and
each of the plurality of processing elements is connected to the memory system by a second network separate from the first network.
5. The inference accelerator of claim 4, wherein the inference accelerator further comprising a controller connected to the second network.
6. The inference accelerator of claim 4, wherein: the GSM is coupled to each of the processing elements via a third network separate from the first network and the second network;
each of the processing elements comprises a local sync manager; and
the GSM is configured to provide configuration information to the local sync manager of each processing element of the plurality of processing elements via the third network.
7. The inference accelerator of claim 1, wherein the first network is configured to implement zero encoding.
8. The inference accelerator of claim 1, wherein:
the synchronization modules of the plurality of processing elements used by the GSM to synchronize operations of the plurality of processing elements are the corresponding TCMs;
each TCM is adapted to store a set of synchronization variables; and
the GSM is adapted to store and adjust the synchronization variables in the TCMs.
9. The inference accelerator of claim 1, wherein the inference accelerator is configured to:
transform a neural network model into a directed acyclic graph;
transform the directed acyclic graph into computation and data movement operations; and
schedule the computation and data movement operations for execution in parallel pipelines by the processing elements, wherein the computation and data movement operations are dispatched using dispatch scaling.
10. The inference accelerator of claim 9, wherein:
the plurality of processing elements is interconnected by a first network configured to perform multicast operations; and
the scheduling of computation and data movement operations includes the replication of data sets using multicast operations on the first network.
11. An apparatus comprising the inference accelerator of claim 1, further comprising a plurality of interconnected additional inference accelerators configured substantially the same as the inference accelerator and connected to the inference accelerator.
12. A method for an inference accelerator having a plurality of processing elements, a memory system coupled to each of the processing elements, and a global synchronization manager (GSM) module coupled to the plurality of processing elements and to the memory system, wherein each processing element comprises a tightly coupled memory (TCM), the method comprising:
accessing, by each processing element, the memory system; and
synchronizing, by the GSM, operations of the plurality of processing elements and memory system using corresponding synchronization modules of each of the plurality of processing elements.
13. The method of claim 12, wherein the processing elements do not implement a coherency protocol with respect to the memory system.
14. The method of claim 12, wherein:
each processing element further comprise a vector processor, a scalar processor, and a matrix processor; and
the method further comprises:
performing floating point operations by the vector processor; and performing floating point operations by the matrix processor.
15. The method of claim 12, wherein:
the plurality of processing elements are interconnected by a first network;
the method further comprises performing multicast operations by the first network; and
each of the plurality of processing elements is connected to the memory system by a second network separate from the first network.
16. The method of claim 15, wherein: the GSM is coupled to each of the processing elements via a third network separate from the first network and the second network;
each of the processing elements comprises a local sync manager;
the method further comprises providing, by the GSM, configuration information to the local sync manager of each processing element of the plurality of processing elements via the third network.
17. The method of claim 12, further comprising implementing zero encoding by the first network.
18. The method of claim 12, wherein:
the synchronization modules of the plurality of processing elements used by the GSM to synchronize operations of the plurality of processing elements are the corresponding TCMs;
each TCM is adapted to store a set of synchronization variables; and
the method further comprises storing and adjusting a set of synchronization variables of the TCM of one of the plurality of processing elements.
19. The method of claim 12, further comprising:
transforming a neural network into a directed acyclic graph;
transforming the directed acyclic graph into computation and data movement operations; and
scheduling the computation and data movement operations for execution in parallel pipelines by the processing elements, wherein the computation and data movement operations are dispatched using dispatch scaling.
20. The method of claim 19, wherein:
the plurality of processing elements is interconnected by a first network configured to perform multicast operations; and
the scheduling of computation and data movement operations includes replicating data sets using multicast operations on the first network.
21. An apparatus including a means for inference acceleration; the inferenceacceleration means comprising:
a means for memory storage and retrieval;
a plurality of means for processing, each means for processing:
having a means for tightly coupling memory (TCM);
coupled to the means for memory storage and retrieval; and adapted to access the means for memory storage and retrieval; and a means for global synchronization management (GSM) coupled to the plurality of means for processing and to the memory means and adapted to synchronize operations of the plurality of means for processing and memory means, using corresponding synchronization modules of each of the plurality of means for processing.
</claims>
</document>
