<document>

<filing_date>
2020-04-22
</filing_date>

<publication_date>
2020-08-20
</publication_date>

<priority_date>
2017-06-05
</priority_date>

<ipc_classes>
B25J13/08,B25J19/02,B25J5/00,B25J9/16,G06F3/0481,G06K9/00,G06T17/00,G06T19/00,G06T7/00
</ipc_classes>

<assignee>
KINDRED SYSTEMS
</assignee>

<inventors>
GILDERT, SUZANNE
KORENKEVYCH, DMYTRO
ROSE, GEORDIE S.
STEININGER, MILES F.H.
</inventors>

<docdb_family_id>
64459892
</docdb_family_id>

<title>
Systems and methods for training robots using augmented reality and machine learning
</title>

<abstract>
Substantially as described and illustrated herein including devices, methods of operation for the systems or devices, articles of manufacture including stores processor-executable instructions, and a system including a robot. The system includes at least one processor. The system may further include a nontransitory processor-readable storage device communicatively coupled to at least one processor and which stores processor-executable instructions which, when executed by the at least one processor, cause the at least one processor to composite environment information that represents an environment and virtual item information that represents the virtual item to produce composited information, present to an agent the composited information, and receive action information that represents an action for the robot to perform via the output system.
</abstract>

<claims>
1. 1-20. (canceled)
21. A computer-implemented method for training and controlling a robot, the method comprising: creating item information that represents a virtual item; compositing the item information with environment information that represents an environment accessible to the robot, to produce composited information that, at least in part, renders a representation of at least a portion of the virtual item relative to a representation of at least a portion of the environment; causing training of a machine learning model with at least a portion of the composited information to generate a trained machine learning model; and generating at least one output signal that includes action information representing an action for the robot to perform based, at least in part, on the trained machine learning model.
22. The method of claim 21, wherein the composited information has a three-dimensional visual form.
23. The method of claim 21, wherein the virtual item corresponds to an object that the robot is trained to avoid picking.
24. The method of claim 21, wherein the virtual item corresponds to an distraction item.
25. The method of claim 24, wherein the distraction item includes a human arm.
26. The method of claim 21, wherein the composited information has an aural or sound form.
27. The method of claim 26, wherein the virtual item corresponds to sounds associated with abnormal robotic operation.
28. The method of claim 27, wherein the sounds associated with abnormal robotic operation include at least one of random clicks or grinding sounds.
29. The method of claim 21, wherein the causing training of a machine learning model comprises providing the composited information to an agent including a processor-based device.
30. The method of claim 29, further comprising receiving the action information from the agent, wherein the action information indicates at least one of a declarative action or an imperative action.
31. A computer-readable medium storing contents that, when executed by one or more processors, cause the one or more processors to perform actions comprising: creating item information that represents a virtual item; compositing the item information with environment information that represents an environment accessible to a robot, to produce composited information that, at least in part, renders a representation of at least a portion of the virtual item relative to a representation of at least a portion of the environment; causing training of a machine learning model with at least a portion of the composited information to generate a trained machine learning model; and causing generation of action information representing an action for the robot to perform based, at least in part, on the trained machine learning model.
32. The computer-readable medium of claim 31, wherein the composited information has at least one of a visual form or aural form.
33. The computer-readable medium of claim 31, wherein the virtual item is not rendered at a location where the robot is designed to pick from.
34. The computer-readable medium of claim 31, wherein the virtual item corresponds to a distraction or defective item that the robot is trained to avoid picking.
35. The computer-readable medium of claim 31, wherein the environment information includes data obtained via one or more sensors.
36. A system comprising: at least one processor; and memory storing processor-executable instructions which, when executed by the at least one processor, cause the system to: create item information that represents a virtual item; composite the item information with environment information that represents an environment accessible to a robot, to produce composited information that, at least in part, renders a representation of at least a portion of the virtual item relative to a representation of at least a portion of the environment; cause training of a machine learning model with at least a portion of the composited information to generate a trained machine learning model; and cause generation of action information representing an action for the robot to perform based, at least in part, on the trained machine learning model.
37. The system of claim 36, wherein the instructions further cause the system to: obtain pose information that represents a pose of the robot in the environment; and update the item information that represents the virtual item based, at least in part, on the pose information.
38. The system of claim 36, wherein the instructions further cause the system to present the composited information to an agent device including a human operator interface.
39. The system of claim 36, wherein the action information represents at least one of a declaration action or an imperative action.
40. The system of claim 36, wherein the action for the robot to perform includes at least one of picking an item or avoiding an item.
</claims>
</document>
