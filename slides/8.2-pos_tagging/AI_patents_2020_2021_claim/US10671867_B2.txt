<document>

<filing_date>
2019-11-25
</filing_date>

<publication_date>
2020-06-02
</publication_date>

<priority_date>
2018-10-23
</priority_date>

<ipc_classes>
G01B11/02,G01B11/03,G06F16/9535,G06F17/18,G06K9/00,G06N20/00,G06T19/00,G06T7/73
</ipc_classes>

<assignee>
CAPITAL ONE SERVICES
</assignee>

<inventors>
CASTANEDA, JONATHAN
DUCKWORTH, STAEVAN
MARTINEZ, DANIEL
LEE, SUSIE
HARDIN, WILLIAM
</inventors>

<docdb_family_id>
67543848
</docdb_family_id>

<title>
Method for determining correct scanning distance using augmented reality and machine learning models
</title>

<abstract>
A smart device is provided with an application program for displaying a video feed received from the smart device's camera. The application can determine the coordinates for an intersection point, which is a point on the ground where the smart device is pointing at. The application can display a target on the visual representation of the intersection point. Based on whether the smart device is at an appropriate distance from the intersection point, the user interface can superimpose an indicator on the video feed received from the camera. This can inform the user whether the smart device is at an optimal scan distance from the intersection point (or an object) so that the object can be identified by a machine learning model.
</abstract>

<claims>
1. A method comprising: displaying a video feed and an indicator over the video feed in a user interface of a device, wherein the indicator corresponds to a visual representation of an intersection point; determining a distance between the device and the intersection point; displaying an attribute for the indicator in the user interface, wherein the attribute is determined based on the distance between the device and the intersection point; receiving a command from a user of the device to detect an object in a frame of the video feed; identifying, using an object recognition model, the object shown in the video feed; receiving information about the object shown in the video feed; and displaying the information in the user interface of the device.
2. The method of claim 1, wherein the intersection point is a point on a ground plane where a perpendicular hypothetical line coming out of the device would hit the ground plane.
3. The method of claim 2, wherein the indicator is displayed over the visual representation of the intersection point.
4. The method of claim 2, wherein the intersection point corresponds to a centerpoint of a field of view of a camera of the device.
5. The method of claim 2, wherein the device is configured to display the indicator coplanar with a visual representation of the ground plane.
6. The method of claim 1, wherein the indicator has at least one circle.
7. The method of claim 6, wherein the attribute of the indicator is a color or size of the at least one circle.
8. The method of claim 1, wherein the user interface is configured to change the attribute for the indicator when the distance between the device and the intersection point changes.
9. The method of claim 1, further comprising: comparing the distance between the device and the intersection point with a threshold distance; and displaying the attribute for the indicator based on the comparison.
10. The method of claim 9, wherein: displaying a first attribute as the attribute for the indicator if the distance between the device and the intersection point exceeds the threshold distance; and displaying a second attribute as the attribute for the indicator if the distance between the device and the intersection point does not exceed the threshold distance.
11. The method of claim 9, wherein: the threshold distance is predetermined, or threshold distance is calculated by determining a probability value for identification of the object shown in the frame.
12. The method of claim 9, wherein the threshold distance is calculated using the following steps: receiving at least two frames from a camera of the device, wherein each frame is associated with a frame distance which is the distance between device and the intersection point at the time of capturing the frame; providing the at least two frames to a machine learning model; determining a probability value for identification of the object in each of the at least two frames; comparing the probability values to a threshold probability; and if at least one of the probability values exceeds the threshold probability, setting the threshold distance as the frame distance of the frame associated with the probability value that exceeds the threshold probability.
13. The method of claim 12, further comprising: if at least one of the probability values exceeds the threshold probability, displaying a third attribute to inform the user that the user is within an appropriate distance from the object; and if the probability values do not exceed the threshold probability, displaying a fourth attribute to inform the user that the user needs to move either closer to the object or away from the object.
14. A non-transitory computer-accessible medium having stored thereon computer-executable instructions for an application for identifying objects executable on a device, wherein, when a computing hardware arrangement executes the instructions, the computing arrangement is configured to perform procedures comprising: displaying a video feed and an indicator over the video feed in a user interface of the application, wherein the indicator corresponds to a visual representation of an intersection point; determining a distance between the device and the intersection point; comparing the distance between the device and the intersection point with a threshold distance; and displaying an attribute for the indicator in the user interface, wherein the attribute is determined based on the comparison.
15. The non-transitory computer-accessible medium of claim 14, wherein the threshold distance is calculated using the following steps: receiving at least two frames from a camera of the device, wherein each frame is associated with a frame distance which is the distance between device and the intersection point at the time of capturing the frame; providing the at least two frames to a machine learning model; determining a probability value for identification of an object in each of the at least two frames; comparing the probability values to a threshold probability; and if at least one of the probability values exceeds the threshold probability, setting the threshold distance as the frame distance of the frame associated with the probability value that exceeds the threshold probability.
16. The non-transitory computer-accessible medium of claim 15, further comprising: if at least one of the probability values exceeds the threshold probability, displaying a third attribute to inform the user that the user is within an appropriate distance from the object; and if the probability values do not exceed the threshold probability, displaying a fourth attribute to inform the user that the user needs to move either closer to the object or away from the object.
17. The non-transitory computer-accessible medium of claim 14, further comprising: capturing a frame of the video feed when a condition is met; conducting a search query for a product shown in the frame; and displaying a listing including the product in the user interface of the application.
18. The non-transitory computer-accessible medium of claim 17, wherein the condition is met when the device is within an optimal scan distance of the product.
19. The non-transitory computer-accessible medium of claim 18, wherein the optimal scan distance is a predetermined distance, a range of distance learned by a model over time, or a threshold percentage of a field of view of a camera being occupied by the product.
20. The non-transitory computer-accessible medium of claim 14, further comprising identifying, using an object recognition model, a product shown in the video feed.
</claims>
</document>
