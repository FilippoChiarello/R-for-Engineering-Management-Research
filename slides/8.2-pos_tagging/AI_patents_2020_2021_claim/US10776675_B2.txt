<document>

<filing_date>
2018-12-10
</filing_date>

<publication_date>
2020-09-15
</publication_date>

<priority_date>
2017-03-31
</priority_date>

<ipc_classes>
G06K9/62,G06K9/66,G06T11/60,G06T5/00,G06T7/33
</ipc_classes>

<assignee>
CLARIFAI
</assignee>

<inventors>
EIGEN, DAVID JOSHUA
ZEILER, MATTHEW
</inventors>

<docdb_family_id>
63669652
</docdb_family_id>

<title>
System and method for facilitating logo-recognition training of a recognition model
</title>

<abstract>
In certain embodiments, training of a prediction model (e.g., recognition or other prediction model) may be facilitated via a training set generated based on one or more logos or other graphics. In some embodiments, graphics information associated with a logo or graphic (e.g., to be recognized via a recognition model) may be obtained. Training media items (e.g., images, videos, etc.) may be generated based on the graphics information, where each of the training media items includes (i) content other than the logo and (ii) a given representation of the logo integrated with the other content. The training media items may be processed via the recognition model to generate predictions (related to recognition of the logo or graphic for the training media items). The recognition model may be updated based on (i) the generated predictions and (ii) corresponding reference indications (related to recognition of the logo for the training media items).
</abstract>

<claims>
1. A method of facilitating logo recognition training of a neural network via an image training set generated based on one or more logos, the method being implemented by a computer system that comprises one or more processors executing computer program instructions that, when executed, perform the method, the method comprising: generating images based on logo information associated with a rendering of a logo, each of the images comprising (i) content other than the logo and (ii) a given rendering of the logo integrated with the other content; processing, via a neural network, the images to generate predictions related to recognition of the logo for the images, the generated predictions comprising an indication of a predicted location of the logo in a first image of the images; causing, via a user interface, presentation of the indication of the predicted location on an area of the first image; obtaining a reference feedback set, the reference feedback set comprising reference indications related to recognition of the logo for the images, the reference indications comprising a reference indication corresponding to a user interaction with the indication of the predicted location of the logo in the first image that is initiated via the user interface; and updating the neural network based the reference feedback set such that the neural network is updated based on the reference indication corresponding to the user interaction with the indication of the predicted location.
2. The method of claim 1, wherein generating the images comprises generating the first image such that at least some content other than the logo appears opaquely over at least a portion of the logo on the first image, and wherein generating the predictions comprises generating the indication of the predicted location based on the first image in which the at least some content appears over at least a portion of the logo.
3. The method of claim 1, wherein generating the predictions comprises generating a predicted label describing the logo for at least one image, the method further comprising: causing, via the user interface, presentation of the predicted label with respect to the at least one image; and obtaining, via the user interface, a reference indication corresponding to a negative user response to the presentation of the predicted label, wherein updating the neural network comprises updating the neural network based on the reference indication corresponding to the negative user response.
4. The method of claim 1, wherein obtaining the reference indications comprises obtaining a reference indication corresponding to a user-initiated movement of the indication of the predicted location to another area of the first image that is initiated via the user interface, and wherein updating the neural network comprises updating the neural network based on the reference indication corresponding to the user-initiated movement.
5. The method of claim 1, wherein obtaining the reference indications comprises obtaining a reference indication corresponding to a user-initiated resizing of the indication of the predicted location that is initiated via the user interface, and wherein updating the neural network comprises updating the neural network based on the reference indication corresponding to the user-initiated resizing.
6. The method of claim 1, further comprising: determining locations at which to place a given rendering of the logo on a given image, each of the locations being different from one another, wherein generating the images comprises generating at least some of the images based on the locations such that each image of the at least some images comprises a given rendering of the logo at different one of the locations.
7. The method of claim 1, further comprising: generating renderings of the logo such that each of the renderings of the logo has a different size from one another, wherein generating the images comprises generating at least some images of the images such that each image of the at least some images comprises a different one of the renderings of the logo from one another.
8. The method of claim 1, further comprising: obtaining transformation parameter sets, each of the transformation parameter sets comprising parameters different from all other ones of the transformation parameter sets, wherein generating the images comprises generating at least some images of the images based on the transformation parameter sets by applying a different one of the transformation parameter sets to the rendering of the logo such that each image of the at least some images has a different transformed rendering of the logo from one another.
9. The method of claim 8, wherein obtaining the transformation parameter sets comprises randomly generating at least some of the transformation parameter sets by randomly obtaining transformation parameters for each of the randomly-generated transformation parameter sets.
10. The method of claim 8, wherein the transformation parameter sets comprise occlusion parameters, and wherein generating the at least some images comprises generating one or more of the at least some images based on the occlusion parameters such that each of the one or more images has a transformed rendering of the logo in which at least of a portion of the rendering of the logo is missing or hidden from view from a viewer perspective.
11. The method of claim 8, wherein the transformation parameter sets comprise blurring effect parameters, camera effect parameters, motion effect parameters, shadow effect parameters, pattern effect parameters, texture effect parameters, sharpening parameters, softening parameters, brightness parameters, contrast parameters, or recoloring parameters.
12. The method of claim 8, wherein the transformation parameter sets comprise compression parameters.
13. The method of claim 1, wherein the neural network (i) determines similarities or differences between the generated predictions and their corresponding reference indications and (ii) updates one or more aspects of the neural network based on the determined similarities or differences.
14. The method of claim 1, further comprising: obtaining template images from a database, wherein generating the images comprises, for each template image of the template images, generating at least one of the images by integrating a given rendering of the logo with the template image.
15. The method of claim 14, wherein integrating a given rendering of the logo with the template image comprising performing multiple insertions of a given rendering of the logo on the template image such that an image generated from the multiple insertions comprises multiple renderings of the logo.
16. A system comprising: a computer system that comprises one or more processors programmed with computer program instructions that, when executed, cause the computer system to: generate training media items based on graphics information associated with a representation of a graphic, each of the training media items comprising (i) content other than the graphic and (ii) a given representation of the graphic integrated with the other content; process, via a recognition model, the training media items to generate predictions related to recognition of the graphic for the training media items, the generated predictions comprising an indication of a predicted location of the graphic in a first media item of the training media items; cause, via a user interface, presentation of the indication of the predicted location on an area of the first media item; obtain a reference feedback set, the reference feedback set comprising reference indications related to recognition of the graphic for the training media items, the reference indications comprising a reference indication corresponding to a user interaction with the indication of the predicted location of the graphic in the first media item that is initiated via the user interface; and update the recognition model based on the reference feedback set such that the recognition model is updated based on the reference indication corresponding to the user interaction with the indication of the predicted location.
17. The system of claim 16, wherein generating the training media items comprises generating the first media item such that at least some content other than the graphic appears opaquely over at least a portion of the graphic on the first media item, and wherein generating the predictions comprises generating the indication of the predicted location based on the first media item in which the at least some content appears over at least a portion of the graphic.
18. The system of claim 16, wherein obtaining the reference indications comprises obtaining a reference indication corresponding to a user-initiated movement of the indication of the predicted location to another area of the first media item that is initiated via the user interface, and wherein updating the recognition model comprises updating the recognition model based on the reference indication corresponding to the user-initiated movement.
19. The system of claim 16, wherein obtaining the reference indications comprises obtaining a reference indication corresponding to a user-initiated resizing of the indication of the predicted location that is initiated via the user interface, and wherein updating the recognition model comprises updating the recognition model based on the reference indication corresponding to the user-initiated resizing.
20. A non-transitory, machine-readable medium storing instructions that, when executed by a data processing apparatus, cause the data processing apparatus to perform the following operations: generating training media items based on graphics information associated with a representation of a graphic, each of the training media items comprising (i) content other than the graphic and (ii) a given representation of the graphic integrated with the other content; processing, via a recognition model, a first media item of the training media items to generate an indication of a predicted location of the graphic in the first media item; causing, via a user interface, presentation of the indication of the predicted location on an area of the first media item; obtaining, via the user interface, a reference indication corresponding to a user interaction with the indication of the predicted location in the first media item that is initiated via the user interface; and updating the recognition model based on the reference indication corresponding to the user interaction with the indication of the predicted location.
</claims>
</document>
