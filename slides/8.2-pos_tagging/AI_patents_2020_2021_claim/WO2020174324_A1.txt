<document>

<filing_date>
2020-02-19
</filing_date>

<publication_date>
2020-09-03
</publication_date>

<priority_date>
2019-02-26
</priority_date>

<ipc_classes>
A61F11/00,A61N1/05,A61N1/36,A61N1/372
</ipc_classes>

<assignee>
COCHLEAR
</assignee>

<inventors>
FUNG, STEPHEN
VON BRASCH, ALEXANDER
</inventors>

<docdb_family_id>
72239254
</docdb_family_id>

<title>
DYNAMIC VIRTUAL HEARING MODELLING
</title>

<abstract>
Presented herein are techniques for generating, updating, and/or using a virtual hearing model associated with a recipient of an auditory prosthesis. The virtual hearing model is generated and updated for the recipient based on psychoacoustics data associated with the recipient and, in certain cases, based on psychoacoustics data gathered from one or more selected populations of auditory prosthesis recipients. The recipient-specific virtual hearing model can be used, in real-time, to determine one or more settings for the auditory prosthesis.
</abstract>

<claims>
What is claimed is:
1. A method, comprising:
at a computing system:
receiving, from an auditory prosthesis configured to be worn by a recipient, a request for updated sound processing settings, wherein the request includes listening situation data representing an expected listening situation for the auditory prosthesis;
determining, based on a recipient-specific virtual hearing model accounting for how the auditory prosthesis operates and aids the perception of the recipient, selected sound processing settings for use by the auditory prosthesis in the expected listening situation; and sending the selected sound processing settings to the auditory prosthesis.
2. The method of claim 1, wherein the recipient-specific virtual hearing model is a holistic model of the hearing system of the recipient accounting for, in a bilateral manner, operation of each of the outer ears, middle ears, and inner ear systems of the recipient, as well as the hearing cognition in auditory cortex and brain of the recipient, and how the auditory prosthesis operates and aids the perception of the recipient.
3. The method of claim 1, wherein determining the selected sound processing settings for use by the auditory prosthesis in the expected listening situation comprises:
iteratively performing simulations for the recipient-specific virtual hearing model in at least one virtual listening situation matching the expected listening situation;
in each iteration, adjusting the simulated operation of the auditory prosthesis in the recipient-specific virtual hearing model to account for different sound processing settings of the auditory prosthesis; and
determining, as the selected sound processing settings for use by the auditory prosthesis, a set of sound processing settings estimated to provide a selected hearing perception for the recipient in the least one virtual listening situation matching the expected listening situation.
4. The method of claim 3, further comprising:
prior to receiving the request from the auditory prosthesis, iteratively performing simulations for the recipient-specific virtual hearing model in a plurality of different virtual listening situations, wherein at least one of the plurality of different virtual listening situations comprises the at least one virtual listening situation matching the expected listening situation; following receipt of the request from the auditory prosthesis, identifying the at least one virtual listening situation matching the expected listening situation; and
determining, as the selected sound processing settings for use by the auditory prosthesis, a set of sound processing settings previously estimated to provide a selected hearing perception for the recipient in the least one virtual listening situation matching the expected listening situation.
5. The method of claim 3, further comprising:
following receipt of the request from the auditory prosthesis, generating, based on the listening situation data, the at least one virtual listening situation matching the expected listening situation;
iteratively performing simulations for the recipient-specific virtual hearing model in the at least one virtual listening situation matching the expected listening situation;
in each iteration, adjusting the simulated operation of the auditory prosthesis in the recipient-specific virtual hearing model to account for different sound processing settings of the auditory prosthesis; and
determining, as the selected sound processing settings for use by the auditory prosthesis, a set of sound processing settings estimated to provide a selected hearing perception for the recipient in the least one virtual listening situation matching the expected listening situation.
6. The method of claim 3, wherein adjusting the simulated operation of the auditory prosthesis in the recipient-specific virtual hearing model in each iteration comprises:
adjusting the simulated operation of the auditory prosthesis in the recipient-specific virtual hearing model using a machine-learning algorithm.
7. The method of claim 3, wherein adjusting the simulated operation of the auditory prosthesis in the recipient-specific virtual hearing model in each iteration comprises:
adjusting the simulated operation of the auditory prosthesis in the recipient-specific virtual hearing model using an artificial intelligence algorithm.
8. The method of claim 1, further comprising:
generating the recipient-specific virtual hearing model based on recipient-specific psychoacoustics data.
9. The method of claim 8, further comprising:
receiving at least part of the recipient-specific psychoacoustics data from a psychoacoustics monitoring system in the auditory prosthesis.
10. The method of claim 8, further comprising:
receiving, over time, additional recipient-specific psychoacoustics data;
dynamically updating the recipient-specific virtual hearing model based on the additional recipient-specific psychoacoustics data.
11. The method of claim 8, further comprising:
generating the recipient-specific virtual hearing model based on psychoacoustics data gathered from one or more selected populations of auditory prosthesis recipients.
12. The method of claim 1, wherein the computing system is a mobile computing device.
13. A method, comprising:
generating, based on recipient-specific psychoacoustics data, a virtual hearing model, wherein the virtual hearing model is holistic model of the hearing system of a recipient of an auditory prosthesis, and wherein the virtual hearing model accounts for, in a bilateral manner, operation of each of the outer ears, middle ears, and inner ear systems of the recipient, as well as the hearing cognition in auditory cortex and brain of the recipient, and how the auditory prosthesis operates and aids the perception of the recipient;
using the virtual hearing model to determine selected sound processing settings for use by the auditory prosthesis; and
instantiating the selected sound processing settings at the auditory prosthesis.
14. The method of claim 13, wherein using the virtual hearing model to determine selected sound processing settings for use by the auditory prosthesis comprises:
receiving, from the auditory prosthesis, a request for updated sound processing settings, wherein the request includes listening situation data representing an expected listening situation for the auditory prosthesis;
iteratively performing simulations for the virtual hearing model in at least one virtual listening situation matching the expected listening situation;
in each iteration, adjusting the simulated operation of the auditory prosthesis in the recipient-specific virtual hearing model to account for different sound processing settings of the auditory prosthesis; and
determining, as the selected sound processing settings for use by the auditory prosthesis, a set of sound processing settings estimated to provide a selected hearing perception for the recipient in the least one virtual listening situation matching the expected listening situation.
15. The method of claim 14, further comprising:
prior to receiving the request from the auditory prosthesis, iteratively performing simulations for the virtual hearing model in a plurality of different virtual listening situations, wherein at least one of the plurality of different virtual listening situations comprises the at least one virtual listening situation matching the expected listening situation;
following receipt of the request from the auditory prosthesis, identifying the at least one virtual listening situation matching the expected listening situation; and
determining, as the selected sound processing settings for use by the auditory prosthesis, a set of sound processing settings previously estimated to provide a selected hearing perception for the recipient in the least one virtual listening situation matching the expected listening situation.
16. The method of claim 14, further comprising:
following receipt of the request from the auditory prosthesis, generating, based on the listening situation data, the at least one virtual listening situation matching the expected listening situation;
iteratively performing simulations for the virtual hearing model in the at least one virtual listening situation matching the expected listening situation;
in each iteration, adjusting the simulated operation of the auditory prosthesis in the virtual hearing model to account for different sound processing settings of the auditory prosthesis; and
determining, as the selected sound processing settings for use by the auditory prosthesis, a set of sound processing settings estimated to provide a selected hearing perception for the recipient in the least one virtual listening situation matching the expected listening situation.
17. The method of claim 14, wherein adjusting the simulated operation of the auditory prosthesis in the recipient-specific virtual hearing model in each iteration comprises:
adjusting the simulated operation of the auditory prosthesis in the virtual hearing model using a machine-learning algorithm.
18. The method of claim 14, wherein adjusting the simulated operation of the auditory prosthesis in the recipient-specific virtual hearing model in each iteration comprises:
adjusting the simulated operation of the auditory prosthesis in the virtual hearing model using an artificial intelligence algorithm.
19. The method of claim 13, further comprising:
receiving at least part of the recipient-specific psychoacoustics data from a psychoacoustics monitoring system in the auditory prosthesis.
20. The method of claim 13, further comprising:
receiving, over time, additional recipient-specific psychoacoustics data;
dynamically updating the virtual hearing model based on the additional recipientspecific psychoacoustics data.
21. The method of claim 13, further comprising:
generating the recipient-specific virtual hearing model based on psychoacoustics data gathered from one or more selected populations of auditory prosthesis recipients.
22. The method of claim 13, wherein instantiating the selected sound processing setting at the auditory prosthesis includes:
sending the selected sound processing setting to the auditory prosthesis.
23. The method of claim 13, wherein instantiating the selected sound processing setting at the auditory prosthesis includes:
sending the selected sound processing setting to the auditory prosthesis
24. The method of claim 13, further comprising:
installing a copy of the virtual hearing model at the auditory prosthesis; and at the auditory prosthesis, using the copy of the virtual hearing model to determine selected sound processing settings for use by the auditory prosthesis.
</claims>
</document>
