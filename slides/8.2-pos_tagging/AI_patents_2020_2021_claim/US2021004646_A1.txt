<document>

<filing_date>
2019-12-03
</filing_date>

<publication_date>
2021-01-07
</publication_date>

<priority_date>
2019-07-06
</priority_date>

<ipc_classes>
G06K9/62,G06N20/00,G06N5/04
</ipc_classes>

<assignee>
TOYOTA RESEARCH INSTITUTE
</assignee>

<inventors>
AMBRUS, RARES A.
GAIDON, ADRIEN DAVID
LI, JIE
Pillai, Sudeep
Guizilini, Vitor
</inventors>

<docdb_family_id>
74065242
</docdb_family_id>

<title>
SYSTEMS AND METHODS FOR WEAKLY SUPERVISED TRAINING OF A MODEL FOR MONOCULAR DEPTH ESTIMATION
</title>

<abstract>
System, methods, and other embodiments described herein relate to semi-supervised training of a depth model for monocular depth estimation. In one embodiment, a method includes training the depth model according to a first stage that is self-supervised and that includes using first training data that comprises pairs of training images. Respective ones of the pairs including separate frames depicting a scene of a monocular video. The method includes training the depth model according to a second stage that is weakly supervised and that includes using second training data to produce depth maps according to the depth model. The second training data comprising individual images with corresponding sparse depth data. The second training data providing for updating the depth model according to second stage loss values that are based, at least in part, on the depth maps and the depth data.
</abstract>

<claims>
1. A depth system for semi-supervised training of a depth model for monocular depth estimation, comprising: one or more processors; a memory communicably coupled to the one or more processors and storing: a training module including instructions that when executed by the one or more processors cause the one or more processors to: train the depth model according to a first stage that is self-supervised and that includes using first training data that comprises pairs of training images, wherein respective ones of the pairs include separate frames depicting a scene from a monocular video, wherein the training module includes instructions to produce first stage loss values that update the depth model and a pose model, wherein the pose model facilitates the first stage according to a structure from motion (SfM) process, and train the depth model according to a second stage that is weakly supervised and that includes using second training data to produce depth maps according to the depth model, wherein the second training data comprising individual images with corresponding sparse depth data, wherein the training module includes instructions to produce second stage loss values that are based, at least in part, on the depth maps and the depth data; and a network module including instructions that when executed by the one or more processors cause the one or more processors to provide the depth model to infer distances from monocular images in a device.
2. The depth system of claim 1, wherein the depth data includes sparse LiDAR data comprising depth information from four beams that correspond with sparse locations in the individual images from the second training data, and wherein the training module includes instructions to train the depth model according to the second stage including instructions to generate the first stage loss values and the second stage loss values to adapt the depth model.
3. The depth system of claim 2, wherein the training module includes instructions to train the depth model according to the second stage including instructions to generate the second stage loss values using a second stage loss function that compares values between the depth maps and corresponding information from the depth data that is the sparse LiDAR data, wherein the training module includes instructions to perform the second stage to refine learned weights of the depth model using the sparse LiDAR data to train the depth model on scale by accounting for scale aware differences between the depth maps and the sparse LiDAR data.
4. The depth system of claim 1, wherein the training module includes instructions to train the depth model according to the first stage includes producing the first stage loss values from a first stage loss function that includes a photometric loss function and a depth smoothness loss function that separately account for pixel-level similarities and irregularities along edge regions between a synthesized image derived from depth predictions of the first stage and a target image of a respective one of the pairs.
5. The depth system of claim 1, wherein the first stage is a self-supervised structure from motion (SfM) training process that accounts for motion of a camera between the training images of the pairs to cause the depth model to learn how to infer depths without annotated training data, and wherein the training module includes instructions to train the depth model according to the second stage including instructions to refine the depth model using the second training data including annotations about depth in the individual images from the corresponding sparse depth data as selective dispersed ground truths providing limited supervision over depth estimates of the individual images.
6. The depth system of claim 1, wherein the training module includes instructions to train the depth model according to the first stage includes processing a first image of one of the pairs according to the depth model and processing the first image and a second image of the one of the pairs according to the pose model, wherein the first training data includes the monocular video that provides a plurality of monocular images for self-supervised training of the depth model without pre-labeled depth data, and wherein the second training data includes the plurality of monocular images in addition to depth information.
7. The depth system of claim 1, wherein the depth model is a machine learning algorithm comprised of an encoder and a decoder that function together to generate depth estimates of a scene from a monocular image, and wherein the pose model is a machine learning algorithm that performs a dimensional reduction of the training images to derive a rigid-body transformation describing a change in pose between images within respective ones of the pairs.
8. The depth system of claim 1, wherein the training module includes instructions to train the depth model according to the first stage and the second stage including instructions to generate a photometric loss by generating a synthesized version of a first image from one of the pairs using one of the depth maps and a transformation derived by the pose model, and wherein the training module includes instructions to generate the photometric loss by calculating the photometric loss according to a comparison of the synthesized version with the first image.
9. A non-transitory computer-readable medium for semi-supervised training of a depth model for monocular depth estimation and including instructions that when executed by one or more processors cause the one or more processors to: train the depth model according to a first stage that is self-supervised and that includes using first training data that comprises pairs of training images, wherein respective ones of the pairs include separate frames depicting a scene from a monocular video, wherein the instructions include instructions to produce first stage loss values that update the depth model and a pose model, wherein the pose model facilitates the first stage according to a structure from motion (SfM) process; train the depth model according to a second stage that is weakly supervised and that includes using second training data to produce depth maps according to the depth model, wherein the second training data comprises individual images with corresponding sparse depth data, wherein the instructions to train according to the second stage include instructions to produce second stage loss values that are based, at least in part, on the depth maps and the depth data; and provide the depth model to infer distances from monocular images in a device.
10. The non-transitory computer-readable medium of claim 9, wherein the depth data includes sparse LiDAR data comprising depth information from four beams that correspond with sparse locations in the individual images from the second training data, and wherein the instructions to train the depth model according to the second stage include instructions to generate the first stage loss values and the second stage loss values to adapt the depth model.
11. The non-transitory computer-readable medium of claim 10, wherein the instructions to train the depth model according to the second stage include instructions to generate the second stage loss values using a second stage loss function that compares values between the depth maps and corresponding information from the depth data that is the sparse LiDAR data, wherein the instructions include instructions to perform the second stage to refine learned weights of the depth model using the sparse LiDAR data to train the depth model on scale by accounting for scale aware differences between the depth maps and the sparse LiDAR data.
12. The non-transitory computer-readable medium of claim 9, wherein the instructions to train the depth model according to the first stage includes producing the first stage loss values from a first stage loss function that includes a photometric loss function and a depth smoothness loss function that separately account for pixel-level similarities and irregularities along edge regions between a synthesized image derived from depth predictions of the first stage and a target image of a respective one of the pairs.
13. A method of semi-supervised training of a depth model for monocular depth estimation, comprising: training the depth model according to a first stage that is self-supervised and that includes using first training data that comprises pairs of training images, wherein respective ones of the pairs including separate frames depicting a scene of a monocular video, wherein the pairs of training images provide for producing first stage loss values to update the depth model and a pose model, wherein the pose model facilitates the first stage according to a structure from motion (SfM) process; training the depth model according to a second stage that is weakly supervised and that includes using second training data to produce depth maps according to the depth model, the second training data comprising individual images with corresponding sparse depth data, the second training data providing for updating the depth model according to second stage loss values that are based, at least in part, on the depth maps and the depth data; and providing the depth model to infer distances from monocular images in a device.
14. The method of claim 13, wherein the depth data includes sparse LiDAR data comprising depth information from four beams that correspond with sparse locations in the individual images from the second training data, and wherein training the depth model according to the second stage includes generating the first stage loss values and the second stage loss values to adapt the depth model.
15. The method of claim 14, wherein training the depth model according to the second stage includes generating the second stage loss values using a second stage loss function that compares values between the depth maps and corresponding information from the depth data that is the sparse LiDAR data, wherein the second stage refines learned weights of the depth model using the sparse LiDAR data to train the depth model on scale by accounting for scale aware differences between the depth maps and the sparse LiDAR data.
16. The method of claim 13, wherein training the depth model according to the first stage includes producing the first stage loss values from a first stage loss function that includes a photometric loss function and a depth smoothness loss function that separately account for pixel-level similarities and irregularities along edge regions between a synthesized image derived from depth predictions of the first stage and a target image of a respective one of the pairs.
17. The method of claim 13, wherein the first stage is a self-supervised structure from motion (SfM) training process that accounts for motion of a camera between the training images of separate pairs to cause the depth model to learn how to infer depths without annotated training data, and wherein training the depth model according to the second stage includes refining the depth model using the second training data using annotations about depth in the individual images from the corresponding sparse depth data as selective dispersed ground truths providing limited supervision over depth estimates of the individual images.
18. The method of claim 13, wherein training the depth model according to the first stage includes processing a first image of one of the pairs according to the depth model and processing the first image and a second image of the one of the pairs according to the pose model, wherein the first training data includes the monocular video that provides a plurality of monocular images for self-supervised training of the depth model without pre-labeled depth data, and wherein the second training data includes the plurality of monocular images in addition to depth information.
19. The method of claim 13, wherein the depth model is a machine learning algorithm comprised of an encoder and a decoder that function together to generate depth estimates of a scene from a monocular image, and wherein the pose model is a machine learning algorithm that performs a dimensional reduction of the training images to derive a rigid-body transformation describing a change in pose between images within respective ones of the pairs.
20. The method of claim 13, wherein training the depth model according to the first stage and the second stage includes generating a photometric loss by generating a synthesized version of a first image from one of the pairs using the depth map and a transformation derived by the pose model, and wherein generating the photometric loss includes calculating the photometric loss according to a comparison of the synthesized version with the first image.
</claims>
</document>
