<document>

<filing_date>
2020-07-20
</filing_date>

<publication_date>
2021-01-28
</publication_date>

<priority_date>
2019-07-22
</priority_date>

<ipc_classes>
G06F12/0893,G06F12/128,G06F13/28,G06F9/30,G06F9/38,G06F9/48,G06N20/00
</ipc_classes>

<assignee>
AMD (ADVANCED MICRO DEVICES)
</assignee>

<inventors>
GOEL, VINEET
KAZAKOV, MAXIM V.
NEMLEKAR, MILIND N.
POMIANOWSKI, ANDREW S.
SALEH, SKYLER JONATHON
Sakharshete, Swapnil P.
</inventors>

<docdb_family_id>
74190249
</docdb_family_id>

<title>
CHIPLET-INTEGRATED MACHINE LEARNING ACCELERATORS
</title>

<abstract>
Techniques for performing machine learning operations are provided. The techniques include configuring a first portion of a first chiplet as a cache; performing caching operations via the first portion; configuring at least a first sub-portion of the first portion of the chiplet as directly-accessible memory; and performing machine learning operations with the first sub-portion by a machine learning accelerator within the first chiplet.
</abstract>

<claims>
1. A method comprising: configuring a first portion of a first chiplet as a cache; performing caching operations via the first portion; configuring at least a first sub-portion of the first portion of the chiplet as directly-accessible memory; and performing machine learning operations with the first sub-portion by a machine learning accelerator within the first chiplet.
2. The method of claim 1, wherein: performing caching operations comprises performing caching operations for a processing core that is on a separate die as the first chiplet.
3. The method of claim 2, wherein: performing caching operations for the processing core comprises one or more of storing a cache line evicted from a cache of the processing core or providing a cache line to the processing core in response to a miss in a cache of the processing core.
4. The method of claim 1, wherein: configuring the first portion as a cache or configuring the first sub-portion as directly-accessible memory is performed in response to a request from a scheduler or a compute unit of a processing core that is on a separate die as the first chiplet.
5. The method of claim 1, further comprising: storing, in response to a request of a processor core that is separate from the chiplet, data within the first sub-portion configured as directly-accessible memory.
6. The method of claim 5, wherein: performing machine learning operations comprises performing the machine learning operations that consume the data as input.
7. The method of claim 1, wherein the machine learning operations comprise matrix multiplication operations.
8. The method of claim 1, wherein: the first portion comprises a first amount of memory of an internal memory of the first chiplet; and the method further comprises: while performing the caching operations via the first portion, performing machine learning operations with a second portion of the memory configured as directly-accessible memory.
9. The method of claim 1, further comprising: transmitting data to or receiving data from a second chiplet that is physically separate from a processing core that requests the first chiplet to perform machine learning operations, wherein the data is transmitted or received via a direct connection between the first chiplet and the second chiplet that does not flow through the processing core.
10. A device comprising: one or more machine learning accelerators; and a chiplet memory, configured to; configure a first portion of the chiplet memory as a cache; perform caching operations via the first portion; configure at least a first sub-portion of the first portion of the chiplet memory as directly-accessible memory; and perform machine learning operations with the first sub-portion by a machine learning accelerator of the one or more machine learning accelerators.
11. The device of claim 10, wherein: performing caching operations comprises performing caching operations for a processing core that is on a separate die as the chiplet memory.
12. The device of claim 11, wherein: performing caching operations for the processing core comprises one or more of storing a cache line evicted from a cache of the processing core or providing a cache line to the processing core in response to a miss in a cache of the processing core.
13. The device of claim 10, wherein: configuring the first portion as a cache or configuring the first sub-portion as directly-accessible memory is performed in response to a request from a scheduler or a compute unit of a processing core that is on a separate die as the chiplet memory.
14. The device of claim 10, wherein the chiplet memory is further configured to: store, in response to a request of a processor core that is separate from the chiplet, data within the first sub-portion configured as directly-accessible memory.
15. The device of claim 14, wherein: performing machine learning operations comprises performing the machine learning operations that consume the data as input.
16. The device of claim 10, wherein the machine learning operations comprise matrix multiplication operations.
17. The device of claim 10, wherein: the first portion comprises a first amount of memory of an internal memory of the first chiplet; and the one or more machine learning accelerators are configured to: while caching operations are being performed via the first portion, perform machine learning operations with a second portion of the memory configured as directly-accessible memory.
18. The device of claim 10, wherein the chiplet memory is further configured to: transmit data to or receive data from a second chiplet that is physically separate from a processing core that requests the first chiplet to perform machine learning operations, wherein the data is transmitted or received via a direct connection between the first chiplet and the second chiplet that does not flow through the processing core.
19. A device, comprising: a first chiplet including a first chiplet memory and a first set of one or more machine learning accelerators; a second chiplet; and a processing core, wherein the first chiplet is configured to: configure a first portion of the first chiplet memory as a cache; perform caching operations via the first portion; configure at least a first sub-portion of the first portion of the chiplet memory as directly-accessible memory; and perform machine learning operations with the first sub-portion by a machine learning accelerator of the one or more machine learning accelerators.
20. The device of claim 19, wherein: performing caching operations comprises performing caching operations for the processing core.
</claims>
</document>
