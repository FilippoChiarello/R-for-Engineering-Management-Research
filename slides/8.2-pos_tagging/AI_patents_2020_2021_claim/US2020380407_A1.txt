<document>

<filing_date>
2019-06-03
</filing_date>

<publication_date>
2020-12-03
</publication_date>

<priority_date>
2019-06-03
</priority_date>

<ipc_classes>
G06N20/00,G06N7/00
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
LU, WEI
BASU, KINJAL
GHOSH, SOUVIK
JIANG, CHENGMING
Gupta, Mansi
</inventors>

<docdb_family_id>
73551302
</docdb_family_id>

<title>
GENERALIZED NONLINEAR MIXED EFFECT MODELS VIA GAUSSIAN PROCESSES
</title>

<abstract>
In an example embodiment, training data is obtained, the training data comprising values for a plurality of different features. Then a global machine learned model is trained using a first machine learning algorithm by feeding the training data into the first machine learning algorithm during a fixed effect training process. A non-linear first random effects machine learned model is trained by feeding a subset of the training data into a second machine learning algorithm, the subset of the training data being limited to training data corresponding to a particular value of one of the plurality of different features.
</abstract>

<claims>
1. A system comprising: a computer-readable medium having instructions stored thereon, which, when executed by a processor, cause the system to: obtain training data, the training data comprising values for a plurality of different features; train a global machine learned model using a first machine learning algorithm by feeding the training data into the first machine learning algorithm during a fixed effect training process; and train a first non-linear random effects machine learned model by feeding a subset of the training data into a second machine learning algorithm, the subset of the training data being limited to training data corresponding to a particular value of one of the plurality of different features.
2. The system of claim 1, wherein the system is further caused to: perform one or more iterations of a machine learned model training process, the one or more iterations continuing until a convergence test is met, each iteration comprising the obtaining training data, training the global machine learned model, and training the first non-linear random effects machine learned model.
3. The system of claim 2, wherein each iteration further comprises: training a second non-linear random effects machine learned model by feeding a second subset of the training data into a third machine learning algorithm, the second subset of the training data being limited to training data corresponding to a particular value of another of the plurality of different features.
4. The system of claim 1, wherein the system is further caused to perform dimension reduction on the subset by applying a transformation to the subset.
5. The system of claim 1, wherein the second machine learning algorithm is a Gaussian process.
6. The system of claim 1, wherein the system is further caused to: feed candidate data into the global machine learned model, producing a first score; feed the candidate data into the first non-linear random effects machine learned model, producing a second score; and combine the first score and the second score into a ranking score, the ranking score used to rank the candidate data against other candidate data.
7. The system of claim 6, wherein the candidate data is job posting results from an online service.
8. A method comprising: obtaining training data, the training data comprising values for a plurality of different features; training a global machine learned model using a first machine learning algorithm by feeding the training data into the first machine learning algorithm during a fixed effect training process; and training a first non-linear random effects machine learned model by feeding a subset of the training data into a second machine learning algorithm, the subset of the training data being limited to training data corresponding to a particular value of one of the plurality of different features.
9. The method of claim 8, further comprising: performing one or more iterations of a machine learned model training process, the one or more iterations continuing until a convergence test is met, each iteration comprising the obtaining training data, training the global machine learned model, and training the first non-linear random effects machine learned model.
10. The method of claim 9, wherein each iteration further comprises: training a second non-linear random effects machine learned model by feeding a second subset of the training data into a third machine learning algorithm, the second subset of the training data being limited to training data corresponding to a particular value of another of the plurality of different features.
11. The method of claim 8, further comprising performing dimension reduction on the subset by applying a transformation to the subset.
12. The method of claim 8, wherein the second machine learning algorithm is a Gaussian process.
13. The method of claim 8, further comprising: feeding candidate data into the global machine learned model, producing a first score; feeding the candidate data into the first non-linear random effects machine learned model, producing a second score; and combining the first score and the second score into a ranking score, the ranking score used to rank the candidate data against other candidate data.
14. The method of claim 13, wherein the candidate data is job posting results from an online service.
15. A non-transitory machine-readable storage medium comprising instructions which, when implemented by one or more machines, cause the one or more machines to perform operations comprising: obtaining training data, the training data comprising values for a plurality of different features; training a global machine learned model by feeding the training data into the first machine learning algorithm during a fixed effect training process; and training a first non-linear random effects machine learned model using a second machine learning algorithm by feeding a subset of the training data into a second machine learning algorithm, the subset of the training data being limited to training data corresponding to a particular value of one of the plurality of different features.
16. The non-transitory machine-readable storage medium of claim 15, wherein the operations further comprise: performing one or more iterations of a machine learned model training process, the one or more iterations continuing until a convergence test is met, each iteration comprising the obtaining training data, training the global machine learned model, and training the first non-linear random effects machine learned model.
17. The non-transitory machine-readable storage medium of claim 16, wherein each iteration further comprises: training a second non-linear random effects machine learned model by feeding a second subset of the training data into a third machine learning algorithm, the second subset of the training data being limited to training data corresponding to a particular value of another of the plurality of different features.
18. The non-transitory machine-readable storage medium of claim 15, wherein the operations further comprise performing dimension reduction on the subset by applying a transformation to the subset.
19. The non-transitory machine-readable storage medium of claim 15, wherein the second machine learning algorithm is a Gaussian process.
20. The non-transitory machine-readable storage medium of claim 15, wherein the operations further comprise: feeding candidate data into the global machine learned model, producing a first score; feeding the candidate data into the first non-linear random effects machine learned model, producing a second score; and combining the first score and the second score into a ranking score, the ranking score used to rank the candidate data against other candidate data.
</claims>
</document>
