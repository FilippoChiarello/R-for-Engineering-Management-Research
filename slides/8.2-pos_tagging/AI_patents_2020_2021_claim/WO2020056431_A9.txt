<document>

<filing_date>
2019-10-11
</filing_date>

<publication_date>
2020-04-16
</publication_date>

<priority_date>
2018-09-12
</priority_date>

<ipc_classes>
G06K9/00
</ipc_classes>

<assignee>
TUSIMPLE
</assignee>

<inventors>
WANG, PANQU
</inventors>

<docdb_family_id>
69056107
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR THREE-DIMENSIONAL (3D) OBJECT DETECTION
</title>

<abstract>
A system and method for three-dimensional (3D) object detection is disclosed. A particular embodiment can be configured to: receive image data from at least one camera associated with an autonomous vehicle, the image data representing at least one image frame; use a trained deep learning module to determine pixel coordinates of a two-dimensional (2D) bounding box around an object detected in the image frame; use the trained deep learning module to determine vertices of a three-dimensional (3D) bounding box around the object; use a fitting module to obtain geological information related to a particular environment associated with the image frame and to obtain camera calibration information associated with the at least one camera; and use the fitting module to determine 3D attributes of the object using the 3D bounding box, the geological information, and the camera calibration information.
</abstract>

<claims>
What is claimed is:
1. A system comprising:
a data processor; and
a 3D image processing system, executable by the data processor, the image processing system being configured to:
receive image data from at least one camera associated with an autonomous vehicle, the image data representing at least one image frame;
use a trained deep learning module to determine pixel coordinates of a two-dimensional (2D) bounding box around an object detected in the image frame;
use the trained deep learning module to determine vertices of a three-dimensional (3D) bounding box around the object;
use a fitting module to obtain geological information related to a particular environment associated with the image frame and to obtain camera calibration information associated with the at least one camera; and
use the fitting module to determine 3D attributes of the object using the 3D bounding box, the geological information, and the camera calibration information.
2. The system of claim 1 being further configured to provide the 3D attributes of the object to an autonomous driving perception system.
3. The system of claim 1 wherein the at least one camera includes a camera lens of a type from the group consisting of: a wide-angle or close-range lens, a medium-range lens, and a long-range lens.
4. The system of claim 1 wherein the vertices of the three-dimensional (3D) bounding box around the object are determined in pixel coordinates.
5. The system of claim 1 wherein the geological information is obtained from a terrain map including global positioning system (GPS) locations with the height of the terrain.
6. The system of claim 1 wherein the camera calibration information includes camera calibration matrices with camera extrinsic and intrinsic matrices.
7. The system of claim 1 wherein the 3D attributes of the object include the object length, height, width, 3D spatial location, and heading of the object.
8. A method comprising:
receiving image data from at least one camera associated with an autonomous vehicle, the image data representing at least one image frame;
using a trained deep learning module to determine pixel coordinates of a two-dimensional (2D) bounding box around an object detected in the image frame;
using the trained deep learning module to determine vertices of a three-dimensional (3D) bounding box around the object;
using a fitting module to obtain geological information related to a particular environment associated with the image frame and to obtain camera calibration information associated with the at least one camera; and
using the fitting module to determine 3D attributes of the object using the 3D bounding box, the geological information, and the camera calibration information.
9. The method of claim 8 including providing the 3D attributes of the object to an autonomous driving perception system.
10. The method of claim 8 wherein the at least one camera includes a camera lens of a type from the group consisting of: a wide-angle or close-range lens, a medium-range lens, and a long-range lens.
11. The method of claim 8 wherein the vertices of the three-dimensional (3D) bounding box around the object are determined in pixel coordinates.
12. The method of claim 8 wherein the geological information is obtained from a terrain map including global positioning system (GPS) locations with the height of the terrain.
13. The method of claim 8 wherein the camera calibration information includes camera calibration matrices with camera extrinsic and intrinsic matrices.
14. The method of claim 8 wherein the 3D attributes of the object include the object length, height, width, 3D spatial location, and heading of the object.
</claims>
</document>
