<document>

<filing_date>
2019-06-28
</filing_date>

<publication_date>
2020-12-31
</publication_date>

<priority_date>
2019-06-28
</priority_date>

<ipc_classes>
G05D1/02,G05D1/08,G06N3/08
</ipc_classes>

<assignee>
FORD GLOBAL TECHNOLOGIES
</assignee>

<inventors>
PUSKORIUS GINTARAS VINCENT
JALES COSTA, BRUNO SIELLY
CHAKRAVARTY, PUNARJAY
</inventors>

<docdb_family_id>
73747291
</docdb_family_id>

<title>
VEHICLE VISUAL ODOMETRY
</title>

<abstract>
A computer, including a processor and a memory, the memory including instructions to be executed by the processor to determine an eccentricity map based on video image data and determine vehicle motion data by processing the eccentricity map and two red, green, blue (RGB) video images with a deep neural network trained to output vehicle motion data in global coordinates. The instructions can further include instructions to operate a vehicle based on the vehicle motion data.
</abstract>

<claims>
1. A computer, comprising a processor; and a memory, the memory including instructions to be executed by the processor to: determine an eccentricity map based on video image data; determine vehicle motion data by processing the eccentricity map and two red, green, blue (RGB) video images with a deep neural network trained to output vehicle motion data in global coordinates; and operate a vehicle based on the vehicle motion data.
2. The computer of claim 1, wherein the two RGB video images are acquired at a time step, where the time step corresponds to a small number of video frames.
3. The computer of claim 1, wherein vehicle motion data includes vehicle location, speed and direction with respect to an external environment of the vehicle.
4. The computer of claim 1, the instructions further including instructions to determine the eccentricity map by determining per-pixel mean μk and per-pixel variance σk2 based on an exponential decay factor α, wherein the eccentricity map measures the motion of objects, edges and surfaces in video stream data.
5. The computer of claim 1, the instructions further including instructions to concatenate the eccentricity map with the two RGB images as input channels to the deep neural network.
6. The computer of claim 5, the instructions further including instructions to process the concatenated eccentricity map and two RGB images using a plurality of convolutional layers to generate hidden variables corresponding to vehicle motion data.
7. The computer of claim 6, the instructions further including instructions to process the hidden variables corresponding to vehicle motion data with a plurality of fully connected layers to generate x, y, and z location coordinates and roll, pitch, and yaw rotational coordinates.
8. The computer of claim 1, the instructions further including instructions to train the deep neural network based on a training dataset including eccentricity maps, RGB images and vehicle motion ground truth in global coordinates.
9. The computer of claim 8, wherein vehicle motion ground truth is generated based on processing dense optical flow images and corresponding RGB image pairs.
10. The computer of claim 1, the instructions further including instructions to acquire the RGB video images from a vehicle video sensor.
11. A method, comprising: determining an eccentricity map based on video image data; determining vehicle motion data by processing the eccentricity map and two red, green, blue (RGB) video images with a deep neural network trained to output vehicle motion data in global coordinates; and operating a vehicle based on the vehicle motion data.
12. The method of claim 11, wherein the two RGB video images are acquired at a time step, where the time step is a small number of video frames.
13. The method of claim 11, wherein vehicle motion data includes vehicle location, speed and direction with respect to an external environment of the vehicle.
14. The method of claim 11, further comprising determining an eccentricity map by determining a per-pixel mean μk and a per-pixel variance σk2 based on an exponential decay factor α, wherein the eccentricity map measures the motion of objects, edges and surfaces in video stream data.
15. The method of claim 11, further comprising concatenating the eccentricity map with the two RGB images as input channels to the deep neural network.
16. The method of claim 15, further comprising processing the concatenated eccentricity map and two RGB images using a plurality of convolutional layers to generate hidden variables corresponding to vehicle motion data.
17. The method of claim 16, further comprising processing the hidden variables corresponding to vehicle motion data with a plurality of fully connected layers to generate x, y, and z location coordinates and roll, pitch, and yaw rotational coordinates.
18. The method of claim 11, further comprising training the deep neural network based on a training dataset including eccentricity maps, RGB images and vehicle motion ground truth in global coordinates.
19. The method of claim 18, wherein vehicle motion ground truth is generated based on processing dense optical flow images and corresponding RGB image pairs.
20. The method of claim 11, further comprising acquiring the RGB video images from a vehicle video sensor.
</claims>
</document>
