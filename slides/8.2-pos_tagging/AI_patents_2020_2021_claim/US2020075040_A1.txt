<document>

<filing_date>
2019-08-30
</filing_date>

<publication_date>
2020-03-05
</publication_date>

<priority_date>
2018-08-31
</priority_date>

<ipc_classes>
G06N20/00,G06N3/08,G10L25/63
</ipc_classes>

<assignee>
UNIVERSITY OF MICHIGAN
</assignee>

<inventors>
GIDEON, JOHN HENRY
KHORRAM, SOHEIL
MATTON, KATHERINE ANNE
MCINNIS, MELVIN
PROVOST, EMILY MOWER
</inventors>

<docdb_family_id>
69641561
</docdb_family_id>

<title>
AUTOMATIC SPEECH-BASED LONGITUDINAL EMOTION AND MOOD RECOGNITION FOR MENTAL HEALTH TREATMENT
</title>

<abstract>
A method of predicting a mood state of a user may include recording an audio sample via a microphone of a mobile computing device of the user based on the occurrence of an event, extracting a set of acoustic features from the audio sample, generating one or more emotion values by analyzing the set of acoustic features using a trained machine learning model, and determining the mood state of the user, based on the one or more emotion values. In some embodiments, the audio sample may be ambient audio recorded periodically, and/or call data of the user recorded during clinical calls or personal calls.
</abstract>

<claims>
1. A method of predicting a mood state of a user, the method comprising: (1) based on the occurrence of an event, recording, via a microphone of a mobile computing device of the user, an audio sample; (2) extracting, from the audio sample, a set of acoustic features; (3) generating one or more emotion values, by analyzing the set of acoustic features using a trained machine learning model; and (4) determining, based on the one or more emotion values, the mood state of the user.
2. The method of claim 1, wherein the event relates to either (i) an initiation of an outgoing telephone call, (ii) an acceptance of an incoming telephone call, or (iii) an ambient audio trigger.
3. The method of claim 1, wherein recording the audio sample includes one or both of (i) storing the audio sample in a memory of the mobile computing device of the user, and (ii) verifying the identity of the user.
4. The method of claim 1, further comprising determining a mood severity of the mood state.
5. The method of claim 1, wherein the set of acoustic features correspond to the Geneva Minimalistic Acoustic Parameter Set (GeMAPS) features.
6. The method of claim 1, wherein the set of acoustic features correspond to log Mel-frequency bank (log-MFB) features.
7. The method of claim 1, wherein the trained machine learning model is a deep feed-forward neural network.
8. The method of claim 1, wherein the trained machine learning model is a convolutional neural network.
9. The method of claim 1, further comprising: correlating the mood state to a mood label, wherein the mood label is selected from the group consisting of euthymic, manic, and depressed.
10. The method of claim 9 wherein correlating the mood state to the mood label includes scoring the mood state using one or both of (i) a Hamilton Depression Scale, and (ii) Young Mania Rating Scale.
11. The method of claim 1, further comprising: repeating steps (1)-(4) at least once during a predetermined time period and at a predetermined time interval, to generate a plurality of mood states of the user, the plurality of mood states including a plurality of emotion values; analyzing the plurality of emotion values to determine a mean emotion value with respect to mania and a mean emotion value with respect to depression; and analyzing the mean emotion value with respect to mania and the mean emotion value with respect to depression to determine a condition of the user.
12. The method of claim 11, wherein the condition of the user is bipolar disorder.
13. The method of claim 1, further comprising: extracting language expressed in the audio sample, wherein generating the one or more emotion values includes analyzing the language expressed in the audio sample.
14. A mood state prediction system, the system comprising: a first computing device comprising a processor, a microphone, and a non-transitory memory, the memory storing instructions that, when executed by the processor, cause the processor to: receive, based on the occurrence of an event, an audio sample via the microphone; and a second computing device comprising a processor and a non-transitory memory, the memory storing instructions that, when executed by the processor, cause the processor to: extract a set of acoustic features from the audio sample, generate one or more emotion values, by analyzing the set of acoustic features using a trained machine learning model; and determine a mood state of the user based on the one or more emotion values.
15. The mood state prediction system of claim 14, wherein the first computing device and the same computing device are the same device.
16. The mood state prediction system of claim 14, including further instructions that, when executed by the processor, cause the processor to: transmit one or both of (i) the audio sample, and (ii) the set of acoustic features from the first computing device to the second computing device.
17. The mood state prediction system of claim 14, wherein the event relates to either (i) an initiation of an outgoing telephone call, (ii) an acceptance of an incoming telephone call, or (iii) an ambient audio trigger.
18. The mood state prediction system of claim 14, wherein the machine learning model is a deep feed-forward neural network.
19. The mood state prediction system of claim 14, wherein the machine learning model is a convolutional neural network.
20. The method of claim 1, including further instructions that, when executed by the processor, cause the processor to: correlate the mood state to a mood label, wherein the mood label is selected from the group consisting of euthymic, manic, and depressed.
</claims>
</document>
