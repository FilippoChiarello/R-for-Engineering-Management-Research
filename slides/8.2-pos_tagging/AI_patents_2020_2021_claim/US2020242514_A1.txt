<document>

<filing_date>
2020-04-16
</filing_date>

<publication_date>
2020-07-30
</publication_date>

<priority_date>
2016-09-26
</priority_date>

<ipc_classes>
G06F17/16,G06F17/18,G06F7/58,G06N20/00,G06N7/00,H04L29/06
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
MCMAHAN, HUGH BRENDAN
KONECNY, JAKUB
YU, XINNAN
BACON, DAVE MORRIS
</inventors>

<docdb_family_id>
59982468
</docdb_family_id>

<title>
Communication Efficient Federated Learning
</title>

<abstract>
The present disclosure provides efficient communication techniques for transmission of model updates within a machine learning framework, such as, for example, a federated learning framework in which a high-quality centralized model is trained on training data distributed overt a large number of clients each with unreliable network connections and low computational power. In an example federated learning setting, in each of a plurality of rounds, each client independently updates the model based on its local data and communicates the updated model back to the server, where all the client-side updates are used to update a global model. The present disclosure provides systems and methods that reduce communication costs. In particular, the present disclosure provides at least: structured update approaches in which the model update is restricted to be small and sketched update approaches in which the model update is compressed before sending to the server.
</abstract>

<claims>
1. 1.-20. (canceled)
21. A client computing device, comprising: at least one processor; and at least one non-transitory computer-readable medium that stores instructions that, when executed by the at least one processor, cause the client computing device to perform operations, the operations comprising: obtaining global values for a set of parameters of a machine-learned model; training the machine-learned model based at least in part on a local dataset to obtain an update matrix that is descriptive of updated values for the set of parameters of the machine-learned model, wherein the local dataset is stored locally by the client computing device; encoding the update matrix to obtain an encoded update; and communicating the encoded update to a server computing device.
22. The client computing device of claim 21, wherein encoding the update matrix comprises subsampling the update matrix to obtain the encoded update.
23. The client computing device of claim 22, wherein subsampling the update matrix comprises: generating a parameter mask that specifies a portion of the set of parameters to be sampled; and subsampling the update matrix according to the parameter mask.
24. The client computing device of claim 23, wherein generating the parameter mask comprises generating the parameter mask based at least in part on a seed and a pseudorandom number generator, wherein both the client computing device and the server computing device have knowledge of the seed such that the parameter mask is reproducible by the server computing device.
25. The client computing device of claim 21, wherein encoding the update matrix comprises probabilistically quantizing one or more values included in the update matrix.
26. The client computing device of claim 21, wherein encoding the update matrix comprises performing probabilistic binary quantization for one or more values included in the update matrix to change each of the one or more values to a maximum value included in the update matrix or a minimum value included in the update matrix.
27. The client computing device of claim 21, wherein encoding the update matrix comprises: defining a plurality of intervals between a maximum value included in the update matrix and a minimum value included in the update matrix; and probabilistically changing each of one or more values included in the update matrix to a local interval maximum or a local interval maximum.
28. The client computing device of claim 21, wherein encoding the update matrix comprises multiplying a vector of the update matrix by a rotation matrix to obtain a rotated update.
29. The client computing device of claim 28, wherein encoding the update matrix further comprises: probabilistically quantizing one or more values included in the rotated update.
30. The client computing device of claim 28, wherein the rotation matrix is a structured rotation matrix that does not require complete generation of the rotation matrix by the client computing device.
31. At least one non-transitory computer-readable medium that stores instructions that, when executed by a client computing device, cause the client computing device to perform operations, the operations comprising: obtaining global values for a set of parameters of a machine-learned model; training the machine-learned model based at least in part on a local dataset to obtain an update matrix that is descriptive of updated values for the set of parameters of the machine-learned model, wherein the local dataset is stored locally by the client computing device, and wherein the update matrix is restricted to be at least one of a low-rank matrix and a sparse matrix; encoding the update matrix to obtain an encoded update; and communicating the encoded update to a server computing device.
32. A computer-implemented method, comprising: obtaining global values for a set of parameters of a machine-learned model; training the machine-learned model based at least in part on a local dataset to obtain an update matrix that is descriptive of updated values for the set of parameters of the machine-learned model, wherein the local dataset is stored locally by the client computing device; encoding the update matrix to obtain an encoded update; and communicating the encoded update to a server computing device.
33. The method of claim 32, wherein encoding the update matrix comprises subsampling the update matrix to obtain the encoded update.
34. The method of claim 33, wherein subsampling the update matrix comprises: generating a parameter mask that specifies a portion of the set of parameters to be sampled; and sub sampling the update matrix according to the parameter mask.
35. The method of claim 34, wherein generating the parameter mask comprises generating the parameter mask based at least in part on a seed and a pseudo-random number generator, wherein both the client computing device and the server computing device have knowledge of the seed such that the parameter mask is reproducible by the server computing device.
36. The method of claim 32, wherein encoding the update matrix comprises probabilistically quantizing one or more values included in the update matrix.
37. The method of claim 32, wherein encoding the update matrix comprises performing probabilistic binary quantization for one or more values included in the update matrix to change each of the one or more values to a maximum value included in the update matrix or a minimum value included in the update matrix.
38. The method of claim 32, wherein encoding the update matrix comprises: defining a plurality of intervals between a maximum value included in the update matrix and a minimum value included in the update matrix; and probabilistically changing each of one or more values included in the update matrix to a local interval maximum or a local interval maximum.
39. The method of claim 32, wherein encoding the update matrix comprises multiplying a vector of the update matrix by a rotation matrix to obtain a rotated update.
40. The method of claim 39, wherein encoding the update matrix further comprises: probabilistically quantizing one or more values included in the rotated update.
</claims>
</document>
