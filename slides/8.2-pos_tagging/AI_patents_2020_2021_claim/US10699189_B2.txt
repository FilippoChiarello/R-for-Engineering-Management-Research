<document>

<filing_date>
2018-02-23
</filing_date>

<publication_date>
2020-06-30
</publication_date>

<priority_date>
2017-02-23
</priority_date>

<ipc_classes>
G06F9/455,G06N3/00,G06N3/04,G06N3/063,G06N3/08,G06N3/10
</ipc_classes>

<assignee>
CEREBRAS SYSTEMS
</assignee>

<inventors>
AREKAPUDI, SRIKANTH
JAMES, MICHAEL EDWIN
LAUTERBACH, GARY R.
LIE, SEAN
MORRISON, MICHAEL
</inventors>

<docdb_family_id>
63253606
</docdb_family_id>

<title>
Accelerated deep learning
</title>

<abstract>
Techniques in advanced deep learning provide improvements in one or more of accuracy, performance, and energy efficiency, such as accuracy of learning, accuracy of prediction, speed of learning, performance of learning, and energy efficiency of learning. An array of processing elements performs flow-based computations on wavelets of data. Each processing element has a respective compute element and a respective routing element. Each compute element has processing resources and memory resources. Each router enables communication via wavelets with at least nearest neighbors in a 2D mesh. Stochastic gradient descent, mini-batch gradient descent, and continuous propagation gradient descent are techniques usable to train weights of a neural network modeled by the processing elements. Reverse checkpoint is usable to reduce memory usage during the training.
</abstract>

<claims>
1. A system comprising: a fabric of processor elements, each processor element comprising a fabric router and a compute engine enabled to perform dataflow-based and instruction-based processing; wherein each processor element selectively communicates fabric packets with others of the processor elements; wherein each compute engine selectively performs the processing in accordance with a virtual channel specifier and a task specifier of at least some of the fabric packets the compute engine receives; wherein the instruction-based processing is in accordance with the task specifier; wherein each compute engine is configured to perform a predefined set of basic operations in response to receiving a corresponding basic instruction selected from a predefined native instruction set of codes; further comprising a training workload comprising a first set of machine codes selected from the native instruction set for performing a mapping of at least a part of a neuron onto the compute engine of the processor element, the mapping comprising managing at least one partial-neuron weight, a second set of machine codes selected from the native instruction set for performing a forward pass to propagate activations in a forward logical direction based at least in part on the at least one partial-neuron weight, the forward pass initiated responsive to an input sample, a third set of machine codes selected from the native instruction set for performing a delta pass in a backward logical direction to generate deltas, the delta pass initiated responsive to completion of the forward pass, a fourth set of machine codes selected from the native instruction set for performing a chain pass to calculate gradients based on the deltas, and a fifth set of machine codes selected from the native instruction set for performing a selective update of the at least one partial-neuron weight in accordance with a predetermined learning rule and based at least in part on the deltas; wherein each compute engine comprises storage for the at least one partial-neuron weight; wherein an iteration of the training workload is performed for each of a plurality of input samples collectively comprising a training set; wherein for each input sample, the system is enabled to selectively update the at least one partial-neuron weight in accordance with the predetermined learning rule responsive to completion of the forward pass, the delta pass, and the chain pass corresponding to the input sample; wherein the system is enabled for each forward pass to use weight information provided by the most recent selective update of the at least one partial-neuron weight; and wherein the system is enabled to perform the delta pass and the chain pass for each input sample based at least in part on activations that are recomputed based at least in part on a first partial-neuron weight.
2. The system of claim 1, wherein the mapping is in accordance with initializing the fabric to implement a partitioning of a neural network into a plurality of layers, the neuron is a first neuron of a plurality of neurons of the neural network, the first neuron is comprised in a first layer of the plurality of layers, and each of the plurality of neurons is mapped in a distributed manner across a plurality of the processor elements of the fabric.
3. The system of claim 2, wherein the plurality of layers operates as a logical fabric pipeline comprising logical fabric pipeline stages, each logical fabric pipeline stage comprising completion of all of the passes for each layer, the completion for each layer taking a time step comprising the same amount of time.
4. The system of claim 2, wherein as each input sample of a training set streams through at least a first plurality of the processor elements across the plurality of layers, the neuron weights are selectively updated in the first plurality of the processor elements across the plurality of layers.
5. The system of claim 1, wherein the training set is partitioned into a plurality of so-called mini-batches and the predetermined learning rule specifies that the at least one partial-neuron weight is updated after the completion of all the passes for each input sample of each of the mini-batches.
6. The system of claim 5, wherein the forward pass incorporates weight updates within a first plurality of the processor elements while the mini-batch learning is ongoing within the first plurality of the processor elements.
7. The system of claim 1, wherein the storage is comprised in a memory local to the compute engine.
8. The system of claim 1, wherein each compute engine further comprises storage for gradient accumulation, forward partial sums, delta partial sums, and forward pass activations.
9. A method comprising: in each of a fabric of processor elements, selectively communicating fabric packets with others of the processor elements, each processor element comprising a fabric router and a compute engine enabled to perform dataflow-based and instruction-based processing; in each compute engine, selectively performing the processing in accordance with a virtual channel specifier and a task specifier of at least some of the fabric packets the compute engine receives; wherein the instruction-based processing is in accordance with the task specifier; wherein each compute engine is configured to perform a predefined set of basic operations in response to receiving a corresponding basic instruction selected from a predefined native instruction set of codes; further comprising processing a training workload comprising a first set of machine codes selected from the native instruction set for performing a mapping of at least a part of a neuron onto the compute engine of the processor element, the mapping comprising managing at least one partial-neuron weight, a second set of machine codes selected from the native instruction set for performing a forward pass to propagate activations in a forward logical direction based at least in part on the at least one partial-neuron weight, the forward pass initiated responsive to an input sample, a third set of machine codes selected from the native instruction set for performing a delta pass in a backward logical direction to generate deltas, the delta pass initiated responsive to completion of the forward pass, a fourth set of machine codes selected from the native instruction set for performing a chain pass to calculate gradients based on the deltas, and a fifth set of machine codes selected from the native instruction set for performing a selective update of the at least one partial-neuron weight in accordance with a predetermined learning rule and based at least in part on the deltas; wherein each compute engine comprises storage for the at least one partial-neuron weight; further comprising performing an iteration of the training workload for each of a plurality of input samples collectively comprising a training set; further comprising, for each input sample, selectively updating the at least one partial-neuron weight in accordance with the predetermined learning rule responsive to completion of the forward pass, the delta pass, and the chain pass corresponding to the input sample; further comprising, for each forward pass, selectively using weight information provided by the most recent selective update of the at least one partial-neuron weight; and further comprising selectively performing the delta pass and the chain pass for each input sample based at least in part on activations that are recomputed based at least in part on a first partial-neuron weight.
10. The method of claim 9, wherein the mapping is in accordance with initializing the fabric to implement a partitioning of a neural network into a plurality of layers, the neuron is a first neuron of a plurality of neurons of the neural network, the first neuron is comprised in a first layer of the plurality of layers, and each of the plurality of neurons is mapped in a distributed manner across a plurality of the processor elements of the fabric.
11. The method of claim 10, wherein the plurality of layers operates as a logical fabric pipeline comprising logical fabric pipeline stages, each logical fabric pipeline stage comprising completion of all of the passes for each layer, the completion for each layer taking a time step comprising the same amount of time.
12. The method of claim 10, wherein as each input sample of a training set streams through at least a first plurality of the processor elements across the plurality of layers, the neuron weights are selectively updated in the first plurality of the processor elements across the plurality of layers.
13. The method of claim 9, further comprising partitioning the training set into a plurality of so-called mini-batches and the predetermined learning rule specifies that the at least one partial-neuron weight is updated after the completion of all the passes for each input sample of each of the mini-batches.
14. The method of claim 13, wherein the forward pass incorporates weight updates within a first plurality of the processor elements while the mini-batch learning is ongoing within the first plurality of the processor elements.
15. The method of claim 9, wherein the storage is comprised in a memory local to the compute engine.
16. The method of claim 9, wherein each compute engine further comprises storage for gradient accumulation, forward partial sums, delta partial sums, and forward pass activations.
17. A system comprising: in each of a fabric of processor elements, means for selectively communicating fabric packets with others of the processor elements, each processor element comprising a fabric router and a compute engine enabled to perform dataflow-based and instruction-based processing; in each compute engine, means for selectively performing the processing in accordance with a virtual channel specifier and a task specifier of at least some of the fabric packets the compute engine receives; wherein the instruction-based processing is in accordance with the task specifier; wherein each compute engine is configured to perform a predefined set of basic operations in response to receiving a corresponding basic instruction selected from a predefined native instruction set of codes; further comprising a training workload comprising a first set of machine codes selected from the native instruction set for performing a mapping of at least a part of a neuron onto the compute engine of the processor element, the mapping comprising managing at least one partial-neuron weight, a second set of machine codes selected from the native instruction set for performing a forward pass to propagate activations in a forward logical direction based at least in part on the at least one partial-neuron weight, the forward pass initiated responsive to an input sample, a third set of machine codes selected from the native instruction set for performing a delta pass in a backward logical direction to generate deltas, the delta pass initiated responsive to completion of the forward pass, a fourth set of machine codes selected from the native instruction set for performing a chain pass to calculate gradients based on the deltas, and a fifth set of machine codes selected from the native instruction set for performing a selective update of the at least one partial-neuron weight in accordance with a predetermined learning rule and based at least in part on the deltas; wherein each compute engine comprises storage for the at least one partial-neuron weight; further comprising means for performing an iteration of the training workload for each of a plurality of input samples collectively comprising a training set; further comprising, for each input sample, means for selectively updating the at least one partial-neuron weight in accordance with the predetermined learning rule responsive to completion of the forward pass, the delta pass, and the chain pass corresponding to the input sample; further comprising means for selectively using weight information provided by the most recent selective update of the at least one partial-neuron weight for each forward pass; and further comprising means for initiating a forward pass of a particular iteration of the training workload independent of whether the selective update of the at least one partial-neuron weight corresponding to a prior iteration of the training workload has occurred.
18. The system of claim 17, wherein the mapping is in accordance with initializing the fabric to implement a partitioning of a neural network into a plurality of layers, the neuron is a first neuron of a plurality of neurons of the neural network, the first neuron is comprised in a first layer of the plurality of layers, and each of the plurality of neurons is mapped in a distributed manner across a plurality of the processor elements of the fabric.
19. The system of claim 18, wherein the plurality of layers operates as a logical fabric pipeline comprising logical fabric pipeline stages, each logical fabric pipeline stage comprising completion of all of the passes for each layer, the completion for each layer taking a time step comprising the same amount of time.
20. The system of claim 18, wherein as each input sample of a training set streams through at least a first plurality of the processor elements across the plurality of layers, the neuron weights are selectively updated in the first plurality of the processor elements across the plurality of layers.
21. The system of claim 18, wherein as each input sample of a training set streams through at least a first plurality of the processor elements across the plurality of layers, the neuron weights are selectively updated in the first plurality of the processor elements across the plurality of layers, and the streaming and updating is ongoing for each time step over a plurality of time steps.
22. The system of claim 19, further comprising means for selectively updating the at least one partial-neuron weight within a first plurality of the processor elements in response to changes in backward propagating data within the first plurality of the processor elements for each time step over a plurality of time steps while forward propagation of activations are ongoing.
23. The system of claim 19, further comprising means for selectively updating the at least one partial-neuron weight each time step over a plurality of time steps.
24. The system of claim 20, 21, 22, or 23, wherein the selectively updating is in accordance with a continuous propagation gradient descent process.
25. The system of claim 17, further comprising means for partitioning the training set into a plurality of so-called mini-batches and the predetermined learning rule specifies that the at least one partial-neuron weight is updated after the completion of all the passes for each input sample of each of the mini-batches.
26. The system of claim 25, wherein the predetermined learning rule is in accordance with a continuous propagation gradient descent process.
27. The system of claim 25, wherein the forward pass incorporates weight updates within a first plurality of the processor elements while the mini-batch learning is ongoing within the first plurality of the processor elements.
28. The system of claim 17, wherein the storage is comprised in a memory local to the compute engine.
29. The system of claim 17, wherein each compute engine further comprises storage for gradient accumulation, forward partial sums, delta partial sums, and forward pass activations.
30. The system of claim 17, wherein the predetermined learning rule is in accordance with a continuous propagation gradient descent process.
31. A system comprising: in each of a fabric of processor elements, means for selectively communicating fabric packets with others of the processor elements, each processor element comprising a fabric router and a compute engine enabled to perform dataflow-based and instruction-based processing; in each compute engine, means for selectively performing the processing in accordance with a virtual channel specifier and a task specifier of at least some of the fabric packets the compute engine receives; wherein the instruction-based processing is in accordance with the task specifier; wherein each compute engine is configured to perform a predefined set of basic operations in response to receiving a corresponding basic instruction selected from a predefined native instruction set of codes; further comprising a training workload comprising a first set of machine codes selected from the native instruction set for performing a mapping of at least a part of a neuron onto the compute engine of the processor element, the mapping comprising managing at least one partial-neuron weight, a second set of machine codes selected from the native instruction set for performing a forward pass to propagate activations in a forward logical direction based at least in part on the at least one partial-neuron weight, the forward pass initiated responsive to an input sample, a third set of machine codes selected from the native instruction set for performing a delta pass in a backward logical direction to generate deltas, the delta pass initiated responsive to completion of the forward pass, a fourth set of machine codes selected from the native instruction set for performing a chain pass to calculate gradients based on the deltas, and a fifth set of machine codes selected from the native instruction set for performing a selective update of the at least one partial-neuron weight in accordance with a predetermined learning rule and based at least in part on the deltas; wherein each compute engine comprises storage for the at least one partial-neuron weight; further comprising means for performing an iteration of the training workload for each of a plurality of input samples collectively comprising a training set; further comprising, for each input sample, means for selectively updating the at least one partial-neuron weight in accordance with the predetermined learning rule responsive to completion of the forward pass, the delta pass, and the chain pass corresponding to the input sample; further comprising means for selectively using weight information provided by the most recent selective update of the at least one partial-neuron weight for each forward pass; and further comprising means for selectively initiating a forward pass of a particular iteration of the training workload independent of whether the delta pass of a prior iteration of the training workload has begun.
32. The system of claim 31, wherein the mapping is in accordance with initializing the fabric to implement a partitioning of a neural network into a plurality of layers, the neuron is a first neuron of a plurality of neurons of the neural network, the first neuron is comprised in a first layer of the plurality of layers, and each of the plurality of neurons is mapped in a distributed manner across a plurality of the processor elements of the fabric.
33. The system of claim 31, wherein the storage is comprised in a memory local to the compute engine.
34. The system of claim 31, wherein each compute engine further comprises storage for gradient accumulation, forward partial sums, delta partial sums, and forward pass activations.
35. A system comprising: in each of a fabric of processor elements, means for selectively communicating fabric packets with others of the processor elements, each processor element comprising a fabric router and a compute engine enabled to perform dataflow-based and instruction-based processing; in each compute engine, means for selectively performing the processing in accordance with a virtual channel specifier and a task specifier of at least some of the fabric packets the compute engine receives; wherein the instruction-based processing is in accordance with the task specifier; wherein each compute engine is configured to perform a predefined set of basic operations in response to receiving a corresponding basic instruction selected from a predefined native instruction set of codes; further comprising a training workload comprising a first set of machine codes selected from the native instruction set for performing a mapping of at least a part of a neuron onto the compute engine of the processor element, the mapping comprising managing at least one partial-neuron weight, a second set of machine codes selected from the native instruction set for performing a forward pass to propagate activations in a forward logical direction based at least in part on the at least one partial-neuron weight, the forward pass initiated responsive to an input sample, a third set of machine codes selected from the native instruction set for performing a delta pass in a backward logical direction to generate deltas, the delta pass initiated responsive to completion of the forward pass, a fourth set of machine codes selected from the native instruction set for performing a chain pass to calculate gradients based on the deltas, and a fifth set of machine codes selected from the native instruction set for performing a selective update of the at least one partial-neuron weight in accordance with a predetermined learning rule and based at least in part on the deltas; wherein each compute engine comprises storage for the at least one partial-neuron weight; further comprising means for performing an iteration of the training workload for each of a plurality of input samples collectively comprising a training set; further comprising, for each input sample, means for selectively updating the at least one partial-neuron weight in accordance with the predetermined learning rule responsive to completion of the forward pass, the delta pass, and the chain pass corresponding to the input sample; further comprising means for selectively using weight information provided by the most recent selective update of the at least one partial-neuron weight for each forward pass; and further comprising, in at least one of the compute engines, means for performing at least a portion of a forward pass for a subsequent iteration of the training workload after performing at least a portion of a forward pass for a prior iteration of the training workload and before performing a portion of the selective update of the at least one partial-neuron weight corresponding to the prior iteration of the training workload.
36. The system of claim 35, wherein the mapping is in accordance with initializing the fabric to implement a partitioning of a neural network into a plurality of layers, the neuron is a first neuron of a plurality of neurons of the neural network, the first neuron is comprised in a first layer of the plurality of layers, and each of the plurality of neurons is mapped in a distributed manner across a plurality of the processor elements of the fabric.
37. The system of claim 35, wherein the storage is comprised in a memory local to the compute engine.
38. The system of claim 35, wherein each compute engine further comprises storage for gradient accumulation, forward partial sums, delta partial sums, and forward pass activations.
39. A system comprising: in each of a fabric of processor elements, means for selectively communicating fabric packets with others of the processor elements, each processor element comprising a fabric router and a compute engine enabled to perform dataflow-based and instruction-based processing; in each compute engine, means for selectively performing the processing in accordance with a virtual channel specifier and a task specifier of at least some of the fabric packets the compute engine receives; wherein the instruction-based processing is in accordance with the task specifier; wherein each compute engine is configured to perform a predefined set of basic operations in response to receiving a corresponding basic instruction selected from a predefined native instruction set of codes; further comprising a training workload comprising a first set of machine codes selected from the native instruction set for performing a mapping of at least a part of a neuron onto the compute engine of the processor element, the mapping comprising managing at least one partial-neuron weight, a second set of machine codes selected from the native instruction set for performing a forward pass to propagate activations in a forward logical direction based at least in part on the at least one partial-neuron weight, the forward pass initiated responsive to an input sample, a third set of machine codes selected from the native instruction set for performing a delta pass in a backward logical direction to generate deltas, the delta pass initiated responsive to completion of the forward pass, a fourth set of machine codes selected from the native instruction set for performing a chain pass to calculate gradients based on the deltas, and a fifth set of machine codes selected from the native instruction set for performing a selective update of the at least one partial-neuron weight in accordance with a predetermined learning rule and based at least in part on the deltas; wherein each compute engine comprises storage for the at least one partial-neuron weight; further comprising means for performing an iteration of the training workload for each of a plurality of input samples collectively comprising a training set; further comprising, for each input sample, means for selectively updating the at least one partial-neuron weight in accordance with the predetermined learning rule responsive to completion of the forward pass, the delta pass, and the chain pass corresponding to the input sample; further comprising means for selectively using weight information provided by the most recent selective update of the at least one partial-neuron weight for each forward pass; and further comprising means for selectively performing the delta pass and the chain pass for each input sample based at least in part on activations that are recomputed based at least in part on a first partial-neuron weight.
40. The system of claim 39, wherein the first partial-neuron weight is the partial-neuron weight produced by the most recent selective update.
41. The system of claim 40, wherein the recomputed activations need not be stored between computations, thereby decreasing the total memory required for a given system training configuration.
42. The system of claim 39, wherein the mapping is in accordance with initializing the fabric to implement a partitioning of a neural network into a plurality of layers, the neuron is a first neuron of a plurality of neurons of the neural network, the first neuron is comprised in a first layer of the plurality of layers, and each of the plurality of neurons is mapped in a distributed manner across a plurality of the processor elements of the fabric.
43. The system of claim 39, wherein the storage is comprised in a memory local to the compute engine.
44. The system of claim 39, wherein each compute engine further comprises storage for gradient accumulation, forward partial sums, delta partial sums, and forward pass activations.
45. A system comprising: in each of a fabric of processor elements, means for selectively communicating fabric packets with others of the processor elements, each processor element comprising a fabric router and a compute engine enabled to perform dataflow-based and instruction-based processing; in each compute engine, means for selectively performing the processing in accordance with a virtual channel specifier and a task specifier of at least some of the fabric packets the compute engine receives; wherein the instruction-based processing is in accordance with the task specifier; wherein each compute engine is configured to perform a predefined set of basic operations in response to receiving a corresponding basic instruction selected from a predefined native instruction set of codes; further comprising a training workload comprising a first set of machine codes selected from the native instruction set for performing a mapping of at least a part of a neuron onto the compute engine of the processor element, the mapping comprising managing at least one partial-neuron weight, a second set of machine codes selected from the native instruction set for performing a forward pass to propagate activations in a forward logical direction based at least in part on the at least one partial-neuron weight, the forward pass initiated responsive to an input sample, a third set of machine codes selected from the native instruction set for performing a delta pass in a backward logical direction to generate deltas, the delta pass initiated responsive to completion of the forward pass, a fourth set of machine codes selected from the native instruction set for performing a chain pass to calculate gradients based on the deltas, and a fifth set of machine codes selected from the native instruction set for performing a selective update of the at least one partial-neuron weight in accordance with a predetermined learning rule and based at least in part on the deltas; wherein each compute engine comprises storage for the at least one partial-neuron weight; further comprising means for performing an iteration of the training workload for each of a plurality of input samples collectively comprising a training set; further comprising, for each input sample, means for selectively updating the at least one partial-neuron weight in accordance with the predetermined learning rule responsive to completion of the forward pass, the delta pass, and the chain pass corresponding to the input sample; further comprising means for selectively using weight information provided by the most recent selective update of the at least one partial-neuron weight for each forward pass; and further comprising, in each compute element, means for selectively performing portions of a delta pass and portions of a chain pass for an input sample based at least in part on activations that are recomputed based at least in part on a first partial-neuron weight.
46. The system of claim 45, wherein the mapping is in accordance with initializing the fabric to implement a partitioning of a neural network into a plurality of layers, the neuron is a first neuron of a plurality of neurons of the neural network, the first neuron is comprised in a first layer of the plurality of layers, and each of the plurality of neurons is mapped in a distributed manner across a plurality of the processor elements of the fabric.
47. The system of claim 45, wherein the storage is comprised in a memory local to the compute engine.
48. The system of claim 45, wherein each compute engine further comprises storage for gradient accumulation, forward partial sums, delta partial sums, and forward pass activations.
</claims>
</document>
