<document>

<filing_date>
2015-09-11
</filing_date>

<publication_date>
2020-03-24
</publication_date>

<priority_date>
2015-09-11
</priority_date>

<ipc_classes>
H04N11/02,H04N11/04,H04N19/146,H04N19/176,H04N19/30,H04N19/436,H04N19/467,H04N19/51,H04N19/90,H04N7/12
</ipc_classes>

<assignee>
FACEBOOK
</assignee>

<inventors>
PUNTAMBEKAR, AMIT
PARSONS-KEIR, WURZEL DAVID
COWARD, MICHAEL HAMILTON
</inventors>

<docdb_family_id>
58257819
</docdb_family_id>

<title>
Ultra-high video compression
</title>

<abstract>
Various of the disclosed embodiments relate to multiple video encoders that are used to simultaneously encode a video using encoders configured using different encoding parameters. A segment selector selects an encoded version of the encoded video segment using operational criteria such as video quality and bandwidth. A configuration determination module may analyze the video segment to make a decision about which encoding parameter configurations may be suitable for encoding the video segment. The configuration determination module may be trainable, based on past encoding results.
</abstract>

<claims>
1. A computer-implemented method comprising: dividing, by a computing system, a video into multiple segments for encoding; deriving, by the computing system, for each segment, a corresponding encoding fingerprint, wherein the deriving comprises analyzing a ratio of bits used in the video for encoding motion based blocks and bits used in the video for encoding non-motion based blocks; training, by the computing system, a machine learning model to determine a plurality of encoder settings for each segment based on the corresponding encoding fingerprint, wherein the number of encoder settings for each segment is a configurable parameter; encoding, by the computing system, at least some segments of the video concurrently, wherein each segment is encoded multiple times using the plurality of encoder settings for each segment to obtain a plurality of encoded segments for each segment; selecting, by the computing system, among the plurality of encoded segments for each segment, a smallest sized encoded segment for each segment that meets a pre-determined quality requirement; and concatenating, by the computing system, the smallest sized encoded segments for all segments into a single bitstream representing an encoded representation of the video.
2. The computer-implemented method of claim 1, wherein the encoding is performed using distributed resources in which at least some of the encoding is performed by different encoders.
3. The computer-implemented method of claim 1, wherein the deriving comprises analyzing a ratio of bit utilization by different frame types in the video, wherein the frame types include an intra-encoded frame type and an inter-encoded frame type.
4. The computer-implemented method of claim 1, wherein the training comprises searching a training database of encoder settings for a best match with the encoding fingerprint of each segment.
5. The computer-implemented method of claim 4, wherein the training database of encoder settings is produced by encoding multiple training videos using multiple encoder settings and collecting quality measurements of resulting videos.
6. The computer-implemented method of claim 1, wherein the concatenating comprises: assigning, by the computing system, to multiple worker nodes, all segments of the video for encoding along with encoding settings to be used; receiving, by the computing system, from the multiple worker nodes, a resulting bitstream for each segment; and combining together, by the computing system, the received resulting bitstreams to produce the encoded version of the video.
7. The computer-implemented method of claim 1, wherein all segments of the video are concurrently encoded by different video encoders.
8. A non-transitory computer-readable storage medium including instructions that, when executed by at least one processor of a computing system, cause the computing system to perform a method comprising: dividing a video into multiple segments for encoding; deriving for each segment, a corresponding encoding fingerprint, wherein the deriving comprises analyzing a ratio of bits used in the video for encoding motion based blocks and bits used in the video for encoding non-motion based blocks; training a machine learning model to determine a plurality of encoder settings for each segment based on the corresponding encoding fingerprint, wherein the number of encoder settings for each segment is a configurable parameter; encoding at least some segments of the video concurrently, wherein each segment is encoded multiple times using the plurality of encoder settings for each segment to obtain a plurality of encoded segments for each segment; selecting among the plurality of encoded segments for each segment, a smallest sized encoded segment for each segment that meets a pre-determined quality requirement; and concatenating the smallest sized encoded segments for all segments into a single bitstream representing an encoded representation of the video.
9. The non-transitory computer-readable storage medium of claim 8, wherein the encoding is performed using distributed resources in which at least some of the encoding is performed by different encoders.
10. The non-transitory computer-readable storage medium of claim 8, wherein the deriving comprises analyzing a ratio of bit utilization by different frame types in the video, wherein the frame types include an intra-encoded frame type and an inter-encoded frame type.
11. The non-transitory computer-readable storage medium of claim 8, wherein the training comprises searching a training database of encoder settings for a best match with the encoding fingerprint of each segment.
12. The non-transitory computer-readable storage medium of claim 8, wherein the concatenating comprises: assigning, to multiple worker nodes, all segments of the video for encoding along with encoding settings to be used; receiving, from the multiple worker nodes, a resulting bitstream for each segment; and combining together the received resulting bitstreams to produce the encoded version of the video.
13. A system, comprising: at least one processor; and at least one memory storing instructions that, when executed by the at least one processor, cause the system to perform: dividing a video into multiple segments for encoding; deriving for each segment, a corresponding encoding fingerprint, wherein the deriving comprises analyzing a ratio of bits used in the video for encoding motion based blocks and bits used in the video for encoding non-motion based blocks; training a machine learning model to determine a plurality of encoder settings for each segment based on the corresponding encoding fingerprint, wherein the number of encoder settings for each segment is a configurable parameter; encoding at least some segments of the video concurrently, wherein each segment is encoded multiple times using the plurality of encoder settings for each segment to obtain a plurality of encoded segments for each segment; selecting among the plurality of encoded segments for each segment, a smallest sized encoded segment for each segment that meets a pre-determined quality requirement; and concatenating the smallest sized encoded segments for all segments into a single bitstream representing an encoded representation of the video.
14. The system of claim 13, wherein the encoding is performed using distributed resources in which at least some of the encoding is performed by different encoders.
15. The system of claim 13, wherein the deriving comprises analyzing a ratio of bit utilization by different frame types in the video, wherein the frame types include an intra-encoded frame type and an inter-encoded frame type.
16. The system of claim 13, wherein the training comprises searching a training database of encoder settings for a best match with the encoding fingerprint of each segment.
17. The system of claim 13, wherein the concatenating comprises: assigning, to multiple worker nodes, all segments of the video for encoding along with encoding settings to be used; receiving, from the multiple worker nodes, a resulting bitstream for each segment; and combining together the received resulting bitstreams to produce the encoded version of the video.
18. The non-transitory computer-readable storage medium of claim 11, wherein the training database of encoder settings is produced by encoding multiple training videos using multiple encoder settings and collecting quality measurements of resulting videos.
19. The non-transitory computer-readable storage medium of claim 8, wherein all segments of the video are concurrently encoded by different video encoders.
20. The system of claim 16, wherein the training database of encoder settings is produced by encoding multiple training videos using multiple encoder settings and collecting quality measurements of resulting videos.
</claims>
</document>
