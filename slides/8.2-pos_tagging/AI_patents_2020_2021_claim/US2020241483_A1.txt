<document>

<filing_date>
2020-04-14
</filing_date>

<publication_date>
2020-07-30
</publication_date>

<priority_date>
2017-10-31
</priority_date>

<ipc_classes>
G05B13/02,G06K9/62,G06N20/10,G06N20/20,G06N3/04,G06N3/08,G06N7/00
</ipc_classes>

<assignee>
GUANGDONG OPPO MOBILE TELECOMMUNICATIONS CORPORATION
</assignee>

<inventors>
LIANG, KUN
</inventors>

<docdb_family_id>
61681681
</docdb_family_id>

<title>
Method and Device for Managing and Controlling Application, Medium, and Electronic Device
</title>

<abstract>
A method and device for managing and controlling an application, a medium, and an electronic device are provided. The method includes the following. Historical feature information xi is obtained. A first training model is generated based on a back propagation (BP) neural network algorithm. A second training model is generated based on a non-linear support vector machine algorithm. Upon detecting that the application is switched to background, current feature information s associated with the application is taken into the first training model and the second training model for calculation. Whether the application needs to be closed is determined.
</abstract>

<claims>
1. A method for managing and controlling an application, the method being applicable to an electronic device and comprising: obtaining a sample vector set associated with the application, the sample vector set containing a plurality of sample vectors, and each of the plurality of sample vectors comprising multi-dimensional historical feature information xi associated with the application; generating a first training model by performing calculation on the sample vector set based on a back propagation (BP) neural network algorithm, and generating a second training model based on a non-linear support vector machine algorithm; obtaining first closing probability by taking current feature information s associated with the application into the first training model for calculation upon detecting that the application is switched to background; obtaining second closing probability by taking the current feature information s associated with the application into the second training model for calculation when the first closing probability is within a hesitation interval; and closing the application when the second closing probability is greater than a predetermined value.
2. The method of claim 1, wherein generating the first training model by performing calculation on the sample vector set based on the BP neural network algorithm comprises: defining a network structure; and obtaining the first training model by taking the sample vector set into the network structure for calculation.
3. The method of claim 2, wherein defining the network structure comprises: setting an input layer, wherein the input layer comprises N nodes, and the number of nodes of the input layer is the same as the number of dimensions of the historical feature information xi; setting a hidden layer, wherein the hidden layer comprises M nodes; setting a classification layer, wherein the classification layer is based on a softmax function, wherein the softmax function is: wherein p is predicted probability, Zk is a median value, C is the number of predicted result categories, and eZj is a jth median value; setting an output layer, wherein the output layer comprises two nodes; setting an activation function, wherein the activation function is based on a sigmoid function, wherein the sigmoid function is: wherein f(x) has a range of 0 to 1; setting a batch size, wherein the batch size is A; and setting a learning rate, wherein the learning rate is B.
4. The method of claim 3, wherein obtaining the first training model by taking the sample vector set into the network structure for calculation comprises: obtaining an output value of the input layer by inputting the sample vector set into the input layer for calculation; obtaining an output value of the hidden layer by inputting the output value of the input layer into the hidden layer; obtaining predicted probability [p1 p2]T by inputting the output value of the hidden layer into the classification layer for calculation, wherein p1 represents predicted closing probability and p2 represents predicted retention probability; obtaining a predicted result y by inputting the predicted probability into the output layer for calculation, wherein y=[1 0]T when p1 is greater than p2, and y=[0 1]T when p1 is smaller than or equal to p2; and obtaining the first training model by modifying the network structure according to the predicted result y.
5. The method of claim 1, wherein generating the second training model based on the non-linear support vector machine algorithm comprises: for each of the sample vectors of the sample vector set, generating a labeling result yi for the sample vector by labeling the sample vector; and obtaining the second training model by defining a Gaussian kernel function.
6. The method of claim 5, wherein obtaining the second training model by defining the Gaussian kernel function comprises: defining the Gaussian kernel function; and obtaining the second training model by defining a model function and a classification decision function according to the Gaussian kernel function, wherein the model function is: and the classification decision function is: wherein f(x) is a classification decision value, ai is a Lagrange factor, and b is a bias coefficient.
7. The method of claim 5, wherein obtaining the second training model by defining the Gaussian kernel function comprises: defining the Gaussian kernel function; defining a model function and a classification decision function according to the Gaussian kernel function, wherein the model function is: and the classification decision function is: wherein f(x) is a classification decision value, ai is a Lagrange factor, and b is a bias coefficient; defining an objective optimization function according to the model function and the classification decision function; and obtaining the second training model by obtaining an optimal solution of the objective optimization function according to a sequential minimal optimization algorithm, wherein the objective optimization function is: wherein the objective optimization function is used to obtain a minimum value for parameters (a1, a2, . . . , ai), ai, corresponds to a training sample (xi, yi,), and the total number of variables is equal to capacity m of the training samples.
8. The method of claim 1, further comprising: retaining the application when the second closing probability is smaller than the predetermined value.
9. The method of claim 1, further comprising: determining whether the first closing probability is smaller than a minimum value of the hesitation interval or greater than a maximum value of the hesitation interval, when the first closing probability is beyond the hesitation interval; retaining the application, upon determining that the first closing probability is smaller than the minimum value of the hesitation interval; and closing the application, upon determining that the first closing probability is greater than the maximum value of the hesitation interval.
10. The method of claim 1, wherein obtaining the first closing probability and the second closing probability comprises: collecting the current feature information s associated with the application; upon detecting that the application is switched to the background, obtaining probability [p1′ p2′]T by taking the current feature information s into the first training model for calculation, and setting p1′ to be the first closing probability; determining whether the first closing probability is within the hesitation interval; and when the first closing probability is within the hesitation interval, obtaining the second closing probability by taking the current feature information s associated with the application into the second training model for calculation.
11. A non-transitory computer-readable storage medium, configured to store instructions which, when executed by a processor, cause the processor to carry out actions, comprising: obtaining a sample vector set associated with an application, the sample vector set containing a plurality of sample vectors, and each of the plurality of sample vectors comprising multi-dimensional historical feature information associated with the application; generating a first training model by performing calculation on the sample vector set based on a back propagation (BP) neural network algorithm, and generating a second training model based on a non-linear support vector machine algorithm; obtaining first closing probability by taking current feature information s associated with the application into the first training model for calculation upon detecting that the application is switched to background; obtaining second closing probability by taking the current feature information s associated with the application into the second training model for calculation when the first closing probability is within a hesitation interval; and closing the application when the second closing probability is greater than a predetermined value.
12. An electronic device, comprising: at least one processor; and a computer readable storage, coupled to the at least one processor and storing at least one computer executable instruction thereon which, when executed by the at least one processor, is operable with the at least one processor to: obtain a sample vector set associated with an application, the sample vector set containing a plurality of sample vectors, and each of the plurality of sample vectors comprising multi-dimensional historical feature information xi associated with the application; generate a first training model by performing calculation on the sample vector set based on a back propagation (BP) neural network algorithm, and generate a second training model based on a non-linear support vector machine algorithm; obtain first closing probability by taking current feature information s associated with the application into the first training model for calculation upon detecting that the application is switched to background; obtain second closing probability by taking the current feature information s associated with the application into the second training model for calculation when the first closing probability is within a hesitation interval; and close the application when the second closing probability is greater than a predetermined value.
13. The electronic device of claim 12, wherein the at least one computer executable instruction operable with the at least one processor to generate the first training model by performing calculation on the sample vector set based on the BP neural network algorithm is operable with the at least one processor to: define a network structure; and obtain the first training model by taking the sample vector set into the network structure for calculation.
14. The electronic device of claim 13, wherein the at least one computer executable instruction operable with the at least one processor to define the network structure is operable with the at least one processor to: set an input layer, wherein the input layer comprises N nodes, and the number of nodes of the input layer is the same as the number of dimensions of the historical feature information xi; set a hidden layer, wherein the hidden layer comprises M nodes; set a classification layer, wherein the classification layer is based on a softmax function, wherein the softmax function is: wherein p is predicted probability, Zk is a median value, C is the number of predicted result categories, and eZj is a jth median value; set an output layer, wherein the output layer comprises two nodes; set an activation function, wherein the activation function is based on a sigmoid function, wherein the sigmoid function is: wherein f(x) has a range of 0 to 1; set a batch size, wherein the batch size is A; and set a learning rate, wherein the learning rate is B.
15. The electronic device of claim 14, wherein the at least one computer executable instruction operable with the at least one processor to obtain the first training model by taking the sample vector set into the network structure for calculation is operable with the at least one processor to: obtain an output value of the input layer by inputting the sample vector set into the input layer for calculation; obtain an output value of the hidden layer by inputting the output value of the input layer into the hidden layer; obtain predicted probability [p1 p2]T by inputting the output value of the hidden layer into the classification layer for calculation, wherein p1 represents predicted closing probability and p2 represents predicted retention probability; obtain a predicted result y by inputting the predicted probability into the output layer for calculation, wherein y=[1 0]T when p1 is greater than p2, and y=[0 1]T when p1 is smaller than or equal to p2; and obtain the first training model by modifying the network structure according to the predicted result y.
16. The electronic device of claim 12, wherein the at least one computer executable instruction operable with the at least one processor to generate the second training model based on the non-linear support vector machine algorithm is operable with the at least one processor to: for each of the sample vectors of the sample vector set, generate a labeling result yi for the sample vector by labeling the sample vector; and obtain the second training model by defining a Gaussian kernel function.
17. The electronic device of claim 16, wherein the at least one computer executable instruction operable with the at least one processor to obtain the second training model by defining the Gaussian kernel function is operable with the at least one processor to: define the Gaussian kernel function; and obtain the second training model by defining a model function and a classification decision function according to the Gaussian kernel function, wherein the model function is: and the classification decision function is: wherein f(x) is a classification decision value, ai is a Lagrange factor, and b is a bias coefficient.
18. The electronic device of claim 12, wherein the at least one computer executable instruction is further operable with the processor to: retain the application when the second closing probability is smaller than the predetermined value.
19. The electronic device of claim 12, wherein the at least one computer executable instruction is further operable with the processor to: determine whether the first closing probability is smaller than a minimum value of the hesitation interval or greater than a maximum value of the hesitation interval, when the first closing probability is beyond the hesitation interval; retain the application, upon determining that the first closing probability is smaller than the minimum value of the hesitation interval; and close the application, upon determining that the first closing probability is greater than the maximum value of the hesitation interval.
20. The electronic device of claim 12, wherein the at least one computer executable instruction operable with the at least one processor to obtain the first closing probability and the second closing probability is operable with the at least one processor to: collect the current feature information s associated with the application; upon detecting that the application is switched to the background, obtain probability [p1′ p2′]T by taking the current feature information s into the first training model for calculation, and set p1′ to be the first closing probability; determine whether the first closing probability is within the hesitation interval; and when the first closing probability is within the hesitation interval, obtain the second closing probability by taking the current feature information s associated with the application into the second training model for calculation.
</claims>
</document>
