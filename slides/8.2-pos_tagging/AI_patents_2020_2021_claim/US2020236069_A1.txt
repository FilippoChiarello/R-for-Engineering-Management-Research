<document>

<filing_date>
2020-03-27
</filing_date>

<publication_date>
2020-07-23
</publication_date>

<priority_date>
2017-11-02
</priority_date>

<ipc_classes>
G06Q10/10,G10L15/18,G10L15/22,H04L12/18,H04L12/58,H04M3/493,H04M3/527
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
LAMECKI, JAN
BEHZADI, BEHSHAD
NOWAK-PRZYGODZKI, MARCIN
</inventors>

<docdb_family_id>
66244543
</docdb_family_id>

<title>
AUTOMATED ASSISTANTS WITH CONFERENCE CAPABILITIES
</title>

<abstract>
Techniques are described related to enabling automated assistants to enter into a "conference mode" in which they can "participate" in meetings between multiple human participants and perform various functions described herein. In various implementations, an automated assistant implemented at least in part on conference computing device(s) may be set to a conference mode in which the automated assistant performs speech-to-text processing on multiple distinct spoken utterances, provided by multiple meeting participants, without requiring explicit invocation prior to each utterance. The automated assistant may perform semantic processing on first text generated from the speech-to-text processing of one or more of the spoken utterances, and generate, based on the semantic processing, data that is pertinent to the first text. The data may be output to the participants at conference computing device(s). The automated assistant may later determine that the meeting has concluded, and may be set to a non-conference mode.
</abstract>

<claims>
1. A method implemented by one or more processors, comprising: setting an automated assistant implemented at least in part on one or more conference computing devices to a conference mode in which the automated assistant performs speech-to-text processing on multiple distinct spoken utterances exchanged during a conversation between multiple participants, without requiring explicit invocation of the automated assistant prior to each of the multiple distinct spoken utterances; automatically performing, by the automated assistant, semantic processing on first text generated from the speech-to-text processing of one or more of the multiple distinct spoken utterances, wherein the semantic processing is performed without explicit participant invocation; generating, by the automated assistant, based on the semantic processing, data that is pertinent to the first text; monitoring, by the automated assistant, the conversation for a pause; in response to detecting, based on the monitoring, the pause in the conversation, audibly outputting the data that is pertinent to the first text to the multiple participants at one or more of the conference computing devices while the automated assistant is in conference mode; in response to determining that one or more criteria is met before the pause is detected, refraining from audibly outputting the data that is pertinent to the first text.
2. The method of claim 1, further comprising, in response to the determining that the one or more criteria was met before the pause was detected, visually outputting the data that is pertinent to the first text on a display that is accessible to at least one of the multiple participants.
3. The method of claim 1, wherein the one or more criteria comprise passage of a predetermined time interval since the one or more of the multiple distinct spoken utterances.
4. The method of claim 1, wherein the one or more criteria comprise detecting a new topic of the conversation.
5. The method of claim 1, wherein the one or more criteria comprise detecting a change in context of the conversation.
6. The method of claim 1, wherein the refraining includes discarding the data that is pertinent to the first text.
7. The method of claim 1, further comprising, in response to determining that the one or more criteria is met before the pause is detected, outputting the data after a conclusion of the conversation.
8. A system comprising one or more processors and memory storing instructions that, in response to execution of the instructions by the one or more processors, cause the one or more processors to: set an automated assistant implemented at least in part on one or more conference computing devices to a conference mode in which the automated assistant performs speech-to-text processing on multiple distinct spoken utterances exchanged during a conversation between multiple participants, without requiring explicit invocation of the automated assistant prior to each of the multiple distinct spoken utterances; automatically perform, by the automated assistant, semantic processing on first text generated from the speech-to-text processing of one or more of the multiple distinct spoken utterances, wherein the semantic processing is performed without explicit participant invocation; generate, by the automated assistant, based on the semantic processing, data that is pertinent to the first text; monitor, by the automated assistant, the conversation for a pause; in response to detection of the pause in the conversation, audibly outputting the data that is pertinent to the first text to the multiple participants at one or more of the conference computing devices while the automated assistant is in conference mode; in response to a determination that one or more criteria is met before the pause is detected, refrain from audibly outputting the data that is pertinent to the first text.
9. The system of claim 8, further comprising, in response to the determination that the one or more criteria was met before the pause was detected, visually output the data that is pertinent to the first text on a display that is accessible to at least one of the multiple participants.
10. The system of claim 8, wherein the one or more criteria comprise passage of a predetermined time interval since the one or more of the multiple distinct spoken utterances.
11. The system of claim 8, wherein the one or more criteria comprise detecting a new topic of the conversation.
12. The system of claim 8, wherein the one or more criteria comprise detecting a change in context of the conversation.
13. The system of claim 8, wherein the refraining includes discarding the data that is pertinent to the first text.
14. The system of claim 8, further comprising instructions to, in response to the determination that the one or more criteria is met before the pause is detected, output the data after a conclusion of the conversation.
15. At least one non-transitory computer-readable medium comprising instructions that, in response to execution of the instructions by one or more processors, cause the one or more processors to perform the following operations: setting an automated assistant implemented at least in part on one or more conference computing devices to a conference mode in which the automated assistant performs speech-to-text processing on multiple distinct spoken utterances exchanged during a conversation between multiple participants, without requiring explicit invocation of the automated assistant prior to each of the multiple distinct spoken utterances; automatically performing, by the automated assistant, semantic processing on first text generated from the speech-to-text processing of one or more of the multiple distinct spoken utterances, wherein the semantic processing is performed without explicit participant invocation; generating, by the automated assistant, based on the semantic processing, data that is pertinent to the first text; monitoring, by the automated assistant, the conversation for a pause; in response to detecting, based on the monitoring, the pause in the conversation, audibly outputting the data that is pertinent to the first text to the multiple participants at one or more of the conference computing devices while the automated assistant is in conference mode; in response to determining that one or more criteria is met before the pause is detected, refraining from audibly outputting the data that is pertinent to the first text.
16. The at least one non-transitory computer-readable medium of claim 15, further comprising, in response to the determining that the one or more criteria was met before the pause was detected, visually outputting the data that is pertinent to the first text on a display that is accessible to at least one of the multiple participants.
17. The at least one non-transitory computer-readable medium of claim 15, wherein the one or more criteria comprise passage of a predetermined time interval since the one or more of the multiple distinct spoken utterances.
18. The at least one non-transitory computer-readable medium of claim 15, wherein the one or more criteria comprise detecting a new topic of the conversation.
19. The at least one non-transitory computer-readable medium of claim 15, wherein the one or more criteria comprise detecting a change in context of the conversation.
20. The at least one non-transitory computer-readable medium of claim 15, wherein the refraining includes discarding the data that is pertinent to the first text.
</claims>
</document>
