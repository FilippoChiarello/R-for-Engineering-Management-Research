<document>

<filing_date>
2019-11-13
</filing_date>

<publication_date>
2020-03-12
</publication_date>

<priority_date>
2018-01-11
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,G10L13/033,G10L13/047,G10L13/10
</ipc_classes>

<assignee>
NEOSAPIENCE
</assignee>

<inventors>
LEE, YOUNGGUN
KIM, TAESU
</inventors>

<docdb_family_id>
67511992
</docdb_family_id>

<title>
MULTILINGUAL TEXT-TO-SPEECH SYNTHESIS
</title>

<abstract>
A multilingual text-to-speech synthesis method and system are disclosed. The method includes receiving first learning data including a learning text of a first language and learning speech data of the first language corresponding to the learning text of the first language, receiving second learning data including a learning text of a second language and learning speech data of the second language corresponding to the learning text of the second language, and generating a single artificial neural network text-to-speech synthesis model by learning similarity information between phonemes of the first language and phonemes of the second language based on the first learning data and the second learning data.
</abstract>

<claims>
1. A multilingual text-to-speech synthesis method comprising: receiving first learning data including a learning text of a first language and learning speech data of the first language corresponding to the learning text of the first language; receiving second learning data including a learning text of a second language and learning speech data of the second language corresponding to the learning text of the second language; and generating a single artificial neural network text-to-speech synthesis model by learning similarity information between phonemes of the first language and phonemes of the second language based on the first learning data and the second learning data.
2. The multilingual text-to-speech synthesis method of claim 1, further comprising: receiving an articulatory feature of a speaker regarding the first language; receiving an input text of the second language; and generating output speech data for the input text of the second language that simulates the speaker's speech by inputting the input text of the second language and the articulatory feature of the speaker regarding the first language to the single artificial neural network text-to-speech synthesis model.
3. The multilingual text-to-speech synthesis method of claim 2, wherein the articulatory feature of the speaker regarding the first language is generated by extracting a feature vector from speech data that the speaker utters in the first language.
4. The multilingual text-to-speech synthesis method of claim 2, further comprising: receiving an emotion feature; and generating output speech data for the input text of the second language that simulates the speaker's speech by inputting the input text of the second language, the articulatory feature of the speaker regarding the first language, and the emotion feature to the single artificial neural network text-to-speech synthesis model.
5. The multilingual text-to-speech synthesis method of claim 2, further comprising: receiving a prosody feature; and generating output speech data for the input text of the second language that simulates the speaker's speech by inputting the input text of the second language, the articulatory feature of the speaker regarding the first language, and the prosody feature to the single artificial neural network text-to-speech synthesis model.
6. The multilingual text-to-speech synthesis method of claim 5, wherein the prosody feature includes at least one of information on utterance speed, information on accentuation, information on voice pitch, and information on pause duration.
7. The multilingual text-to-speech synthesis method of claim 1, further comprising: receiving an input speech of the first language; extracting a feature vector from the input speech of the first language to generate an articulatory feature of a speaker regarding the first language; converting the input speech of the first language into an input text of the first language; converting the input text of the first language into an input text of the second language; and generating output speech data of the second language for the input text of the second language that simulates the speaker's speech by inputting the input text of the second language and the articulatory feature of the speaker regarding the first language to the single artificial neural network text-to-speech synthesis model.
8. The multilingual text-to-speech synthesis method of claim 1, wherein the learning text of the first language and the learning text of the second language are converted into a phoneme sequence using a grapheme-to-phoneme (G2P) algorithm.
9. The multilingual text-to-speech synthesis method of claim 1, wherein the single artificial neural network text-to-speech synthesis model is generated without an input of similarity information on at least one of pronunciations and notations between phonemes of the first language and phonemes of the second language.
10. A computer-recordable storage medium having recorded thereon a program comprising instructions for performing each step according to the multilingual text-to-speech synthesis method of claim 1.
</claims>
</document>
