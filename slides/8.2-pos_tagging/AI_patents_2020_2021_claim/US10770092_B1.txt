<document>

<filing_date>
2017-09-22
</filing_date>

<publication_date>
2020-09-08
</publication_date>

<priority_date>
2017-09-22
</priority_date>

<ipc_classes>
G10L13/08,G10L21/00,G10L21/06,G10L21/10,G10L21/18
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
</assignee>

<inventors>
ADAMS, ZOE
DELLER, DERICK
KLEIN, PETE
RANGANATH, ANIRUDH
RICHARDS, BRADLEY MICHAEL
</inventors>

<docdb_family_id>
72290156
</docdb_family_id>

<title>
Viseme data generation
</title>

<abstract>
Systems and methods for viseme data generation are disclosed. Uncompressed audio data is generated and/or utilized to determine the beats per minute of the audio data. Visemes are associated with the audio data utilizing a Viterbi algorithm and the beats per minute. A time-stamped list of viseme data is generated that associates the visemes with the portions of the audio data that they correspond to. An animatronic toy and/or an animation is caused to lip sync using the viseme data while audio corresponding to the audio data is output.
</abstract>

<claims>
1. A system comprising: one or more processors; and computer-readable media storing computer-executable instructions that, when executed by the one or more processors, cause the one or more processors to perform operations comprising: generating, in an uncompressed format, audio data corresponding to a song, the audio data representing frequencies and amplitudes of the song; analyzing the audio data to determine a number of beats per minute associated with the audio data; determining, using a Viterbi algorithm and the beats per minute, a portion of the audio data representing human sound; determining viseme data to associate with the audio data, the viseme data determined from an amplitude of the amplitudes; generating lip-sync data including the viseme data and the audio data; and sending the lip-sync data to a first device, the lip-sync data causing the first device to output the audio data and causing a second device to present the viseme data while the audio data is output.
2. The system of claim 1, wherein determining the viseme data to associate with the audio data comprises: determining that a first amplitude associated with a first portion of the audio data is within a first amplitude range; determining that a second amplitude associated with a second portion of the audio data is within a second amplitude range, the first amplitude range being greater than the second amplitude range; associating a first viseme with the first portion of the audio data based on the first amplitude being within the first amplitude range; and associating a second viseme with the second portion of the audio data based on the second amplitude being within the second amplitude range, the second viseme corresponding to a mouth position that is more closed than the first viseme.
3. The system of claim 1, the operations further comprising: identifying contextual information associated with the song, the contextual information including at least one of an artist associated with the song or a genre associated with the song; determining an audio frequency range of speech associated with the contextual information; and wherein determining the portion of the audio data corresponding to the human sound comprises determining the portion of the audio data corresponding to the human sound based at least in part on the audio frequency range.
4. The system of claim 1, wherein a first portion of the audio data corresponds to the song and a second portion of the audio data corresponds to text-to-speech audio data, and the operations further comprising: associating the first portion of the audio data with the second portion of the audio data; causing the second device to output audio corresponding to the text-to-speech audio data; and causing the first device to output audio corresponding to the song.
5. A method, comprising: determining a portion of audio data representing human sound; determining amplitude data corresponding to one or more amplitudes of the portion of the audio data; determining viseme identifiers to associate with the audio data, the viseme identifiers determined based at least in part on the amplitude data; generating viseme data that associates the viseme identifiers and portions of the audio data corresponding to the viseme identifiers; generating at least one command associated with the viseme data, the at least one command configured to, when sent to a first device: cause, by a first processor of the first device, the first device to output the audio data, the first device being a voice-controlled device including a voice user interface; and cause, by a second processor of a second device corresponding to a different device than the first device, the second device to present a visual representation of the viseme identifiers while the audio data is output; and sending the viseme data, the audio data, and the at least one command to the first device.
6. The method of claim 5, wherein the audio data corresponds to a song, and further comprising: identifying contextual information associated with the song, the contextual information including at least one of an artist associated with the song or a genre associated with the song; determining an audio frequency range of human sound associated with the contextual information; and wherein determining the portion of the audio data representing the human sound comprises determining the portion of the audio data representing the human sound based at least in part on the audio frequency range.
7. The method of claim 5, further comprising: analyzing the audio data to determine a number of beats per minute associated with the audio data; and determining, based at least in part on a Viterbi algorithm and the number of beats per minute, the portion of the audio data representing the human sound.
8. The method of claim 5, wherein the second device is an animatronic device, and sending the viseme data and the at least one command causes a face portion of the animatronic device to physically move mouth positions based at least in part on the viseme data.
9. The method of claim 5, further comprising: identifying a portion of the audio data corresponding to a frequency that is outside of a range of frequencies corresponding to human sound; and wherein determining the portion of the audio data representing the human sound comprises determining the portion of the audio data representing the human sound based at least in part on excluding the portion outside of the range of frequencies.
10. The method of claim 5, further comprising: determining that a first amplitude associated with a first portion of the audio data is within a first amplitude range; determining that a second amplitude associated with a second portion of the audio data is within a second amplitude range, the first amplitude range being greater than the second amplitude range; associating a first viseme identifier of the viseme identifiers with the first portion of the audio data based at least in part on the first amplitude being within the first amplitude range; and associating a second viseme identifier of the viseme identifiers with the second portion of the audio data based at least in part on the second amplitude being within the second amplitude range, the second viseme identifier corresponding to a mouth position that is more closed than the first viseme identifier.
11. The method of claim 5, wherein the audio data comprises speech-based first audio data, and further comprising: receiving, from the first device, second audio data representing a request for the first device to output a speech-based response; identifying a word corresponding to the speech-based response; determining at least one of the viseme identifiers to associate with the word; causing the first device to output audio corresponding to the speech-based response; and causing the second device to present a visual representation of the at least one of the viseme identifiers during output of the audio.
12. The method of claim 5, further comprising: generating text data based at least in part on automatic speech recognition performed on the portion of the audio data corresponding to the human sound, the text data indicating words corresponding to the human sound; and wherein determining the viseme identifiers to associate with the audio data comprises determining the viseme identifiers to associate with the audio data based at least in part on the text data.
13. The method of claim 5, further comprising analyzing, based at least in part on a Viterbi algorithm, the audio data at least until the portion of the audio data representing the human sound is determined at a threshold confidence level.
14. The method of claim 5, wherein determining the portion of the audio data representing the human sound comprises determining the portion of the audio data representing the human sound based at least in part on a Viterbi algorithm, and further comprising: receiving feedback data indicating an accuracy of the viseme data; and generating a modified Viterbi algorithm based at least in part on the feedback data and the Viterbi algorithm.
15. A system, comprising: one or more processors; and computer-readable media storing computer-executable instructions that, when executed by the one or more processors, cause the one or more processors to perform operations comprising: receiving, at a first device and from a remote system, audio data; receiving, at the first device and from the remote system, viseme data indicating visemes associated with the audio data; receiving, at the first device and from the remote system, at least one command associated with the viseme data, the at least one command configured to cause a second device to present a visual representation of the viseme data while audio representing the audio data is output; determining that the second device is in communication with the first device; and causing the second device, via a first processor of the second device, to present a visual representation of the viseme data while a second processor of the first device causes the first device to output audio representing the audio data.
16. The system of claim 15, wherein the second device comprises an animatronic device that includes a mouth portion, and causing the second device to present the visual representation of the viseme data comprises causing the mouth portion to present the viseme data.
17. The system of claim 15, wherein the second device includes a display, and causing the second device to present the visual representation of the viseme data comprises causing animation of mouth movements corresponding to the visemes on the display.
18. The system of claim 15, wherein the viseme data includes timing data indicating which portions of the audio data are associated with the viseme data.
19. The system of claim 15, the operations further comprising: determining an identity of a user in proximity to the first device; identifying a user profile associated with the user; determining that the user profile indicates that the user is hearing impaired; and wherein causing the second device to present the visual representation of the viseme data comprises causing the second device to present the visual representation of the viseme data based at least in part on the user profile indicating that the user is hearing impaired.
20. The system of claim 15, wherein the audio data corresponds to a song, and the operations further comprising: receiving beat data indicating a number of beats per minute associated with the song; and causing presentation of a visual representation of the song based at least in part on the number of beats per minute.
</claims>
</document>
