<document>

<filing_date>
2018-05-31
</filing_date>

<publication_date>
2020-05-13
</publication_date>

<priority_date>
2017-12-15
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,G10L15/06,G10L15/16
</ipc_classes>

<assignee>
MITSUBISHI ELECTRIC CORPORATION
</assignee>

<inventors>
HERSHEY, JOHN
HORI, TAKAAKI
LE ROUX, JONATHAN
SEKI, HIROSHI
WATANABE, SHINJI
</inventors>

<docdb_family_id>
62842174
</docdb_family_id>

<title>
METHOD AND SYSTEM FOR TRAINING A MULTI-LANGUAGE SPEECH RECOGNITION NETWORK
</title>

<abstract>
A method for training a multi-language speech recognition network includes providing utterance datasets corresponding to predetermined languages, inserting language identification (ID) labels into the utterance datasets, wherein each of the utterance datasets is labelled by each of the language ID labels, concatenating the labeled utterance datasets, generating initial network parameters from the utterance datasets, selecting the initial network parameters according to a predetermined sequence, and training, iteratively, an end-to-end network with a series of the selected initial network parameters and the concatenated labeled utterance datasets until a training result reaches a threshold.
</abstract>

<claims>
1. A method for training a multi-language speech recognition network, comprising: providing utterance datasets corresponding to predetermined languages; inserting language identification, ID, labels into the utterance datasets, wherein each of the utterance datasets is labeled by each of the language ID labels, and wherein a language ID label is inserted into a transcript of each utterance dataset in the speech corpora of different languages (110); concatenating the labeled utterance datasets in a random order, wherein the corresponding transcripts are also concatenated in the same order as the concatenated utterance datasets; generating initial network parameters (116) from the utterance datasets, wherein the initial network parameters (116) are generated using a universal label set obtained as a union of grapheme sets and language IDs; and training, iteratively, an end-to-end network with a series of the initial network parameters (116) and the concatenated labeled utterance datasets until a training result reaches a threshold, wherein the training an end-to-end network comprises optimizing the initial network parameters (116) using the concatenated utterance datasets and the transcripts.
2. The method of claim 1, wherein each of the utterance datasets includes a pair of an acoustic dataset (110) and ground truth labels corresponding to the acoustic dataset (110).
3. The method of claim 1, wherein the end-to-end network is a language independent model.
4. The method of claim 3, wherein the language independent model uses a deep BLSTM encoder network.
5. The method of claim 4, wherein a number of layers in the deep BLSTM encoder network is 7 or more than 7.
6. The method of claim 1, wherein the ID labels are arranged to the utterance datasets according to an arrangement rule,
7. The method of claim 6, wherein the arrangement rule causes each of the ID labels to be added to a head position of each of the utterance datasets.
8. The method of claim 1, further comprising generating trained network parameters (118) when the training result has reached the threshold.
9. The method of claim 1, wherein the end-to-end network jointly optimizes the series of the initial network parameters (116) and the concatenated labeled utterance datasets based on a predetermined method.
10. A multi-lingual speech recognition system for generating trained network parameters (118) for a multi-language speech recognition, comprising: one or more processors; and one or more storage devices storing parameters and program modules including instructions executable by the one or more processors which, when executed, cause the one or more processors to perform operations comprising: providing utterance datasets corresponding to predetermined languages; inserting language identification, ID, labels into the utterance datasets, wherein each of the utterance datasets is labeled by each of the language ID labels and wherein a language ID label is inserted into a transcript of each utterance dataset in the speech corpora of different languages (110); concatenating the labeled utterance datasets in a random order, wherein the corresponding transcripts are also concatenated in the same order as the concatenated utterance datasets; generating initial network parameters (116) from the utterance datasets, wherein the initial network parameters (116) are generated using a universal label set obtained as a union of grapheme sets and language IDs; selecting the initial network parameters (116) according to a predetermined sequence; and training, iteratively, an end-to-end network with a series of the selected initial network parameters (116) and the concatenated labeled utterance datasets until a training result reaches a threshold, wherein the training an end-to-end network comprises optimizing the initial network parameters (116) using the concatenated utterance datasets and the transcripts.
11. The system of claim 10, wherein each of the utterance datasets includes a pair of an acoustic dataset (110) and ground truth labels corresponding to the acoustic dataset (110).
12. The system of claim 10, wherein the end-to-end network is a language independent model.
13. The system of claim 12, wherein the language independent model uses a deep BLSTM encoder network.
14. The system of claim 13, wherein a number of layers in the deep BLSTM encoder network is 7 or more than 7,
15. The system of claim 10, wherein the ID labels are arranged to the utterance datasets according to an arrangement rule.
16. The system of claim 15, wherein the arrangement rule causes each of the ID labels to be added to a head position of each of the utterance datasets.
17. The system of claim 10, wherein the instructions executable by the one or more processors, when executed, further cause the one or more processors to perform the operation
generating trained network parameters (118) when the training result has reached the threshold.
</claims>
</document>
