<document>

<filing_date>
2020-04-23
</filing_date>

<publication_date>
2020-10-29
</publication_date>

<priority_date>
2019-04-23
</priority_date>

<ipc_classes>
G06F3/01,G06F3/0481,G06N3/00,G06T19/00
</ipc_classes>

<assignee>
APPLE
</assignee>

<inventors>
MEIER, PETER
MORGAN, Bo
DUNN, Cameron J.
RICHTER, Ian M.
RUSSELL, John Christopher
DRUMMOND, Mark
SIVAPURAPU, Siva Chandra Mouli
</inventors>

<docdb_family_id>
70680673
</docdb_family_id>

<title>
TRAINING A MODEL WITH HUMAN-INTUITIVE INPUTS
</title>

<abstract>
In one implementation, a method of generating environment states is performed by a device including one or more processors and non-transitory memory. The method includes displaying a computer-generated reality (CGR) environment including an asset associated with a neural network model and having a plurality of asset states. The method includes receiving a user input indicative of a training request. The method includes selecting, based on the user input, a training focus indicating one or more of the plurality of asset states. The method includes generating a set of training data including a plurality of training instances weighted according to the training focus. The method includes training the neural network model on the set of training data.
</abstract>

<claims>
1. A method comprising:
at an electronic device including a processor and non-transitory memory:
displaying a computer-generated reality (CGR) environment including an asset associated with a model and having a plurality of asset states;
receiving a user input indicative of a training request;
selecting, based on the user input, a training focus indicating one or more of the plurality of asset states;
generating a set of training data including a plurality of training instances weighted according to the training focus; and
training the model on the set of training data.
2. The method of claim 1, wherein the user input includes speech.
3. The method of claim 2, wherein selecting the training focus includes:
converting the speech to a text representation of the speech;
parsing the text representation of the speech with a natural language parsing algorithm to identify one or more of the plurality of asset states; and
selecting the training focus based on the identified one or more of the plurality of asset states.
4. The method of any of claims 1-3, wherein the user input indicates a video.
5. The method of claim 4, wherein selecting the training focus includes:
performing video analysis on the video to identify one or more of the plurality of asset states; and
selecting the training focus based on the identified one or more of the plurality of asset states.
6. The method of any of claims 1-5, wherein selecting the training focus includes:
determining a plurality of candidate training focuses, each indicating a different set of one or more of the plurality of asset states; and
selecting one of the plurality of candidate training focuses as the training focus.
7. The method of claim 6, wherein at least one of the plurality of candidate training focuses indicates a single one of the plurality of asset states.
8. The method of claims 6 or 7, wherein at least one of the plurality of candidate training focuses indicates a function of two or more of the plurality of asset states.
9. The method of any of claims 6-8, wherein selecting one of the plurality of candidate training focuses as the training focus includes:
ranking the plurality of candidate training focuses; and
selecting one of the candidate training focuses as the training focus based on the ranking.
10. The method of claim 9, wherein ranking the plurality of candidate training focuses is based on asset state recency.
11. The method of claims 9 or 10, wherein ranking the plurality of candidate training focuses is based on the user input.
12. The method of any of claims 1-11, wherein selecting the training focus includes: selecting a potential training focus indicating one or more of the plurality of asset states; and
presenting a natural language confirmation of the potential training focus.
13. The method of claim 12, wherein selecting the training focus further includes receiving a user input confirming the potential training focus and selecting the potential training focus as the training focus.
14. The method of claim 12, wherein selecting the training focus further includes receiving a user input modifying the potential training focus and selecting the modified potential training focus as the training focus.
15. The method of claim 12, wherein selecting the training focus further includes receiving a user input negating the potential training focus and selecting a different potential training focus as the training focus.
16. The method of any of claims 1-15, wherein the model includes a neural network model.
17. A device comprising:
one or more processors;
a nontransitory memory; and
one or more programs stored in the non-transitory memory, which, when executed by the one or more processors, cause the device to perform any of the methods of claims 1-17.
18. A non-transitory memory storing one or more programs, which, when executed by one or more processors of a device, cause the device to perform any of the methods of claims 1-17.
19. A device comprising:
one or more processors;
a non-transitory memory; and means for causing the device to perform any of the methods of claims 1-17.
20. A device comprising:
a nontransitory memory; and
one or more processors to:
display a computer-generated reality (CGR) environment including an asset associated with a model and having a plurality of asset states;
receive a user input indicative of a training request;
select, based on the user input, a training focus indicating one or more of the plurality of asset states;
generate a set of training data including a plurality of training instances weighted according to the training focus; and
train the model on the set of training data.
</claims>
</document>
