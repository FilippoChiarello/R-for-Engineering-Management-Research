<document>

<filing_date>
2019-09-17
</filing_date>

<publication_date>
2020-01-09
</publication_date>

<priority_date>
2017-03-23
</priority_date>

<ipc_classes>
G01C21/34,G06N3/08
</ipc_classes>

<assignee>
BEIJING DIDI INFINITY TECHNOLOGY AND DEVELOPMENT COMPANY
</assignee>

<inventors>
WANG YU
YE, ZHOU
SHAO, DAN
</inventors>

<docdb_family_id>
63584860
</docdb_family_id>

<title>
SYSTEMS AND METHODS FOR ROUTE SEARCHING
</title>

<abstract>
The present disclosure relates to systems and methods for searching for a route. The systems may perform the methods to obtain route information of a first route; encode the route information of the first route into a first code based on a target model; access a target database in at least one storage medium, wherein the target database includes a plurality of candidate codes encoded through the target model from a plurality of candidate routes; identify, from the plurality of candidate codes, a second code based on the first code, the second code being associated with at least one second route; and send information associated with the at least one second route to a receiving device.
</abstract>

<claims>
1. A system for displaying information of a similar route on an interface of a terminal, comprising: at least one storage medium including a set of instructions for searching for a route; at least one processor in communication with the at least one storage medium, wherein when executing the set of instructions, the at least one processor is directed to: receive a service request from a terminal of a service requestor or a service provider, wherein the service request includes information relating to a start location and a destination of a first route; in response to the service request, obtain route information of the first route; encode the route information of the first route into a first code based on a target model, wherein: the target model is configured to encode a route into a code having a predetermined length of n bytes, n being an integer larger than 1, and encode substantially similar routes into a same code; the target model is a recurrent neural network (RNN) model having a hidden layer with at least m neurons; and the RNN model is obtained by a training process, the training process including: setting initial parameters of the RNN model, wherein the initial parameters include a first parameter associated with an input layer of the RNN model and a second parameter associated with the hidden layer of the RNN model: obtaining sample route information of a plurality of sample routes, wherein each of the plurality of sample routes includes a plurality of sample waypoints, and the sample route information includes sample location information of the plurality of sample waypoints; for each of the plurality of sample routes, determining output information associated with the at least m neurons in the hidden layer based on the initial parameters, a predetermined nonlinear activation function, and the sample route information, wherein for a first neuron in the at least m neurons, an input is sample location information of a first sample waypoint in the plurality of sample waypoints and for an ith neuron in the at least m neurons, the input includes sample location information of an ith waypoint in the plurality of waypoints and an output of an (i−1)th neuron in the at least m neurons; and determining trained parameters of the RNN model based on the output information; access a target database in the at least one storage medium, wherein the target database includes a plurality of candidate codes encoded through the target model from a plurality of candidate routes; identify, from the plurality of candidate codes, a second code based on the first code, the second code being associated with at least one second route; and generate on an interface of the terminal a presentation of information associated with the at least one second route.
2. The system of claim 1, wherein the route information of the first route includes location information of m waypoints along the first route, a sequence of the m waypoints corresponding to a direction of the first route, wherein m is an integer larger than 1 and independent from n.
3. (canceled)
4. The system of claim 1, wherein the determining output information associated with the at least m neurons in the hidden layer includes: determining a first 1*n vector corresponding to the output of the (i−1)th neuron; determining a 1*2 vector corresponding to the sample location information of the ith sample waypoint; determining a 1*(n+2) matrix based on the 1*n vector corresponding to the (i−1)th neuron and the 1*2 vector corresponding to the sample location information of the ith sample waypoint; determining a (n+2)*n matrix based on the first parameter and the second parameters, wherein the first parameter is an n*n matrix and the second parameter is a 2*n matrix; determining a 1*n matrix corresponding to the ith neuron based on the 1*(n+2) matrix and the (n+2)*n matrix; and determining a second 1*n vector corresponding to an output of the ith neuron based on the 1*n matrix and the predetermined nonlinear activation function.
5. The system of claim 1, wherein the determining output information associated with the at least m neurons in the hidden layer includes: determining a first n*1 vector corresponding to the output of the (i−1)th neuron; determining a 2*1 vector corresponding to the sample location information of the ith sample waypoint; determining a (n+2)*1 matrix based on the n*1 vector corresponding to the (i−1)th neuron and the 2*1 vector corresponding to the sample location information of the ith sample waypoint; determining an n*(n+2) matrix based on the first parameter and the second parameters, wherein the first parameter is an n*n matrix and the second parameter is an n*2 matrix; determining a n*1 matrix corresponding to the ith neuron based on the 1*(n+2) matrix and the (n+2)*n matrix; and determining a second n*1 vector corresponding to an output of the ith neuron based on the n*1 matrix and the predetermined nonlinear activation function.
6. The system of claim 1, wherein the determining trained parameters of the target model based on the output information includes: determining the trained parameters based on a backpropagation algorithm or a gradient descent algorithm.
7. The system of claim 1, wherein the predetermined nonlinear activation function includes at least one of a sigmoid function, a hyperbolic tangent (tanh) function, or a Rectified Linear Units (ReLu) function.
8. The system of claim 1, wherein to encode the route information of the first route into the first code based on the target model, the at least one processor is further directed to: determine an n-dimensional vector based on the target model; and generate the first code by encoding the n-dimensional vector based on an encoding function.
9. The system of claim 8, wherein the encoding function is a function below: where x refers to a value of an element in the n-dimensional vector, and h(x) refers to a code value corresponding to the element in the first code.
10. The system of claim 1, wherein the target model includes a long short-term memory network (LSTM) model.
11. A method implemented on a computing device having at least one processor, at least one storage medium, and a communication platform connected to a network, the method comprising: receiving a service request from a terminal of a service requestor or a service provider, wherein the service request includes information relating to a start location and a destination of a first route; in response to the service request, obtaining route information of the first route; encoding the route information of the first route into a first code based on a target model, wherein: the target model is configured to encode a route into a code having a predetermined length of n bytes, n being an integer larger than 1, and encode substantially similar routes into a same code; the target model is a recurrent neural network (RNN) model having a hidden layer with at least m neurons; and the RNN model is obtained by a training process, the training process including: setting initial parameters of the RNN model, wherein the initial parameters include a first parameter associated with an input layer of the RNN model and a second parameter associated with the hidden layer of the RNN model; obtaining sample route information of a plurality of sample routes, wherein each of the plurality of sample routes includes a plurality of sample waypoints, and the sample route information includes sample location information of the plurality of sample waypoints; for each of the plurality of sample routes, determining output information associated with the at least m neurons in the hidden layer based on the initial parameters, a predetermined nonlinear activation function, and the sample route information, wherein for a first neuron in the at least m neurons, an input is sample location information of a first sample waypoint in the plurality of sample waypoints and for an ith neuron in the at least m neurons, the input includes sample location information of an ith waypoint in the plurality of waypoints and an output of an (i−1)th neuron in the at least m neurons; and determining trained parameters of the RNN model based on the output information; accessing a target database in the at least one storage medium, wherein the target database includes a plurality of candidate codes encoded through the target model from a plurality of candidate routes; identifying from the plurality of candidate codes, a second code based on the first code, the second code being associated with at least one second route; and generating on an interface of the terminal a presentation of information associated with the at least one second route.
12. The method of claim 11, wherein the route information of the first route includes location information of m waypoints along the first route, a sequence of the m waypoints corresponding to a direction of the first route, wherein m is an integer larger than 1 and independent from n.
13. (canceled)
14. The method of claim 11, wherein the determining output information associated with the at least m neurons in the hidden layer includes: determining a first 1*n vector corresponding to the output of the (i−1)th neuron; determining a 1*2 vector corresponding to the sample location information of the ith sample waypoint; determining a 1*(n+2) matrix based on the 1*n vector corresponding to the (i−1)th neuron and the 1*2 vector corresponding to the sample location information of the ith sample waypoint; determining a (n+2)*n matrix based on the first parameter and the second parameters, wherein the first parameter is an n*n matrix and the second parameter is a 2*n matrix; determining a 1*n matrix corresponding to the ith neuron based on the 1*(n+2) matrix and the (n+2)*n matrix; and determining a second 1*n vector corresponding to an output of the ith neuron based on the 1*n matrix and the predetermined nonlinear activation function.
15. The method of claim 11, wherein the determining output information associated with the at least m neurons: determining a first n*1 vector corresponding to the output of the (i−1)th neuron; determining a 2*1 vector corresponding to the sample location information of the ith sample waypoint; determining a (n+2)*1 matrix based on the n*1 vector corresponding to the (i−1)th neuron and the 2*1 vector corresponding to the sample location information of the ith sample waypoint; determining an n*(n+2) matrix based on the first parameter and the second parameters, wherein the first parameter is an n*n matrix and the second parameter is an n*2 matrix; determining a n*1 matrix corresponding to the ith neuron based on the 1*(n+2) matrix and the (n+2)*n matrix; and determining a second n*1 vector corresponding to an output of the ith neuron based on the n*1 matrix and the predetermined nonlinear activation function.
16. The method of claim 11, wherein the determining trained parameters of the target model based on the output information includes: determining the trained parameters based on a backpropagation algorithm or a gradient descent algorithm.
17. The method of claim 11, wherein the predetermined nonlinear activation function includes at least one of a sigmoid function, a hyperbolic tangent (tanh) function, or a Rectified Linear Units (ReLu) function.
18. The method of claim 11, wherein the encoding of the route information of the first route into the first code based on the target model includes: determine an n-dimensional vector based on the target model; and generate the first code by encoding the n-dimensional vector based on an encoding function.
19. The method of claim 18, wherein the encoding function is a function below: where x refers to a value of an element in the n-dimensional vector, and h(x) refers to a code value corresponding to the element in the first code.
20. The method of claim 11, wherein the target model includes a long short-term memory network (LSTM) model.
21. A non-transitory computer readable medium comprising a set of instructions for searching for a route that, when executed by at least one processor, cause the at least one processor to effectuate a method comprising: receiving a service request from a terminal of a service requestor or a service provider, wherein the service request includes information relating to a start location and a destination of a first route; in response to the service request, obtaining route information of the first route; encoding the route information of the first route into a first code based on a target model, wherein: the target model is configured to encode a route into a code having a predetermined length of n bytes, n being an integer larger than 1, and encode substantially similar routes into a same code; the target model is a recurrent neural network (RNN) model having a hidden layer with at least m neurons; and the RNN model is obtained by a training process, the training process including: setting initial parameters of the RNN model, wherein the initial parameters include a first parameter associated with an input layer of the RNN model and a second parameter associated with the hidden layer of the RNN model; obtaining sample route information of a plurality of sample routes, wherein each of the plurality of sample routes includes a plurality of sample waypoints, and the sample route information includes sample location information of the plurality of sample waypoints; for each of the plurality of sample routes, determining output information associated with the at least m neurons in the hidden layer based on the initial parameters, a predetermined nonlinear activation function, and the sample route information, wherein for a first neuron in the at least m neurons, an input is sample location information of a first sample waypoint in the plurality of sample waypoints and for an ith neuron in the at least m neurons, the input includes sample location information of an ith waypoint in the plurality of waypoints and an output of an (i−1)th neuron in the at least m neurons; and determining trained parameters of the RNN model based on the output information; accessing a target database in the at least one storage medium, wherein the target database includes a plurality of candidate codes encoded through the target model from a plurality of candidate routes; identifying from the plurality of candidate codes, a second code based on the first code, the second code being associated with at least one second route; and generating on an interface of the terminal a presentation of information associated with the at least one second route.
22. The non-transitory computer readable medium of claim 21, wherein the determining output information associated with the at least m neurons in the hidden layer includes: determining a first 1*n vector corresponding to the output of the (i−1)th neuron; determining a 1*2 vector corresponding to the sample location information of the ith sample waypoint; determining a 1*(n+2) matrix based on the 1*n vector corresponding to the (i−1)th neuron and the 1*2 vector corresponding to the sample location information of the ith sample waypoint; determining a (n+2)*n matrix based on the first parameter and the second parameters, wherein the first parameter is an n*n matrix and the second parameter is a 2*n matrix; determining a 1*n matrix corresponding to the ith neuron based on the 1*(n+2) matrix and the (n+2)*n matrix; and determining a second 1*n vector corresponding to an output of the ith neuron based on the 1*n matrix and the predetermined nonlinear activation function.
</claims>
</document>
