<document>

<filing_date>
2020-03-10
</filing_date>

<publication_date>
2021-01-28
</publication_date>

<priority_date>
2019-07-22
</priority_date>

<ipc_classes>
G06F40/279,G06F40/30,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
ADVANCED NEW TECHNOLOGIES COMPANY
</assignee>

<inventors>
HUANG JING
Sun, Mengshu
Lin, Xiexiong
Wang, Taifeng
</inventors>

<docdb_family_id>
74187431
</docdb_family_id>

<title>
GENERATING RECOMMENDATION INFORMATION
</title>

<abstract>
Implementations of this disclosure provide methods and apparatuses for generating recommendation information. An example method includes matching text content from a text content library based on a plurality of predetermined scenario-related words; extracting keywords from the related text content, to generate a plurality of training samples; and for each training sample, providing a source sequence of a sequence pair corresponding to the training sample as an input to a recommendation information generation model, obtaining, from the recommendation information generation model, a predicted word, and adjusting a model parameter of the recommendation information generation model based on a comparison between the predicted word and a corresponding word in a target sequence of the sequence pair corresponding to the training sample, to train the recommendation information generation model.
</abstract>

<claims>
1. A computer-implemented method comprising: matching text content from a text content library based on a plurality of predetermined scenario-related words to provide related text content; extracting keywords from the related text content, to generate a plurality of training samples, wherein each training sample of the plurality of training samples corresponds to a respective sequence pair, each sequence pair comprising a corresponding source sequence and a corresponding target sequence, the source sequence being a sequence of keywords extracted from the related text content, and the target sequence being a word sequence obtained after word segmentation processing is performed on the related text content; and for each training sample of the plurality of training samples, providing the source sequence of the sequence pair corresponding to the training sample as an input to a recommendation information generation model, wherein the recommendation information generation model is an encoder-decoder network, obtaining, from the recommendation information generation model, a predicted word, wherein the recommendation information generation model adjusts a probability distribution of words in a word list using pointer distribution to increase a probability that keywords in the source sequence and input to the recommendation information generation model will appear in a prediction result, and adjusting a model parameter of the recommendation information generation model based on a comparison between the predicted word of the recommendation information generation model and a corresponding word in the target sequence of the sequence pair corresponding to the training sample, to train the recommendation information generation model.
2. The computer-implemented method of claim 1, wherein matching text content from the text content library based on the plurality of predetermined scenario-related words comprises: determining a quantity of scenario-related words in text content from the text content library; and in response to the quantity of scenario-related words in the text content being greater than a predetermined quantity of scenario-related words, identifying the text content as the related text content.
3. The computer-implemented method of claim 1, wherein matching text content from the text content library based on the plurality of predetermined scenario-related words comprises: extracting keywords from text content in the text content library; determining a word matching degree between the keywords extracted from the text content and the plurality of predetermined scenario-related words; and in response to the word matching degree being greater than a matching degree threshold, identifying the text content as the related text content.
4. The computer-implemented method of claim 1, further comprising: after identifying the related text content, inputting the related text content to a pre-trained classification model, to determine at least one text content category of the related text content; and screening out text content for which the at least one text content category does not include a predetermined category.
5. The computer-implemented method of claim 1, wherein the recommendation information generation model comprises an encoder-decoder network comprising an encoder neural network and a decoder neural network, and adjusting the model parameter of the recommendation information generation model comprises: transmitting, to a neuron of the decoder neural network, a received source sequence and an output result obtained by the encoder neural network based on the source sequence, and sequentially inputting, to the decoder neural network, a target sequence obtained after receiving a sequence start identifier, wherein the predicted word is predicted by the decoder neural network and the corresponding word in the target sequence is obtained after receiving the sequence start identifier; and wherein the model parameter of the recommendation information generation model is a parameter of the encoder-decoder network.
6. The computer-implemented method of claim 1, wherein a pointer distribution weight is positively correlated with weighted values of a state of a neuron at a current moment, an output of the neuron at the current moment, and an input word at the current moment, and weights applied to each word in a sequence are model parameters; and using pointer distribution to adjust the probability distribution of words in the word list comprises: using the pointer distribution weight as a weight coefficient of a word distribution probability of one of the words in the word list, and using a difference between 1 and the pointer distribution weight as a weight coefficient of a word distribution probability of the input word received by an encoder neural network of the recommendation information generation model at the current moment.
7. The computer-implemented method of claim 6, wherein the weight coefficient in the pointer distribution weight is adjusted based on a loss function to which a penalty of similarity between a current attention vector and a sum of historical attention vectors is added, the loss function being positively correlated with a similarity between the current attention vector and the sum of historical attention vectors.
8. A non-transitory, computer-readable medium storing one or more instructions executable by a computer system to perform operations comprising: matching text content from a text content library based on a plurality of predetermined scenario-related words to provide related text content; extracting keywords from the related text content, to generate a plurality of training samples, wherein each training sample of the plurality of training samples corresponds to a respective sequence pair, each sequence pair comprising a corresponding source sequence and a corresponding target sequence, the source sequence being a sequence of keywords extracted from the related text content, and the target sequence being a word sequence obtained after word segmentation processing is performed on the related text content; and for each training sample of the plurality of training samples, providing the source sequence of the sequence pair corresponding to the training sample as an input to a recommendation information generation model, wherein the recommendation information generation model is an encoder-decoder network, obtaining, from the recommendation information generation model, a predicted word, wherein the recommendation information generation model adjusts a probability distribution of words in a word list using pointer distribution to increase a probability that keywords in the source sequence and input to the recommendation information generation model will appear in a prediction result, and adjusting a model parameter of the recommendation information generation model based on a comparison between the predicted word of the recommendation information generation model and a corresponding word in the target sequence of the sequence pair corresponding to the training sample, to train the recommendation information generation model.
9. The computer-readable medium of claim 8, wherein matching text content from the text content library based on the plurality of predetermined scenario-related words comprises: determining a quantity of scenario-related words in text content from the text content library; and in response to the quantity of scenario-related words in the text content being greater than a predetermined quantity of scenario-related words, identifying the text content as the related text content.
10. The computer-readable medium of claim 8, wherein matching text content from the text content library based on the plurality of predetermined scenario-related words comprises: extracting keywords from text content in the text content library; determining a word matching degree between the keywords extracted from the text content and the plurality of predetermined scenario-related words; and in response to the word matching degree being greater than a matching degree threshold, identifying the text content as the related text content.
11. The computer-readable medium of claim 8, the operations further comprising: after identifying the related text content, inputting the related text content to a pre-trained classification model, to determine at least one text content category of the related text content; and screening out text content for which the at least one text content category does not include a predetermined category.
12. The computer-readable medium of claim 8, wherein the recommendation information generation model comprises an encoder-decoder network comprising an encoder neural network and a decoder neural network, and adjusting the model parameter of the recommendation information generation model comprises: transmitting, to a neuron of the decoder neural network, a received source sequence and an output result obtained by the encoder neural network based on the source sequence, and sequentially inputting, to the decoder neural network, a target sequence obtained after receiving a sequence start identifier, wherein the predicted word is predicted by the decoder neural network and the corresponding word in the target sequence is obtained after receiving the sequence start identifier; and wherein the model parameter of the recommendation information generation model is a parameter of the encoder-decoder network.
13. The computer-readable medium of claim 8, wherein a pointer distribution weight is positively correlated with weighted values of a state of a neuron at a current moment, an output of the neuron at the current moment, and an input word at the current moment, and weights applied to each word in a sequence are model parameters; and using pointer distribution to adjust the probability distribution of words in the word list comprises: using the pointer distribution weight as a weight coefficient of a word distribution probability of one of the words in the word list, and using a difference between 1 and the pointer distribution weight as a weight coefficient of a word distribution probability of the input word received by an encoder neural network of the recommendation information generation model at the current moment.
14. The computer-readable medium of claim 13, wherein the weight coefficient in the pointer distribution weight is adjusted based on a loss function to which a penalty of similarity between a current attention vector and a sum of historical attention vectors is added, the loss function being positively correlated with a similarity between the current attention vector and the sum of historical attention vectors.
15. A computer-implemented system, comprising: one or more computers; and one or more computer memory devices interoperably coupled with the one or more computers and having tangible, non-transitory, machine-readable media storing one or more instructions that, when executed by the one or more computers, perform one or more operations comprising: matching text content from a text content library based on a plurality of predetermined scenario-related words to provide related text content; extracting keywords from the related text content, to generate a plurality of training samples, wherein each training sample of the plurality of training samples corresponds to a respective sequence pair, each sequence pair comprising a corresponding source sequence and a corresponding target sequence, the source sequence being a sequence of keywords extracted from the related text content, and the target sequence being a word sequence obtained after word segmentation processing is performed on the related text content; and for each training sample of the plurality of training samples, providing the source sequence of the sequence pair corresponding to the training sample as an input to a recommendation information generation model, wherein the recommendation information generation model is an encoder-decoder network, obtaining, from the recommendation information generation model, a predicted word, wherein the recommendation information generation model adjusts a probability distribution of words in a word list using pointer distribution to increase a probability that keywords in the source sequence and input to the recommendation information generation model will appear in a prediction result, and adjusting a model parameter of the recommendation information generation model based on a comparison between the predicted word of the recommendation information generation model and a corresponding word in the target sequence of the sequence pair corresponding to the training sample, to train the recommendation information generation model.
16. The computer-implemented system of claim 15, wherein matching text content from the text content library based on the plurality of predetermined scenario-related words comprises: determining a quantity of scenario-related words in text content from the text content library; and in response to the quantity of scenario-related words in the text content being greater than a predetermined quantity of scenario-related words, identifying the text content as the related text content.
17. The computer-implemented system of claim 15, wherein matching text content from the text content library based on the plurality of predetermined scenario-related words comprises: extracting keywords from text content in the text content library; determining a word matching degree between the keywords extracted from the text content and the plurality of predetermined scenario-related words; and in response to the word matching degree being greater than a matching degree threshold, identifying the text content as the related text content.
18. The computer-implemented system of claim 15, the operations further comprising: after identifying the related text content, inputting the related text content to a pre-trained classification model, to determine at least one text content category of the related text content; and screening out text content for which the at least one text content category does not include a predetermined category.
19. The computer-implemented system of claim 15, wherein the recommendation information generation model comprises an encoder-decoder network comprising an encoder neural network and a decoder neural network, and adjusting the model parameter of the recommendation information generation model comprises: transmitting, to a neuron of the decoder neural network, a received source sequence and an output result obtained by the encoder neural network based on the source sequence, and sequentially inputting, to the decoder neural network, a target sequence obtained after receiving a sequence start identifier, wherein the predicted word is predicted by the decoder neural network and the corresponding word in the target sequence is obtained after receiving the sequence start identifier; and wherein the model parameter of the recommendation information generation model is a parameter of the encoder-decoder network.
20. The computer-implemented system of claim 15, wherein a pointer distribution weight is positively correlated with weighted values of a state of a neuron at a current moment, an output of the neuron at the current moment, and an input word at the current moment, and weights applied to each word in a sequence are model parameters; and using pointer distribution to adjust the probability distribution of words in the word list comprises: using the pointer distribution weight as a weight coefficient of a word distribution probability of one of the words in the word list, and using a difference between 1 and the pointer distribution weight as a weight coefficient of a word distribution probability of the input word received by an encoder neural network of the recommendation information generation model at the current moment.
21. The computer-implemented system of claim 20, wherein the weight coefficient in the pointer distribution weight is adjusted based on a loss function to which a penalty of similarity between a current attention vector and a sum of historical attention vectors is added, the loss function being positively correlated with a similarity between the current attention vector and the sum of historical attention vectors.
</claims>
</document>
