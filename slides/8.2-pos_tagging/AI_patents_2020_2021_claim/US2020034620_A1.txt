<document>

<filing_date>
2017-08-05
</filing_date>

<publication_date>
2020-01-30
</publication_date>

<priority_date>
2016-08-05
</priority_date>

<ipc_classes>
B64C39/02,G01C11/10,G01C11/28,G05D1/10,G06K9/00,G06K9/62,G06T7/73
</ipc_classes>

<assignee>
NEUROBOTICS
</assignee>

<inventors>
LUTTERODT, CYRIL
</inventors>

<docdb_family_id>
59974848
</docdb_family_id>

<title>
SELF-RELIANT AUTONOMOUS MOBILE PLATFORM
</title>

<abstract>
A drone (105) and a method for stitching video data in three dimensions. The method comprises generating video data, localizing and mapping the video data, generating a three-dimensional stitched map, and wirelessly transmitting data for the stitched map. The data is generated using at least one camera (225) mounted on a drone (105), and includes multiple viewpoints of objects in an area. The data, including the multiple viewpoints, is localized and mapped by at least one processor (210) on the drone. The three-dimensional stitched map of the area is generated using the localized and mapped video data. The data for the stitched map is wirelessly transmitted by a transceiver (220) on the drone.
</abstract>

<claims>
1. A method for stitching video data in three dimensions, the method comprising: generating video data including multiple viewpoints of objects in an area using at least one camera mounted on a drone; localizing and mapping, by at least one processor on the drone, the video data including the multiple viewpoints; generating a three-dimensional stitched map of the area using the localized and mapped video data; and wirelessly transmitting, by a transceiver on the drone, data for the stitched map.
2. The method of claim 1, further comprising: identifying one or more of the objects in the generated video data; comparing image data, from the generated video data, of at least one of the one or more identified objects to object image data stored in a database; identifying a type of the one identified object based on the comparing; and including information about the identified type of the one identified object in the stitched map in a location proximate the one identified object.
3. The method of claim 2, wherein: the drone includes a memory configured to store the database on board the drone, and identifying a type of the one identified object comprises identifying the type of the one identified object by searching the memory without using the transceiver.
4. The method of claim 2, wherein: the database is an internet connected database, and comparing the image data of at least one of the identified objects to the object image data stored in the database comprises performing, using the transceiver, a search of the internet connected database to identify the type of the one identified object.
5. The method of claim 1, further comprising compressing, by the at least one processor on the drone, the data for the stitched map, wherein transmitting the data for the stitched map comprises transmitting, by the transceiver, the compressed data to a server for real-time streaming to a client device.
6. The method of claim 1, further comprising receiving path planning data from a server, wherein: generating the video data comprises generating the video data based on received the path planning data, and the drone is one of a plurality of drones generating the video data of the area.
7. The method of claim 6, further comprising: identifying one or more other drones in the plurality of drones and a location of the one or more other drones relative to the drone by comparing image data, from the generated video data, of the one or more other drones to object image data stored in a database; monitoring the location of the one or more other drones relative to the drone; practicing obstacle avoidance of the one or more other drones based on the monitored location; and including and dynamically updating the location of the one or more other drones in the stitched map that is transmitted to the server.
8. A drone comprising: a camera configured to generate video data including multiple viewpoints of objects in an area; at least one processor operably connected to the camera, the at least one processor configured to: localize and map the video data including the multiple viewpoints; and generate a three-dimensional stitched map of the area using the localized and mapped video data; and a transceiver operably connected to the at least one processor, the transceiver configured to wirelessly transmit data for the stitched map.
9. The drone of claim 8, wherein the at least one processor is further configured to: identify one or more of the objects in the generated video data; compare image data, from the generated video data, of at least one of the one or more identified objects to object image data stored in a database; identify a type of the one identified object based on the comparison; and include information about the identified type of the one identified object in the stitched map in a location proximate the one identified object.
10. The drone of claim 9, further comprising: a memory configured to store the database on board the drone, wherein the at least one processor is further configured to identify the type of the one identified object by searching the memory without using the transceiver.
11. The drone of claim 9, wherein: the database is an internet connected database, and the at least one processor is further configured to perform, using the transceiver, a search of the internet connected database to identify the type of the one identified object.
12. The drone of claim 8, wherein: the at least one processor is further configured to compress the data for the stitched map, and the transceiver is further configured to transmit the compressed data to a server for real-time streaming to a client device.
13. The drone of claim 8, wherein: the transceiver is further configured to receiving path planning data from a server; the at least one processor is further configured to generate the video data based on received the path planning data; and the drone is one of a plurality of drones generating the video data of the area.
14. The drone of claim 13, wherein the at least one processor is further configured to: identify one or more other drones in the plurality of drones and a location of the one or more other drones relative to the drone by comparing image data, from the generated video data, of the one or more other drones to object image data stored in a database, monitor the location of the one or more other drones relative to the drone, and control a motor of the drone to practice obstacle avoidance of the one or more other drones based on the monitored location.
15. The drone of claim 14, wherein the at least one processor is further configured to include and dynamically update the location of the one or more other drones in the stitched map that is transmitted to the server.
</claims>
</document>
