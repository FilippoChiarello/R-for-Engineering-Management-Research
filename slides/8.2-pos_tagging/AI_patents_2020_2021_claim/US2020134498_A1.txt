<document>

<filing_date>
2018-10-31
</filing_date>

<publication_date>
2020-04-30
</publication_date>

<priority_date>
2018-10-31
</priority_date>

<ipc_classes>
G06N7/00,G06N99/00
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
OSOGAMI, TAKAYUKI
DASGUPTA, SAKYASINGHA
HARRY PUTRA, RUDY RAYMOND
</inventors>

<docdb_family_id>
70325305
</docdb_family_id>

<title>
DYNAMIC BOLTZMANN MACHINE FOR PREDICTING GENERAL DISTRIBUTIONS OF TIME SERIES DATASETS
</title>

<abstract>
A computer-implemented method includes employing a dynamic Boltzmann machine (DyBM) to solve a maximum likelihood of generalized normal distribution (GND) of time-series datasets. The method further includes acquiring the time-series datasets transmitted from a source node to a destination node of a neural network including a plurality of nodes, learning, by the processor, a time-series generative model based on the GND with eligibility traces, and, performing, by the processor, online updating of internal parameters of the GND based on a gradient update to predict updated times-series datasets generated from non-Gaussian distributions.
</abstract>

<claims>
1. A computer-implemented method executed on a processor for employing a dynamic Boltzmann machine (DyBM) to solve a maximum likelihood of generalized normal distribution (GND) of time-series datasets, the method comprising: acquiring the time-series datasets transmitted from a source node to a destination node of a neural network including a plurality of nodes; learning, by the processor, a time-series generative model based on the GND with eligibility traces; and performing, by the processor, online updating of internal parameters of the GND based on a gradient update to predict updated times-series datasets generated from non-Gaussian distributions.
2. The method of claim 1, further comprising adjusting a direction of the online updates by refreshing guessing values of the internal parameters of the GND after every fixed number of gradient updates.
3. The method of claim 1, wherein the time-series datasets are financial time-series data.
4. The method of claim 1, wherein a conditional probability density is: where x is a series of patterns, t is time, N is a number of layers, T is layers of units, and i, j are units in layers.
5. The method of claim 4, wherein a Gaussian distribution for each j unit of the conditional probability density is: where σ is a variance, μ is an expected value, x is a series of patterns, t is time, and T is layers of units, and i, j are units in layers.
6. The method of claim 5, wherein the expected value of the j-th unit is: where b is a bias, w is a weight, d is a conduction delay, K are column vectors, U is a learning parameter, t is time, and δ is a time point difference.
7. The method of claim 1, wherein a conditional probability density of a generalized dynamic Boltzmann machine (DyBM) is: where λj=(2−2/υjΓ(1/υj)Γ(3/υj))1/2, and where σ is a variance, λ is a decay rate, t is time, T is layers of units, x is a series of patterns, υ is a learning parameter, γ is an update parameter, μ is an expected value, and Γ is the gamma function.
8. The method of claim 7, wherein a log-likelihood function of the generalized DyBM is: where σ is a variance, λ is a decay rate, t is time, x is a series of patterns, υ is a learning parameter, μ is an expected value, and Γ is the gamma function.
9. A non-transitory computer-readable storage medium comprising a computer-readable program executed on a processor for employing a dynamic Boltzmann machine (DyBM) to solve a maximum likelihood of generalized normal distribution (GND) of time-series datasets, wherein the computer-readable program when executed on the processor causes a computer to perform the steps of: acquiring the time-series datasets transmitted from a source node to a destination node of a neural network including a plurality of nodes; learning, by the processor, a time-series generative model based on the GND with eligibility traces; and performing, by the processor, online updating of internal parameters of the GND based on a gradient update to predict updated times-series datasets generated from non-Gaussian distributions.
10. The non-transitory computer-readable storage medium of claim 9, wherein a direction of the online updates is adjusted by refreshing guessing values of the internal parameters of the GND after every fixed number of gradient updates.
11. The non-transitory computer-readable storage medium of claim 9, wherein the time-series datasets are financial time-series data.
12. The non-transitory computer-readable storage medium of claim 9, wherein a conditional probability density is: where x is a series of patterns, t is time, N is a number of layers, T is layers of units, and i, j are units in layers.
13. The non-transitory computer-readable storage medium of claim 12, wherein a Gaussian distribution for each j unit of the conditional probability density is: where σ is a variance, μ is an expected value, x is a series of patterns, t is time, and T is layers of units, and i, j are units in layers.
14. The non-transitory computer-readable storage medium of claim 13, wherein the expected value of the j-th unit is: where b is a bias, w is a weight, d is a conduction delay, K are column vectors, U is a learning parameter, t is time, and δ is a time point difference.
15. The non-transitory computer-readable storage medium of claim 9, wherein a conditional probability density of a generalized dynamic Boltzmann machine (DyBM) is: where λj=(2−2/υjΓ(1/υj)Γ(3/υj))1/2, and where σ is a variance, λ is a decay rate, t is time, T is layers of units, x is a series of patterns, υ is a learning parameter, γ is an update parameter, μ is an expected value, and Γ is the gamma function.
16. The non-transitory computer-readable storage medium of claim 15, wherein a log-likelihood function of the generalized DyBM is: where σ is a variance, λ is a decay rate, t is time, x is a series of patterns, υ is a learning parameter, μ is an expected value, and Γ is the gamma function.
17. A system for employing a dynamic Boltzmann machine (DyBM) to solve a maximum likelihood of generalized normal distribution (GND) of time-series datasets, the system comprising: a memory; and one or more processors in communication with the memory configured to: acquire the time-series datasets transmitted from a source node to a destination node of a neural network including a plurality of nodes; learn, by the processor, a time-series generative model based on the GND with eligibility traces; and perform, by the processor, online updating of internal parameters of the GND based on a gradient update to predict updated times-series datasets generated from non-Gaussian distributions.
18. The system of claim 17, wherein a direction of the online updates are adjusted by refreshing guessing values of the internal parameters of the GND after every fixed number of gradient updates.
19. The system of claim 17, wherein the time-series datasets are financial time-series data.
20. The system of claim 17, wherein a conditional probability density of a generalized dynamic Boltzmann machine (DyBM) is: where λj=(2−2/υjΓ(1/υj)Γ(3/υj))1/2, and where σ is a variance, λ is a decay rate, t is time, T is layers of units, x is a series of patterns, υ is a learning parameter, γ is an update parameter, μ is an expected value, and Γ is the gamma function.
</claims>
</document>
