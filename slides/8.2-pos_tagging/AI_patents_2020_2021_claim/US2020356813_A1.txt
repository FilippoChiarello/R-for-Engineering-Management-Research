<document>

<filing_date>
2020-05-21
</filing_date>

<publication_date>
2020-11-12
</publication_date>

<priority_date>
2016-10-05
</priority_date>

<ipc_classes>
G06K9/62,G06Q30/06
</ipc_classes>

<assignee>
DIGIMARC CORPORATION
</assignee>

<inventors>
CONWELL, WILLIAM, Y.
SHARMA, RAVI, K.
FILLER, TOMAS
SEDIGHIANARAKI, VAHID
DESHMUKH, UTKARSH
</inventors>

<docdb_family_id>
70775011
</docdb_family_id>

<title>
IMAGE PROCESSING ARRANGEMENTS
</title>

<abstract>
Aspects of the detailed technologies concern training and use of neural networks for fine-grained classification of large numbers of items, e.g., as may be encountered in a supermarket. Mitigating false positive errors is an exemplary area of emphasis. Novel network topologies are also detailedâ€”some employing recognition technologies in addition to neural networks. A great number of other features and arrangements are also detailed.
</abstract>

<claims>
1. 1-16. (canceled)
17. A neural network comprising: one or more input stages for receiving image data; one or more intermediate stages, cascaded following said one or more input stages, each of said intermediate stages having an input coupled to an output of a previous stage; and plural concluding stages, each having an input coupled to an output of an intermediate stage; wherein a first of said plural concluding stages has N output neurons, and a second of said plural concluding stages has M output neurons.
18. The neural network of claim 17 in which M is less than N.
19. The neural network of claim 17 in which the first concluding stage comprises plural layers.
20. The neural network of claim 17 in which a first neuron in a first of said concluding stages, and a first neuron in a second of said concluding stages, are each configured to fire when an image depicting a first particular grocery item is presented to the one or more input stages.
21. The neural network of claim 20 wherein the particular grocery item is identified not by a single neuron in said first and second concluding stages that fires with a greatest output signal, but rather by a combination of (a) said first neuron in the first concluding stage, which fires with an output signal greater than all other neurons in the first concluding stage, together with (b) said first neuron in the second concluding stage, which fires with an output signal greater than all other neurons in the second concluding stage.
22. The neural network of claim 20 in which a second neuron in the first concluding stage, and a second neuron in the second concluding stage, are each configured to fire when an image depicting a second particular grocery item is presented to the one or more input stages, wherein the first and second particular grocery items are two brandmates of a particular type of product, distinguished as lemon and lime variants, or regular and low-sodium variants, or seeded and unseeded variants, of said particular type of product.
23. The neural network of claim 17 in which at least one of said concluding stages comprises a convolutional stage.
24. A method comprising the acts: providing a neural network including one or more input stages for receiving image data; one or more intermediate stages, cascaded following said one or more input stages, each of said intermediate stages having an input coupled to an output of a previous stage; and plural concluding stages, each having an input coupled to an output of an intermediate stage; wherein a first of said plural concluding stages has N output neurons, and a second of said plural concluding stages has M output neurons, where M is less than N; training the network to respond to images depicting legacy grocery products by firing neurons in the first concluding stage; and training the network to respond to images depicting newer grocery products by firing neurons in the second concluding stage; the method further included retraining the network when items are added to a grocery's offerings, said retraining comprising retraining the second, but not the first, of said concluding stages.
25. A method comprising the acts: providing a neural network including one or more input stages for receiving image data; one or more intermediate stages, cascaded following said one or more input stages, each of said intermediate stages having an input coupled to an output of a previous stage; and plural concluding stages, each having an input coupled to an output of an intermediate stage; wherein a first of said plural concluding stages has N output neurons, and a second of said plural concluding stages has M output neurons, where M is less than N; training the network to respond to images depicting first and second grocery products by firing first and second neurons, respectively, in the first concluding stage; and training the network to respond to images depicting said first and second grocery products by firing first and second neurons, respectively, in the second concluding stage.
26. The method of claim 25 wherein the first and second grocery items are two brandmates of a particular type of product.
27. The method of claim 26 wherein the first and second grocery items are brandmates distinguished as flavor variants or ingredient variants, or regular and low-sodium variants, or seeded and unseeded variants, of said particular type of product.
28. The method of claim 27 in which the first and second grocery items are brandmates distinguished as flavor variants, namely lemon or lime variants.
29. The method of claim 27 in which the first and second grocery items are brandmates distinguished as ingredient variants, namely regular sodium or low sodium variants.
30. The method of claim 27 in which the first and second grocery items are brandmates distinguished as ingredient variants, namely seeded or seedless variants.
31. 31-63. (canceled)
</claims>
</document>
