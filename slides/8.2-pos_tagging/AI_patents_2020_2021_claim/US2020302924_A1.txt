<document>

<filing_date>
2019-02-07
</filing_date>

<publication_date>
2020-09-24
</publication_date>

<priority_date>
2018-11-21
</priority_date>

<ipc_classes>
G06N3/08,G10L15/02,G10L15/22
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
ANDREICA, MUGUREL IONUT
LANGE, JOSEPH
NOWAK-PRZYGODZKI, MARCIN
Stovezky, Sharon
Vuskovic, Vladimir
</inventors>

<docdb_family_id>
65494654
</docdb_family_id>

<title>
ORCHESTRATING EXECUTION OF A SERIES OF ACTIONS REQUESTED TO BE PERFORMED VIA AN AUTOMATED ASSISTANT
</title>

<abstract>
Implementations are set forth herein for creating an order of execution for actions that were requested by a user, via a spoken utterance to an automated assistant. The order of execution for the requested actions can be based on how each requested action can, or is predicted to, affect other requested actions. In some implementations, an order of execution for a series of actions can be determined based on an output of a machine learning model, such as a model that has been trained according to supervised learning. A particular order of execution can be selected to mitigate waste of processing, memory, and network resourcesâ€”at least relative to other possible orders of execution. Using interaction data that characterizes past performances of automated assistants, certain orders of execution can be adapted over time, thereby allowing the automated assistant to learn from past interactions with one or more users.
</abstract>

<claims>
1. A method implemented by one or more processors, the method comprising: receiving audio data that that characterizes a spoken utterance from a user, wherein the spoken utterance includes a request for multiple actions to be performed via an automated assistant and the spoken utterance is received at an automated assistant interface of a computing device; identifying, based on the audio data characterizing the spoken utterance, each action of the multiple actions requested by the user to be performed via the automated assistant, wherein requests for the multiple actions to be performed are set forth in the spoken utterance according to a first order of actions; determining, based on identifying each action of the multiple actions, an execution characteristic of each action of the multiple actions, wherein a particular execution characteristic of an action of the multiple actions affects a temporal aspect of execution of the multiple actions when the multiple actions are executed according to the first order of actions by one or more computing devices, and wherein determining the execution characteristic of each action of the multiple actions includes accessing data that is generated based on past executions of one or more actions of the multiple actions at the computing device and/or a separate computing device; determining, based on the particular execution characteristic of the action of the multiple actions, a second order of actions for executing the multiple actions, wherein the second order of actions, when executed by the one or more computing devices, causes the one or more computing devices to exhibit a different the temporal aspect of execution of the multiple actions; and causing, based on determining the second order of actions, the automated assistant to initialize performance of one or more actions of the multiple actions according to the second order of actions.
2. The method of claim 1, wherein determining the second order of actions includes: processing output data from a trained neural network model, the trained neural network model having been trained using historical interaction data that characterizes at least one or more previous interactions between the user and the automated assistant.
3. The method of claim 2, wherein the historical interaction data further characterizes multiple interactions involving other users that have previously interacted with the automated assistant in furtherance of causing the automated assistant to perform various sequences of actions.
4. The method of claim 2, wherein the historical interaction data further characterizes feedback provided by the user to the automated assistant in order to influence an order of execution of previously requested actions.
5. The method of claim 1, wherein the particular execution characteristic of the action of the multiple actions characterizes the action as a dialog initiating action, and wherein a supplemental dialog session between the user and the automated assistant is to occur for the user to identify a value to be assigned to a parameter of the action.
6. The method of claim 5, wherein the temporal aspect of the execution of the multiple actions, according to the first order of actions, includes at least an estimated time of execution for one or more actions of the multiple actions, and wherein the method further comprises: determining that the supplemental dialog session is predicted to extend the estimated time of execution for the one or more actions when the multiple actions are executed according to the first order of actions.
7. The method of claim 6, wherein another action of the multiple actions includes providing continuous media playback, and wherein the second order of the actions prioritizes the dialog initiating action over the other action that includes providing the continuous media playback.
8. The method of claim 5, wherein causing the automated assistant to initialize performance of the at least one action of the multiple actions according to the second order of actions includes: generating a natural language output that provides the user with an indication that the at least one action of the multiple actions has been initialized according to the second order of actions.
9. A method implemented by one or more processors, the method comprising: processing audio data that that characterizes a spoken utterance from a user requesting that an automated assistant perform multiple actions, wherein the multiple actions are characterized by the user in the spoken utterance according a first order of actions; determining, based on processing the audio data, an action classification for each action of the multiple actions requested by the user, wherein a particular action classification of a particular action of the multiple actions includes a dialog initiating action that is executed according to at least one parameter; determining whether a value for the at least one parameter is specified by the user in the spoken utterance; and when the value for the at least one parameter was unspecified in the spoken utterance: generating a second order of actions for the multiple actions, wherein the second order of actions causes the dialog initiating action to have a reduced priority relative to another action of the multiple actions based on the value for at least one parameter being unspecified in the spoken utterance.
10. The method of claim 9, further comprising: when the at least one parameter is specified in the spoken utterance: generating a third order of actions for the multiple actions, wherein the third order of actions causes the dialog initiating action to have priority that is unaffected by the user specifying the value for at least one parameter in the spoken utterance.
11. The method of claim 10, wherein determining the action classification includes determining, each action of the multiple actions, whether the action corresponds to a continuous playback of media, and the method further comprises: when a requested action of the multiple actions includes the continuous playback of media action: generating the second order of actions or the third order of actions to prioritize the requested action such that the requested action is executed later in time relative to the other action of the multiple actions.
12. The method of claim 11, further comprising: determining whether the user explicitly specified a temporal condition for executing at least one action of the multiple actions; and when the user has explicitly specified the temporal condition for executing the at least one action of the multiple actions: generating the second order of actions or the third order of actions to comply with the temporal condition for executing at least one action of the multiple actions.
13. The method of claim 12, wherein the automated assistant is configured to override the second order of actions or the third order of actions according to the temporal condition when the particular action is the at least one action explicitly requested by the user to be affected by the temporal condition.
14. A method implemented by one or more processors, the method comprising: determining that a user has provided a spoken utterance that includes requests for an automated assistant to perform multiple actions that include a first type of action and a second type of action, wherein the automated assistant is accessible to the user via an automated assistant interface of a computing device; generating, in response to the user providing the spoken utterance, an estimated delay for the first type of action when the second type of action is prioritized over the first type of action during execution of the multiple actions; determining, based on the estimated delay, whether the estimated delay for the first type of action satisfies a threshold, wherein, when the estimated delay for the first type of action satisfies the threshold, execution of the first type of action is prioritized over the second type of action; generating, based on whether the estimated delay satisfies the threshold, a preferred order of execution for the multiple actions requested by the user; and causing the automated assistant to initialize performance of the multiple actions according to the preferred order of execution.
15. The method of claim 14, further comprising: determining an action classification for each action of the multiple actions requested by the user, wherein the automated assistant is configured to prioritize at least one particular classification of actions over at least one other classification of actions.
16. The method of claim 14, wherein the first type of action includes a dialog initiating action and the second type of action includes a media playback action.
17. The method of claim 16, wherein the media playback action is configured to be at least partially performed at a separate computing device, and the method further comprises: when the dialog initiating action is prioritized over the media playback action: causing the dialogue initiating action to be initialized at the computing device simultaneous to causing the separate device to initialize an application for executing the media playback action.
18. The method of claim 17, further comprising: when the media playback action is prioritized over the dialog initiating action: causing the automated assistant to provide a natural language output corresponding to dialogue in furtherance of completing the dialog initiating action, and when the dialogue initiating action is completed: causing the automated assistant to initialize performance of the media playback action at the computing device or the separate computing device.
19. The method of claim 16, wherein the dialog initiating action, when executed, includes initializing a dialog session between the user and the automated assistant in order for the user to identify a value to be assigned to a parameter in furtherance of completing the dialog initiating action.
20. The method of claim 16, wherein the media playback action, when executed, includes initializing playback of media that is accessible via one or more files, and the estimated delay is based on a total of file lengths for the one or more files.
21. 21-24. (canceled)
</claims>
</document>
