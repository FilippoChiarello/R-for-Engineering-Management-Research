<document>

<filing_date>
2017-03-29
</filing_date>

<publication_date>
2020-05-05
</publication_date>

<priority_date>
2017-03-29
</priority_date>

<ipc_classes>
G10L15/14,G10L15/16,G10L15/18,G10L15/22,G10L15/30,G10L25/51
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
</assignee>

<inventors>
POGUE, MICHAEL ALAN
CHOUDHARY, PRACHI
MCDOWELL, SAMUEL KEITH
</inventors>

<docdb_family_id>
70461584
</docdb_family_id>

<title>
Selecting speech inputs
</title>

<abstract>
A speech-processing system can engage in time synchronization between audio capture devices so that if multiple devices detect and process a same utterance, the system may determine which device is closer to a speaker of the utterance by comparing time stamp data from the multiple devices. The devices may synchronize their individual clocks to some time standard that may be disseminated among the devices and/or to a server. When an utterance is captured, the capturing devices may create a time stamp corresponding to when the respective devices detected the utterance. The time stamps may be linked to the synchronized time standard or may be converted to the time standard by another device. The system may then compare the audio data for incoming utterances, and if the same utterance comes from multiple devices, may use the time stamp data to determine which device first detected the utterance. That device may be selected as the device most likely to be closest to the user for various purposes, including sending output data.
</abstract>

<claims>
1. A computer-implemented method comprising: by a first device: determining first clock data corresponding to operation of a processor clock of the first device, receiving second clock data from a second device, the second clock data corresponding to operation of a processor clock of the second device, determining an offset value between the first clock data and the second clock data, sending the offset value to the second device, detecting, at a first processor clock time, first audio corresponding to an utterance, combining the offset value to the first processor clock time to determine a first time stamp, determining first audio data corresponding to the first audio, and sending the first audio data and the first time stamp to a remote device; by the second device: receiving the offset value from the first device, detecting, at a second processor clock time, second audio corresponding to the utterance, combining the offset value to the second processor clock time to determine a second time stamp, determining second audio data corresponding to the second audio, and sending the second audio data and the second time stamp to the remote device; by the remote device: receiving the first audio data and the first time stamp from the first device, receiving the second audio data and the second time stamp from the second device, determining that the first device and the second device are associated with a same user profile; determining that the first audio data is correlated to the second audio data, determining that the first audio data and the second audio data correspond to a same utterance, determining that the first time stamp indicates an earlier time than the second time stamp, and determining that the first device detected the first audio before the second device detected the second audio.
2. The computer-implemented method of claim 1, further comprising, by the remote device: sending, to the second device, a command to close a process associated with the second audio data; performing automatic speech recognition (ASR) on the first audio data to obtain first text data; performing natural language understanding (NLU) processing on the first text data to determine a command; determining output data corresponding to the command; and sending the output data to the first device.
3. The computer-implemented method of claim 1, further comprising, by the remote device: prior to receiving the first time stamp, receiving, from the first device, an indication that the first device has determined the offset value corresponding to the first device and second device.
4. The computer-implemented method of claim 1, further comprising, by the remote device: receiving, from the first device, the offset value; and storing an association between the offset value and the user profile.
5. A system comprising: at least one processor; and memory including instructions operable to be executed by the at least one processor to configure the system to: receive first audio data corresponding to first audio detected by a first device; receive first time data corresponding to a time of detection of the first audio by the first device; receive second audio data corresponding to second audio detected by a second device; receive second time data corresponding to a time of detection of the second audio by the second device; determine an offset value between the first time data and the second time data; determine a correlation between the first audio data and the second audio data; determine that the first device and the second device are associated with a same user profile; determine, based in part on the offset value and the correlation, that the first audio data and the second audio data correspond to a same utterance; and use the first time data and the second time data to determine that the first device detected the utterance prior to the second device detecting the utterance.
6. The system of claim 5, wherein the memory further comprises instructions that further configure the system to: perform speech processing on the first audio data to determine a command; determine output data corresponding to the command; and in response to determining that the first device detected the utterance prior to the second device detecting the utterance, send the output data to the first device.
7. The system of claim 5, wherein the memory further comprises instructions that further configure the system to: send, to the second device, a command to close a process associated with the second audio data.
8. The system of claim 5, wherein the memory further comprises instructions that further configure the system to: receive an indication that time data received from the first device and second device are synchronized; wherein using the first time data and the second time data to determine that the first device detected the utterance prior to the second device comprises comparing the first time data directly to the second time data.
9. The system of claim 5, wherein the memory further comprises instructions that further configure the system to: receive, from the first device, offset data corresponding to a synchronization between clock data of the first device and clock data of the second device; and store an association between the offset data and the user profile.
10. The system of claim 9, wherein the memory further comprises instructions that further configure the system to: combine the offset data with the first time data to determine first offset time data; and combine the offset data with the second time data to determine second offset time data; wherein using the first time data and the second time data to determine that the first device detected the utterance prior to the second device comprises comparing the first offset time data to the second offset time data.
11. A computer-implemented method comprising: receiving first audio data corresponding to first audio detected by a first device; receiving first time data corresponding to a time of detection of the first audio by the first device; receiving second audio data corresponding to second audio detected by a second device; receiving second time data corresponding to a time of detection of the second audio by the second device; determining an offset value between the first time data and the second time data; determining that the first device and the second device are associated with a same user profile; performing speech processing on the first audio data to obtain first text data; performing speech processing on the second audio data to obtain second text data; determining a correlation between the first text data and the second text data; determining, based in part on the offset value and the correlation, that the first audio data and the second audio data correspond to a same utterance; and using the first time data and the second time data to determine that the first device detected the utterance prior to the second device detecting the utterance.
12. The computer-implemented method of claim 11, further comprising: determining output data corresponding to the first text data; and in response to determining that the first device detected the utterance prior to the second device detecting the utterance, sending the output data to the first device.
13. The computer-implemented method of claim 11, further comprising: sending, to the second device, a command to close a process associated with the second audio data.
14. The computer-implemented method of claim 11, further comprising: receiving an indication that time data received from the first device and second device are synchronized; wherein using the first time data and the second time data to determine that the first device detected the utterance prior to the second device comprises comparing the first time data directly to the second time data.
15. The computer-implemented method of claim 11, further comprising: receiving, from the first device, offset data corresponding to a synchronization between clock data of the first device and clock data of the second device; and storing an association between the offset data and the user profile.
16. The computer-implemented method of claim 15, further comprising: combining the offset data with the first time data to determine first offset time data; and combining the offset data with the second time data to determine second offset time data; wherein using the first time data and the second time data to determine that the first device detected the utterance prior to the second device comprises comparing the first offset time data to the second offset time data.
</claims>
</document>
