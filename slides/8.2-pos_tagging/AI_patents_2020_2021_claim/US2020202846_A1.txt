<document>

<filing_date>
2017-06-18
</filing_date>

<publication_date>
2020-06-25
</publication_date>

<priority_date>
2017-06-18
</priority_date>

<ipc_classes>
G06F16/332,G06N3/04,G06N3/08,G10L15/16,G10L15/18,G10L15/22
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
BAPNA, ANKUR
HECK, LARRY PAUL
</inventors>

<docdb_family_id>
59258380
</docdb_family_id>

<title>
PROCESSING NATURAL LANGUAGE USING MACHINE LEARNING TO DETERMINE SLOT VALUES BASED ON SLOT DESCRIPTORS
</title>

<abstract>
Determining slot value(s) based on received natural language input and based on descriptor(s) for the slot(s). In some implementations, natural language input is received as part of human-to-automated assistant dialog. A natural language input embedding is generated based on token(s) of the natural language input. Further, descriptor embedding(s) are generated (or received), where each of the descriptor embeddings is generated based on descriptor(s) for a corresponding slot that is assigned to a domain indicated by the dialog. The natural language input embedding and the descriptor embedding(s) are applied to layer(s) of a neural network model to determine, for each of the slot(s), which token(s) of the natural language input correspond to the slot. A command is generated that includes slot value(s) for slot(s), where the slot value(s) for one or more of slot(s) are determined based on the token(s) determined to correspond to the slot(s).
</abstract>

<claims>
1. A natural language processing method using a neural network model, the method implemented by one or more processors and comprising: receiving a natural language data stream comprising a sequence of tokens; generating one or more embedded representations of one or more of the sequence of tokens; applying, to a layer of the neural network model, one or more embedded representations of one or more descriptors for one or more slots; identifying a correspondence between a descriptor and one or more tokens, wherein identifying said correspondence comprises processing the one or more embedded representations of the one or more tokens and the one or more embedded representations of the one or more descriptors, using the neural network model; transmitting data relating to the identified correspondence from a first network node to a second network node, the second network node being configured to perform one or more actions in response to receiving said data, the one or more actions including causing responsive content to be generated and transmitted; and receiving the responsive content at the first network node.
2. The method of claim 1, wherein one or more of the received embedded representations of one or more descriptors relate to a domain that is not among the one or more domains on which the neural network was trained.
3. The method of claim 1, wherein the neural network model is a neural network model trained using multiple sample natural language data streams and sample embedded representations of descriptors for one or more slot tags from a plurality of domains.
4. The method of claim 1, wherein the one or more embedded representations of the one or more descriptors are applied as input to a combining layer of the neural network model, along with the one or more embedded representations of the one or more tokens.
5. The method of claim 4, wherein the combining layer comprises a feed forward layer.
6. The method of claim 4, wherein the combining layer is located in the neural network model between two memory layers.
7. The method of claim 1, wherein the final layer of the neural network model comprises a soft-max layer configured to output an in-out-begin representation of the received sequence of tokens.
8. The method of claim 1, wherein the method further comprises classifying the sequence of tokens into a semantic domain, prior to inputting an embedded representation of the sequence of tokens into a layer of the neural network model.
9. The method of claim 1, wherein the method further comprises: receiving, at a microphone, a voice input; and generating the sequence of tokens from the voice input using a voice-to-text algorithm.
10. The method of claim 1, wherein generating the one or more embedded representations of one or more of the sequence of tokens is performed based on applying the one or more of the sequence of tokens to the neural network model.
11. The method of claim 1, wherein generating the one or more embedded representations of one or more of the sequence of tokens is performed within a further neural network model.
12. A method of training a neural network model for natural language processing, the method comprising the steps of: applying, to a layer of the neural network model, one or more embedded representations of one or more descriptors for one or more slots; receiving natural language data streams relating to multiple domains, the data streams comprising tokens having a known correspondence to the one or more descriptors; generating one or more embedded representations of one or more of said tokens; processing, using the neural network model, the one or more embedded representations of the one or more tokens and the one or more embedded representations of the one or more descriptors to identify a correspondence between a descriptor and one or more tokens; comparing the identified correspondence with the known correspondence; and updating weights in the neural network model in dependence on the comparison.
13. The method of claim 12, wherein the method further comprises tuning hyper-parameters of the neural network model in dependence on a determined performance of the neural network model.
14. The method of claim 12, wherein the one or more embedded representations of descriptors are applied at a combining layer of the neural network model, wherein the combining layer is configured to combine the one or more embedded representations of one or more descriptors with the one or more embedded representations of the one or more tokens.
15. The method of claim 14, wherein the combining layer comprises a feed forward layer.
16. The method of claim 14, wherein the combining layer is located in the neural network model between two memory layers.
17. An apparatus comprising a processor and a memory, wherein the memory includes computer-readable code which, when executed by the processor, causes the processor to provide a neural network for processing data relating to a sequence of tokens in a natural language data stream, wherein the neural network comprises a plurality of layers including at least one memory layer, the neural network being configured to: receive, at one of said plurality of layers, one or more embedded representations of one or more descriptors for one or more slot tags; identify a correspondence between a descriptor and one or more tokens, wherein identifying said correspondence comprises processing the one or more embedded representations of the one or more tokens and the one or more embedded representations of the one or more descriptors, using the neural network.
18. The apparatus of claim 17, wherein the one or more embedded representations of the one or more descriptors are received at a combining layer, the combining layer being configured to combine the one or more embedded representations of one or more descriptors with the one or more embedded representations of the one or more tokens.
19. The apparatus of claim 18, wherein the neural network comprises first and second memory layers, wherein the combining layer is located between the first and second memory layers.
20. 20.-36. (canceled)
</claims>
</document>
