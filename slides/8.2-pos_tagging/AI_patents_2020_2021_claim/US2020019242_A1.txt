<document>

<filing_date>
2018-07-12
</filing_date>

<publication_date>
2020-01-16
</publication_date>

<priority_date>
2018-07-12
</priority_date>

<ipc_classes>
G06F1/16,G06F3/01,G06F3/16,G06N99/00
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
MUEHLHAUSEN, ANDREW FREDERICK
JAKUBZAK, KENNETH MITCHELL
MCBETH, SEAN KENNETH
ATLAS, CHARLENE MARY
</inventors>

<docdb_family_id>
67138189
</docdb_family_id>

<title>
DIGITAL PERSONAL EXPRESSION VIA WEARABLE DEVICE
</title>

<abstract>
Examples are disclosed that relate to evoking an emotion and/or other an expression of an avatar via a gesture and/or posture sensed by a wearable device. One example provides a computing device including a logic subsystem and memory storing instructions executable by the logic subsystem to receive, from a wearable device configured to be worn on a hand of a user, an input of data indicative of one or more of a gesture and a posture. The instructions are further executable to, based on the input of data received, determine a digital personal expression corresponding to the one or more of the gesture and the posture, and output the digital personal expression.
</abstract>

<claims>
1. A computing device, comprising: a logic subsystem; and memory comprising instructions executable by the logic subsystem to receive an input; receive, from a wearable device configured to be worn on a hand of a user, an input of data indicative of one or more of a gesture and a posture of the hand of the user; based on the input of data received, determine a digital personal expression corresponding to the one or more of the gesture and the posture; store the digital personal expression determined as associated with the input; and output the digital personal expression.
2. The computing device of claim 1, wherein the instructions are executable to output, via a display, an avatar of the user, the avatar of the user comprising a feature representing the digital personal expression.
3. The computing device of claim 1, wherein the input comprises an input of one or more of a video, a speech, an image, and/or a text.
4. The computing device of claim 1, wherein the instructions are executable to output the digital personal expression by sending the digital personal expression to another computing device.
5. The computing device of claim 1, wherein the wearable device comprises one or more of a ring and a glove.
6. The computing device of claim 1, wherein the instructions are executable to output, via a speaker, an audio avatar having a sound characteristic representative of the digital personal expression.
7. The computing device of claim 1, wherein receiving the input of data indicative of the one or more of the gesture and the posture comprises receiving data indicative of an input received by a user-selectable input mechanism of the wearable device.
8. The computing device of claim 1, wherein the instructions are executable to determine the digital personal expression based on a trained machine learning model.
9. The computing device of claim 1, wherein the instructions are executable to determine the digital personal expression based on a mapping of the one or more of the gesture and/posture to a corresponding digital personal expression.
10. A wearable device configured to be worn on a hand of a user, the wearable device comprising: an input subsystem comprising one or more sensors; a logic subsystem; and memory holding instructions executable by the logic subsystem to receive, from the input subsystem, information comprising one or more of hand pose data and/or hand motion data; based at least on the information received, determine a digital personal expression corresponding to the one or more of the hand pose data and/or the hand motion data; and send, to an external computing device, the digital personal expression.
11. The wearable device of claim 10, wherein the one or more sensors comprises one or more of a gyroscope, an accelerometer, and/or a magnetometer.
12. The wearable device of claim 10, wherein the instructions are executable to determine the digital personal expression based on mapping the one or more of the hand pose data and/or the hand motion data received to a corresponding digital personal expression.
13. The wearable device of claim 10, wherein the instructions are executable to determine the digital personal expression via a trained machine learning model.
14. The wearable device of claim 10, wherein the wearable device comprises one or more of a ring and a glove.
15. The wearable device of claim 10, wherein the instructions are executable to receive a user input mapping a selected gesture and/or a selected posture to a corresponding digital personal expression.
16. The wearable device of claim 10, wherein the input subsystem comprises one or more of a button and/or a touch sensor, and wherein the information comprising the one or more of the hand pose data and/or the hand motion data comprises an input received via the one or more of the button and/or the touch sensor.
17. A method for designating a digital personal expression to data, the method comprising: receiving an input; receiving, from a wearable device worn on a hand of a user, an input of information, the information comprising hand tracking data; inputting the information received into a trained machine learning model; obtaining from the trained machine learning model a probable digital personal expression corresponding to one or more of a sensed pose and/or a sensed movement of the hand; storing the probable digital personal expression as associated with the input; and outputting the probable digital personal expression via an avatar.
18. The method of claim 17, wherein the hand tracking data comprises data capturing natural conversational motion of the hand.
19. The method of claim 17, further comprising receiving user feedback regarding whether the probable digital personal expression obtained was a correct digital personal expression, and inputting the feedback as training data for the trained machine learning model.
20. The method of claim 17, wherein the trained machine learning model is trained based upon data obtained from one or more of a cohort comprising the user and/or a population of users.
</claims>
</document>
