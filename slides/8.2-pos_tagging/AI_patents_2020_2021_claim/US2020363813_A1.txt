<document>

<filing_date>
2019-05-15
</filing_date>

<publication_date>
2020-11-19
</publication_date>

<priority_date>
2019-05-15
</priority_date>

<ipc_classes>
G05D1/02,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
BAIDU USA
</assignee>

<inventors>
HU JIANGTAO
LUO, QI
MIAO, JINGHAO
SONG, SHIYU
WANG, YU
ZHOU, JINYUN
XU, JIAXUAN
JIANG, SHU
HE, RUNXIN
</inventors>

<docdb_family_id>
73228094
</docdb_family_id>

<title>
ONLINE AGENT USING REINFORCEMENT LEARNING TO PLAN AN OPEN SPACE TRAJECTORY FOR AUTONOMOUS VEHICLES
</title>

<abstract>
In one embodiment, a system uses an actor-critic reinforcement learning model to generate a trajectory for an autonomous driving vehicle (ADV) in an open space. The system perceives an environment surrounding an ADV. The system applies a RL algorithm to an initial state of a planning trajectory based on the perceived environment to determine a plurality of controls for the ADV to advance to a plurality of trajectory states based on map and vehicle control information for the ADV. The system determines a reward prediction by the RL algorithm for each of the plurality of controls in view of a target destination state. The system generates a first trajectory from the trajectory states by maximizing the reward predictions to control the ADV autonomously according to the first trajectory.
</abstract>

<claims>
1. A computer-implemented method for operating an autonomous driving vehicle, the method comprising: perceiving an environment surrounding an autonomous driving vehicle (ADV); applying a reinforcement learning (RL) algorithm to an initial state of an initially planned trajectory based on the perceived environment to determine a plurality of controls for the ADV to advance to a plurality of trajectory states based on map and vehicle control information for the ADV; determining a reward prediction by the RL algorithm for each of the plurality of controls in view of a target destination state; and generating a first trajectory from the trajectory states by maximizing the reward predictions to control the ADV autonomously according to the first trajectory.
2. The method of claim 1, further comprising applying a judgment logic to the first trajectory to determine a judgment score for the first trajectory.
3. The method of claim 2, wherein the judgment score includes scores for whether the first trajectory ends at the destination state, whether the first trajectory is smooth, and whether the first trajectory avoids one or more obstacles in the perceived environment.
4. The method of claim 3, further comprising, if the judgment score is below a predetermined threshold, generating a second trajectory based on an open space optimization model to control the ADV autonomously according to the second trajectory.
5. The method of claim 4, wherein the open space optimization model is to generate a trajectory for the ADV without following a reference line or traffic lines.
6. The method of claim 4, wherein the open space optimization model includes a vehicle dynamic model for the ADV.
7. The method of claim 1, wherein the RL algorithm is performed by an actor neural network and a critic neural network, and wherein the actor neural network and the critic neural network are deep neural networks.
8. A non-transitory machine-readable medium having instructions stored therein, which when executed by a processor, cause the processor to perform operations, the operations comprising: perceiving an environment surrounding an autonomous driving vehicle (ADV); applying a reinforcement learning (RL) algorithm to an initial state of an initially planned trajectory based on the perceived environment to determine a plurality of controls for the ADV to advance to a plurality of trajectory states based on map and vehicle control information for the ADV; determining a reward prediction by the RL algorithm for each of the plurality of controls in view of a target destination state; and generating a first trajectory from the trajectory states by maximizing the reward predictions to control the ADV autonomously according to the first trajectory.
9. The non-transitory machine-readable medium of claim 8, wherein the operations further comprise applying a judgment logic to the first trajectory to determine a judgment score for the first trajectory.
10. The non-transitory machine-readable medium of claim 9, wherein the judgment score includes scores for whether the first trajectory ends at the destination state, whether the first trajectory is smooth, and whether the first trajectory avoids one or more obstacles for the perceived environment.
11. The non-transitory machine-readable medium of claim 10, wherein the operations further comprise, if the judgment score is below a predetermined threshold, generating a second trajectory based on an open space optimization model to control the ADV autonomously according to the second trajectory.
12. The non-transitory machine-readable medium of claim 11, wherein the open space optimization model is to generate a trajectory for the ADV without following a reference line or traffic lines.
13. The non-transitory machine-readable medium of claim 11, wherein the open space optimization model includes a vehicle dynamic model for the ADV.
14. The non-transitory machine-readable medium of claim 8, wherein the RL algorithm is performed by an actor neural network and a critic neural network, and wherein the actor neural network and the critic neural network are deep neural networks.
15. A data processing system, comprising: a processor; and a memory coupled to the processor to store instructions, which when executed by the processor, cause the processor to perform operations, the operations including perceiving an environment surrounding an autonomous driving vehicle (ADV), applying a reinforcement learning (RL) algorithm to an initial state of an initially planned trajectory based on the perceived environment to determine a plurality of controls for the ADV to advance to a plurality of trajectory states based on map and vehicle control information for the ADV, determining a reward prediction by the RL algorithm for each of the plurality of controls in view of a target destination state, and generating a first trajectory from the trajectory states by maximizing the reward predictions to control the ADV autonomously according to the first trajectory.
16. The system of claim 15, wherein the operations further comprise applying a judgment logic to the first trajectory to determine a judgment score for the first trajectory.
17. The system of claim 16, wherein the judgment score includes scores for whether the first trajectory ends at the destination state, whether the first trajectory is smooth, and whether the first trajectory avoids one or more obstacles for the perceived environment.
18. The system of claim 17, wherein the operations further comprise, if the judgment score is below a predetermined threshold, generating a second trajectory based on an open space optimization model to control the ADV autonomously according to the second trajectory.
19. The system of claim 18, wherein the open space optimization model is to generate a trajectory for the ADV without following a reference line or traffic lines.
20. The system of claim 18, wherein the open space optimization model includes a vehicle dynamic model for the ADV.
21. The system of claim 15, wherein the RL algorithm is performed by an actor neural network and a critic neural network, and wherein the actor neural network and the critic neural network are deep neural networks.
</claims>
</document>
