<document>

<filing_date>
2020-03-25
</filing_date>

<publication_date>
2020-07-16
</publication_date>

<priority_date>
2016-10-10
</priority_date>

<ipc_classes>
B25J9/16,G05B13/02,G06N3/00,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
DEEPMIND TECHNOLOGIES
</assignee>

<inventors>
RUSU, ANDREI-ALEXANDRU
PASCANU, RAZVAN
HEESS, NICOLAS MANFRED OTTO
HADSELL, RAIA THAIS
VECERIK, MEL
ROTHOERL, THOMAS
</inventors>

<docdb_family_id>
60164852
</docdb_family_id>

<title>
NEURAL NETWORKS FOR SELECTING ACTIONS TO BE PERFORMED BY A ROBOTIC AGENT
</title>

<abstract>
A system includes a neural network system implemented by one or more computers. The neural network system is configured to receive an observation characterizing a current state of a real-world environment being interacted with by a robotic agent to perform a robotic task and to process the observation to generate a policy output that defines an action to be performed by the robotic agent in response to the observation. The neural network system includes: (i) a sequence of deep neural networks (DNNs), in which the sequence of DNNs includes a simulation-trained DNN that has been trained on interactions of a simulated version of the robotic agent with a simulated version of the real-world environment to perform a simulated version of the robotic task, and (ii) a first robot-trained DNN that is configured to receive the observation and to process the observation to generate the policy output.
</abstract>

<claims>
1. 1.-20. (canceled)
21. A computer-implemented method comprising: receiving an observation characterizing a current state of a real-world environment being interacted with by a robotic agent to perform a robotic task; processing the observation using a neural network system to generate a policy output, wherein the neural network system comprises a sequence of deep neural networks (DNNs), and wherein the sequence of DNNs comprises: a simulation-trained DNN that has been trained on interactions of a simulated version of the robotic agent with a simulated version of the real-world environment to perform a simulated version of the robotic task, wherein: the simulation-trained DNN comprises a first plurality of indexed layers, and the simulation-trained DNN is configured to receive the observation and process the observation through each layer in the first plurality of indexed layers to generate a respective layer output for each layer in the first plurality of indexed layers; and a first robot-trained DNN that has been trained on interactions of the robotic agent with the real-world environment to perform the robotic task to determine trained values of parameters of the first robot-trained DNN while holding trained values of the parameters of the simulation-trained DNN fixed, wherein the first robot-trained DNN comprises a second plurality of indexed layers, the first robot-trained DNN is configured to receive the observation and to process the observation through each layer in the second plurality of indexed layers to generate the policy output, and one or more of the layers in the second plurality of indexed layers are each configured to receive as input (i) a layer output generated by a preceding layer of the first robot-trained DNN, and (ii) a layer output generated by a preceding layer of the simulation-trained DNN, wherein a preceding layer is a layer whose index is one less than the index of the layer; and selecting an action to be performed by the robotic agent in response to the observation using the policy output.
22. The method of claim 21, wherein the first robot-trained DNN has a smaller capacity than the simulation-trained DNN.
23. The method of claim 21, wherein the first robot-trained DNN has a narrow architecture and the simulation-trained DNN has a wide architecture.
24. The method of claim 21, wherein, for each of one or more layers of the second plurality of indexed layers, a corresponding layer in the first plurality of indexed layers having a same index is a neural network layer of a same type but of a larger dimension than the layer in the second plurality of indexed layers.
25. The method of claim 21, wherein the first plurality of indexed layers and the second plurality of indexed layers each include a respective recurrent neural network layer.
26. The method of claim 25, wherein the recurrent neural network layer in the second plurality of indexed layers is configured to receive as input (i) a layer output of a layer preceding the recurrent neural network layer in the first plurality of indexed layers, (ii) an internal state of the recurrent neural network layer in the first plurality of indexed layers, and (iii) a layer output of a layer preceding the recurrent neural network layer in the second plurality of indexed layers.
27. The method of claim 21, wherein each of the one or more of the layers in the second plurality of indexed layers that are configured to receive as input (i) a layer output generated by a preceding layer of the first robot-trained DNN, and (ii) a layer output generated by a preceding layer of the simulation-trained DNN is further configured to: apply a respective first set of parameters to the layer output generated by the preceding layer of the first robot-trained DNN; and apply a respective second set of parameters to the layer output generated by the preceding layer of the simulation-trained DNN.
28. The method of claim 21, wherein the policy output defines a respective change in position of each of a plurality of degrees of freedom of the robotic agent.
29. The method of claim 21, wherein the plurality of degrees of freedom include one or more joints of the robotic agent and one or more actuators of the robotic agent.
30. The method of claim 21, wherein the sequence of DNNs further comprises: a second robot-trained DNN, wherein: the second robot-trained DNN comprises a third plurality of indexed layers, and one or more of the layers in the third plurality of indexed layers are each configured to receive as input (i) a layer output generated by a preceding layer of the first robot-trained DNN, (ii) a layer output generated by a preceding layer of the simulation-trained DNN, and (iii) a layer output generated by a preceding layer of the second robot-trained DNN.
31. The method of claim 30, wherein: the second robot-trained DNN is configured to receive different data characterizing the current state in conjunction with the observation; and the second robot-trained DNN is configured to process the different data through the third plurality of indexed layers to generate a second policy output that defines an action to be performed by the robotic agent to perform a second, different robotic task.
32. The method of claim 31, wherein the observation is visual data and the different data is proprioceptive data.
33. One or more non-transitory computer-readable storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising: receiving an observation characterizing a current state of a real-world environment being interacted with by a robotic agent to perform a robotic task; processing the observation using a neural network system to generate a policy output, wherein the neural network system comprises a sequence of deep neural networks (DNNs), and wherein the sequence of DNNs comprises: a simulation-trained DNN that has been trained on interactions of a simulated version of the robotic agent with a simulated version of the real-world environment to perform a simulated version of the robotic task, wherein: the simulation-trained DNN comprises a first plurality of indexed layers, and the simulation-trained DNN is configured to receive the observation and process the observation through each layer in the first plurality of indexed layers to generate a respective layer output for each layer in the first plurality of indexed layers; and a first robot-trained DNN that has been trained on interactions of the robotic agent with the real-world environment to perform the robotic task to determine trained values of parameters of the first robot-trained DNN while holding trained values of the parameters of the simulation-trained DNN fixed, wherein the first robot-trained DNN comprises a second plurality of indexed layers, the first robot-trained DNN is configured to receive the observation and to process the observation through each layer in the second plurality of indexed layers to generate the policy output, and one or more of the layers in the second plurality of indexed layers are each configured to receive as input (i) a layer output generated by a preceding layer of the first robot-trained DNN, and (ii) a layer output generated by a preceding layer of the simulation-trained DNN, wherein a preceding layer is a layer whose index is one less than the index of the layer; and selecting an action to be performed by the robotic agent in response to the observation using the policy output.
34. A system comprising: one or more computers; and one or more non-transitory computer-readable storage media storing instructions that, when executed by the one or more computers, cause the one or more computers to perform operations comprising: receiving an observation characterizing a current state of a real-world environment being interacted with by a robotic agent to perform a robotic task; processing the observation using a neural network system to generate a policy output, wherein the neural network system comprises a sequence of deep neural networks (DNNs), and wherein the sequence of DNNs comprises: a simulation-trained DNN that has been trained on interactions of a simulated version of the robotic agent with a simulated version of the real-world environment to perform a simulated version of the robotic task, wherein: the simulation-trained DNN comprises a first plurality of indexed layers, and the simulation-trained DNN is configured to receive the observation and process the observation through each layer in the first plurality of indexed layers to generate a respective layer output for each layer in the first plurality of indexed layers; and a first robot-trained DNN that has been trained on interactions of the robotic agent with the real-world environment to perform the robotic task to determine trained values of parameters of the first robot-trained DNN while holding trained values of the parameters of the simulation-trained DNN fixed, wherein the first robot-trained DNN comprises a second plurality of indexed layers, the first robot-trained DNN is configured to receive the observation and to process the observation through each layer in the second plurality of indexed layers to generate the policy output, and one or more of the layers in the second plurality of indexed layers are each configured to receive as input (i) a layer output generated by a preceding layer of the first robot-trained DNN, and (ii) a layer output generated by a preceding layer of the simulation-trained DNN, wherein a preceding layer is a layer whose index is one less than the index of the layer; and selecting an action to be performed by the robotic agent in response to the observation using the policy output.
35. One or more non-transitory computer-readable storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform operations for training a neural network system comprising a sequence of deep neural networks (DNNs) to determine trained values of parameters of one or more of the DNNs in the sequence of DNNs, wherein the neural network system is configured to receive an observation characterizing a current state of a real-world environment being interacted with by a robotic agent to perform a robotic task and to process the observation to generate a policy output that defines an action to be performed by the robotic agent in response to the observation, wherein the sequence of DNNs comprises: a simulation-trained DNN, wherein the simulation-trained DNN comprises a first plurality of indexed layers and is configured to receive the observation and process the observation through each layer in the first plurality of indexed layers to generate a respective layer output for each layer in the first plurality of indexed layers; and a first robot-trained DNN that has been trained on interactions of the robotic agent with the real-world environment to perform the robotic task to determine trained values of parameters of the first robot-trained DNN while holding trained values of the parameters of the simulation-trained DNN fixed, wherein the first robot-trained DNN comprises a second plurality of indexed layers, the first robot-trained DNN is configured to receive the observation and to process the observation through each layer in the second plurality of indexed layers to generate the policy output, and one or more of the layers in the second plurality of indexed layers are each configured to receive as input (i) a layer output generated by a preceding layer of the first robot-trained DNN, and (ii) a layer output generated by a preceding layer of the simulation-trained DNN, wherein a preceding layer is a layer whose index is one less than the index of the layer, wherein the operations comprise: training the first robot-trained DNN on interactions of the robotic agent with the real-world environment to perform the robotic task to determine trained values of parameters of the first robot-trained DNN while holding trained values of the parameters of the simulation-trained DNN fixed.
36. The one or more non-transitory computer-readable storage media of claim 35, wherein an output layer of the first robot-trained DNN is configured to: receive as input (i) a layer output generated by a layer preceding the output layer of the first robot-trained DNN, and (ii) a layer output generated by a layer preceding an output layer of the simulation-trained DNN; apply a first set of parameters to the layer output generated by a layer preceding the output layer the first robot-trained DNN; and apply a second set of parameters to the layer output generated by a layer preceding the output layer of the simulation-trained DNN, and wherein the method further comprises: initializing values of the second set of parameters to match trained values of parameters of the output layer of the simulation-trained DNN.
37. The one or more non-transitory computer-readable storage media of claim 36, wherein the operations further comprise: initializing values of the first set of parameters to zero.
38. The one or more non-transitory computer-readable storage media of claim 36, wherein the operations further comprise: initializing values of parameters of layers of the first robot-trained DNN other than the output layer to random values.
39. The one or more non-transitory computer-readable storage media of claim 35, wherein the operations further comprise: training the simulation-trained DNN on interactions of a simulated version of the robotic agent with a simulated version of the real-world environment to perform a simulated version of the robotic task to determine trained values of parameters of the simulation-trained DNN.
40. The one or more non-transitory computer-readable storage media of claim 39, wherein training the simulation-trained DNN comprises training the simulation-trained DNN on the interactions of the simulated version of the robotic agent using a first reinforcement learning technique.
41. The one or more non-transitory computer-readable storage media of claim 40, wherein training the first robot-trained DNN comprises training the first robot-trained DNN on the interactions of the robotic agent using a second reinforcement learning technique.
42. A system comprising: one or more computers; and one or more non-transitory computer-readable storage media storing instructions that, when executed by the one or more computers, cause the one or more computers to perform operations for training a neural network system comprising a sequence of deep neural networks (DNNs) to determine trained values of parameters of one or more of the DNNs in the sequence of DNNs, wherein the neural network system is configured to receive an observation characterizing a current state of a real-world environment being interacted with by a robotic agent to perform a robotic task and to process the observation to generate a policy output that defines an action to be performed by the robotic agent in response to the observation, wherein the sequence of DNNs comprises: a simulation-trained DNN, wherein the simulation-trained DNN comprises a first plurality of indexed layers and is configured to receive the observation and process the observation through each layer in the first plurality of indexed layers to generate a respective layer output for each layer in the first plurality of indexed layers; and a first robot-trained DNN that has been trained on interactions of the robotic agent with the real-world environment to perform the robotic task to determine trained values of parameters of the first robot-trained DNN while holding trained values of the parameters of the simulation-trained DNN fixed, wherein the first robot-trained DNN comprises a second plurality of indexed layers, the first robot-trained DNN is configured to receive the observation and to process the observation through each layer in the second plurality of indexed layers to generate the policy output, and one or more of the layers in the second plurality of indexed layers are each configured to receive as input (i) a layer output generated by a preceding layer of the first robot-trained DNN, and (ii) a layer output generated by a preceding layer of the simulation-trained DNN, wherein a preceding layer is a layer whose index is one less than the index of the layer, wherein the operations comprise: training the first robot-trained DNN on interactions of the robotic agent with the real-world environment to perform the robotic task to determine trained values of parameters of the first robot-trained DNN while holding trained values of the parameters of the simulation-trained DNN fixed.
</claims>
</document>
