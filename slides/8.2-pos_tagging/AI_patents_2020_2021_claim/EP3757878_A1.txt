<document>

<filing_date>
2020-06-15
</filing_date>

<publication_date>
2020-12-30
</publication_date>

<priority_date>
2019-06-24
</priority_date>

<ipc_classes>
G06K9/00,G06T7/73
</ipc_classes>

<assignee>
SYNAPTICS
</assignee>

<inventors>
GAUR, Utkarsh
IVANOV BONEV, Boyan
</inventors>

<docdb_family_id>
71096597
</docdb_family_id>

<title>
HEAD POSE ESTIMATION
</title>

<abstract>
A method and apparatus for estimating a user's head pose relative to a sensing device. The sensing device detects a face of the user in an image. The sensing device further identifies a plurality of points in the image corresponding to respective features of the detected face. The plurality of points includes at least a first point corresponding to a location of a first facial feature. The sensing device determines a position of the face relative to the sensing device based at least in part on a distance between the first point in the image and one or more of the remaining points. For example, the sensing device may determine a pitch, yaw, distance, or location of the user's face relative to the sensing device.
</abstract>

<claims>
1. A method of determining a head pose of a user by a sensing device, comprising: detecting a face of the user in an image; identifying a plurality of points in the image corresponding to respective features of the detected face, the plurality of points including at least a first point corresponding to a location of a first facial feature; and determining a position of the face relative to the sensing device based at least in part on a distance between the first point in the image and one or more of the remaining points.
2. The method of claim 1, wherein the first facial feature is a nose and the remaining points correspond to locations of a mouth or eyes.
3. The method of claim 2, wherein the determining comprises: determining a pitch of the face relative to the sensing device based on a ratio of the distances between the first point and a pair of points associated with the mouth and the eyes; optionally wherein determining the pitch comprises: calculating a first midpoint between the location of a left eye and the location of a right eye; calculating a second midpoint between the location of a left portion of the mouth and the location of a right portion of the mouth; and determining the pitch of the face based on a ratio of the distances from the first point to each of the first and second midpoints.
4. The method of claim 2, wherein the determining comprises: determining a yaw of the face relative to the sensing device based on a ratio of the distances between the first point and a pair of points associated with the mouth and the eyes; optionally, wherein determining the yaw comprises: calculating a third midpoint between the location of a left eye and the location of a left portion of the mouth; calculating a fourth midpoint between the location of a right eye and the location of a right portion of the mouth; and determining the yaw of the face based on a ratio of the distances from the first point to each of the third and fourth midpoints.
5. The method of claim 2, wherein the determining comprises:
determining a distance of the face relative to the sensing device as a proportion of the distances between the first point and two or more pairs of points associated the mouth and the eyes.
6. The method of claim 5, wherein determining the distance of the face comprises: calculating a first midpoint between the location of a left eye and the location of a right eye; calculating a second midpoint between the location of a left portion of the mouth and the location of a right portion of the mouth; calculating a third midpoint between the location of the left eye and the location of the left portion of the mouth; calculating a fourth midpoint between the location of the right eye and the location of the right portion of the mouth; and determining the distance of the face as a proportion of the distances from the first point to each of the first and second midpoints or as a proportion of the distances from the first point to each of the third and fourth midpoints.
7. The method of claim 6, wherein determining the distance of the face further comprises: calculating a first combined distance from the first point to each of the first midpoint and the second midpoint; calculating a second combined distance from the first point to each of the third midpoint and the fourth midpoint; and determining the distance of the face based on the greater of the first combined distance or the second combined distance.
8. The method of claim 1, wherein the determining comprises: receiving one or more images of a user while the user is in contact with the sensing device; determining a scalar quantity based on the one or more images of the user; and determining the position of the face in the received image based at least in part on the scalar quantity.
9. A sensing device, comprising: processing circuitry; and memory storing instructions that, when executed by the processing circuitry, causes the sensing device to: detect a face in an image; identify a plurality of points in the image corresponding to respective features of the detected face, the plurality of points including at least a first point corresponding to a location of a first facial feature; and determining a position of the face relative to the sensing device based at least in part on a distance between the first point in the image and one or more of the remaining points.
10. The sensing device of claim 9, wherein the first facial feature is a nose and the remaining points correspond to locations of a mouth or eyes.
11. The sensing device of claim 10, wherein execution of the instructions for determining the position of the face causes the sensing device to: calculate a first midpoint between the location of a left eye and the location of a right eye; calculate a second midpoint between the location of a left portion of the mouth and the location of a right portion of the mouth; and determine a pitch of the face relative to the sensing device based on a ratio of the distances from the first point to each of the first and second midpoints.
12. The sensing device of claim 10, wherein execution of the instructions for determining the position of the face causes the sensing device to: calculate a third midpoint between the location of a left eye and the location of a left portion of the mouth; calculate a fourth midpoint between the location of a right eye and the location of a right portion of the mouth; and determine a yaw of the face relative to the sensing device based on a ratio of the distances from the first point to each of the third and fourth midpoints.
13. The sensing device of claim 10, wherein execution of the instructions for determining the position of the face causes the sensing device to: calculate a first midpoint between the location of a left eye and the location of a right eye; calculate a second midpoint between the location of a left portion of the mouth and the location of a right portion of the mouth; calculate a third midpoint between the location of the left eye and the location of the left portion of the mouth; calculate a fourth midpoint between the location of the right eye and the location of the right portion of the mouth; and calculate a first combined distance from the first midpoint to the first point and from the first point to the second midpoint; calculate a second combined distance from the third midpoint to the first point and from the first point to the fourth midpoint; and determine a distance of the face from the sensing device based on the greater of the first combined distance or the second combined distance.
14. The sensing device of claim 9, wherein execution of the instructions for determining the position of the face causes the sensing device to: receive one or more images of a user while the user is in contact with the sensing device; determine a scalar quantity based on the one or more images of the user; and determine the position of the face in the received image based at least in part on the scalar quantity.
15. A system, comprising a camera configured to capture an image of a scene; and the sensing device according to any one of claims 9 to 14.
</claims>
</document>
