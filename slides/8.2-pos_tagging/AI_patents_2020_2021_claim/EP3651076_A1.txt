<document>

<filing_date>
2018-09-03
</filing_date>

<publication_date>
2020-05-13
</publication_date>

<priority_date>
2018-02-13
</priority_date>

<ipc_classes>
G06F7/499,G06N3/063
</ipc_classes>

<assignee>
SHANGHAI CAMBRICON INFORMATION TECHNOLOGY COMPANY
</assignee>

<inventors>
WANG, BINGRUI
ZHANG, YAO
</inventors>

<docdb_family_id>
67618498
</docdb_family_id>

<title>
COMPUTATION DEVICE AND METHOD
</title>

<abstract>
The present disclosure provides a computation device. The computation device is configured to perform a machine learning computation, and includes an operation unit, a controller unit, and a conversion unit. The storage unit is configured to obtain input data and a computation instruction. The controller unit is configured to extract and parse the computation instruction from the storage unit to obtain one or more operation instructions, and to transmit the one or more operation instructions and the input data to the operation unit. The operation unit is configured to perform operations on the input data according to one or more operation instructions to obtain a computation result of the computation instruction. In the examples of the present disclosure, the input data involved in machine learning computations is represented by fixed-point data, thereby improving the processing speed and processing efficiency of training operations.
</abstract>

<claims>
1. A computation device, comprising an operation unit (12), a controller unit (11), and a conversion unit (13), wherein: the controller unit (11) is configured to obtain first input data and transmit the first input data to the conversion unit (13); the conversion unit (13) is configured to convert the first input data into second input data and transmit the second input data to the operation unit (12), wherein the second input data is fixed-point data; and the operation unit (12) is configured to operate the second input data to obtain a computation result; wherein the operation unit (12) includes a data cache unit configured to cache one or more intermediate results obtained by operating the second input data, wherein the intermediate result of which a data type is floating-point data in the one or more intermediate results are not truncated.
2. The computation device of claim 1, wherein the controller unit (11) is further configured to obtain one or more operation instructions, wherein obtaining the one or more operation instructions include:
obtaining, by the controller unit (11), a computation instruction, and parsing, by the controller unit (11), the computation instruction to obtain a data conversion instruction and the one or more operation instructions, wherein: the data conversion instruction includes an opcode field and an opcode, wherein the opcode is configured to indicate information of a function of the data conversion instruction, and the opcode field includes information of a decimal point position, a flag bit indicating a data type of the first input data, and an identifier of data type conversion, the conversion unit (13) is configured to convert the first input data into the second input data, wherein converting the first input data into the second input data includes converting the first input data into the second input data according to the data conversion instruction, and operating the second input data to obtain the computation result includes operating the second input data according to the one or more operation instruction to obtain the computation result.
3. The computation unit of claim 2, configured to execute a machine learning computation, wherein: the machine learning computation includes an artificial neural network operation, the first input data includes an input neuron and a weight, and the computation result is an output neuron.
4. The computation device of claim 2 or 3, wherein the operation unit (12) includes a primary processing circuit (101) and a plurality of secondary processing circuits (102), wherein: the primary processing circuit (101) is configured to perform pre-processing on the second input data and to transmit data and the plurality of operation instructions between the plurality of secondary processing circuits (102) and the primary processing circuit (101), the plurality of secondary processing circuits (102) are configured to perform an intermediate operation to obtain a plurality of intermediate results according to the second input data and the plurality of operation instructions transmitted from the primary processing circuit (101), to transmit the plurality of intermediate results to the primary processing circuit (101), and to store the intermediate result of which the data type is the floating-point data in the one or more intermediate results in the data cache unit without performing truncation processing, and the primary processing circuit (101) is further configured to perform post-processing on the plurality of intermediate results to obtain the computation result of the computation instruction.
5. The device of claim 4, wherein not truncating, by the plurality of secondary processing circuits (102), the intermediate results of which the data type is the floating-point data in the plurality of intermediate results includes:
when the intermediate result obtained by performing operations on the data of a same type or the data of a same layer exceeds a value range corresponding to the decimal point position and the bit width of the data of the same type or the data of the same layer, not performing truncation processing on the intermediate result by the plurality of secondary processing circuits (102) do.
6. The computation device of any of claims 4 to 5, wherein when the first input data is fixed-point data, the operation unit (12) further includes: a derivation unit configured to derive a decimal point position of the one or more intermediate results according to the decimal point position of the first input data, wherein the one or more intermediate results are obtained by computing according to the first input data, and the derivation unit is further configured to shift the decimal point position of the intermediate results to the left by M bits when the intermediate results exceed the range indicated by the corresponding decimal point position, such that the accuracy of the intermediate result is within a precision range indicated by the decimal point position of the intermediate result, and the M is an integer greater than zero.
7. A combination processing device, comprising a machine learning operation device, universal interconnection interfaces, a storage device, and other processing devices, wherein:
the machine learning operation device, comprising one or more computation devices according to any of claims 2 to 6, wherein the at least one computation device is configured to obtain data to be processed and control information from other processing devices, to perform specified machine learning computations, and to transmit an execution result to the other processing devices through I/O interfaces, wherein: when the machine learning operation device includes a plurality of the computation devices, the plurality of computation devices is configured to couple and transmit data with each other through a specific structure, and the plurality of computation devices is configured to: interconnect and transmit data through a peripheral component interconnect express, PCIE, bus to support larger-scale machine learning computations, share the same one control system or have respective control systems, share the same one memory or have respective memories, and deploy an interconnection manner of any arbitrary interconnection topology, the machine learning operation device is configured to interact with the other processing devices to jointly perform user-specified computing operations, and the storage device is configured to couple with the machine learning operation device and the other processing devices for storing data of the machine learning operation device and the other processing devices.
8. An electronic device, comprising a neural network chip, wherein the neural network chip comprises the combination processing device of claim 7.
9. A board, comprising a storage device (390), an interface device, a control device (392), and the neural network chip of claim 8, wherein: the neural network chip is respectively coupled with the storage device (390), the control device (392), and the interface device, the storage device (390) is configured to store data, the interface device is configured to implement data transmission between the neural network chip and external devices, and the control device (392) is configured to monitor a status of the neural network chip, wherein the storage device (390) includes a plurality of groups of storage units (393), wherein each group of the plurality of groups of the storage units (393) is coupled with the neural network chip through a bus, and the storage unit (393) is a double data rate synchronous dynamic random access memory, the neural network chip includes a double data rate controller for controlling data transmission and data storage of each of the storage units (393), and the interface device is a standard PCIE interface.
10. A computation method, comprising: obtaining, by the controller unit (11), the first input data and the one or more operation instructions, converting, by the conversion unit (13), the first input data into the second input data, wherein the second input data is the fixed-point data, and operating, by the operation unit (12), the second input data according to the plurality of operation instructions to obtain the computation result; wherein the operation unit (12) is further configured to cache the one or more intermediate results obtained by operating the second input data, wherein the intermediate result of which the data type is the floating-point data in the one or more intermediate results are not truncated.
11. The method of claim 10, wherein obtaining the one or more operation instructions by the controller unit (11) includes: obtaining the computation instruction when obtaining the first input data, and parsing the computation instruction to obtain the data conversion instruction and the one or more operation instructions, wherein the data conversion instruction includes the opcode field and the opcode, wherein the opcode is configured to indicate information of the function of the data conversion instruction, the opcode field include information of the decimal point position, and the flag bit indicates the data type of the first input data, and the identifier of data type conversion, and the conversion unit (13) is configured to convert the first input data into the second input data, wherein converting the first input data into the second input data includes converting the first input data into the second input data according to the data conversion instruction.
12. The method of claim 11, wherein the method is configured to execute a machine learning operation, wherein
the machine learning computation includes an artificial neural network operation,
the input data includes an input neuron and a weight, and
the computation result is an output neuron.
13. The method of claim 11 or 12, wherein converting the first input data into second input data according to the data conversion instruction includes: parsing the data conversion instruction to obtain information of the decimal point position, the flag bit indicating a data type of the first input data, and the data type conversion, determining the data type of the first input data according to the flag bit indicating the data type of the first input data, and converting the first input data into the second input data according to the decimal point position and the data type conversion, and the data type of the first input data is inconsistent with that of the second input data.
14. The method of claim 12 or 13, wherein:
if the first input data and the second input data are both fixed-point data, the decimal point position of the first input data is inconsistent with that of the second input data.
15. The method of claim 14, wherein if the first input data is fixed-point data, the method further includes: deriving the decimal point position of the one or more intermediate results according to the decimal point position of the first input data, wherein the one or more intermediate results are obtained by operating according to the first input data, and the operation unit (12) is further configured to shift the decimal point position of the intermediate results to the left by M bits when the intermediate results exceed the range indicated by the corresponding decimal point position, such that the accuracy of the intermediate result is within the precision range indicated by the decimal point position of the intermediate result, and the M is the integer greater than zero.
</claims>
</document>
