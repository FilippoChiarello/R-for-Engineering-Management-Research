<document>

<filing_date>
2018-12-11
</filing_date>

<publication_date>
2021-01-26
</publication_date>

<priority_date>
2018-08-13
</priority_date>

<ipc_classes>
G05D1/00,G05D1/02,G06K9/00,G06K9/62,G06T7/246
</ipc_classes>

<assignee>
NVIDIA CORPORATION
</assignee>

<inventors>
DIPIETRO, ROBERT STEPHEN
BROWN, ABEL KARL
</inventors>

<docdb_family_id>
69406093
</docdb_family_id>

<title>
Scene embedding for visual navigation
</title>

<abstract>
Navigation instructions are determined using visual data or other sensory information. Individual frames can be extracted from video data, captured from passes through an environment, to generate a sequence of image frames. The frames are processed using a feature extractor to generate frame-specific feature vectors. Image triplets are generated, including a representative image frame (or corresponding feature vector), a similar image frame adjacent in the sequence, and a disparate image frame that is separated by a number of frames in the sequence. The embedding network is trained using the triplets. Image data for a current position and a target destination can then be provided as input to the trained embedding model, which outputs a navigation vector indicating a direction and distance over which the vehicle is to be navigated in the physical environment.
</abstract>

<claims>
1. A computer-implemented method, comprising: capturing video data representative of different views of an environment; segmenting the video data into a sequence of image frames; generating a set of frame triplets, each frame triplet including a representative image frame, a similar image frame adjacent the representative image frame in the sequence, and a disparate image separated by a number of image frames in the sequence; training a convolutional neural network using the set of frame triplets to obtain a trained embedding network; capturing an origination image representative of a current location of a device to be navigated; obtaining a destination image representative of a target destination in the environment; processing the origination image and the destination image using the trained embedding network to obtain a navigation vector indicating a relative distance and direction of the target destination relative to the current location; and providing the navigation vector to a navigation application to enable the device to be translated to the target destination.
2. The computer-implemented method of claim 1, further comprising: processing the image frames of the image frame triplets using a feature extractor to generate a respective feature vector representative of each of the processed video frames, the convolutional neural network being trained using the respective feature vectors for the set of frame triplets.
3. The computer-implemented method of claim 1, further comprising: performing a visualization for the navigation vector in three dimensions, the trained embedding network being in more than three dimensions.
4. The computer-implemented method of claim 1, further comprising: obtaining the navigation vector based on the origination image and the destination image without any physical location information for the device to be navigated.
5. The computer-implemented method of claim 1, further comprising: causing the video data to be captured over a set of paths through the environment, each path returning to at least one previously-visited location in the environment.
6. A computer-implemented method, comprising: obtaining first image data representative of a current location of an object in a physical environment; obtaining second image data representative of a destination location for the object in the physical environment; processing the first image data and the second image data using a trained embedding network, the trained embedding network trained using a set of image triplets, the image triplets each including image data for a representative image, a similar image, and a disparate image representative of respective portions of the physical environment; and receiving, from the trained embedding network, a navigation vector indicating a relative direction and distance to the destination location from the current location of the object.
7. The computer-implemented method of claim 6, further comprising: capturing video data representative of different views of the physical environment; segmenting the video data into a sequence of image frames; generating the set of image triplets, each image triplet including the representative image frame, the similar image frame that is adjacent the representative image frame in the sequence, and the disparate image that is separated by a number of image frames in the sequence; and training a convolutional neural network using the set of image triplets to obtain the trained embedding network.
8. The computer-implemented method of claim 7, further comprising: navigating a vehicle along multiple paths through the physical environment, the vehicle having at least one camera for capturing the video data.
9. The computer-implemented method of claim 7, further comprising: capturing additional sensory data at various locations in the physical environment; and training the embedding network further based on the additional sensory data.
10. The computer-implemented method of claim 7, further comprising: processing the image frames of the image triplets using a feature extractor to generate a respective feature vector representative of each of the processed video frames, the embedding network being trained using the respective feature vectors for the set of image triplets.
11. The computer-implemented method of claim 10, wherein at least one of the feature extractor or the trained embedding network is a convolutional neural network.
12. The computer-implemented method of claim 6, further comprising: training the embedding network using a large-margin nearest-neighbor technique accounting for triplet loss.
13. The computer-implemented method of claim 6, further comprising: performing a visualization for the navigation vector in three dimensions, the trained embedding network being in more than three dimensions.
14. The computer-implemented method of claim 6, further comprising: obtaining the navigation vector based on the first image data and the second image data without any physical location information for the device to be navigated.
15. The computer-implemented method of claim 6, wherein the trained embedding network provides a visualization that is topologically consistent with the physical environment.
16. A system, comprising: at least one processor; and memory including instructions that, when executed by the at least one processor, cause the system to: obtain first image data representative of a current location of an object in a physical environment; obtain second image data representative of a destination location for the object in the physical environment; process the first image data and the second image data using a trained embedding network, the trained embedding network trained using a set of image triplets, the image triplets each including image data for a representative image, a similar image, and a disparate image representative of respective portions of the physical environment; and receive, from the trained embedding network, a navigation vector indicating a relative direction and distance to the destination location from the current location of the object.
17. The system of claim 16, wherein the instructions when executed further cause the system to: capture video data representative of different views of the physical environment; segment the video data into a sequence of image frames; generate the set of image triplets, each image triplet including the representative image frame, the similar image frame that is adjacent the representative image frame in the sequence, and the disparate image that is separated by a number of image frames in the sequence; and cause a convolutional neural network to be trained using the set of image triplets to obtain the trained embedding network.
18. The system of claim 16, wherein the instructions when executed further cause the system to: navigate a vehicle along multiple paths through the physical environment, the vehicle having at least one camera for capturing the video data.
19. The system of claim 16, wherein the instructions when executed further cause the system to: provide the navigation vector to a navigation subsystem for navigating the object to minimize the navigation vector along a navigable space determined in part using data captured by one or more object sensors.
20. The computer-implemented method of claim 7, further comprising: process the image frames of the image triplets using a feature extractor to generate a respective feature vector representative of each of the processed video frames, the embedding network being trained using the respective feature vectors for the set of image triplets.
</claims>
</document>
