<document>

<filing_date>
2017-12-12
</filing_date>

<publication_date>
2020-11-17
</publication_date>

<priority_date>
2017-12-12
</priority_date>

<ipc_classes>
G10L15/06,G10L17/04,G10L17/18,H04M3/22
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
</assignee>

<inventors>
LIBERTY, EDO
JHA, MADHAV
</inventors>

<docdb_family_id>
73264068
</docdb_family_id>

<title>
Online training with delayed feedback
</title>

<abstract>
Bandwidth-efficient (i.e., compressed) representations of audio data can be utilized for near real-time presentation of the audio on one or more receiving devices. Persons identified as having speech represented in the audio data can have trained speech models provided to the devices. These trained models can be used to classify the compressed audio in order to improve the quality to correspond more closely to the uncompressed version, without experiencing lag that might otherwise be associated with transmission of the uncompressed audio. The uncompressed audio is also received, with potential lag, and is used to further train the speech models in near real time. The ability to utilize the uncompressed audio as it is received prevents a need to store or further transmit the audio data for offline processing, and enables the further trained model to be used during the communication session.
</abstract>

<claims>
1. A computer-implemented method, comprising: initiating an audio communication between a first communication device, associated with a first person, and a second communication device, associated with a second person; identifying the first person; causing the second communication device to obtain a speech model for the first person, the speech model generated by an initial training using prior speech data for the first person; causing audio, spoken by the first person and captured by the first communication device, to be encoded into a compressed version; causing an uncompressed version of the audio to be transmitted to the second communication device along with the compressed version; causing the second communication device to generate decoded audio from the compressed version; causing at least a portion of the decoded audio to be modified, using the speech model for the first person, for audio presentation via the second device; and causing the speech model to be further trained, during the audio communication, based at least in part on the speech model and the uncompressed version of the audio, substantially contemporaneously as the uncompressed version is received, the uncompressed version capable of being received with a delay relative to receiving of the compressed version.
2. The computer-implemented method of claim 1, further comprising: causing the first communication device to obtain a second speech model for the second person; causing audio, spoken by the second person and captured by the second communication device, to be encoded into a compressed version; causing an uncompressed version of the audio to be transmitted to the first communication device along with the compressed version; causing the first communication device to generate decoded audio from the compressed version; causing the decoded audio to be modified, according to the second speech model for the second person, for audio presentation via the first device; and causing the second speech model to be further trained, during the audio communication, using the uncompressed version of the audio as the uncompressed version is received.
3. The computer-implemented method of claim 1, further comprising: training the speech model to classify one or more features of the decoded audio selected from the group consisting of a pitch, timber, tone, inflection, accent, pattern of speech, or pace of speech of the first person.
4. The computer-implemented method of claim 1, further comprising: initiating the audio transmission as part of a communication session involving at least the first communication device and the second communication device, the communication further including at least video content captured using at least one of the first communication device or the second communication device.
5. The computer-implemented method of claim 1, further comprising: causing the portion of the decoded audio to be modified to cause one or more features of the compressed version to more closely resemble corresponding features of the uncompressed version.
6. A computer-implemented method, comprising: receiving a compressed version and an uncompressed version of an audio signal; training a machine learning model based at least in part on the uncompressed version of the audio signal, the training the machine learning model including an initial training using prior speech data for a person having speech represented in the audio signal; initiating an audio session between a first device, associated with the person, and a second device receiving the audio signal; obtaining a trained speech model for the person; and classifying, substantially contemporaneously with the training of the machine learning model and based at least in part the trained speech model, the compressed version of the audio signal using the trained machine learning model.
7. The computer-implemented method of claim 6, further comprising: identifying the person using at least one of login information for the person, information associated with the first device, or voice recognition.
8. The computer-implemented method of claim 6, further comprising: determining a set of features representative of human speech for use in training the machine learning model, the set of features including at least one of a pitch, timber, tone, inflection, accent, pattern of speech, or pace of speech of the person.
9. The computer-implemented method of claim 6, further comprising: buffering the uncompressed version for use in training the machine learning model; and discarding the uncompressed version after the training.
10. The computer-implemented method of claim 6, further comprising: identifying a plurality of persons having speech represented in the audio signal; and training a respective machine learning model for each of the plurality of persons, the respective machine learning models used to classify portions of the audio corresponding to speech of the corresponding persons.
11. The computer-implemented method of claim 6, further comprising: decoding the compressed version before the classifying.
12. The computer-implemented method of claim 6, further comprising: receiving the compressed version and the uncompressed version contemporaneously over separate communication channels.
13. The computer-implemented method of claim 6, wherein the machine learning model is based on one of a generative adversarial network (GAN), a convolutional neural network (CNN), a recurrent neural network (RNN), transfer learning, domain adaptation, or neural style transfer.
14. A system, comprising: at least one processor; and memory including instructions that, when executed by the system, cause the system to: receive a compressed version and an uncompressed version of an audio signal; train a machine learning model based at least in part on the uncompressed version of the audio signal, the training the machine learning model including an initial training using prior speech data for a person having speech represented in the audio signal; initiate an audio session between a first device, associated with the person, and a second device receiving the audio signal; obtain a trained speech model for the person; and classify, substantially contemporaneously with the training of the machine learning model and based at least in part the trained speech model, the compressed version of the audio signal using the trained machine learning model.
15. The system of claim 14, wherein the instructions when executed further cause the system to: identify the person using at least one of login information for the person, information associated with the first device, or voice recognition.
16. The system of claim 14, wherein the instructions when executed further cause the system to: determine a set of features representative of human speech for use in training the machine learning model, the set of features including at least one of a pitch, timber, tone, inflection, accent, pattern of speech, or pace of speech of the person.
</claims>
</document>
