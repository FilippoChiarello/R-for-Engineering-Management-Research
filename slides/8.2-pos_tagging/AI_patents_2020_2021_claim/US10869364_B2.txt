<document>

<filing_date>
2019-08-14
</filing_date>

<publication_date>
2020-12-15
</publication_date>

<priority_date>
2018-10-05
</priority_date>

<ipc_classes>
G06N20/00,H04W4/00,H04W48/00,H04W48/20,H04W88/14
</ipc_classes>

<assignee>
AIRSPAN NETWORKS
</assignee>

<inventors>
LOGOTHETIS, ANDREW
LIVINGSTONE, MICHAEL DAVID
PARROTT, STUART
KHAN, QASIM
</inventors>

<docdb_family_id>
67734690
</docdb_family_id>

<title>
Apparatus and method for configuring a communication link
</title>

<abstract>
An apparatus and method are provided for configuring a communication link. Wherein the apparatus has a plurality of antenna elements to support RF communication using a plurality of frequency channels, a plurality of RF processing circuits, and configuration circuitry to apply a selected configuration from a plurality of different configurations, where each configuration identifies which RF processing circuit each antenna element coupled to, and which channel allocated to each RF processing circuit. The configuration circuitry arranged to a reinforcement learning process in order to dynamically alter which of the plurality of different configurations to apply a currently selected configuration. The reinforcement learning process maintaining a future rewards record having a plurality of entries, where each entry maintains, for an associated combination of link state and configuration, an estimated future rewards indication determined using a discounted rewards mechanism. A selection policy is to select a configuration for a current link state, and a new reward is observed is dependent on how the selected configuration alters a chosen performance metric for the communication link. The estimated future rewards indication in the associated entry is then updated in dependence on the new reward.
</abstract>

<claims>
1. An apparatus comprising: a plurality of antenna elements to support radio frequency (RF) communication over a communication link using a plurality of frequency channels; a plurality of RF processing circuits for processing RF signals; configuration circuitry to apply a selected configuration from a plurality of different configurations, each configuration identifying which RF processing circuit each antenna element is coupled to, and which frequency channel is allocated to each RF processing circuit; the configuration circuitry being arranged to employ a reinforcement learning process in order to dynamically alter which of the plurality of different configurations to apply as a currently selected configuration, the reinforcement learning process comprising: maintaining a future rewards record having a plurality of entries, each entry being arranged to maintain, for an associated combination of link state and configuration, an estimated future rewards indication determined using a discounted rewards mechanism; employing a selection policy to select a configuration for a current link state; observing a new reward that is dependent on how the selected configuration alters a chosen performance metric for the communication link; and updating the estimated future rewards indication in the associated entry of the future rewards record in dependence on the new reward, the updating comprising, when the associated entry is first encountered following a reset event, storing in the associated entry a predicted estimated future rewards indication generated by assuming, when using the discounted rewards mechanism, that all rewards that will be used in future to update the estimated future rewards indication in the associated entry will have the same value as the new reward.
2. An apparatus as claimed in claim 1, wherein the configuration circuitry is further arranged when employing the reinforcement learning process to: observe a new link state resulting from the selected configuration; and the updating comprises, when the associated entry has previously been encountered following a reset event, but at least one entry associated with the new link state has not yet been encountered following the reset event, to generate an updated estimated future rewards indication to be stored in the associated entry by combining a current estimated future rewards indication stored in the associated entry with the predicted estimated future rewards indication.
3. An apparatus as claimed in claim 2, wherein the combining comprises performing a weighted averaging of the current estimated future rewards indication stored in the associated entry and the predicted estimated future rewards indication.
4. An apparatus as claimed in claim 2, wherein the updating comprises, when the associated entry for the current link state and all active entries associated with the new link state have previously been encountered following the reset event, generating an updated estimated future rewards indication to be stored in the associated entry by combining a current estimated future rewards indication stored in the associated entry with an adjustment value determined from the new reward and the maximum estimated future rewards indication for an entry associated with the new link state.
5. An apparatus as claimed in claim 4, wherein the combining comprises performing a weighted averaging of the current estimated future rewards indication stored in the associated entry and the adjustment value.
6. An apparatus as claimed in claim 3, wherein the weighted averaging is performed using a learning rate indication such that the current estimated future rewards indication is given a higher weighting when a learning rate is lower.
7. An apparatus as claimed in claim 6, wherein the configuration circuitry is further arranged when employing the reinforcement learning process to: perform a learning rate update operation to adjust the learning rate between upper and lower defined bounds in dependence on a difference between the current estimated future rewards indication stored in the associated entry and a predicted estimated future rewards indication.
8. An apparatus as claimed in claim 7, wherein performance of the learning rate update operation causes the learning rate to increase when the difference between the current estimated future rewards indication stored in the associated entry and the predicted estimated future rewards indication is above a threshold.
9. An apparatus as claimed in claim 7, wherein the learning rate is initialised to the lower defined bound.
10. An apparatus as claimed in claim 1, wherein the configuration circuitry is arranged to reinitialise the reinforcement learning process upon occurrence of the reset event, reinitialising of the reinforcement learning process causing at least the entries in the future rewards record to be set to an initial value.
11. An apparatus as claimed in claim 10, wherein the initial value for each valid entry is chosen so as to identify that valid entry as having not yet been encountered.
12. An apparatus as claimed in claim 10, wherein the reset event is caused by at least one of: (i) movement of the apparatus beyond a determined threshold amount; (ii) one or more trigger events associated with an adverse effect on link quality; (iii) a periodic trigger event associated with elapse of a chosen time period.
13. An apparatus as claimed in claim 1, wherein the selection policy is a behaviour policy that employs both exploitation and exploration phases based on a chosen ratio, during the exploitation phase the behaviour policy referring to the future rewards record in order to determine the selected configuration, and during the exploration phase the behaviour policy using an alternative technique to choose the selected configuration.
14. An apparatus as claimed in claim 13, wherein the chosen ratio is a link state dependent chosen ratio.
15. An apparatus as claimed in claim 13, wherein during the exploitation phase, the behaviour policy is arranged to reference the future rewards record in order to identify, from the entries associated with the current link state, an entry having the best estimated future rewards indication, and to then select the configuration associated with that identified entry.
16. An apparatus as claimed in claim 15, wherein the alternative technique comprises randomly selecting a configuration from the plurality of configurations.
17. An apparatus as claimed in claim 1, wherein the configuration circuitry is arranged, when employing the reinforcement learning process, to observe link state information for the communication link in order to determine the current link state.
18. An apparatus as claimed in claim 1, wherein the chosen performance metric is throughput.
19. An apparatus as claimed in claim 1, wherein the communication link provides a backhaul connection from the apparatus to a base station.
20. An apparatus as claimed in claim 1, wherein the configuration circuitry is arranged to perform the reinforcement learning process in order to dynamically change the configuration for: a downlink communication path over the communication link; an uplink communication path over the communication link; both downlink and uplink communication paths over the communication link.
21. A method of operating an apparatus having a plurality of antenna elements to support radio frequency (RF) communication over a communication link using a plurality of frequency channels, and a plurality of RF processing circuits for processing RF signals, the method comprising: employing configuration circuitry to apply a selected configuration from a plurality of different configurations, each configuration identifying which RF processing circuit each antenna element is coupled to, and which frequency channel is allocated to each RF processing circuit; causing the configuration circuitry to employ a reinforcement learning process in order to dynamically alter which of the plurality of different configurations to apply as a currently selected configuration, the reinforcement learning process comprising: maintaining a future rewards record having a plurality of entries, each entry being arranged to maintain, for an associated combination of link state and configuration, an estimated future rewards indication determined using a discounted rewards mechanism; employing a selection policy to select a configuration for a current link state; observing a new reward that is dependent on how the selected configuration alters a chosen performance metric for the communication link; and updating the estimated future rewards indication in the associated entry of the future rewards record in dependence on the new reward, the updating comprising, when the associated entry is first encountered following a reset event, storing in the associated entry a predicted estimated future rewards indication generated by assuming, when using the discounted rewards mechanism, that all rewards that will be used in future to update the estimated future rewards indication in the associated entry will have the same value as the new reward.
22. An apparatus comprising: a plurality of antenna element means for supporting radio frequency (RF) communication over a communication link using a plurality of frequency channels; a plurality of RF processing means for processing RF signals; configuration means for applying a selected configuration from a plurality of different configurations, each configuration identifying which RF processing means each antenna element means is coupled to, and which frequency channel is allocated to each RF processing means; the configuration means for employing a reinforcement learning process in order to dynamically alter which of the plurality of different configurations to apply as a currently selected configuration, the reinforcement learning process comprising: maintaining a future rewards record having a plurality of entries, each entry being arranged to maintain, for an associated combination of link state and configuration, an estimated future rewards indication determined using a discounted rewards mechanism; employing a selection policy to select a configuration for a current link state; observing a new reward that is dependent on how the selected configuration alters a chosen performance metric for the communication link; and updating the estimated future rewards indication in the associated entry of the future rewards record in dependence on the new reward, the updating comprising, when the associated entry is first encountered following a reset event, storing in the associated entry a predicted estimated future rewards indication generated by assuming, when using the discounted rewards mechanism, that all rewards that will be used in future to update the estimated future rewards indication in the associated entry will have the same value as the new reward.
</claims>
</document>
