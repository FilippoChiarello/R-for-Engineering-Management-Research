<document>

<filing_date>
2019-01-11
</filing_date>

<publication_date>
2020-11-18
</publication_date>

<priority_date>
2018-01-11
</priority_date>

<ipc_classes>
G10L13/033,G10L13/08,G10L15/02,G10L25/30
</ipc_classes>

<assignee>
Neosapience, Inc.
</assignee>

<inventors>
KIM, Tae Su
LEE, Young Gun
</inventors>

<docdb_family_id>
67511988
</docdb_family_id>

<title>
TEXT-TO-SPEECH SYNTHESIS METHOD AND APPARATUS USING MACHINE LEARNING, AND COMPUTER-READABLE STORAGE MEDIUM
</title>

<abstract>
A text-to-speech synthesis method using machine learning, the text-to-speech synthesis method is disclosed. The method includes generating a single artificial neural network text-to-speech synthesis model by performing machine learning based on a plurality of learning texts and speech data corresponding to the plurality of learning texts, receiving an input text, receiving an articulatory feature of a speaker, generating output speech data for the input text reflecting the articulatory feature of the speaker by inputting the articulatory feature of the speaker to the single artificial neural network text-to-speech synthesis model.
</abstract>

<claims>
1. A text-to-speech synthesis method using machine learning, the text-to-speech synthesis method comprising: generating a single artificial neural network text-to-speech synthesis model by performing machine learning based on a plurality of learning texts and speech data corresponding to the plurality of learning texts; receiving an input text; receiving an articulatory feature of a speaker; and generating output speech data for the input text reflecting the articulatory feature of the speaker by inputting the articulatory feature of the speaker to the single artificial neural network text-to-speech synthesis model.
2. The text-to-speech synthesis method of claim 1, wherein receiving of the articulatory feature of the speaker comprises: receiving a speech sample; and extracting an embedding vector indicative of the articulatory feature of the speaker from the speech sample.
3. The text-to-speech synthesis method of claim 2, wherein,
extracting the embedding vector indicative of the articulatory feature of the speaker from the speech sample comprises extracting a first sub-embedding vector indicative of a prosody feature of the speaker, wherein the prosody feature includes at least one of information on utterance speed, information on accentuation, information on pause duration, or information on voice pitch, and
generating the output speech data for the input text reflecting the articulatory feature of the speaker comprises generating output speech data for the input text reflecting the prosody feature of the speaker by inputting the first sub-embedding vector indicative of the prosody feature to the single artificial neural network text-to-speech synthesis model.
4. he text-to-speech synthesis method of claim 2, wherein,
extracting the embedding vector indicative of the articulatory feature of the speaker from the speech sample comprises extracting a second sub-embedding vector indicative of an emotion feature of the speaker, wherein the emotion feature includes information on an emotion implied in what the speaker utters, and
generating the output speech data for the input text reflecting the articulatory feature of the speaker comprises generating output speech data for the input text reflecting the emotion feature of the speaker by inputting the second sub-embedding vector indicative of the emotion feature to the single artificial neural network text-to-speech synthesis model.
5. The text-to-speech synthesis method of claim 2, wherein,
extracting the embedding vector indicative of the articulatory feature of the speaker from the speech sample comprises extracting a third sub-embedding vector indicative of a feature related to a voice tone and pitch of the speaker, and
generating the output speech data for the input text reflecting the articulatory feature of the speaker comprises generating output speech data for the input text reflecting the feature related to the voice tone and pitch of the speaker by inputting the third sub-embedding vector indicative of the feature related to the voice tone and pitch of the speaker to the single artificial neural network text-to-speech synthesis model.
6. The text-to-speech synthesis method of claim 2, wherein generating the output speech data for the input text reflecting the articulatory feature of the speaker comprises: receiving an additional input for the output speech data; modifying the embedding vector indicative of the articulatory feature of the speaker based on the additional input; and converting the output speech data into speech data for the input text reflecting information included in the additional input by inputting the modified embedding vector to the single artificial neural network text-to-speech synthesis model.
7. The text-to-speech synthesis method of claim 6, wherein the information included in the additional input for the output speech data comprises at least one of gender information, age information, regional accent information, articulation speed information, voice pitch information, or articulation level information.
8. The text-to-speech synthesis method of claim 2, wherein receiving the speech sample comprises receiving a speech input from the speaker within a predetermined time period as the speech sample in real time.
9. The text-to-speech synthesis method of claim 2, wherein receiving the speech sample comprises receiving a speech input from the speaker within a predetermined time period from a speech database.
10. A computer-readable storage medium having a program recorded thereon, the program comprising instructions of performing operations of the text-to-speech synthesis method using the machine learning of claim 1.
</claims>
</document>
