<document>

<filing_date>
2018-10-19
</filing_date>

<publication_date>
2020-04-23
</publication_date>

<priority_date>
2018-10-19
</priority_date>

<ipc_classes>
G10L15/02,G10L15/06,G10L15/14,G10L15/16,G10L15/18,G10L15/22,G10L15/26,G10L21/10,G10L21/12,G10L21/18,G10L25/63
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
SU, CONGYONG
HUANG, LI
HU, HOUDONG
</inventors>

<docdb_family_id>
68000073
</docdb_family_id>

<title>
Transforming audio content into images
</title>

<abstract>
A technique is described herein for transforming audio content into images. The technique may include: receiving the audio content from a source; converting the audio content into a temporal stream of audio features; and converting the stream of audio features into one or more images using one or more machine-trained models. The technique generates the image(s) based on recognition of: semantic information that conveys one or more semantic topics associated with the audio content; and sentiment information that conveys one or more sentiments associated with the audio content. The technique then generates an output presentation that includes the image(s), which it provides to one or more display devices for display thereat. The output presentation serves as a summary of salient semantic and sentiment-related characteristics of the audio content.
</abstract>

<claims>
1. One or more computing devices for processing digital audio content, comprising: hardware logic circuitry, the hardware logic circuitry corresponding to: (a) one or more hardware processors that perform operations by executing machine-readable instructions stored in a memory, and/or (b) one or more other hardware logic components that perform operations using a task-specific collection of logic gates, the operations including: receiving audio content from a source; forming a temporal stream of audio features, in a series of frames, that represents the audio content; generating one or more images based on the stream of audio features using one or more machine-trained models, said generating being based on recognition of: semantic information that conveys one or more semantic topics associated with the audio content; and sentiment information that conveys one or more sentiments associated with the audio content; producing a graphical output presentation that includes said one or more images; and providing the output presentation to one or more display devices for display thereat, the output presentation serving as a summary of semantic and sentiment-related characteristics of the audio content.
2. The one or more computing devices of claim 1, wherein the operations further include: converting the stream of audio features into text information; and identifying the sentiment information based on the text information and the stream of audio features, wherein said generating comprises generating said one or more images based on the text information and the sentiment information.
3. The one or more computing devices of claim 2, wherein said converting uses a recurrent neural network (RNN).
4. The one or more computing devices of claim 2, wherein said generating uses a machine-trained generative model to synthesize said one or more images.
5. The one or more computing devices of claim 4, wherein the generative model is produced using a training system that employs a generative adversarial network (GAN).
6. The one or more computing devices of claim 2, wherein said generating comprises retrieving one or more preexisting images that match the text information and the sentiment information.
7. The one or more computing devices of claim 2, wherein said one or more images correspond to one or more original images, and wherein the operations further include modifying said one or more original images into one or more style-enhanced images based on the sentiment information.
8. The one or more computing devices of claim 7, wherein said modifying uses a neural network that is trained to duplicate first-level content associated with said one said one or more original images, and second-level content associated with a style image, wherein the first-level content is higher than the second-level content, and wherein the style image is associated with the sentiment information.
9. The one or more computing devices of claim 7, wherein said modifying comprises: identifying an instance of style information from a set of possible instances of style information, based on the sentiment information; and applying the instance of style information that is identified to said one or more original images.
10. The one or more computing devices of claim 7, wherein said generating is performed at a first rate, and said modifying is performed at a second rate, the second rate being greater than the first rate.
11. The one or more computing devices of claim 1, wherein the operations are implemented by an end-to-end machine-trained model that maps the stream of audio features into said one or more images.
12. The one or more computing devices of claim 1, wherein said receiving includes receiving a message from a sender, over a computer network, which contains the audio content, wherein the output presentation corresponds to a user notification that contains said one or more images, the user notification notifying a recipient of the message sent by the sender, and wherein said forming and generating are performed by the recipient of the message or the sender of the message.
13. The one or more computing devices of claim 1, wherein said receiving comprises dynamically receiving the audio content in response to real-time speech of a first user captured by at least one microphone, and wherein said providing comprises providing the output presentation to a second user to assist the second user in understanding the speech of the first user.
14. The one or more computing devices of claim 1, wherein the audio content is associated with an audio file stored in a data store, wherein said receiving comprises accessing the audio file from the data store, wherein the operations further include generating a visual identifier based on the audio content, and associating the visual identifier with the audio file, and wherein the output presentation includes the visual identifier.
15. The one or more computing devices of claim 1, wherein said one or more images correspond to plural images that represent a temporal flow of the semantic information and sentiment information conveyed by the audio content, and wherein the output presentation includes a dynamic presentation of the plural images synchronized with a temporal presentation of the audio content.
16. A method, implemented by one or more computing devices, for processing digital audio content, comprising: receiving audio content from a source; forming a temporal stream of audio features that represents the audio content; converting the stream of audio features into text information using a first machine-trained model; identifying sentiment information based on the text information and the stream of audio features using a second machine-trained model; generating one or more images based on the text information and the sentiment information using a third machine-trained model; producing a graphical output presentation that includes said one or more images; and providing the output presentation to one or more display devices for display thereat.
17. The method of claim 16, wherein said one or more images correspond to one or more original images, and wherein the method further comprises modifying said one or more original images into one or more style-enhanced images based on the sentiment information.
18. The method of claim 16, wherein the first, second, and third machine-trained models are different respective machine-trained models.
19. The method of claim 16, wherein the first, second, and third machine-trained models correspond to parts of a single end-to-end machine-trained model.
20. A computer-readable storage medium for storing computer-readable instructions, the computer-readable instructions, when executed by one or more hardware processors, performing a method that comprises: receiving audio content from a source; forming a temporal stream of audio features that represents the audio content; converting the stream of audio features into text information; identifying sentiment information based on the text information and the stream of audio features; generating one or more images based on the text information and the sentiment information using a machine-trained generative model, wherein the generative model is produced using a training system that employs a generative adversarial network (GAN) system; producing a graphical output presentation that includes said one or more images; and providing the output presentation to one or more display devices for display thereat.
</claims>
</document>
