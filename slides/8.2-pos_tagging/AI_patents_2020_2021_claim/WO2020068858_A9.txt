<document>

<filing_date>
2019-09-24
</filing_date>

<publication_date>
2020-04-23
</publication_date>

<priority_date>
2018-09-24
</priority_date>

<ipc_classes>
G10L13/08,G10L15/00,G10L15/183
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
BRESLIN, CATHERINE
</assignee>

<inventors>
VERMA, ALOK
MOERCHEN, FABIAN
GASPERS, JUDITH
LATIN-STOERMER, TOBY R.
REITER, KLAUS
TRIEFENBACH, FABIAN
BUECHE, EDWARD
FEINSTEIN, JONATHAN B.
DURHAM, BRANDON SCOTT
SHABBEER, AMINA
KARANASOU, PANAGIOTA
BRESLIN, CATHERINE
</inventors>

<docdb_family_id>
68165798
</docdb_family_id>

<title>
TECHNIQUES FOR LANGUAGE MODEL TRAINING FOR A REFERENCE LANGUAGE
</title>

<abstract>
Techniques are provided for training a language recognition model. For example, a language recognition model may be maintained and associated with a reference language (e.g., English). The language recognition model may be configured to accept as input an utterance in the reference language and to identify a feature to be executed in response to receiving the utterance. New language data (e.g., other utterances) provided in a different language (e.g., German) may be obtained. This new language data may be translated to English and utilized to retrain the model to recognize reference language data as well as language data translated to the reference language. Subsequent utterances (e.g., English utterances, or German utterances translated to English) may be provided to the updated model and a feature may be identified. One or more instructions may be sent to a user device to execute a set of instructions associated with the feature.
</abstract>

<claims>
WHAT IS CLAIMED IS:
1. A computer-implemented method, comprising:
maintaining, by a computing device, a language recognition model associated with a reference language, the language recognition model configured to accept an utterance in the reference language as input and provide as output a feature to be executed by a user device;
obtaining, by the computing device, a user-provided utterance of a target language, the target language being different from the reference language;
generating, by the computing device, a set of attributes for the user-provided utterance;
generating, by the computing device, training data by at least machine translating the user-provided utterance and the set of attributes to the reference language;
updating the language recognition model utilizing the generated training data; receiving, by the computing device, subsequent user input comprising a subsequent utterance; and
sending, by the computing device to a user device, instructions to execute subsequent features based at least in part on the subsequent utterance and the updated language recognition model.
2. The computer-implemented method of claim 1, further comprising:
identifying, by the computing device, a frequency at which the user-provided utterance has been historically received as user input;
obtaining, by the computing device, a success criteria associated with a feature associated with the user-provided utterance;
calculating, by the computing device, a success score for the user-provided utterance based at least in part on the frequency and the success criteria; and
excluding, by the computing device, the user-provided utterance or at least one of the set of attributes associated with the user-provided utterance from the training data.
3. The computer-implemented method of claim 1, wherein generating the training data further comprises:
obtaining, by the computing device, historical language data utilized to train the language recognition model;
identifying, by the computing device, a carrier phrase from the historical language data or the user-provided utterance; generating, by the computing device, a plurality of utterances based at least in part on the carrier phrase identified; and
including, by the computing device, the plurality of utterances in the training data
4. A computing device, comprising:
a processor; and
a memory storing computer-readable instructions that, upon execution by the processor, configure the computing device to:
maintain a language recognition model associated with a reference language, the language recognition model configured to accept a first utterance in the reference language as input and provide as output a feature to be executed by a user device;
obtain target language data of a target language, the target language data comprising a second utterance in the target language and a set of attributes associated with the second utterance;
generate training data for the language recognition model based at least in part by machine translating the target language data to the reference language;
update the language recognition model utilizing the training data; and send an instruction to execute a subsequent feature at a user device based at least in part on a received utterance and the updated language recognition model.
5. The computing device of claim 4, wherein the computing device is further configured to:
deploy the updated language recognition model to a plurality of user devices; obtain, from the user device, user input of the target language, the user input comprising an utterance provided by a user, the utterance being in the target language;
translate the user input from the target language to the reference language;
provide the translated user input to the language recognition model; receive output of the language recognition model, the output indicating a particular feature to be executed by the user device; and
send instructions for the particular feature to be executed by the user device.
6. The computing device of claim 5, wherein the computing device is further configured to:
identify success criteria associated with the feature executed by the user device; determine that the feature executed by the user device was incorrect based at least in part on the success criteria; and
request feedback from the user device related to the user input.
7. The computing device of claim 6, wherein requesting feedback from the user device further comprises:
identifying a number of questions related to at least one attribute associated with user input; and
providing at least one of the number of questions to the user device.
8. The computing device of claim 7, wherein the computing device is further configured to:
receive at least one answer corresponding the at least one of the number of questions presented to the user device;
determine whether one or more attributes associated with the user input is inaccurate based at least in part on the at least one answer;
modify the one or more attributes based at least in part on determining that the one or more attributes associated with the user input is inaccurate; and
update the updated language recognition model based at least in part on the one or more attributes that were modified.
9. The computing device of claim 4, wherein the computing device is further configured to:
deploy the updated language recognition model to a plurality of user devices; obtain, from the user device, user input of the target language, the user input comprising an utterance provided by a user, the utterance being in the target language;
translate the user input from the target language to the reference language;
identify success criteria associated with a particular feature executed based on the user input;
determine that the particular feature executed by a user device was incorrect based at least in part on the success criteria; and
exclude the user input from subsequent training data utilized to further update the updated language recognition model.
10. The computing device of claim 4, wherein the computing device is further configured to:
obtain historical language data utilized to train the language recognition model; identify a carrier phrase from the historical language data or the training data; generate a plurality of utterances based at least in part on the carrier phrase identified; and
include the plurality of utterances in the training data.
11. The computing device of claim 10, wherein the plurality of utterances are generated by:
identifying a set of catalog attributes from catalog data; and
combining the identified carrier phrase with each attribute of the set of catalog attributes to generate the plurality of utterances.
12. A computer-readable storage medium having stored thereon computerexecutable instructions that, when executed by a processor, cause the processor to perform operations comprising:
maintaining a language recognition model associated with a reference language, the language recognition model configured to accept a first utterance in the reference language as input and provide as output a feature to be executed by a user device;
obtaining target language data of a target language, the target language data comprising a second utterance in the target language and a set of attributes associated with the second utterance;
generating training data for the language recognition model based at least in part by machine translating the target language data to the reference language;
updating the language recognition model utilizing the training data; and send instructions to execute a subsequent feature at a user device based at least in part on a subsequent utterance and the updated language recognition model.
13. The computer-readable storage medium of claim 12, wherein the processor performs additional operations comprising:
obtaining a user-provided translation in the target language, the user-provided translation being associated with the target language data;
comparing the user-provided translation to a machine-provided translation of the training data; and
excluding the machine-provided translation from the training data based at least in part on the comparison.
14. The computer-readable storage medium of claim 12, wherein the set of attributes associated with the second utterance comprises a feature identifier of the user device, and wherein the feature identifier is associated with a set of instructions that are executable by the user device. 15. The computer-readable storage medium of claim 14, wherein the processor performs additional operations comprising:
maintaining a plurality of language recognition models associated with respective reference languages; and
selecting the language recognition model from the plurality of language recognition models based at least in part on a predetermined association between the reference language and the target language.
</claims>
</document>
