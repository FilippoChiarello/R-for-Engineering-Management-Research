<document>

<filing_date>
2020-03-18
</filing_date>

<publication_date>
2020-09-24
</publication_date>

<priority_date>
2019-03-21
</priority_date>

<ipc_classes>
G06K9/00,G06K9/62,G06T7/73
</ipc_classes>

<assignee>
TOYOTA RESEARCH INSTITUTE
</assignee>

<inventors>
HASHIMOTO, KUNIMATSU
TEDRAKE, RUSSELL L.
Cousineau, Eric A.
Huynh, Duy-Nguyen Ta
</inventors>

<docdb_family_id>
72515417
</docdb_family_id>

<title>
SYSTEMS, DEVICES, AND METHODS FOR GENERATING A POSE ESTIMATE OF AN OBJECT
</title>

<abstract>
In an embodiment, a pose estimation device obtains an image of an object, and generates a pose estimate of the object. The pose estimate includes a respective heatmap for each of a plurality of pose components of a pose of the object, and the respective heatmap for each of the pose components includes a respective uncertainty indication of an uncertainty of the pose component at each of one or more pixels of the image.
</abstract>

<claims>
1. A method carried out by a pose estimation device, the method comprising: obtaining an image of an object; and generating a pose estimate of the object, the pose estimate comprising a respective heatmap for each of a plurality of pose components of a pose of the object, the respective heatmap for each of the pose components comprising a respective uncertainty indication of an uncertainty of the pose component at each of one or more pixels of the image.
2. The method of claim 1, wherein the plurality of pose components comprises an elevation of the object, an azimuth of the object, an object center of the object, an object north point of the object, an in-plane rotation of the object, and an object boundary of the object.
3. The method of claim 2, wherein: a respective elevation heatmap of the elevation of the object comprises a respective three-dimensional heatmap, a respective azimuth heatmap of the azimuth of the object comprises a respective three-dimensional heatmap, a respective keypoint heatmap of the object center of the object comprises a respective two-dimensional heatmap, a respective north keypoint heatmap of the object north point of the object comprises a respective two-dimensional heatmap, a respective in-plane rotation heatmap of the in-plane rotation of the object comprises a respective three-dimensional heatmap, and a respective object boundary heatmap of the object boundary of the object comprises a respective two-dimensional heatmap.
4. The method of claim 3, wherein: a first dimension and a second dimension of the heatmaps of the pose correspond to a first axis and a second axis of the image, respectively, a third dimension of the elevation heatmap corresponds to an elevation angle of the object, a third dimension of the azimuth heatmap corresponds to an azimuth angle of the object, and a third dimension of the in-plane rotation heatmap corresponds to an in-plane rotation angle of the object.
5. The method of claim 4, wherein the respective heatmap for each of the pose components comprises a respective grid of heatmap components that is scaled along the first dimension and the second dimension with respect to a grid of pixels that form the image.
6. The method of claim 4, wherein: a three-dimensional object space comprises a three-dimensional representation of the object, an origin of the object space corresponds to the object center of the object, a two-dimensional camera plane comprises a two-dimensional projection of the three-dimensional object space onto the camera plane, obtaining the image of the object comprises obtaining the image via a sensor, and a position of a camera point in the object space corresponds to a position of the image sensor in the scene.
7. The method of claim 6, wherein: generating the pose estimate comprises estimating a sphere boundary of the object in the object space, wherein the uncertainty indications of the object boundary heatmap are based on a two-dimensional projection of the sphere boundary onto the camera plane, and the method further comprises: obtaining an estimated object radius based on the object boundary heatmap; identifying a three-dimensional object model of the object from among a database of object models; and based on a comparison of the obtained estimated radius to a size of the identified object model, estimating both a scale of the object and a distance between the object and the image sensor.
8. A pose estimation device comprising a processor and a non-transitory computer-readable storage medium having instructions that, when executed by the processor, cause the pose estimation device to: obtain an image of an object; and generate a pose estimate of the object, the pose estimate comprising a respective heatmap for each of a plurality of pose components of a pose of the object, the respective heatmap for each of the pose components comprising a respective uncertainty indication of an uncertainty of the pose component at each of one or more pixels of the image.
9. The pose estimation device of claim 8, wherein the plurality of pose components comprises an elevation of the object, an azimuth of the object, an object center of the object, an object north point of the object, an in-plane rotation of the object, and an object boundary of the object.
10. The pose estimation device of claim 9, wherein: a respective elevation heatmap of the elevation of the object comprises a respective three-dimensional heatmap, a respective azimuth heatmap of the azimuth of the object comprises a respective three-dimensional heatmap, a respective keypoint heatmap of the object center of the object comprises a respective two-dimensional heatmap, a respective north keypoint heatmap of the object north point of the object comprises a respective two-dimensional heatmap, a respective in-plane rotation heatmap of the in-plane rotation of the object comprises a respective three-dimensional heatmap, and a respective object boundary heatmap of the object boundary of the object comprises a respective two-dimensional heatmap.
11. The pose estimation device of claim 10, wherein: a first dimension and a second dimension of the heatmaps of the pose correspond to a first axis and a second axis of the image, respectively, a third dimension of the elevation heatmap corresponds to an elevation angle of the object, a third dimension of the azimuth heatmap corresponds to an azimuth angle of the object, and a third dimension of the in-plane rotation heatmap corresponds to an in-plane rotation angle of the object.
12. The pose estimation device of claim 11, wherein the respective heatmap for each of the pose components comprises a respective grid of heatmap components that is scaled along the first dimension and the second dimension with respect to a grid of pixels that form the image.
13. The pose estimation device of claim 11, wherein: a three-dimensional object space comprises a three-dimensional representation of the object, an origin of the object space corresponds to the object center of the object, a two-dimensional camera plane comprises a two-dimensional projection of the three-dimensional object space onto the camera plane, the instructions to obtain the image of the object comprise instructions that cause the pose estimation device to obtain the image via a sensor, and a position of a camera point in the object space corresponds to a position of the image sensor in the scene.
14. The pose estimation device of claim 13, wherein: generating the pose estimate comprises estimating a sphere boundary of the object in the object space, wherein the uncertainty indications of the object boundary heatmap are based on a two-dimensional projection of the sphere boundary onto the camera plane, and the method further comprises: obtaining an estimated object radius based on the object boundary heatmap; identifying a three-dimensional object model of the object from among a database of object models; and based on a comparison of the obtained estimated radius to a size of the identified object model, estimating both a scale of the object and a distance between the object and the image sensor.
15. A method carried out by a pose estimation device, the method comprising: generating one or more training images, each comprising a two-dimensional projection of an object model in a respective pose; generating a respective heatmap set for each of the training images, the respective heatmap set comprising a respective ground-truth uncertainty heatmap for each of a plurality of pose components of the respective pose of the object model projected onto the training image, the respective ground-truth uncertainty heatmap comprising a respective uncertainty assignment of an uncertainty of the pose component at each of one or more pixels of the respective training image; training a neural network based on the training images and the ground-truth uncertainty heatmaps in the respective heatmap sets generated for the training images; receiving an image of an object obtained via a sensor; and generating a respective uncertainty heatmap via the neural network for each of a plurality of pose components of a pose of the object, the respective uncertainty heatmap comprising a respective uncertainty indication of an uncertainty of the pose component at each of one or more pixels of the image.
16. The method of claim 15, wherein the plurality of pose components of the object comprise an elevation of the object, an azimuth of the object, an object center of the object, an object north point of the object, an in-plane rotation of the object, and an object boundary of the object.
17. The method of claim 16, wherein: a respective elevation heatmap of the elevation of the object comprises a respective three-dimensional heatmap, a respective azimuth heatmap of the azimuth of the object comprises a respective three-dimensional heatmap, a respective keypoint heatmap of the object center of the object comprises a respective two-dimensional heatmap, a respective north keypoint heatmap of the object north point of the object comprises a respective two-dimensional heatmap, a respective in-plane rotation heatmap of the in-plane rotation of the object comprises a respective three-dimensional heatmap, and a respective object boundary heatmap of the object boundary of the object comprises a respective two-dimensional heatmap.
18. The method of claim 17, wherein: a first dimension and a second dimension of the heatmaps of the generated pose estimate correspond to a first axis and a second axis of the image, respectively, a third dimension of the elevation heatmap corresponds to an elevation angle of the object, a third dimension of the azimuth heatmap corresponds to an azimuth angle of the object, and a third dimension of the in-plane rotation heatmap corresponds to an in-plane rotation angle of the object.
19. The method of claim 18, wherein: a three-dimensional object space comprises a three-dimensional representation of the object, an origin of the object space corresponds to the object center of the object, a two-dimensional camera plane comprises a two-dimensional projection of the three-dimensional object space onto the camera plane, and a position of a camera point in the object space corresponds to a position of the image sensor in the scene.
20. The method of claim 19, wherein: generating the pose estimate comprises estimating a sphere boundary of the object in the object space, wherein the uncertainty indications of the object boundary heatmap are based on a two-dimensional projection of the sphere boundary onto the camera plane, and the method further comprises: obtaining an estimated object radius based on the object boundary heatmap; identifying a three-dimensional object model of the object from among a database of object models; and based on a comparison of the obtained estimated radius to a size of the identified object model, estimating both a scale of the object and a distance between the object and the image sensor.
</claims>
</document>
