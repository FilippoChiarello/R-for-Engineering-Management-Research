<document>

<filing_date>
2013-08-26
</filing_date>

<publication_date>
2020-05-12
</publication_date>

<priority_date>
2013-08-26
</priority_date>

<ipc_classes>
G06K9/00,G06K9/46,G06K9/62,G06T7/00,H04N7/18
</ipc_classes>

<assignee>
HE CHAO
YU MENG
NCR CORPORATION
AMORA, KRISTOFFER DOMINIC
PARAC, EM
QUIPANES, RICHARD C.
</assignee>

<inventors>
HE CHAO
YU MENG
AMORA, KRISTOFFER DOMINIC
PARAC, EM
QUIPANES, RICHARD C.
</inventors>

<docdb_family_id>
52480012
</docdb_family_id>

<title>
Produce and non-produce verification using hybrid scanner
</title>

<abstract>
Approaches to produce verification are addressed which may be suitably employed in conjunction with a low resolution image scanner. Features, such as lines, corners, text, texture, edges and the like are extracted and compared with features for an identified item or evaluated to determine if the features collectively are indicative of a produce item or a non-produce item.
</abstract>

<claims>
We claim:
1. Apparatus for verifying whether an item is produce or not comprising: a scale configured to weigh produce items; a hybrid scanner including: i) a laser scanner configured to read optical codes affixed to the items and 2) a camera configured to generate an image of the item when the item is placed within a field of view of the camera on the scale, wherein the hybrid scanner is configured to read the optical codes and generate the image at the same time; an illumination mechanism configured to selectively illuminate the field of view of the camera; a produce verification model database configured to store data for produce items to be verified; and a processor that executes software, which causes the processor to: control the camera and the illumination mechanism; retrieve a model for a particular item of produce to be weighed from the produce verification model database; control the camera to capture image data for the item; extract features from the captured image data; use the features to classify the image into categories representative of a produce item or a non-produce item based on: line features, corner features, edge features, texture features, and any characters or text features; wherein a total number of the line features is a factor relevant to determining the non-produce item as compared to the produce item, wherein a total number of the corner features is a second factor relevant to determining the non-produce item as compared to the produce item, wherein a total number of uniform patterns within the texture features is a third factor relevant to determining the produce item as compared to the non-produce item, and wherein a total number of the characters or the text features is fourth factor relevant to determining the non-produce item as compared to the produce item; use the classified image to sub-classify the image from the categories into sub-categories based on particular extracted features representing 1) packaging materials for the item that distinguishes between boxes, bottles, and cans, 2) a design of the packaging materials that distinguishes between high-contrast or low-contrast present in the image, 3) item object shape, 4) item object amount indicating a single item or a bundle of multiple items; fit the extracted features, the categories, and sub-categories to a retrieved model for the particular item of produce; and verify the item as produce or not based upon the fit of the extracted features to the retrieved model.
2. The apparatus of claim 1 wherein the processor further operates to verify the item is the particular item or not.
3. The apparatus of claim 1 further comprising: a touchscreen, and wherein the particular item is identified by a selection by a user from a menu displayed on the touchscreen.
4. The apparatus of claim 1 wherein if the item is not identified as produce, the processor communicates a potential error message to an attendant display station.
5. The apparatus of claim 1 wherein the illumination mechanism comprises a plurality of light emitting diodes (LEDs); and the processor controls the camera and LEDs to generate illuminated and non-illuminated background images, and illuminated and non-illuminated item images.
6. The apparatus of claim 5 wherein the processor computes a first difference of the two background images, bksub, and a second difference of the two item images, imgsub.
7. The apparatus of claim 6 wherein the processor subtracts bksub from imgsub to obtain, img, a difference of the first and second differences.
8. The apparatus of claim 7 wherein the camera comprises a low resolution gray scale camera also utilized to perform reading of two dimensional bar codes.
9. The apparatus of claim 8 wherein the processor generates a binary item segmentation map by thresholding img.
10. The apparatus of claim 1 wherein the processor extracts the line features, the corner features, the edge features and the texture features and applies feature detectors to select a group of most discriminating features.
11. Apparatus for categorizing an item as produce or non-produce comprising: a hybrid scanner including: i) a laser scanner configured to read optical codes affixed to the item, and ii) a camera configured to generate an image of the item when the item is placed within a field of view of the camera on the scale, and wherein the hybrid scanner is configured to read the optical codes and generate the image at the same time; a categorization database configured to store data for produce and non-produce items to be categorized; and a processor configured to execute software, which causes the processor to: retrieve product categorization criteria from the categorization database; control the camera to capture image data for the item; extract features from the captured image data including line features, corner features, edge features, texture features, and any characters or text features; apply a retrieved categorization criteria; and categorize the item as produce or non-produce based upon the application of the categorization criteria using the extracted features from the captured image data; wherein a total number of the line features is a first factor relevant to determining the non-produce as compared to the produce, wherein a total number of the corner features is a second factor relevant to determining of the non-produce as compared to the produce item, wherein a total number of uniform patterns within the texture features is third factor relevant to determining the produce as compared to non-produce, and wherein a total number any of the characters or the text features is a fourth factor relevant to the non-produce as compared to the produce; wherein classify includes processing to further sub-classify the item based the extracted features into sub-categories identified from: 1) packaging materials with the item that distinguishes between boxes, bottles, and cans, 2) a package design for the packaging materials that distinguishes between high-contrast or low-contrast present in the image, 3) item object shape, and 4) item object amount representing a single item or a bundle of multiple items.
12. The apparatus of claim 11 further comprising: an illumination mechanism for selectively illuminating the field of view of the camera, and wherein the processor operates to control the illumination mechanism.
13. The apparatus of claim 12 wherein the illumination mechanism comprises a plurality of light emitting diodes (LEDs); and the processor controls the camera and LEDs to generate illuminated and non-illuminated background images, and illuminated and non-illuminated item images.
14. The apparatus of claim 13 wherein the processor computes a first difference of the two background images, bksub, and a second difference of the two item images, imgsub.
15. The apparatus of claim 14 wherein the processor subtracts bksub from imgsub to obtain, img, a difference of the first and second differences.
16. The apparatus of claim 15 wherein the camera comprises a low resolution gray scale camera also utilized to perform reading of two dimensional bar codes.
17. The apparatus of claim 16 wherein the processor generates a binary item segmentation map by thresholding img.
18. The apparatus of claim 17 wherein the processor extracts item pixels using the segmentation map.
19. The apparatus of claim 18 wherein the processor extracts the line features, the corner features, the edge features and the texture features and applies feature detectors to select a group of most discriminating features.
20. The apparatus of claim 19 wherein the processor applies a probabilistic approach to classification and further classifies the item into a sub-category.
</claims>
</document>
