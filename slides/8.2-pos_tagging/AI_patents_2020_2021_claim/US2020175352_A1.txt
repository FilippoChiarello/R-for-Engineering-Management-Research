<document>

<filing_date>
2019-09-13
</filing_date>

<publication_date>
2020-06-04
</publication_date>

<priority_date>
2017-03-14
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,G06T7/00
</ipc_classes>

<assignee>
UNIVERSITY OF MANITOBA
</assignee>

<inventors>
CHA, YOUNG JIN
CHOI, WOORAM
</inventors>

<docdb_family_id>
63521698
</docdb_family_id>

<title>
STRUCTURE DEFECT DETECTION USING MACHINE LEARNING ALGORITHMS
</title>

<abstract>
Structure defect detection is performed using computer-implemented arrangements employing machine learning algorithms in the form of neural networks. In one arrangement, a convolutional neural network is trained using a database of images formed to optimize accuracy of the convolutional neural network to detect, for example, a crack in a concrete surface. A two-stage scanning process each performing a plurality of scans of a test image is incorporated in the foregoing arrangement of convolutional neural network, with the two-stages forming overlapping capture areas to reduce likelihood of a crack lying on a boundary of the individual scans going undetected. Also, region-based convolutional neural networks are trained to detect various types of defects.
</abstract>

<claims>
1. A computer-implemented method of analyzing an image of a surface to detect a defect in the surface, comprising: receiving the image of the surface; processing the image using a machine learning algorithm configured to detect the defect, the machine learning algorithm comprising a convolutional neural network including: at least one convolution layer; and at least one max pooling layer; and displaying the image with location of the defect being indicated if determined to be present by the convolutional neural network.
2. The computer-implemented method of claim 1 wherein said processing includes: scanning a first set of non-overlapping areas of the image; scanning a second set of non-overlapping areas of the image each which overlap more than one of the first set of non-overlapping areas so as to capture information at edges of the first set of non-overlapping areas which is otherwise unavailable to the convolutional neural network from the scanning of the first set of non-overlapping areas.
3. The computer-implemented method of claim 1 wherein the convolutional neural network comprises: an input layer having a height of n pixels, a width of n pixels, and a depth of d channels; said at least one convolution layer comprising a preliminary convolution layer, a secondary convolution layer, a tertiary convolution layer and a quaternary convolution layer; said at least one pooling layer comprising a preliminary pooling layer and a secondary pooling layer; the preliminary convolution layer having a height of Hc1 pixels, a width of Hc1 pixels, and a depth of Dc1 channels formed by a first convolution operator having a height of hc1 pixels, a width of hc1 pixels, and a depth of dc1 channels with a stride of sc1 performed upon the input layer;
description="In-line Formulae" end="lead"?wherein Hc1=[(n−hc1)/sc1]+1;description="In-line Formulae" end="tail"?
description="In-line Formulae" end="lead"?wherein Dc1=dc1;description="In-line Formulae" end="tail"? the preliminary pooling layer having a height of Hp1 pixels, a width of Hp1 pixels, and a depth of Dp1 channels formed by a first pooling operator having a height of hp1 pixels and a width of hp1 pixels with a stride of sp1 performed on the preliminary convolution layer;
description="In-line Formulae" end="lead"?wherein Hp1=[(Hc1−hp1)/sp1]+1;description="In-line Formulae" end="tail"?
description="In-line Formulae" end="lead"?wherein Dp1=Dc1;description="In-line Formulae" end="tail"? the secondary convolution layer having a height of Hc2 pixels, a width of Hc2 pixels, and a depth of Dc2 channels formed by a second convolution operator having a height of hc2 pixels, a width of hc2 pixels, and a depth of dc2 channels with a stride of sc2 performed upon the preliminary pooling layer;
description="In-line Formulae" end="lead"?wherein Hc2=[(Hp1−hc2)/sc2]+1;description="In-line Formulae" end="tail"?
description="In-line Formulae" end="lead"?wherein Dc2=dc2 description="In-line Formulae" end="tail"? the secondary pooling layer having a height of Hp2 pixels, a width of Hp2 pixels, and a depth of Dp2 channels formed by a second pooling operator having a height of hp2 pixels and a width of hp2 pixels with a stride of sp2 performed upon the secondary convolution layer;
description="In-line Formulae" end="lead"?wherein Hp2=[(Hc1−hp2)/sp2]+1;description="In-line Formulae" end="tail"?
description="In-line Formulae" end="lead"?wherein Dp2=Dc2 description="In-line Formulae" end="tail"? the tertiary convolution layer having a height of Hc3 pixels, a width of Hc3 pixels, and a depth of Dc3 channels formed by a third convolution operator having a height of hc3 pixels, a width of hc3 pixels, and a depth of dc3 channels with a stride of sc3 that is performed upon the secondary pooling layer;
description="In-line Formulae" end="lead"?wherein Hc3=[(Hp2−hc3)/sc3]+1;description="In-line Formulae" end="tail"?
description="In-line Formulae" end="lead"?wherein Dc3=dc3;description="In-line Formulae" end="tail"? an activation layer having a height of Ha1 pixels, a width of Ha1 pixel, and a depth of Da1 channels formed by a nonlinear activation function operator performed upon the tertiary convolution layer;
description="In-line Formulae" end="lead"?wherein Ha1=Hc3;description="In-line Formulae" end="tail"?
description="In-line Formulae" end="lead"?wherein Da1=Dc3;description="In-line Formulae" end="tail"? the quaternary convolution layer having a height of Hc4 pixels, a width of Hc4 pixels, and a depth of Dc4 channels formed by a fourth convolution operator having a height of hc4 pixel, a width of hc4 pixel, and a depth of dc4 channels with a stride of sc4 performed upon the activation layer;
description="In-line Formulae" end="lead"?wherein Hc4=[(Ha1−hc4)/sc4]+1;description="In-line Formulae" end="tail"?
description="In-line Formulae" end="lead"?wherein Dc4=dc4;description="In-line Formulae" end="tail"? and a softmax layer having a height of Sm1 pixels, a width of Sm1 pixels, and a depth of Dsm1 channels formed by a softmax operator performed upon the quaternary convolution layer such that a continuously extending line in an image can be detected;
description="In-line Formulae" end="lead"?wherein Sm1=Hc4;description="In-line Formulae" end="tail"?
description="In-line Formulae" end="lead"?wherein Dsm1=Dc4.description="In-line Formulae" end="tail"?
4. The computer-implemented method of claim 3 wherein the first convolution operator has a height of 20 pixels, a width of 20 pixels, and a depth of 3 channels with a stride of 2.
5. The computer-implemented method of claim 3 wherein the first pooling operator has a height of 7 pixels and a width of 7 pixels with a stride of 2.
6. The computer-implemented method of claim 3 wherein the second convolution operator has a height of 15 pixels, a width of 15 pixels, and a depth of 24 channels with a stride of 2.
7. The computer-implemented method of claim 3 wherein the second pooling operator has a height of 4 pixels and a width of 4 pixels with a stride of 2.
8. The computer-implemented method of claim 3 wherein the third convolution operator has a height of 10 pixels, a width of 10 pixels, and a depth of 48 channels with a stride of 2.
9. The computer-implemented method of claim 3 wherein the fourth convolution operator has height of 1 pixel, a width of 1 pixel, and a depth of 96 channels with a stride of 1.
10. The computer-implemented method of claim 1 wherein the convolutional neural network comprises: an input layer having a height of 256 pixels, a width of 256 pixels, and a depth of 3 channels; said at least one convolution layer comprising a preliminary convolution layer, a secondary convolution layer, a tertiary convolution layer and a quaternary convolution layer; said at least one pooling layer comprising a preliminary pooling layer and a secondary pooling layer; the preliminary convolution layer having a height of 119 pixels, a width of 119 pixels, and a depth of 24 channels formed by a first convolution operator having a height of 20 pixels, a width of 20 pixels, and a depth of 3 channels with a stride of 2 performed upon the input layer; the preliminary pooling layer having a height of 57 pixels, a width of 57 pixels, and a depth of 24 channels formed by a first pooling operator having a height of 7 pixels and a width of 7 pixels with a stride of 2 performed on the preliminary convolution layer; the secondary convolution layer having a height of 22 pixels, a width of 22 pixels, and a depth of 48 channels formed by a second convolution operator having a height of 15 pixels, a width of 15 pixels, and a depth of 24 channels with a stride of 2 performed upon the preliminary pooling layer; the secondary pooling layer having a height of 10 pixels, a width of 10 pixels, and a depth of 48 channels formed by a second pooling operator having a height of 4 pixels and a width of 4 pixels with a stride of 2 performed upon the secondary convolution layer; the tertiary convolution layer having a height of 1 pixel, a width of 1 pixel, and a depth of 96 channels formed by a third convolution operator having a height of 10 pixels, a width of 10 pixels, and a depth of 48 channels with a stride of 2 performed upon the secondary pooling layer; an activation layer having a height of 1 pixel, a width of 1 pixel, and a depth of 96 channels formed by a nonlinear activation function operator performed upon the tertiary convolution layer; the quaternary convolution layer having a height of 1 pixel, a width of 1 pixel, and a depth of 2 channels formed by a fourth convolution operator having a height of 1 pixel, a width of 1 pixel, and a depth of 96 channels with a stride of 1 performed upon the activation layer; and a softmax layer having a height of 1 pixel, a width of 1 pixel, and a depth of 2 channels formed by a softmax operator performed upon the quaternary convolution layer such that a continuously extending line in an image can be detected.
11. The computer-implemented method of claim 3 wherein the convolutional neural network further comprises a dropout layer intermediate the tertiary convolution layer and the activation layer.
12. The computer-implemented method of claim 3 wherein the nonlinear activation function operator comprises a rectified linear unit function.
13. The computer-implemented method of claim 2 wherein the first set of non-overlapping areas are arranged such that each one thereof is contiguous with at least one other of the first set at an edge of said each one of the first set.
14. The computer-implemented method of claim 2 wherein the second set of non-overlapping areas are arranged such that each one thereof is contiguous with at least one other of the second set at an edge of said each one of the second set.
15. The computer-implemented method of claim 2 wherein scanning the first set of non-overlapping areas captures an entirety of the image.
16. The computer-implemented method of claim 2 wherein scanning the second set of non-overlapping areas captures only a portion of the image.
17. The computer-implemented method of claim 16 wherein said portion of the image captured by the second set of non-overlapping areas has a periphery which substantially follows a periphery of the image, the periphery of the said portion of the image extending through each one of a series of the first set of non-overlapping areas arranged along the periphery of the image.
18. The computer-implemented method of claim 1 wherein when the defect to be detected includes a crack, the convolutional neural network is trained by programming instructions stored on a computer readable medium comprising: a data structure including a set of training images each having a top edge, a bottom edge, and opposite sides edges; wherein a first portion of the set of training images includes a crack and a second portion of the set of training images lack a crack; the first portion of the set of training images comprising: a first category of crack-containing images having a crack extending substantially horizontally across the image in a direction from one side edge to the other and spaced from the top and bottom edges of the image; a second category of crack-containing images having a crack extending substantially vertically across the image in a direction from the top edge to the bottom edge and spaced from the side edges of the image; a third category of crack-containing images having a crack extending diagonally across the image such that terminal ends of the crack are spaced substantially vertically and substantially horizontally apart from one another that is located within a region of the respective image spanning vertically between the top and bottom edges and horizontally between the side edges but excluding triangular areas each at one corner of the image formed by a portion of each of two edges of the image and a diagonal line interconnecting said portions; and training instructions stored on the medium and executable by a computer processor for training the convolutional neural network with said data structure so that the convolutional neural network is enabled to detect the crack in the image of the surface.
19. The computer-implemented method of claim 18 wherein the ratio of a number of images forming each of the first portion of the set of training images including a crack and the second portion of the set of training images lacking a crack is 1:1.
20. The computer-implemented method of claim 18 wherein each of the set of training images has a 1:1 aspect ratio.
21. The computer-implemented method of claim 18 wherein the set of training images is formed from a plurality of photographs cropped to form smaller images each having a 1:1 aspect ratio.
22. The computer-implemented method of claim 21 wherein each photograph forms a plurality of smaller images with 1:1 aspect ratio.
23. The computer-implemented method of claim 18 wherein the cracks in each of the first category of crack-containing images, the second category of crack-containing images and the third category of crack-containing images are located generally centrally with respect to a direction transverse to the direction in which a respective one of the cracks extends across a respective one of the training images.
24. The computer-implemented method of claim 1 wherein the machine learning algorithm comprises a region-based convolutional neural network which includes the convolutional neural network, the region-based convolutional neural network further including: a region of interest pooling layer for receiving regions of interest of a feature map formed by an output of the convolutional neural network and by object proposals generated by a selective search performed on the image, the regions of interest being delimited by the object proposals, and for generating feature vectors; a set of fully connected layers for receiving the feature vectors; and parallel softmax and regressor layers after the set of fully connected layers for classifying and identifying the defect which can be found in the image.
25. The computer-implemented method of claim 24 wherein the convolutional neural network includes: a first convolutional layer; a second convolutional layer; a first max pooling layer after the second convolutional layer; a third convolutional layer after the first max pooling layer; a fourth convolutional layer; a second max pooling layer after the fourth convolutional layer; a fifth convolutional layer after the second max pooling layer; a sixth convolutional layer; a seventh convolutional layer; a third max pooling layer after the fourth convolutional layer; an eighth convolutional layer after the third max pooling layer; a ninth convolutional layer; a tenth convolutional layer; a fourth max pooling layer after the tenth convolutional layer; an eleventh convolutional layer after the fourth max pooling layer; a twelfth convolutional layer; and a thirteenth convolutional layer; the region of interest pooling layer of the convolutional network being after the thirteenth convolutional layer.
26. The computer-implemented method of claim 24 wherein the region-based convolutional neural network is a first region-based convolutional neural network of the machine learning algorithm; and wherein the machine learning algorithm includes a second region-based convolutional neural network including the first region-based neural network and a region proposal network; the first region-based neural network and the region proposal network sharing said at least one convolutional layer and said at least one max pooling layer which collectively define shared convolutional neural network layers; the region proposal network including, in addition to the shared convolutional neural network layers: a sliding convolutional layer after the shared convolutional neural network layers; a fully connected layer; and parallel softmax and regressor layers.
27. The computer-implemented method of claim 26 wherein the shared convolutional neural network layers include: a first convolutional layer; a first local response normalization layer after the first convolutional layer; a first max pooling layer after the first local response normalization layer; a second convolutional layer after the first max pooling layer; a second local response normalization layer after the second convolutional layer; a second max pooling layer after the second local response normalization layer; a third convolutional layer after the second max pooling layer; a fourth convolutional layer; and a fifth convolutional layer.
28. The computer-implemented method of claim 24 wherein there is provided a dropout layer between each consecutive pair of the fully connected layers.
29. The computer-implemented method of claim 24 wherein each convolutional layer is followed by a rectified linear unit activation function.
30. The computer-implemented method of claim 24 wherein the image comprises a sequence of images forming a video, and displaying the image comprises displaying the video with the location of the defect, if determined to be present, being indicated in each one of the sequence of images.
</claims>
</document>
