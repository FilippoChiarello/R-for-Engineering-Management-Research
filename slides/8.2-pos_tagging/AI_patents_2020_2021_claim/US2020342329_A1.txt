<document>

<filing_date>
2019-04-25
</filing_date>

<publication_date>
2020-10-29
</publication_date>

<priority_date>
2019-04-25
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
SAP
</assignee>

<inventors>
KAIN, STEFAN
</inventors>

<docdb_family_id>
72917099
</docdb_family_id>

<title>
ARCHITECTURE SEARCH WITHOUT USING LABELS FOR DEEP AUTOENCODERS EMPLOYED FOR ANOMALY DETECTION
</title>

<abstract>
Methods, systems, and computer-readable storage media for defining an autoencoder architecture including a neural network, during training of the autoencoder, recording a loss value at each iteration to provide a plurality of loss values, the autoencoder being trained using a data set that is associated with a domain, and a learning rate to provide a trained autoencoder, calculating a penalty score using at least a portion of the plurality of loss values, the penalty score being based on a loss span penalty PLS, a convergence penalty PC, and a fluctuation penalty PF, comparing the penalty score P to a threshold penalty score to affect a comparison, and selectively employing the trained autoencoder for anomaly detection within the domain based on the comparison.
</abstract>

<claims>
1. A computer-implemented method for selecting a machine-learning (ML) model for application in anomaly detection, the method being executed by one or more processors and comprising: defining an autoencoder architecture comprising a neural network; during training of the autoencoder, recording a loss value at each iteration to provide a plurality of loss values, the autoencoder being trained using a data set that is associated with a domain, and a learning rate to provide a trained autoencoder; calculating a penalty score using at least a portion of the plurality of loss values, the penalty score being based on a loss span penalty PLS, a convergence penalty PC, and a fluctuation penalty PF; comparing the penalty score P to a threshold penalty score to affect a comparison; and selectively employing the trained autoencoder for anomaly detection within the domain based on the comparison.
2. The method of claim 1, wherein the loss span penalty PLS is calculated as a minimum of a smoothed loss divided by a maximum of the smoothed loss, the smoothed loss being determined based on the plurality of loss values.
3. The method of claim 1, wherein determining the convergence penalty PC comprises: selecting an interval of iterations, over which loss values in the plurality of loss are each below a threshold loss; determining a number of iterations in the interval of iterations; and calculating the convergence penalty PC as the quotient of the number of iterations and a total number of iterations in training of the autoencoder.
4. The method of claim 1, wherein the fluctuation penalty PF is determined as a difference between a maximum of a loss residual and the minimum of the loss residual, the loss residual being determine as a difference between a smoothed loss and the plurality of loss values.
5. The method of claim 1, wherein, for training of the autoencoder, the data set is randomly divided into a training sub-set, and a validation sub-set.
6. The method of claim 1, wherein defining the auto-encoder architecture at least partially comprises providing a number of hidden layers of the neural network, and a size of each hidden layer.
7. The method of claim 1, wherein employing the trained autoencoder for anomaly detection comprises processing a data stream from one or more Internet-of-Things (IoT) devices that monitor an environment to selectively detect in anomalous condition within the environment.
8. A non-transitory computer-readable storage medium coupled to one or more processors and having instructions stored thereon which, when executed by the one or more processors, cause the one or more processors to perform operations for selecting a machine-learning (ML) model for application in anomaly detection, the operations comprising: defining an autoencoder architecture comprising a neural network; during training of the autoencoder, recording a loss value at each iteration to provide a plurality of loss values, the autoencoder being trained using a data set that is associated with a domain, and a learning rate to provide a trained autoencoder; calculating a penalty score using at least a portion of the plurality of loss values, the penalty score being based on a loss span penalty PLS, a convergence penalty PC, and a fluctuation penalty PF; comparing the penalty score P to a threshold penalty score to affect a comparison; and selectively employing the trained autoencoder for anomaly detection within the domain based on the comparison.
9. The computer-readable storage medium of claim 8, wherein the loss span penalty PLS is calculated as a minimum of a smoothed loss divided by a maximum of the smoothed loss, the smoothed loss being determined based on the plurality of loss values.
10. The computer-readable storage medium of claim 8, wherein determining the convergence penalty PC comprises: selecting an interval of iterations, over which loss values in the plurality of loss are each below a threshold loss; determining a number of iterations in the interval of iterations; and calculating the convergence penalty PC as the quotient of the number of iterations and a total number of iterations in training of the autoencoder.
11. The computer-readable storage medium of claim 8, wherein the fluctuation penalty PF is determined as a difference between a maximum of a loss residual and the minimum of the loss residual, the loss residual being determine as a difference between a smoothed loss and the plurality of loss values.
12. The computer-readable storage medium of claim 8, wherein, for training of the autoencoder, the data set is randomly divided into a training sub-set, and a validation sub-set.
13. The computer-readable storage medium of claim 8, wherein defining the auto-encoder architecture at least partially comprises providing a number of hidden layers of the neural network, and a size of each hidden layer.
14. The computer-readable storage medium of claim 8, wherein employing the trained autoencoder for anomaly detection comprises processing a data stream from one or more Internet-of-Things (IoT) devices that monitor an environment to selectively detect in anomalous condition within the environment
15. A system, comprising: a computing device; and a computer-readable storage device coupled to the computing device and having instructions stored thereon which, when executed by the computing device, cause the computing device to perform operations for selecting a machine-learning (ML) model for application in anomaly detection, the operations comprising: defining an autoencoder architecture comprising a neural network; during training of the autoencoder, recording a loss value at each iteration to provide a plurality of loss values, the autoencoder being trained using a data set that is associated with a domain, and a learning rate to provide a trained autoencoder; calculating a penalty score using at least a portion of the plurality of loss values, the penalty score being based on a loss span penalty PLS, a convergence penalty PC, and a fluctuation penalty PF; comparing the penalty score P to a threshold penalty score to affect a comparison; and selectively employing the trained autoencoder for anomaly detection within the domain based on the comparison.
16. The system of claim 15, wherein the loss span penalty PLS is calculated as a minimum of a smoothed loss divided by a maximum of the smoothed loss, the smoothed loss being determined based on the plurality of loss values.
17. The system of claim 15, wherein determining the convergence penalty PC comprises: selecting an interval of iterations, over which loss values in the plurality of loss are each below a threshold loss; determining a number of iterations in the interval of iterations; and calculating the convergence penalty PC as the quotient of the number of iterations and a total number of iterations in training of the autoencoder.
18. The system of claim 15, wherein the fluctuation penalty PF is determined as a difference between a maximum of a loss residual and the minimum of the loss residual, the loss residual being determine as a difference between a smoothed loss and the plurality of loss values.
19. The system of claim 15, wherein, for training of the autoencoder, the data set is randomly divided into a training sub-set, and a validation sub-set.
20. The system of claim 15, wherein defining the auto-encoder architecture at least partially comprises providing a number of hidden layers of the neural network, and a size of each hidden layer.
</claims>
</document>
