<document>

<filing_date>
2018-11-28
</filing_date>

<publication_date>
2020-05-28
</publication_date>

<priority_date>
2018-11-28
</priority_date>

<ipc_classes>
G06F9/48,G06K9/62,G06N20/00,G06N3/08,G06N7/00
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
KIM, SUNGCHUL
KVETON, BRANISLAV
LI, SHENG
WEN, ZHENG
ZHAO, HANDONG
</inventors>

<docdb_family_id>
70769981
</docdb_family_id>

<title>
Multi-task Equidistant Embedding
</title>

<abstract>
Systems and techniques for multi-task equidistant embedding are described that process categorical feature data to explore feature interactions. A digital analytics system enforces an equidistant relationship among features within a category while extracting high-order feature interactions by punishing both positive correlations and negative correlations among low-dimensional representations of different features. By enforcing an equidistant embedding, information is retained and accuracy is increased while higher order feature interactions are determined. Further, the digital analytics system shares knowledge among different tasks by connecting a shared network representation common to multiple tasks with exclusive network representations specific to particular tasks.
</abstract>

<claims>
1. In a digital medium environment, a method implemented by at least one computing device, the method comprising: receiving, by the at least one computing device, a dataset corresponding to a plurality of tasks, the dataset describing a plurality of categories and a plurality of features included within respective categories of the plurality of categories; generating, by the at least one computing device, a reduced dimension dataset as an equidistant embedding of the plurality of features, respectively, within the respective categories of the plurality of categories from the dataset; extracting, by the at least one computing device, complementary information from the plurality of tasks; training, by the at least one computing device, a machine learning model based on the reduced dimension dataset and the complementary information.
2. The method of claim 1, further comprising: receiving, by the at least one computing device, another dataset that corresponds to a particular task from the plurality of tasks, the another dataset describing the plurality of categories and the plurality of features; generating, by the least one computing device, a prediction of an outcome of the particular task based on the another dataset and the trained machine learning model; and outputting, by the at least one computing device, the generated prediction of the outcome of the particular task.
3. The method of claim 1, wherein the plurality of tasks comprises at least a first task and a second task, and wherein the training the machine learning model comprises training a first machine learning model corresponding to the first task and training a second machine learning model corresponding to the second task.
4. The method of claim 3, wherein the first task is a supervised task and the second task is one of a supervised task or an unsupervised task.
5. The method of claim 3, wherein the first machine learning model and the second machine learning model are trained from a neural network that includes shared layers corresponding to the first task and the second task, exclusive layers corresponding to the first task, and exclusive layers corresponding to the second task.
6. The method of claim 3, wherein the second machine learning model is trained utilizing information corresponding to the first task.
7. The method of claim 1, wherein the equidistant embedding enforces, for each respective category of the plurality of categories, an equidistant relationship among the plurality of features included within the respective category.
8. The method of claim 1, wherein the generating the reduced dimension dataset is performed as a part of the training the machine learning model.
9. At least one computing device in a digital medium environment, the at least one computing device including a processing system and at least one computer-readable storage medium, the at least one computing device comprising: equidistant embedding layers of a neural network, the equidistant embedding layers configured to enforce, for each respective category of a plurality of categories described in a dataset, an equidistant relationship among a plurality of features included within the respective category; shared layers of the neural network, the shared layers configured to extract feature interactions between the features described in the dataset corresponding to at least one of a first task and a second task; exclusive layers of the neural network corresponding to the first task, the exclusive layers corresponding to the first task configured to utilize the extracted feature interactions to generate a first machine learning model corresponding to the first task; and exclusive layers of the neural network corresponding to the second task, the exclusive layers corresponding to the second task configured to utilize the extracted feature interactions to generate a second machine learning model corresponding to the second task.
10. The at least one computing device of claim 9, the at least one computer-readable storage medium storing processor-executable instructions that, responsive to execution by the processing system, cause the processing system to perform operations comprising: receiving a second dataset corresponding to the first task; generating a prediction of an outcome of the first task based on the second dataset and the first machine learning model; and outputting the generated prediction of the outcome of the first task.
11. The at least one computing device of claim 9, wherein the first machine learning model and the second machine learning model are generated concurrently.
12. The at least one computing device of claim 9, wherein the first task is a supervised task and the second task is an unsupervised task.
13. The at least one computing device of claim 9, wherein the first task is a supervised task and the second task is a supervised task.
14. The at least one computing device of claim 9, wherein the extracting feature interactions includes determining complementary information beneficial to the first task and the second task.
15. The at least one computing device of claim 9, wherein the extracting feature interactions includes determining a feature interaction beneficial to the first task, and wherein the generating the second machine learning model corresponding to the second task includes utilizing the determined feature interaction.
16. The at least one computing device of claim 9, wherein the generating the first machine learning model and the generating the second machine learning model are subject to the same training criterion.
17. At least one computing device in a digital medium environment, the at least one computing device including a processing system and at least one computer-readable storage medium, the at least one computing device comprising: means for receiving a first dataset corresponding to at least one task, the first dataset describing a plurality of categories and a plurality of features included within respective categories of the plurality of categories; means for training a machine learning model that enforces an equidistant relationship between features included within a respective category; means for receiving an input corresponding to the at least one task; means for generating a prediction of an outcome of the at least one task based on the input and the trained machine learning model; and means for outputting the generated prediction of the outcome of the at least one task.
18. The at least one computing device of claim 17, wherein the at least one task comprises at least a first task and a second task, and wherein the means for training a machine learning model comprises means for training a first machine learning model corresponding to the first task and means for training a second machine learning model corresponding to the second task.
19. The at least one computing device of claim 18, wherein the first task is a supervised task and the second task is an unsupervised task.
20. The at least one computing device of claim 18, wherein the first task is a supervised task and the second task is a supervised task.
</claims>
</document>
