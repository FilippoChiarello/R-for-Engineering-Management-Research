<document>

<filing_date>
2019-05-01
</filing_date>

<publication_date>
2020-09-29
</publication_date>

<priority_date>
2019-05-01
</priority_date>

<ipc_classes>
G06F30/327,G06N3/04,G06N3/063,G06N3/08
</ipc_classes>

<assignee>
XILINX
</assignee>

<inventors>
Koppada, Venkataraju
Jain, Vishal K.
Menon, Sairam K. M.
Martha, Anil K.
Karumannil, Abid
Perla, Anusha
Barri, Anitha
Vemuri, Kumar S. S.
</inventors>

<docdb_family_id>
72614831
</docdb_family_id>

<title>
Compiler and hardware abstraction layer architecture for a neural network accelerator
</title>

<abstract>
Examples herein describe a method for a compiler and hardware-abstraction-layer architecture for a programmable integrated circuit (IC). In one embodiment, a method for mapping and porting a neural network to an integrated circuit (IC) is disclosed. The method includes receiving a network description of the neural network; generating a framework independent network graph based on the network description; performing a plurality of back-end operations on the network graph to generate an execution sequence vector; and configuring the IC based on the execution sequence vector.
</abstract>

<claims>
1. A method for mapping and porting a neural network to an integrated circuit (IC), the method comprising: receiving a framework dependent network description of the neural network; generating a framework independent and hardware agnostic network graph based on the framework dependent network description and a type of a framework in which the network description was generated; performing a plurality of back-end operations on the framework independent and hardware agnostic network graph to generate an execution sequence vector; and configuring the IC based on the execution sequence vector.
2. The method of claim 1, wherein performing the plurality of back-end operations comprises performing at least one hardware-independent optimization.
3. The method of claim 2, wherein performing the at least one hardware-independent optimization comprises performing one of the following: parallel 1Ã—1 convolutions fuse optimizations, software fuse optimizations, dropout optimizations, reshape optimizations, flatten optimizations, concatenation layer optimizations, custom layer optimizations, prior box optimizations, and training layer removal optimizations.
4. The method of claim 1, wherein performing the plurality of back-end operations comprises performing at least one hardware-dependent optimization.
5. The method of claim 4, wherein performing the at least one hardware-dependent optimization comprises performing one of the following: convolution and Rectified Linear Unit (ReLU) optimizations, hardware fusion optimization, Concatenated ReLu (CReLU) optimizations, ElementWise Addition optimizations, ReLU optimizations, 3D separable convolution optimizations, and deconvolution optimizations.
6. The method of claim 1, wherein the plurality of back-end operations comprises: refining the framework independent and hardware agnostic network graph into a hardware-dependent network graph via hardware-independent optimizations and hardware-dependent optimizations; assigning sequence identifiers to each layer of the hardware-dependent network graph; and generating the execution sequence vector comprising a sequential queue of the layers of the hardware-dependent network graph.
7. The method of claim 6, wherein generating the execution sequence vector comprises optimizing buffer handles for reuse between layers of the hardware-dependent network graph.
8. The method of claim 1, wherein generating the framework independent and hardware agnostic network graph comprises generating layer nodes corresponding to layers of the neural network and buffer nodes between the layer nodes.
9. The method of claim 1, wherein the plurality of back-end operations comprises allocating buffer handles for a plurality of layers of the framework independent and hardware agnostic network graph.
10. The method of claim 1, wherein configuring the IC comprises calibrating a plurality of hardware runtime parameters of the IC based on the execution sequence vector.
11. The method of claim 10, wherein calibrating the plurality of hardware runtime parameters of the IC based on the execution sequence vector comprises computing scalar parameters to program the IC.
12. The method of claim 1, wherein configuring the IC comprises scheduling the plurality of commands of the execution sequence vector for a plurality of processing elements of the IC.
13. The method of claim 12, wherein scheduling the plurality of commands of the execution sequence vector for the plurality of processing elements of the IC comprises dispatching the plurality of commands to DPEs of the IC for processing.
14. The method of claim 12, wherein scheduling the plurality of commands comprises: separating the plurality of commands into a plurality of command queues based on a processing element of the plurality of processing elements used to process a command; determining whether the command has dependencies; and asynchronously dispatching the command to the processing element for processing.
15. The method of claim 12, wherein scheduling the plurality of commands further comprises: receiving a command completion response from a processing element of the plurality of processing elements; and asynchronously dispatching a next command to the processing element.
16. The method of claim 1, further comprising allocating a plurality of software and hardware buffers based on the execution sequence vector by configuring the plurality of software and hardware buffers based on a plurality of commands of the execution sequence vector.
17. A system comprising: a processor; and non-transitory computer-readable storage medium embodying computer program instructions for mapping and porting a neural network to an integrated circuit (IC), the computer program instructions implementing a method, the method comprising: receiving a network description of the neural network; generating a framework independent and hardware agnostic network graph based on the network description and a type of a framework in which the network description was generated; performing a plurality of back-end operations on the framework independent and hardware agnostic network graph to generate an execution sequence vector; and configuring the IC based on the execution sequence vector.
18. The system of claim 17, wherein configuring the IC comprises scheduling the plurality of commands of the execution sequence vector for a plurality of components of the IC by dispatching the plurality of commands to DPEs of the IC for processing.
19. The system of claim 17, wherein configuring the IC comprises: scheduling the plurality of commands of the execution sequence vector for a plurality of components of the IC by separating the plurality of commands into a plurality of command queues based on a processing element used to process a command; determining whether the command has dependencies; and asynchronously dispatching the command to the processing element for processing.
20. The system of claim 17, wherein the plurality of back-end operations comprises: refining the framework independent and hardware agnostic network graph into a hardware-dependent network graph via hardware-independent optimizations and hardware-dependent optimizations; assigning sequence identifiers to each layer of the hardware-dependent network graph; and generating the execution sequence vector comprising a sequential queue of the layers of the hardware-dependent network graph.
</claims>
</document>
