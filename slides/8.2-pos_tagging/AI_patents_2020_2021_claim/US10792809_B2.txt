<document>

<filing_date>
2017-12-12
</filing_date>

<publication_date>
2020-10-06
</publication_date>

<priority_date>
2017-12-12
</priority_date>

<ipc_classes>
B25J13/08,B25J15/02,B25J15/08,B25J19/02,B25J9/10,B25J9/16
</ipc_classes>

<assignee>
X DEVELOPMENT
</assignee>

<inventors>
HOMBERG, BIANCA
BINGHAM, JEFFREY
DELPRETO, JOSEPH
ALEXANDER, TAYLOR
</inventors>

<docdb_family_id>
66734992
</docdb_family_id>

<title>
Robot grip detection using non-contact sensors
</title>

<abstract>
A method is provided that includes controlling a robotic gripping device to cause a plurality of digits of the robotic gripping device to move towards each other in an attempt to grasp an object. The method also includes receiving, from at least one non-contact sensor on the robotic gripping device, first sensor data indicative of a region between the plurality of digits of the robotic gripping device. The method further includes receiving, from the at least one non-contact sensor on the robotic gripping device, second sensor data indicative of the region between the plurality of digits of the robotic gripping device, where the second sensor data is based on a different sensing modality than the first sensor data. The method additionally includes determining, using an object-in-hand classifier that takes as input the first sensor data and the second sensor data, a result of the attempt to grasp the object.
</abstract>

<claims>
1. A method comprising: controlling a robotic gripping device to cause a plurality of digits of the robotic gripping device to move towards each other in an attempt to grasp an object, wherein the plurality of digits are coupled to a palm of the robotic gripping device; receiving, from a first non-contact sensor on the palm of the robotic gripping device, first sensor data indicative of a region extending from the palm between the plurality of digits of the robotic gripping device, wherein the first sensor data is indicative of the region after the attempt to grasp the object; receiving, from a second non-contact sensor on palm of the robotic gripping device, second sensor data indicative of the region extending from the palm between the plurality of digits of the robotic gripping device, wherein the second sensor data is based on a different sensing modality than the first sensor data, wherein the second sensor data is indicative of the region after the attempt to grasp the object; and determining, using an object-in-hand classifier that takes as input the first sensor data and the second sensor data, whether the robotic gripping device is currently holding the object after the attempt to grasp the object.
2. The method of claim 1, wherein the object-in-hand classifier is a machine learning model trained based on at least one of (i) past grasp attempts by the robotic gripping device, (ii) past grasp attempts by a similar robotic gripping device, or (iii) simulated grasping events.
3. The method of claim 2, wherein the machine learning model is a convolutional neural network (CNN).
4. The method of claim 1, wherein the first non-contact sensor comprises a time-of-flight sensor, wherein the first sensor data comprises time-of-flight distance data from the time-of-flight sensor.
5. The method of claim 4, wherein the object-in-hand classifier is a linear kernel SVM that takes as input the time-of-flight distance data.
6. The method of claim 1, wherein the first non-contact sensor is an infrared camera, wherein the first sensor data comprises grayscale image data, wherein the second non-contact sensor is a time-of-flight sensor, and wherein the second sensor data comprises time-of-flight distance data.
7. The method of claim 6, wherein the object-in-hand classifier takes as further input third sensor data, wherein the third sensor data comprises reflectance data from the time-of-flight sensor.
8. The method of claim 7, wherein the object-in-hand classifier is an SVM that takes as input a single image from the infrared camera, a single distance measurement from the time-of-flight sensor, and a single reflectance measurement from the time-of-flight sensor.
9. The method of claim 6, further comprising receiving, from an additional time-of-flight sensor on the robotic gripping device, additional sensor data indicative of the region between the plurality of digits of the robotic gripping device, wherein the object-in-hand classifier takes as further input the additional sensor data.
10. The method of claim 9, wherein the time-of-flight sensor is a short-range time-of-flight sensor configured to measure distance within a first range, wherein the additional time-of-flight sensor is a long-range time-of-flight sensor configured to measure distance within a second range, wherein the second range extends further from the robotic gripping device than the first range, and wherein each of the short-range time-of-flight sensor and the long-range time-of-flight sensor are configured to generate a separate reflectance measurement.
11. The method of claim 1, wherein the result of the attempt to grasp the object comprises one of a grasp success or a grasp failure, the method further comprising controlling the robotic gripping device to attempt to grasp the object a second time when the result comprises a grasp failure.
12. A robot comprising: a robotic gripping device, comprising: a palm; a plurality of digits coupled to the palm; a first non-contact sensor on the palm; and a second non-contact sensor on the palm; and a control system configured to: control the robotic gripping device to cause the plurality of digits of the robotic gripping device to move towards each other in an attempt to grasp an object; receive, from the first non-contact sensor on the palm of the robotic gripping device, first sensor data indicative of a region extending from the palm between the plurality of digits of the robotic gripping device, wherein the first sensor data is indicative of the region after the attempt to grasp the object; receive, from the second non-contact sensor on the robotic gripping device, second sensor data indicative of the region extending from the palm between the plurality of digits of the robotic gripping device, wherein the second sensor data is based on a different sensing modality than the first sensor data, wherein the second sensor data is indicative of the region after the attempt to grasp the object; and determine, using an object-in-hand classifier that takes as input the first sensor data and the second sensor data, whether the robotic gripping device is currently holding the object after the attempt to grasp the object.
13. The robot of claim 12, wherein the plurality of digits comprise two underactuated digits, each underactuated digit comprising: a deformable gripping surface; and a plurality of members coupled together end-to-end by one or more unactuated joints, wherein the plurality of members are configured to cause the deformable gripping surface to conform to a shape of the object when the object is grasped between the two underactuated digits.
14. The robot of claim 13, wherein the control system is further configured to determine, based on the first sensor data and the second sensor data, a grasp quality indicative of a quality of grip on the object by the two underactuated digits.
15. The robot of claim 12, wherein the first non-contact sensor comprises an RGB camera and the second non-contact sensor comprises a time-of-flight sensor, wherein the object-in-hand classifier used by the control system takes as input a color image from the RGB camera, a distance measurement from the time-of-flight sensor, and a reflectance measurement from the time-of-flight sensor.
16. A non-transitory computer readable medium having stored therein instructions executable by one or more processors to cause the one or more processors to perform functions comprising: controlling a robotic gripping device to cause a plurality of digits of the robotic gripping device to move towards each other in an attempt to grasp an object, wherein the plurality of digits are coupled to a palm of the robotic gripping device; receiving, from a first non-contact sensor on the palm of the robotic gripping device, first sensor data indicative of a region extending from the palm between the plurality of digits of the robotic gripping device, wherein the first sensor data is indicative of the region after the attempt to grasp the object; receiving, from a second non-contact sensor on palm of the robotic gripping device, second sensor data indicative of the region extending from the palm between the plurality of digits of the robotic gripping device, wherein the second sensor data is based on a different sensing modality than the first sensor data, wherein the second sensor data is indicative of the region after the attempt to grasp the object; and determining, using an object-in-hand classifier that takes as input the first sensor data and the second sensor data, whether the robotic gripping device is currently holding the object after the attempt to grasp the object.
17. The non-transitory computer readable medium of claim 16, the functions further comprising: controlling the robotic gripping device to perform one or more additional operations based on whether the robotic gripping device is currently holding the object after the attempt to grasp the object.
18. The non-transitory computer readable medium of claim 16, wherein the object-in-hand classifier is a neural network trained based on past grasp attempts by a plurality of similar robotic gripping devices coupled to a plurality of other robots.
19. The method of claim 1, wherein the plurality of digits of the robotic gripping comprise a plurality of underactuated digits, wherein each underactuated digit has less control inputs than degrees of freedom.
20. The method of claim 1, wherein the first non-contact sensor is a one-dimensional (1D) time-of-flight sensor configured to generate a time-of-flight distance measurement indicative of distance to a nearest object in a direction extending between the plurality of digits of the robotic gripping device.
</claims>
</document>
