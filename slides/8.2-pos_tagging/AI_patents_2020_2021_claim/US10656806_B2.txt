<document>

<filing_date>
2019-04-19
</filing_date>

<publication_date>
2020-05-19
</publication_date>

<priority_date>
2018-04-21
</priority_date>

<ipc_classes>
G06F3/01,G06F3/0481,G06F3/0484,G06F3/16
</ipc_classes>

<assignee>
AUGMENTALIS
</assignee>

<inventors>
JHAWAR, MANOJ
SIMMONS, ANDY
</inventors>

<docdb_family_id>
66429646
</docdb_family_id>

<title>
Display interface systems and methods
</title>

<abstract>
Systems and methods are provided for implementing an interactive application, which incorporates voice-driven and motion-driven interactions with media content in a display device. An application instance can be initialized for interacting with media content output to a display device, such as an head mounted display (HMD). Then, a determination whether a received user interaction is interpretable into an interactive command defined by an operating system (OS) is performed. If the OS can interpret the user interaction, the interactive command can be executed with actions generated by the OS. Alternatively, an emulation of the interactive command may be executed, when the user interaction cannot be interpreted by the OS. Subsequently, the media content is presented within a user interface based on the interactive command. For example, user interaction can be the head movement of a user that is interpreted into a command that controls presentation of web content in the HMD.
</abstract>

<claims>
1. A method, comprising: running a first application for interacting with media content presented on a mobile display device, the mobile display device comprising a head mounted display (HMD) or a handheld mobile display device with a touchscreen; monitoring, via a first operating system (OS) service of a mobile OS of the mobile display device, one or more user interactions with the mobile display device; receiving input associated with a first user interaction of the one or more user interactions, wherein the input indicates an interactive command associated with a user interface (UI) control for interacting with the media content, wherein the first user interaction is: a voice interaction with an audio device coupled to the HMD or handheld mobile display device; a touch-based interaction with the touchscreen of the handheld mobile display device; or a motion interaction with the HMD or the handheld mobile display device; after receiving the input, generating, using the first OS service, an event associated with the UI control; transmitting the event from the first OS service to a second application; determining that the first user interaction is not interpretable into an interactive command defined by the mobile OS of the mobile display device; in response to determining that the first user interaction is not interpretable into the interactive command defined by the mobile OS, initiating, via the second application, execution of an emulation of the interactive command by generating an emulation control event based on the event received from the first OS service, wherein the emulation control event comprises an action description and coordinates of a cursor; transmitting the emulation control event from the second application to a second OS service of the mobile OS, wherein the second OS service is configured to render content for presentation on the mobile display device; and rendering, via the second OS service of the mobile OS, the media content for presentation within a user interface associated with the first application based on the execution of a software procedure associated with the emulation control event, wherein rendering the media content comprises drawing and manipulating the cursor within the user interface of the first application.
2. The method of claim 1, wherein the first user interaction is the voice interaction, wherein the voice interaction is associated with receiving voice input, the method further comprising: triggering speech recognition for interpreting the voice input into a click event or a touch event, wherein the event is generated by the first OS service after the voice input is interpreted into the click event or the touch event.
3. The method of claim 2, wherein the voice input is received through a microphone communicatively coupled to a head mounted display.
4. The method of claim 1, wherein the first user interaction is the motion interaction, wherein the motion interaction is associated with receiving motion input, the method further comprising: using the motion input to track, via the second application, a change in position of a user wearing the HMD or holding the handheld mobile display device; and translating, using the second application, the tracked change in position into a cursor position change, wherein the cursor position change is used by the second application to generate the emulation control event.
5. The method of claim 4, wherein the motion input is head movement of the user wearing the HMD.
6. The method of claim 1, further comprising: tracking a series of multiple user interactions with the media content; tracking each of a plurality of actions generated to execute a sequence of interactive commands corresponding to the series of multiple user interactions; storing each of the actions to define a re-executable sequence of interactive commands; and upon triggering an optimized replay of interactions with the media content, automatically retrieving and executing each of the actions of the defined re-executable sequence of interactive commands in response to receiving a sub-series of the multiple user interactions, wherein the sub-series precludes repeating each of the user interactions in the series of multiple user interactions.
7. The method of claim 1, wherein the first application is a web application instance, the media content is a webpage, and the UI control is a control associated with the webpage.
8. The method of claim 1, wherein the first user interaction is the touch-based interaction with the touchscreen of the handheld mobile display device.
9. The method of claim 1, wherein the second application is configured to use one or more software procedures that define controls and associated actions for each of a plurality of we b-based applications that present web-based media content.
10. The method of claim 1, wherein the emulation control event comprises information corresponding to information contained in an event defined by the mobile OS; and wherein rendering the media content comprises modifying a viewport's presentation of the media content.
11. The method of claim 10, wherein the action description comprises: a click action, a zoom action, an action to draw the cursor, or some combination thereof.
12. The method of claim 1, further comprising: simultaneously rendering multiple viewports on the mobile display device, wherein the media content is rendered on one of the multiple viewports.
13. A non-transitory computer-readable storage medium having executable instructions stored thereon that, when executed by a processor, perform operations of: running a first application for interacting with media content presented on a mobile display device, the mobile display device comprising a head mounted display (HMD) or a handheld mobile display device with a touchscreen; monitoring, via a first operating system (OS) service of a mobile OS of the mobile display device, one or more user interactions with the mobile display device; receiving input associated with a first user interaction of the one or more user interactions, wherein the input indicates an interactive command associated with a user interface (UI) control for interacting with the media content, wherein the first user interaction is: a voice interaction with an audio device coupled to the HMD or handheld mobile display device; a touch-based interaction with the touchscreen of the handheld mobile display device; or a motion interaction with the HMD or the handheld mobile display device; after receiving the input, generating, using the first OS service, an event associated with the UI control; transmitting the event from the first OS service to a second application; determining that the first user interaction is not interpretable into an interactive command defined by the mobile OS of the mobile display device; in response to determining that the first user interaction is not interpretable into the interactive command defined by the mobile OS, initiating, via the second application, execution of an emulation of the interactive command by generating an emulation control event based on the event received from the first OS service, wherein the emulation control event comprises an action description and coordinates of a cursor; transmitting the emulation control event from the second application to a second OS service of the mobile OS, wherein the second OS service is configured to render content for presentation on the mobile display device; and rendering, via the second OS service of the mobile OS, the media content for presentation within a user interface associated with the first application based on the execution of a software procedure associated with the emulation control event, wherein rendering the media content comprises drawing and manipulating the cursor within the user interface of the first application.
14. The non-transitory computer-readable storage medium of claim 13, wherein the first user interaction is the voice interaction, wherein the voice interaction is associated with receiving voice input, the method further comprising: triggering speech recognition for interpreting the voice input into a click event or touch event, wherein the event is generated by the first OS service after the voice input is interpreted into the click event or the touch event.
15. The non-transitory computer-readable storage medium of claim 14, wherein the first user interaction is the motion interaction, wherein the motion interaction is associated with receiving motion input, the method further comprising: using the motion input to track, via the second application, a change in position of a user wearing the HMD or holding the handheld mobile display device; and translating, using the second application, the tracked change in position into a cursor position change, wherein the cursor position change is used by the second application to generate the emulation control event.
16. The non-transitory computer-readable storage medium of claim 15, wherein the motion input is head movement of the user wearing the HMD.
</claims>
</document>
