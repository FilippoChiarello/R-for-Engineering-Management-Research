<document>

<filing_date>
2019-04-11
</filing_date>

<publication_date>
2020-10-13
</publication_date>

<priority_date>
2017-11-29
</priority_date>

<ipc_classes>
G06N3/04,G06N3/063,G06N3/08,G11C11/34,G11C11/54,G11C16/04,H01L27/115,H01L27/11517,H01L27/11521,H01L27/11524,H01L29/423,H01L29/788
</ipc_classes>

<assignee>
SILICON STORAGE TECHNOLOGY
</assignee>

<inventors>
TRAN HIEU VAN
DO, NHAN
TIWARI, VIPIN
REITEN, MARK
LEMKE, STEVEN
</inventors>

<docdb_family_id>
67391554
</docdb_family_id>

<title>
Neural network classifier using array of four-gate non-volatile memory cells
</title>

<abstract>
A neural network device with synapses having memory cells each having a floating gate and a first gate over first and second portions of a channel region, and second and third gates over the floating gate and over the source region. First lines each electrically connect the first gates in one of the memory cell rows, second lines each electrically connect the second gates in one of the memory cell rows, third lines each electrically connect the third gates in one of the memory cell rows, fourth lines each electrically connect the source regions in one of the memory cell rows, and fifth lines each electrically connect the drain regions in one of the memory cell columns. The synapses receive a first plurality of inputs as electrical voltages on the first, second or third lines, and provide a first plurality of outputs as electrical currents on the fifth lines.
</abstract>

<claims>
1. A neural network device, comprising: a first plurality of synapses configured to receive a first plurality of inputs and to generate therefrom a first plurality of outputs, wherein the first plurality of synapses comprises: a plurality of memory cells, wherein each of the memory cells includes spaced apart source and drain regions formed in a semiconductor substrate with a channel region extending there between, a floating gate disposed over and insulated from a first portion of the channel region, a first gate disposed over and insulated from a second portion of the channel region, a second gate disposed over and insulated from the floating gate, and a third gate disposed over and insulated from the source region; each of the plurality of memory cells is configured to store a weight value corresponding to a number of electrons on the floating gate; the plurality of memory cells are configured to generate the first plurality of outputs based upon the first plurality of inputs and the stored weight values; wherein the memory cells of the first plurality of synapses are arranged in rows and columns, and wherein the first plurality of synapses comprises: a plurality of first lines each electrically connecting together the first gates in one of the rows of the memory cells; a plurality of second lines each electrically connecting together the second gates in one of the rows of the memory cells; a plurality of third lines each electrically connecting together the third gates in one of the rows of the memory cells; a plurality of fourth lines each electrically connecting together the source regions in one of the rows of the memory cells; a plurality of fifth lines each electrically connecting together the drain regions in one of the columns of the memory cells; wherein the first plurality of synapses is configured to receive the first plurality of inputs as electrical voltages on the plurality of first lines or on the plurality of second lines or on the plurality of third lines or the plurality of fourth lines, and to provide the first plurality of outputs as electrical currents on the plurality of fifth lines.
2. The neural network device of claim 1, wherein the first plurality of synapses is configured to receive the first plurality of inputs as electrical voltages on the plurality of first lines.
3. The neural network device of claim 1, wherein the first plurality of synapses is configured to receive the first plurality of inputs as electrical voltages on the plurality of second lines.
4. The neural network device of claim 1, wherein the first plurality of synapses is configured to receive the first plurality of inputs as electrical voltages on the plurality of third lines.
5. The neural network device of claim 1, wherein the first plurality of synapses is configured to receive the first plurality of inputs as electrical voltages on the plurality of fourth lines.
6. The neural network device of claim 1, further comprising: a first plurality of neurons configured to receive the first plurality of outputs.
7. The neural network device of claim 6, further comprising: a second plurality of synapses configured to receive a second plurality of inputs from the first plurality of neurons and to generate therefrom a second plurality of outputs, wherein the second plurality of synapses comprises: a plurality of second memory cells, wherein each of the second memory cells includes spaced apart second source and second drain regions formed in the semiconductor substrate with a second channel region extending there between, a second floating gate disposed over and insulated from a first portion of the second channel region, a fourth gate disposed over and insulated from a second portion of the second channel region, a fifth gate disposed over and insulated from the second floating gate, and a sixth gate disposed over and insulated from the second source region; each of the plurality of second memory cells is configured to store a second weight value corresponding to a number of electrons on the second floating gate; the plurality of second memory cells are configured generate the second plurality of outputs based upon the second plurality of inputs and the stored second weight values; wherein the second memory cells of the second plurality of synapses are arranged in rows and columns, and wherein the second plurality of synapses comprises: a plurality of sixth lines each electrically connecting together the fourth gates in one of the rows of the second memory cells; a plurality of seventh lines each electrically connecting together the fifth gates in one of the rows of the second memory cells; a plurality of eighth lines each electrically connecting together the sixth gates in one of the rows of the second memory cells; a plurality of ninth lines each electrically connecting together the second source regions in one of the rows of the second memory cells; a plurality of tenth lines each electrically connecting together the second drain regions in one of the columns of the second memory cells; wherein the second plurality of synapses is configured to receive the second plurality of inputs as electrical voltages on the plurality of sixth lines or on the plurality of seventh lines or on the plurality of eighth lines or on the plurality of ninth lines, and to provide the second plurality of outputs as electrical currents on the plurality of tenth lines.
8. The neural network device of claim 7, wherein the second plurality of synapses is configured to receive the second plurality of inputs as electrical voltages on the plurality of sixth lines.
9. The neural network device of claim 7, wherein the second plurality of synapses is configured to receive the second plurality of inputs as electrical voltages on the plurality of seventh lines.
10. The neural network device of claim 7, wherein the second plurality of synapses is configured to receive the second plurality of inputs as electrical voltages on the plurality of eighth lines.
11. The neural network device of claim 7, wherein the second plurality of synapses is configured to receive the second plurality of inputs as electrical voltages on the plurality of ninth lines.
12. The neural network device of claim 7, further comprising: a second plurality of neurons configured to receive the second plurality of outputs.
13. A neural network device, comprising: a first plurality of synapses configured to receive a first plurality of inputs and to generate therefrom a first plurality of outputs, wherein the first plurality of synapses comprises: a plurality of memory cells, wherein each of the memory cells includes spaced apart source and drain regions formed in a semiconductor substrate with a channel region extending there between, a floating gate disposed over and insulated from a first portion of the channel region, a first gate disposed over and insulated from a second portion of the channel region, a second gate disposed over and insulated from the floating gate, and a third gate disposed over and insulated from the source region; each of the plurality of memory cells is configured to store a weight value corresponding to a number of electrons on the floating gate; the plurality of memory cells are configured to generate the first plurality of outputs based upon the first plurality of inputs and the stored weight values; wherein the memory cells of the first plurality of synapses are arranged in rows and columns, and wherein the first plurality of synapses comprises: a plurality of first lines each electrically connecting together the first gates in one of the rows of the memory cells; a plurality of second lines each electrically connecting together the second gates in one of the columns of the memory cells; a plurality of third lines each electrically connecting together the third gates in one of the rows of the memory cells; a plurality of fourth lines each electrically connecting together the source regions in one of the rows of the memory cells; a plurality of fifth lines each electrically connecting together the drain regions in one of the columns of the memory cells; wherein the first plurality of synapses is configured to receive the first plurality of inputs as electrical voltages on the plurality of second lines or on the plurality of fifth lines, and to provide the first plurality of outputs as electrical currents on the plurality of fourth lines.
14. The neural network device of claim 13, wherein the first plurality of synapses is configured to receive the first plurality of inputs as electrical voltages on the plurality of second lines.
15. The neural network device of claim 13, wherein the first plurality of synapses is configured to receive the first plurality of inputs as electrical voltages on the plurality of fifth lines.
16. The neural network device of claim 13, further comprising: a first plurality of neurons configured to receive the first plurality of outputs.
17. The neural network device of claim 16, further comprising: a second plurality of synapses configured to receive a second plurality of inputs from the first plurality of neurons and to generate therefrom a second plurality of outputs, wherein the second plurality of synapses comprises: a plurality of second memory cells, wherein each of the second memory cells includes spaced apart second source and second drain regions formed in the semiconductor substrate with a second channel region extending there between, a second floating gate disposed over and insulated from a first portion of the second channel region, a fourth gate disposed over and insulated from a second portion of the second channel region, a fifth gate disposed over and insulated from the second floating gate, and a sixth gate disposed over and insulated from the second source region; each of the plurality of second memory cells is configured to store a second weight value corresponding to a number of electrons on the second floating gate; the plurality of second memory cells are configured generate the second plurality of outputs based upon the second plurality of inputs and the stored second weight values; wherein the second memory cells of the second plurality of synapses are arranged in rows and columns, and wherein the second plurality of synapses comprises: a plurality of sixth lines each electrically connecting together the fourth gates in one of the rows of the second memory cells; a plurality of seventh lines each electrically connecting together the fifth gates in one of the columns of the second memory cells; a plurality of eighth lines each electrically connecting together the sixth gates in one of the rows of the second memory cells; a plurality of ninth lines each electrically connecting together the second source regions in one of the rows of the second memory cells; a plurality of tenth lines each electrically connecting together the second drain regions in one of the columns of the second memory cells; wherein the second plurality of synapses is configured to receive the second plurality of inputs as electrical voltages on the plurality of seventh lines or on the plurality of tenth lines, and to provide the second plurality of outputs as electrical currents on the plurality of ninth lines.
18. The neural network device of claim 17, wherein the second plurality of synapses is configured to receive the second plurality of inputs as electrical voltages on the plurality of seventh lines.
19. The neural network device of claim 17, wherein the second plurality of synapses is configured to receive the second plurality of inputs as electrical voltages on the plurality of tenth lines.
20. The neural network device of claim 17, further comprising: a second plurality of neurons configured to receive the second plurality of outputs.
21. A neural network device, comprising: a first plurality of synapses configured to receive a first plurality of inputs and to generate therefrom a first plurality of outputs, wherein the first plurality of synapses comprises: a plurality of memory cells, wherein each of the memory cells includes spaced apart source and drain regions formed in a semiconductor substrate with a channel region extending there between, a floating gate disposed over and insulated from a first portion of the channel region, a first gate disposed over and insulated from a second portion of the channel region, a second gate disposed over and insulated from the floating gate, and a third gate disposed over and insulated from the source region; each of the plurality of memory cells is configured to store a weight value corresponding to a number of electrons on the floating gate; the plurality of memory cells are configured to generate the first plurality of outputs based upon the first plurality of inputs and the stored weight values; wherein the memory cells of the first plurality of synapses are arranged in rows and columns, and wherein the first plurality of synapses comprises: a plurality of first lines each electrically connecting together the first gates in one of the rows of the memory cells; a plurality of second lines each electrically connecting together the second gates in one of the rows of the memory cells; a plurality of third lines each electrically connecting together the third gates in one of the rows of the memory cells; a plurality of fourth lines each electrically connecting together the source regions in one of the rows of the memory cells; a plurality of fifth lines each electrically connecting together the drain regions in one of the columns of the memory cells; a plurality of transistors each electrically connected in series with one of the fifth lines; wherein the first plurality of synapses is configured to receive the first plurality of inputs as electrical voltages on gates of the plurality of transistors, and to provide the first plurality of outputs as electrical currents on the plurality of fourth lines.
22. The neural network device of claim 21, further comprising: a first plurality of neurons configured to receive the first plurality of outputs.
23. The neural network device of claim 22, further comprising: a second plurality of synapses configured to receive a second plurality of inputs from the first plurality of neurons and to generate therefrom a second plurality of outputs, wherein the second plurality of synapses comprises: a plurality of second memory cells, wherein each of the second memory cells includes spaced apart second source and second drain regions formed in the semiconductor substrate with a second channel region extending there between, a second floating gate disposed over and insulated from a first portion of the second channel region, a fourth gate disposed over and insulated from a second portion of the second channel region, a fifth gate disposed over and insulated from the second floating gate, and a sixth gate disposed over and insulated from the second source region; each of the plurality of second memory cells is configured to store a second weight value corresponding to a number of electrons on the second floating gate; the plurality of second memory cells are configured generate the second plurality of outputs based upon the second plurality of inputs and the stored second weight values; wherein the second memory cells of the second plurality of synapses are arranged in rows and columns, and wherein the second plurality of synapses comprises: a plurality of sixth lines each electrically connecting together the fourth gates in one of the rows of the second memory cells; a plurality of seventh lines each electrically connecting together the fifth gates in one of the rows of the second memory cells; a plurality of eighth lines each electrically connecting together the sixth gates in one of the rows of the second memory cells; a plurality of ninth lines each electrically connecting together the second source regions in one of the rows of the second memory cells; a plurality of tenth lines each electrically connecting together the second drain regions in one of the columns of the second memory cells; a second plurality of transistors each electrically connected in series with one of the tenth lines; wherein the second plurality of synapses is configured to receive the second plurality of inputs as electrical voltages on gates of the second plurality of transistors, and to provide the second plurality of outputs as electrical currents on the plurality of ninth lines.
24. The neural network device of claim 23, further comprising: a second plurality of neurons configured to receive the second plurality of outputs.
</claims>
</document>
