<document>

<filing_date>
2019-11-07
</filing_date>

<publication_date>
2020-05-14
</publication_date>

<priority_date>
2018-11-08
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
NEC LABORATORIES AMERICA
</assignee>

<inventors>
CHEN HAIFENG
ZONG, BO
ZHENG, CHENG
NI, JINGCHAO
</inventors>

<docdb_family_id>
70550600
</docdb_family_id>

<title>
METHOD FOR SUPERVISED GRAPH SPARSIFICATION
</title>

<abstract>
A method for employing a supervised graph sparsification (SGS) network to use feedback from subsequent graph learning tasks to guide graph sparsification is presented. The method includes, in a training phase (101), generating sparsified subgraphs by edge sampling (102) from input training graphs following a learned distribution, feeding the sparsified subgraphs to a prediction/classification component (103), collecting a predication/classification error (103), and updating parameters of the learned distribution (104) based on a gradient derived from the predication/classification error. The method further includes, in a testing phase (106), generating sparsified subgraphs by edge sampling (107) from input testing graphs following the learned distribution, feeding the sparsified subgraphs to the prediction/classification component (108), and outputting prediction/classification results to a visualization device (109).
</abstract>

<claims>
1. A computer-implemented method executed on a processor for employing a supervised graph sparsification (SGS) network to use feedback from subsequent graph learning tasks to guide graph sparsification, the method comprising:
in a training phase (101):
generating sparsified subgraphs by edge sampling (102) from input training graphs following a learned distribution;
feeding the sparsified subgraphs to a prediction/classification component
(103);
collecting a predication/classification error (103); and
updating parameters of the learned distribution (104) based on a gradient derived from the predication/classification error; and
in a testing phase (106):
generating sparsified subgraphs by edge sampling (107) from input testing graphs following the learned distribution;
feeding the sparsified subgraphs to the prediction/classification component (108); and
outputting prediction/classification results (109) to a visualization device.
2. The method of claim 1, wherein the edge sampling from input training graphs includes:
determining edge sampling probabilities (201); and
sampling differentiable edges by Gumbel-Softmax (202).
3. The method of claim 2, wherein an importance of an edge is given by: · where 7 ^ is a scalar indicating an importance of edge (u, v), x" is a vector representation of w's attributes, xv is a vector representation of v's attributes, eu,v is a vector representation of edge attributes, and MLPe() is a multi-layer neural network parameterized by Q.
4. The method of claim 3, wherein a probability that the edge (u, v) is sampled is given by:
5. The method of claim 4, wherein the differentiable edge sampling by the Gumbel-Softmax provides a sparse vector is given by:
where x is a constant and .v, is expected to be either close to 0 or 1 with
6. The method of claim 1, wherein the sparsified subgraphs are fed into graph neural networks (GNNs) to leam a graph representation for subsequent prediction components.
7. The method of claim 6, wherein the sparsified subgraphs are ^-neighbor subgraphs.
8. The method of claim 7, wherein the sparsified subgraphs are sampled before applying the GNNs.
9. The method of claim 1, wherein samples from the edge sampling from input training and testing graphs are differentiable.
10. A non-transitory computer-readable storage medium comprising a computerreadable program for employing a supervised graph sparsification (SGS) network to use feedback from subsequent graph learning tasks to guide graph sparsification, wherein the computer-readable program when executed on a computer causes the computer to perform the steps of:
in a training phase (101):
generating sparsified subgraphs by edge sampling (102) from input training graphs following a learned distribution;
feeding the sparsified subgraphs to a prediction/classification component
(103);
collecting a predication/classification error (103); and
updating parameters of the learned distribution (104) based on a gradient derived from the predication/classification error; and
in a testing phase (106):
generating sparsified subgraphs by edge sampling (107) from input testing graphs following the learned distribution; feeding the sparsified subgraphs to the prediction/classification component
(108); and
outputting prediction/classification results (109) to a visualization device.
11. The non-transitory computer-readable storage medium of claim 10, wherein the edge sampling from input training graphs includes:
determining edge sampling probabilities (201); and
sampling differentiable edges by Gumbel-Softmax (202).
12. The non-transitory computer-readable storage medium of claim 11, wherein an importance of an edge is given by: where is a scalar indicating an importance of edge (u, v), xu is a vector representation of w's attributes, xv is a vector representation of v's attributes, eu,v is a vector representation of edge attributes, and MLPe() is a multi-layer neural network parameterized by Q.
13. The non-transitory computer-readable storage medium of claim 12, wherein a probability that the edge (u, v) is sampled is given by:
14. The non-transitory computer-readable storage medium of claim 13, wherein the differentiable edge sampling by the Gumbel-Softmax provides a sparse vector is given by:
where x is a constant and .v, is expected to be either close to 0 or 1 with
15. The non-transitory computer-readable storage medium of claim 10, wherein the sparsified subgraphs are fed into graph neural networks (GNNs) to leam a graph representation for subsequent prediction components.
16. The non-transitory computer-readable storage medium of claim 15, wherein the sparsified subgraphs are ^-neighbor subgraphs.
17. The non-transitory computer-readable storage medium of claim 16, wherein the sparsified subgraphs are sampled before applying the GNNs.
18. The non-transitory computer-readable storage medium of claim 10, wherein samples from the edge sampling from input training and testing graphs are differentiable.
19. A system for employing a supervised graph sparsification (SGS) network to use feedback from subsequent graph learning tasks to guide graph sparsification, the system comprising:
a memory; and one or more processors in communication with the memory configured to:
in a training phase (101):
generate sparsified subgraphs by edge sampling (102) from input training graphs following a learned distribution;
feed the sparsified subgraphs to a prediction/classification component (103);
collect a predication/classification error (103); and
update parameters of the learned distribution (104) based on a gradient derived from the predication/classification error; and in a testing phase (106):
generate sparsified subgraphs by edge sampling (107) from input testing graphs following the learned distribution;
feed the sparsified subgraphs to the prediction/classification component (108); and
output prediction/classification results to a visualization device
(109).
20. The system of claim 19, wherein the edge sampling from input training graphs includes determining edge sampling probabilities (201) and sampling differentiable edges by Gumbel-Softmax (202), and
wherein an importance of an edge is given by:
¾" = MLPe(x" x" euJ where is a scalar indicating an importance of edge (u, v), x" is a vector representation of w's attributes, xv is a vector representation of v's attributes, eu,v is a vector representation of edge attributes, and MLP«( ) is a multi-layer neural network parameterized by Q.
</claims>
</document>
