<document>

<filing_date>
2019-10-23
</filing_date>

<publication_date>
2020-06-24
</publication_date>

<priority_date>
2018-12-19
</priority_date>

<ipc_classes>
G06N3/04,G06N3/063,G06N3/08
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
CHOI, CHANG KYU
HAN, JAEJOON
JUNG, SAN GIL
KWAK, YOUNGJUN
LEE, SEOH YUNG
SON, CHANGYONG
SON, JINWOO
</inventors>

<docdb_family_id>
68342676
</docdb_family_id>

<title>
NEURAL NETWORK PROCESSING METHOD AND APPARATUS BASED ON NESTED BIT REPRESENTATION
</title>

<abstract>
A neural network processing method and apparatus based on nested bit representation is provided. The processing method includes obtaining first weights for a first layer of a source model of a first layer of a neural network, determining a bit-width for the first layer of the neural network, obtaining second weights for the first layer of the neural network by extracting at least one bit corresponding to the determined bit-width from each of the first weights for the first layer of a source model corresponding to the first layer of the neural network, and processing input data of the first layer of the neural network by executing the first layer of the neural network based on the obtained second weights.
</abstract>

<claims>
1. A neural network processor-implemented method, comprising: obtaining first weights for a source model of a neural network; determining a bit-width for the neural network; obtaining second weights for the neural network by extracting at least one bit corresponding to the determined bit-width from each of the first weights for the source model; and processing input data of the neural network by executing the neural network based on the obtained second weights.
2. The method of claim 1, wherein the first weights are configured to have a higher bit-precision than the second weights.
3. The method of claim 1 or 2, wherein the second weights are nested in the first weights.
4. The method of claim 1, 2 or 3, wherein the bit-width for the neural network is determined based on a processing characteristic corresponding to the neural network, and
wherein the processing characteristic comprises at least one of a required processing speed, a required processing accuracy, a processing difficulty, or a terminal performance.
5. The method of any preceding claim, wherein: obtaining first weights is performed for a first layer of the source model corresponding to a first layer of the neural network; determining a bit-width is performed for the first layer of the neural network; obtaining second weights is performed for the first layer of the neural network by extracting at least one bit corresponding to the determined bit-width from each of the first weights for the first layer of the source model corresponding to the first layer of the neural network; and processing input data comprises processing input data of the first layer of the neural network by executing the first layer of the neural network based on the obtained second weights, further comprising: determining a bit-width for a second layer of the neural network; obtaining third weights for a second layer of a source model corresponding to the second layer of the neural network; obtaining fourth weights for the second layer of the neural network by extracting at least one bit corresponding to the determined bit-width for the second layer of the neural network from each of the third weights for the second layer of the source model corresponding to the second layer of the neural network; and processing input data of the second layer of the neural network by executing the second layer of the neural network based on the obtained fourth weights.
6. The method of claim 5, wherein the third weights either have a higher bit-precision than the fourth weights, or the fourth weights are nested therein.
7. The method of claim 5, wherein the first layer of the neural network executed based on the second weights is configured to process a first task based on the input data of the first layer, and
the second layer of the neural network executed based on the fourth weights is configured to process a second task different from the first task based on the input data of the second layer.
8. The method of any of claims 1 - 4, further comprising: determining a bit-width for a second neural network based on a result of the processing of the input data by the executing of the neural network; obtaining third weights for the second neural network by extracting at least one bit corresponding to the determined bit-width for the second neural network from each of the first weights; and processing input data of the second neural network by executing the second neural network based on the obtained third weights.
9. The method of claim 8, wherein the first weights either are configured to have a higher bit-precision than the third weights, or the second weights and the third weights are nested therein.
10. The method of claim 8 or 9, wherein the neural network that is executed based on the second weights is configured to process a first task based on the input data of the neural network, and
the second neural network that is executed based on the third weights is configured to process a second task that is different from the first task based on the input data of the second neural network.
11. The method of any of preceding claims 5 - 10, comprising: receiving multilevel input data from a group, at least comprising: multilevel image data; and multilevel voice data; executing the neural network or the first layer of the neural network based on first weights that are trained to process a first task based on the received input data; executing the second layer of the neural network or the second neural network based on second weights that are trained to process a second task based on the received input data; and outputting multilevel input data based on the processed first task and the processed second task.
12. A processor-implemented training method, comprising: determining weights of a low bit-width corresponding to a neural network or a first layer of a neural network by quantizing weights of a high bit-width corresponding to the neural network or the first layer of the neural network; determining loss values corresponding to the determined weights of the low bit-width by applying input data to the neural network or the first layer of the neural network; and updating the weights of the high bit-width based on the determined loss values.
13. The method of claim 12, further comprising: determining weight sets of the low bit-width corresponding to the first layer by quantizing the weights of the high bit-width, after training associated with the weights of the high bit-width is completed, wherein preferably the weight sets of the low bit-width comprise a weight set of a first bit-width and a weight set of a second bit-width having a lower bit-precision than the weight set of the first bit-width, and the weight set of the second bit-width is nested in the weight set of the first bit-width.
14. A non-transitory computer-readable storage medium storing instructions that, when executed by a processor, cause the processor to perform the method of any preceding claim.
15. A neural network processing or training apparatus, comprising: a processor; and a memory configured to store an instruction readable by the processor,
wherein, when the instruction is executed by the processor, the processor is configured to perform steps of any of the preceding claims 1 - 13.
</claims>
</document>
