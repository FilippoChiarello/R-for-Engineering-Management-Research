<document>

<filing_date>
2018-06-01
</filing_date>

<publication_date>
2020-04-30
</publication_date>

<priority_date>
2017-06-08
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
D5AI
</assignee>

<inventors>
BAKER, JAMES, K.
</inventors>

<docdb_family_id>
64566586
</docdb_family_id>

<title>
DATA SPLITTING BY GRADIENT DIRECTION FOR NEURAL NETWORKS
</title>

<abstract>
Systems and methods improve the performance of a network that has converged such that the gradient of the network and all the partial derivatives are zero (or close to zero) by splitting the training data such that, on each subset of the split training data, some nodes or arcs (i.e., connections between a node and previous or subsequent layers of the network) have individual partial derivative values that are different from zero on the split subsets of the data, although their partial derivatives averaged over the whole set of training data is close to zero. The present system and method can create a new network by splitting the candidate nodes or arcs that diverge from zero and then trains the resulting network with each selected node trained on the corresponding cluster of the data. Because the direction of the gradient i s different for each of the nodes or arcs that are split, the nodes and their arcs in the new network will train to be different. Therefore, the new network is not at a stationary point.
</abstract>

<claims>
1. A method of generating an improved neural network, the method comprising: splitting first training data into N groups of training data, where N>1, based on similarity of a gradient direction of an error cost function with respect to the first training data; and training a base neural network with the first training data, wherein the base neural network comprises N sub-network portions, and wherein each of the N sub-network portions is trained on a respective one of the N groups of training data.
2. The method of claim 1, wherein: the method further comprises, prior to training the base neural network, splitting a first node of the base neural network into a node set that comprises N new nodes, such that one new node in the node set corresponds to one and only one of each of the N groups of training data; and the step of training the base neural network comprises, for each data item in the first training data that belongs to one of the N groups of training data, dropping from the training for that data item all nodes in the node set that do not correspond to the group for the data item.
3. The method of claim 1, wherein: the method further comprises, prior to training the base neural network, generating an ensemble of N ensemble member networks, wherein each of the N ensemble members is identical to the base neural, and such that one ensemble member corresponds to one and only one of each of the N groups of training data; and the step of training the base neural network comprises training the ensemble, wherein, for each data item in the first training data that belongs to one of the N groups of training data, training the ensemble comprises dropping from the training for that data item all ensemble members in the ensemble that do not correspond to the group for the data item.
4. The method of claim 1, further comprising, prior to training the base neural network with the first training data, pre-training the base neural network to a desired performance level.
5. The method of claim 4, wherein pre-training the base neural network comprises pre-training the base neural network to convergence.
6. The method of claim 5, wherein pre-training the base neural network to convergence comprises iteratively pre-training the base neural network through gradient descent until a gradient of an applicable error cost function is below a threshold minimum.
7. The method of claim 1, wherein splitting the first training data into N groups comprises clustering the first training data into the N groups.
8. The method of claim 2, wherein: the first node, prior to splitting, comprises one or more incoming arcs and one or more outgoing arcs, with each of the incoming and outgoing arcs of the first node, prior to splitting, has a respective weight; and splitting the first node into the new node set comprises: having each new node in the new node set have the same number of incoming and outgoing arcs as the first node prior to splitting; and initializing the incoming and outgoing arcs of each new node in the new node set with equivalent weights as the first node prior to splitting, such that after the training, the weights for the incoming and outgoing arcs of each new node N in the new node set diverge.
9. The method of claim 2, wherein training the base neural network comprises, after dropping nodes for the first training data, training the base neural network, with the new node set, without dropping nodes for second training data that is different from the first training data.
10. The method of claim 3, wherein training the ensemble comprises, after dropping ensemble members for the first training data, training the ensemble without dropping ensemble members for second training data that is different from the first training data
11. The method of claim 1, wherein grouping the first training data comprises: computing a norm of partial derivatives over the first training data for each node in the base neural network; and grouping the first training data into the N groups based on similarity of gradient direction.
12. The method of claim 2, wherein grouping the first training data comprises: computing a norm of partial derivatives over the first training data for each node in the base neural network; and grouping the first training data into the N groups based on similarity of gradient direction.
13. The method of claim 12, further comprising, prior to splitting the first node, selecting the first node for splitting, wherein selecting the first node for splitting comprises: ranking all nodes in the base network based on the computed norm of partial derivatives; and selecting the first node from among all nodes in the base network whose computed norm of partial derivatives is above a threshold value.
14. The method of claim 12, further comprising, prior to splitting the first node, selecting the first node for splitting, wherein selecting the first node for splitting comprises: ranking all nodes in a selected, single layer of the base network based on the computed norm of partial derivatives; and selecting the first node from among all nodes in the selected, single layer of the base network whose computed norm of partial derivatives is above a threshold value.
15. The method of claim 12, wherein computing the norm of partial derivatives over first training data for each node in the base network comprises computing, for each node of the base neural network, the norm of the partial derivatives of the node.
16. The method of claim 12, wherein computing the norm of partial derivatives over first training data for each node in the base network comprises computing, for each node, the norm of a gradient of weights on arcs coming into the node.
17. The method of claim 12, wherein computing the norm of partial derivatives over first training data for each node in the base network comprises computing, for each node, the norm of a gradient of weights on arcs leaving the node.
18. The method of claim 7, wherein clustering the first training data into the N clusters comprises: generating a vector for each training example in the first training data; and clustering the vectors into N clusters.
19. The method of claim 18, wherein generating the vector for each training example comprises generating, for each training example, a vector that comprises a set of partial derivatives for nodes in the base network.
20. The method of claim 18, wherein generating the vector for each training example comprises generating, for each training example, a vector that comprises a gradient of partial derivative of incoming arcs for one or more nodes of the base network.
21. The method of claim 18, wherein generating the vector for each training example comprises generating, for each training example, a vector that comprises a gradient of partial derivative of ongoing arcs for one or more nodes of the base network.
22. The method of claim 18, wherein generating the vector for each training example comprises generating, for each training example, a concatenated gradient vectors.
23. The method of claim 18, wherein clustering the vectors comprises clustering the vectors using a clustering algorithm.
24. The method of claim 18, wherein clustering the vector comprises training an autoencoder with a bottleneck layer.
25. A computer system for generating an improved neural network, the computer system comprising one or more computers, wherein the one or more computers comprise at least one processor and associated memory, wherein the associated memory stores software that when executed by the at least one processor, causes the at least one processor to: split first training data into N groups of training data, where N>1, based on similarity of a gradient direction of an error cost function with respect to the first training data; and train a base neural network with the first training data, wherein the base neural network comprises N sub-network portions, and wherein each of the N sub-network portions is trained on a respective one of the N groups of training data.
26. The computer system of claim 25, wherein the at least one processor is programmed to: prior to training the base neural network, split a first node of the base neural network into a node set that comprises N new nodes, such that one new node in the node set corresponds to one and only one of each of the N groups of training data; and train the base neural network by, for each data item in the first training data that belongs to one of the N groups of training data, dropping from the training for that data item all nodes in the node set that do not correspond to the group for the data item.
27. The computer system of claim 25, wherein the at least one processor is programmed to: prior to training the base neural network, generate an ensemble of N ensemble member networks, wherein each of the N ensemble members is identical to the base neural, and such that one ensemble member corresponds to one and only one of each of the N groups of training data; and training the base neural network by training the ensemble, wherein, for each data item in the first training data that belongs to one of the N groups of training data, training the ensemble comprises dropping from the training for that data item all ensemble members in the ensemble that do not correspond to the group for the data item.
28. The computer system of claim 25, wherein the at least one processor is programmed to, prior to training the base neural network with the first training data, pre-training the base neural network to a desired performance level.
29. The computer system of claim 28, wherein the at least one processor is programmed to pre-train the base neural network by pre-training the base neural network to convergence.
30. The computer system of claim 29, wherein the at least one processor is programmed to pre-train the base neural network to convergence by iteratively pre-training the base neural network through gradient descent until a gradient of an applicable error cost function is below a threshold minimum.
31. The computer system of claim 25, wherein the at least one processor is programmed to split the first training data into N groups by clustering the first training data into the N groups.
32. The computer system of claim 26, wherein: the first node, prior to splitting, comprises one or more incoming arcs and one or more outgoing arcs, with each of the incoming and outgoing arcs of the first node, prior to splitting, has a respective weight; and the at least one processor is programmed to split the first node into the new node set by: having each new node in the new node set have the same number of incoming and outgoing arcs as the first node prior to splitting; and initializing the incoming and outgoing arcs of each new node in the new node set with equivalent weights as the first node prior to splitting, such that after the training, the weights for the incoming and outgoing arcs of each new node N in the new node set diverge.
33. The computer system of claim 26, wherein the at least one processor is programmed to train the base neural network by, after dropping nodes for the first training data, training the base neural network, with the new node set, without dropping nodes for second training data that is different from the first training data.
34. The computer system of claim 27, wherein the at least one processor is programmed to train the ensemble by, after dropping ensemble members for the first training data, training the ensemble without dropping ensemble members for second training data that is different from the first training data.
35. The computer system of claim 25, wherein the at least one processor is programmed to group the first training data by: computing a norm of partial derivatives over the first training data for each node in the base neural network; and grouping the first training data into the N groups based on similarity of gradient direction.
36. The computer system of claim 26, wherein the at least one processor is programmed to group the first training data by: computing a norm of partial derivatives over the first training data for each node in the base neural network; and grouping the first training data into the N groups based on similarity of gradient direction.
37. The computer system of claim 36, wherein the at least one processor is programmed to, prior to splitting the first node, selecting the first node for splitting, wherein the at least one processor is programmed to select the first node for splitting by: ranking all nodes in the base network based on the computed norm of partial derivatives; and selecting the first node from among all nodes in the base network whose computed norm of partial derivatives is above a threshold value.
38. The computer system of claim 36, wherein the at least one processor is programmed to, prior to splitting the first node, selecting the first node for splitting, wherein the at least one processor is programmed to select the first node for splitting by: ranking all nodes in a selected, single layer of the base network based on the computed norm of partial derivatives; and selecting the first node from among all nodes in the selected, single layer of the base network whose computed norm of partial derivatives is above a threshold value.
39. The computer system of claim 36, wherein the at least one processor is programmed to compute the norm of partial derivatives over first training data for each node in the base network by computing, for each node of the base neural network, the norm of the partial derivatives of the node.
40. The computer system of claim 36, wherein the at least one processor is programmed to compute the norm of partial derivatives over first training data for each node in the base network by computing, for each node, the norm of a gradient of weights on arcs coming into the node.
41. The computer system of claim 36, wherein the at least one processor is programmed to compute the norm of partial derivatives over first training data for each node in the base network by computing, for each node, the norm of a gradient of weights on arcs leaving the node.
42. The computer system of claim 31, wherein the at least one processor is programmed to cluster the first training data into the N clusters by: generating a vector for each training example in the first training data; and clustering the vectors into N clusters.
43. The computer system of claim 42, wherein the at least one processor is programmed to generate the vector for each training example by generating, for each training example, a vector that comprises a set of partial derivatives for nodes in the base network.
44. The computer system of claim 42, wherein the at least one processor is programmed to generate the vector for each training example by generating, for each training example, a vector that comprises a gradient of partial derivative of incoming arcs for one or more nodes of the base network.
45. The computer system of claim 42, wherein the at least one processor is programmed to generate the vector for each training example by generating, for each training example, a vector that comprises a gradient of partial derivative of ongoing arcs for one or more nodes of the base network.
46. The computer system of claim 42, wherein the at least one processor is programmed to generate the vector for each training example by generating, for each training example, a concatenated gradient vectors.
47. The computer system of claim 42, wherein the at least one processor is programmed to cluster the vectors using a clustering algorithm.
48. The computer system of claim 42, wherein the at least one processor is programmed to cluster the vectors by training an autoencoder with a bottleneck layer.
49. A computer system for generating an improved neural network, the computer system comprising: a first set of one or more processing cores for pre-training a base neural network to a desired performance level; and a second set of one or more processing cores for: splitting first training data into N groups of training data, where N>1, based on similarity of a gradient direction of an error cost function with respect to the first training data; and training the base neural network with the first training data, wherein the base neural network comprises N sub-network portions, and wherein each of the N sub-network portions is trained on a respective one of the N groups of training data.
50. The computer system of claim 49, wherein the second set of one or more processing cores: are further for, prior to training the base neural network, splitting a first node of the base neural network into a node set that comprises N new nodes, such that one new node in the node set corresponds to one and only one of each of the N groups of training data; and train the base neural network by, for each data item in the first training data that belongs to one of the N groups of training data, dropping from the training for that data item all nodes in the node set that do not correspond to the group for the data item.
51. The computer system of claim 49, wherein the second set of one or more processing cores: are further for, prior to training the base neural network, generating an ensemble of N ensemble member networks, wherein each of the N ensemble members is identical to the base neural, and such that one ensemble member corresponds to one and only one of each of the N groups of training data; and train the base neural network by training the ensemble, wherein, for each data item in the first training data that belongs to one of the N groups of training data, training the ensemble comprises dropping from the training for that data item all ensemble members in the ensemble that do not correspond to the group for the data item.
</claims>
</document>
