<document>

<filing_date>
2018-12-27
</filing_date>

<publication_date>
2020-07-02
</publication_date>

<priority_date>
2018-12-27
</priority_date>

<ipc_classes>
G06N20/00
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
TIWARY, SAURABH KUMAR
SONG, XIA
AGHAJANYAN, ARMEN
</inventors>

<docdb_family_id>
71123964
</docdb_family_id>

<title>
Computer-Implemented Generation and Utilization of a Universal Encoder Component
</title>

<abstract>
Computer-implemented techniques are described herein for generating and utilizing a universal encoder component (UEC). The UEC maps a linguistic expression in a natural language to a language-agnostic representation of the linguistic expression. The representation is said to be agnostic with respect to language because it captures semantic content that is largely independent of the syntactic rules associated with the natural language used to compose the linguistic expression. The representations is also agnostic with respect to task because a downstream training system can leverage it to produce different kinds to machine-trained components that serve different respective tasks. The UEC facilitates the generation of downstream machine-trained models by permitting a developer to train a model based on input examples expressed in a language jα, and thereafter apply it to the interpretation of documents in language jβ, with no additional training required.
</abstract>

<claims>
1. One or more computing devices for generating a task-specific machine-trained component, comprising: one or more hardware processors that execute operations based on machine-readable instructions stored in a memory and/or based on logic embodied in a task-specific collection of logic gates, the operations including: receiving a universal encoder component that is produced by a first computer-implemented training system; and using a second computer-implemented training system to generate a task-specific machine-trained component based on a set of input training examples expressed in at least one natural language, the second training system using the universal encoder component to convert each input training example into a language-agnostic representation of the input training example, the task-specific machine-trained component, once trained, providing a computer-implemented tool for mapping an input document, expressed in an input natural language, into an output result, said mapping also applying to a case in which the input natural language of the input document is not among said at least one natural language that was used to train the task-specific machine-trained component, the second training system corresponding to a different training system or a same training system as the first training system.
2. The one or more computing devices of claim 1, wherein the operations further include producing plural task-specific machine-trained components using the universal encoder component that perform plural respective different tasks, the universal encoder component also being agnostic with respect to task.
3. The one or more computing devices of claim 1, wherein the universal encoder component uses a machine-trained model for mapping linguistic content expressed in a given natural language into a language-agnostic representation of the linguistic content.
4. The one or more computing devices of claim 1, wherein the first training system produces the universal encoder component using a generative adversarial network (GAN).
5. The one or more computing devices of claim 1, wherein the first training system generates the universal encoder component by simultaneously training a language model component and a discriminator component, and wherein the first training system generates the universal encoder component based on a training objective that takes into consideration at least: loss information based on a measure of predictive accuracy of the language model component; and loss information based on a measure of coherence among language-agnostic representations of input training examples expressed in different natural languages, said measure of coherence being based on output information generated by the discriminator component.
6. The one or more computing devices of claim 5, wherein the measure of coherence is generated by computing, for each of a plurality of pairs of language-agnostic representations, a distance between a first language-agnostic representation associated with an input training example in a first natural language, and a second language-agnostic representation associated with an input training example expressed in a second natural language.
7. The one or more computing devices of claim 6, wherein the distance is a Wasserstein distance.
8. A computer-readable storage medium for storing computer-readable instructions, the computer-readable instructions, when executed by one or more hardware processors, providing a task-specific machine-trained component that performs operations of: receiving an input document expressed in an input natural language; converting the input document into a language-agnostic representation of the input document using a universal encoder component; and mapping the language-agnostic representation to an output result, the task-specific machine-trained component having been trained based on input training examples expressed in at least one natural language, said mapping applying to a case in which the input natural language of the input document is not among said at least one natural language that was used to train the task-specific machine-trained component.
9. The computer-readable storage medium of claim 8, wherein the universal encoder component is also agnostic with respect to a task performed by the task-specific machine-trained component.
10. The computer-readable storage medium of claim 8, wherein the universal encoder component uses a machine-trained model for mapping linguistic content in the input document into a language-agnostic representation of the linguistic content.
11. The computer-readable storage medium of clam 8, the universal encoder component is produced by a training system that uses a generative adversarial network (GAN).
12. The computer-readable storage medium of claim 8, wherein the universal encoder component is produced by a training system by simultaneously training a language model component and a discriminator component, and wherein the training system generates the universal component based on a training objective that takes into consideration at least: loss information based on a measure of predictive accuracy of the language model component; and loss information based on a measure of coherence among language-agnostic representations of input training examples expressed in different natural languages, said measure of coherence being based on output information generated by the discriminator component.
13. A method, implemented by one or more computing devices, for performing machine-training in a training system, comprising: in a training operation: using plural language-specific encoder components to convert input training examples expressed in different natural languages into respective language-specific representations of the input training examples; using a language-agnostic encoder component to convert each language-specific representation into a language-agnostic representation; for each natural language associated with the input training examples: using a language-specific decoder component to convert the language-agnostic representation, together with rule information which characterizes syntactic principles underling the natural language, into a language-specific decoded representation; and using a language-specific output component to convert the language-specific decoded representation into an output result expressed in the natural language, each language-specific encoder component, the language-agnostic encoder component, the language-specific decoder component, and each language-specific output component corresponding to parts of a language model component; generating first loss information based on a measure of predictive accuracy of the language model component; using a discriminator component to generate output information based on language-agnostic representations of the input training examples; generating second loss information, based on the output information, that reflects a measure of coherence among the language-agnostic representations of input training examples expressed in different natural languages; and adjusting weight values in the training system based on the first loss information and the second loss information; and repeating the training operation until a training objective is achieved, at which time the method provides a universal encoder component corresponding to a trained version of at least one language-specific encoder component and a trained version of the language-agnostic encoder component.
14. The method of claim 13, wherein the language model component and the discriminator component are each implemented using one or more machine-trained models.
15. The method of claim 14, wherein at least one machine-trained model used by the method is a neural network.
16. The method of claim 15, wherein at least one neural network used by the method is a recurrent neural network (RNN).
17. The method of claim 13, wherein the measure of coherence is generated by computing, for each of a plurality of pairs of language-agnostic representations, a distance between a first language-agnostic representation associated with an input training example expressed in a first natural language, and a second language-agnostic representation associated with an input training example expressed in a second natural language.
18. The method of claim 17, wherein the distance is a Wasserstein distance.
19. The method of claim 13, wherein the training system performs its training using a generative adversarial network (GAN).
20. The method of claim 13, further comprising, in a further training operation: using the universal encoder component to generate a task-specific machine-trained component based on a set of input training examples expressed in at least one natural language, the task-specific machine-trained component, once trained, providing a computer-implemented tool for mapping an input document, expressed in an input natural language, into an output result, said mapping also applying to a case in which the input natural language of the input document is not among said at least one natural language that was used to produce the task-specific machine-trained component.
</claims>
</document>
