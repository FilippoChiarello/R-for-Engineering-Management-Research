<document>

<filing_date>
2020-03-19
</filing_date>

<publication_date>
2020-09-24
</publication_date>

<priority_date>
2019-03-21
</priority_date>

<ipc_classes>
G06N3/08,H04N19/13,H04N19/136,H04N19/90
</ipc_classes>

<assignee>
QUALCOMM
</assignee>

<inventors>
HABIBIAN, AMIRHOSSEIN
COHEN, Taco Sebastiaan
</inventors>

<docdb_family_id>
70296035
</docdb_family_id>

<title>
VIDEO COMPRESSION USING DEEP GENERATIVE MODELS
</title>

<abstract>
Certain aspects of the present disclosure are directed to methods and apparatus for compressing video content using deep generative models. One example method generally includes receiving video content for compression. The received video content is generally encoded into a latent code space through an auto-encoder, which may be implemented by a first artificial neural network. A compressed version of the encoded video content is generally generated through a trained probabilistic model, which may be implemented by a second artificial neural network, and output for transmission.
</abstract>

<claims>
What is claimed is:
1. A method for compressing video, comprising:
receiving video content for compression;
encoding the received video content into a latent code space through an auto encoder implemented by a first artificial neural network;
generating a compressed version of the encoded video content through a probabilistic model implemented by a second artificial neural network; and
outputting the compressed version of the encoded video content for transmission.
2. The method of claim 1, wherein the auto-encoder is trained by:
receiving first video content;
encoding the first video content into the latent code space;
generating a reconstructed version of the first video content by decoding the encoded first video content;
comparing the reconstructed version of the first video content to the received first video content; and
adjusting the auto-encoder based on the comparing.
3. The method of claim 2, wherein adjusting the auto-encoder comprises performing a gradient descent.
4. The method of claim 1, wherein the auto-encoder is configured to encode the received video content into the latent code space based on a three-dimensional filter, wherein dimensions of the three-dimensional filter comprise height of a video frame, width of the video frame, and time of the video frame.
5. The method of claim 1, wherein the trained probabilistic model comprises an auto-regressive model of a probability distribution over four-dimensional tensors, the probability distribution illustrating a likelihood that different codes can be used to compress the encoded video content.
6. The method of claim 5, wherein the probabilistic model generates data based on a four-dimensional tensor, wherein dimensions of the four-dimensional tensor comprise time, a channel, and spatial dimensions of the video content.
7. The method of claim 5, wherein the probability distribution is generated based on a factorization of dependencies.
8. The method of claim 7, wherein the factorization of dependencies represents the probability distribution based on a code associated with a current time slice in the video content and a conditioning signal.
9. The method of claim 8, wherein the conditioning signal comprises an output generated by a recurrent neural network for an input of codes associated with previous time slices in the video content other than the current time slice.
10. The method of claim 9, wherein the recurrent neural network comprises a set of convolutional long short-term memory (LSTM) layers.
11. A system for compressing video, comprising:
at least one processor configured to:
receive video content for compression;
encode the received video content into a latent code space through an auto-encoder implemented by a first artificial neural network configured to execute on the at least one processor;
generate a compressed version of the encoded video content through a probabilistic model implemented by a second artificial neural network configured to execute on the at least one processor; and
output the compressed version of the encoded video content for transmission; and a memory coupled to the at least one processor.
12. The system of claim 11, wherein the at least one processor is configured to train the auto-encoder by:
receiving first video content;
encoding the first video content into the latent code space;
generating a reconstructed version of the first video content by decoding the encoded first video content;
comparing the reconstructed version of the first video content to the received first video content; and
adjusting the auto-encoder based on the comparing.
13. The system of claim 12, wherein adjusting the auto-encoder comprises performing a gradient descent.
14. The system of claim 11, wherein the auto-encoder is configured to encode the received video content into the latent code space based on a three-dimensional filter, wherein dimensions of the three-dimensional filter comprise height of a video frame, width of the video frame, and time of the video frame.
15. The system of claim 11, wherein the trained probabilistic model comprises an auto-regressive model of a probability distribution over four-dimensional tensors, the probability distribution illustrating a likelihood that different codes can be used to compress the encoded video content.
16. The system of claim 15, wherein the second artificial neural network
implementing the probabilistic model is configured to generate data based on a four dimensional tensor, wherein dimensions of the four-dimensional tensor comprise time, a channel, and spatial dimensions of the video content.
17. The system of claim 15, wherein the probability distribution is generated based on a factorization of dependencies. 18. The system of claim 17, wherein the factorization of dependencies represents the probability distribution based on a code associated with a current time slice in the video content and a conditioning signal.
19. The system of claim 18, wherein the second artificial neural network comprises a recurrent neural network executing on the at least one processor, wherein the conditioning signal comprises an output generated by the recurrent neural network for an input of codes associated with previous time slices in the video content other than the current time slice.
20. The system of claim 19, wherein the recurrent neural network comprises a set of convolutional long short-term memory (LSTM) layers.
21. A method for decompressing encoded video, comprising:
receiving a compressed version of an encoded video content;
decompressing the compressed version of the encoded video content based on a probabilistic model implemented by a first artificial neural network into a latent code space;
decoding the encoded video content out of the latent code space through an auto encoder implemented by a second artificial neural network; and
outputting the decoded video content for display.
22. The method of claim 21, wherein the auto-encoder is trained by:
receiving first video content;
encoding the first video content into the latent code space;
generating a reconstructed version of the first video content by decoding the encoded first video content;
comparing the reconstructed version of the first video content to the received first video content; and
adjusting the auto-encoder based on the comparing.
23. The method of claim 21, wherein the auto-encoder is configured to decode the encoded video content out of the latent code space based on a three-dimensional filter, wherein dimensions of the three-dimensional filter comprise height of a video frame, width of the video frame, and time of the video frame.
24. The method of claim 21, wherein the probabilistic model comprises an auto regressive model of a probability distribution over four-dimensional tensors, the probability distribution illustrating a likelihood that different codes can be used to decompress the encoded video content.
25. The method of claim 24, wherein the probabilistic model generates data based on a four-dimensional tensor, wherein dimensions of the four-dimensional tensor comprise time, a channel, and spatial dimensions of the video content.
26. The method of claim 25, wherein the probability distribution is generated based on a factorization of dependencies.
27. The method of claim 26, wherein the factorization of dependencies represents the probability distribution based on a code associated with a current time slice in the video content and a conditioning signal.
28. The method of claim 27, wherein the conditioning signal comprises an output generated by a recurrent neural network for an input of codes associated with previous time slices in the video content other than the current time slice.
29. The method of claim 28, wherein the recurrent neural network comprises a set of convolutional long short-term memory (LSTM) layers.
30. A system for decompressing encoded video, comprising:
at least one processor configured to:
receive a compressed version of an encoded video content; decompress the compressed version of the encoded video content into a latent code space based on a probabilistic model implemented by a first artificial neural network configured to execute on the at least one processor;
decode the encoded video content out of the latent code space through an auto-encoder implemented by a second artificial neural network configured to execute on the at least one processor; and
output the decoded video content for display; and
a memory coupled to the at least one processor.
</claims>
</document>
