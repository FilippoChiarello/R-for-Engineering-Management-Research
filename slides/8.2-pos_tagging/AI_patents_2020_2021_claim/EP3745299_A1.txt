<document>

<filing_date>
2019-05-31
</filing_date>

<publication_date>
2020-12-02
</publication_date>

<priority_date>
2019-05-31
</priority_date>

<ipc_classes>
G06K9/00,G06K9/62
</ipc_classes>

<assignee>
INFINEON TECHNOLOGIES
</assignee>

<inventors>
BALAZS, Gabor
</inventors>

<docdb_family_id>
66685438
</docdb_family_id>

<title>
NEURAL NETWORK DEVICE AND METHOD USING A NEURAL NETWORK FOR SENSOR FUSION
</title>

<abstract>
A neural network device and a method using a neural network for sensor fusion are disclosed, wherein the neural network device includes a neural network (136) and wherein the neural network (136) is configured to: process a first grid (132) including a plurality of grid cells, wherein the first grid (132) represents at least a first portion of a field of view (108) of a first sensor (104), wherein at least one grid cell has information about an occupancy of the first portion of the field of view (108) assigned to the at least one grid cell, the information being based on data provided by the first sensor (104); process a second grid (134) including a plurality of grid cells, wherein the second grid (134) represents at least a second portion of a field of view (110) of a second sensor (106), wherein at least one grid cell has information about an occupancy of the second portion of the field of view (110) assigned to the at least one grid cell, the information being based on data provided by the second sensor (106); and fuse the processed first grid (132) with the processed second grid (134) into a fused grid (138), wherein the fused grid (138) includes information about the occupancy of the first portion of the field of view (108) of the first sensor (104) and the occupancy of the second portion of the field of view (110) of the second sensor (106).
</abstract>

<claims>
1. A neural network device comprising:
a neural network (136) configured to process a first grid (132) comprising a plurality of grid cells, wherein the first grid (132) represents at least a first portion of a field of view (108) of a first sensor (104), wherein at least one grid cell has information about an occupancy of the first portion of the field of view (108) assigned to the at least one grid cell, the information being based on data provided by the first sensor (104); process a second grid (134) comprising a plurality of grid cells, wherein the second grid (134) represents at least a second portion of a field of view (110) of a second sensor (106), wherein at least one grid cell has information about an occupancy of the second portion of the field of view (110) assigned to the at least one grid cell, the information being based on data provided by the second sensor (106); and fuse the processed first grid (132) with the processed second grid (134) into a fused grid (138), wherein the fused grid (138) includes information about the occupancy of the first portion of the field of view (108) of the first sensor (104) and the occupancy of the second portion of the field of view (110) of the second sensor (106).
2. The neural network device of claim 1,
wherein at least a portion of the neural network (136) is implemented by one or more processors (122).
3. The neural network device of claim 1 or 2,
wherein the neural network (136) is further configured to determine, for at least one grid cell of the fused grid (138), a probability of the occupancy of the first portion of the field of view (108) of the first sensor (104) and the second portion of the field of view (110) of the second sensor (106) assigned to the at least one grid cell of the fused grid (138).
4. The neural network device of any one of claims 1 to 3, wherein the neural network (136) is further configured to take into account the information about an occupancy of a portion of the field of view (108) next to the first portion and/or next to the second portion when determining information about the occupancy assigned to a respective fused grid cell of the fused grid (138).
5. The neural network device of any one of claims 1 to 4, wherein the neural network (136) comprises a first neural network portion configured to process the first grid (132);
a second neural network portion configured to process the second grid (134); and
a fusion neural network portion configured to fuse the processed first grid (132) with the processed second grid (134) into the fused grid (138).
6. The neural network device of any one of claims 1 to 5, wherein the neural network (136) comprises an auto-encoder;
wherein the auto-encoder comprises: a first encoder configured to process the first grid (132), a second encoder configured to process the second grid (134), and a decoder configured to provide the fused grid (138) based on the processed first grid (132) and the processed second grid (134).
7. The neural network device of any one of claims 1 to 6, wherein the neural network (136) comprises one or more skip connections;
wherein the one or more skip connections bypass code from the first neural network portion and/or the second neural network portion to the fusion neural network portion.
8. The neural network device of any one of claims 1 to 7,
wherein a first inverse sensor model is applied to the data provided by the first sensor (104) to provide the information about the occupancy of the first portion of the field of view (108) of the first sensor (104); and wherein a second inverse sensor model is applied to the data provided by the second sensor (106) to provide the information about the occupancy of the second portion of the field of view (110) of the second sensor (106).
9. The neural network device of claim 1 to 8,
wherein the neural network (136) is further configured to determine a free space within the fused grid (138) based on the fused grid (138).
10. A system, comprising: the device of any one of claims 1 to 9; the first sensor (104) configured to provide data for the information of the first grid (132); and the second sensor (106) configured to provide data for the information of the second grid (134).
11. A vehicle (100), comprising:
a driver assistance system comprising the system of claim 10.
12. A method, comprising:
a neural network (136): processing a first grid (132) comprising a plurality of grid cells, wherein the first grid (132) represents at least a first portion of a field of view (108) of a first sensor (104), wherein at least one grid cell has information about an occupancy of the first portion of the field of view (108) assigned to the at least one grid cell, the information being based on data provided by the first sensor (104); processing a second grid (134) comprising a plurality of grid cells, wherein the second grid (134) represents at least a second portion of a field of view (110) of a second sensor (106), wherein at least one grid cell has information about an occupancy of the second portion of the field of view (110) assigned to the at least one grid cell, the information being based on data provided by the second sensor (106); and fusing the processed first grid (132) with the processed second grid (134) into a fused grid (138), wherein the fused grid (138) comprises information about the occupancy of the first portion of the field of view (108) of the first sensor (104) and the occupancy of the second portion of the field of view (110) of the second sensor (106).
13. The method of claim 12,
wherein at least a portion of the neural network (136) is implemented by one or more processors.
14. The method of any one of claims 12 or 13,
wherein the neural network (136) determines, for at least one grid cell of the fused grid (138), a probability of the occupancy of the first portion of the field of view (108) of the first sensor (104) and the second portion of the field of view (110) of the second sensor (106) assigned to the at least one grid cell of the fused grid (138).
15. The method of any one of claims 12 to 14,
wherein a first inverse sensor model is applied to the data provided by the first sensor (104) to provide the information about the occupancy of the first portion of the field of view (108) of the first sensor (104); and wherein a second inverse sensor model is applied to the data provided by the second sensor (106) to provide the information about the occupancy of the second portion of the field of view (110) of the second sensor (106).
</claims>
</document>
