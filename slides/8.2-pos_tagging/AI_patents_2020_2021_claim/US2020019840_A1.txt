<document>

<filing_date>
2019-07-15
</filing_date>

<publication_date>
2020-01-16
</publication_date>

<priority_date>
2018-07-13
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
LIU HUAN
ARIZONA STATE UNIVERSITY
LI JUNDONG
GUO, RUOCHENG
</assignee>

<inventors>
LIU HUAN
LI JUNDONG
GUO, RUOCHENG
</inventors>

<docdb_family_id>
69138404
</docdb_family_id>

<title>
SYSTEMS AND METHODS FOR SEQUENTIAL EVENT PREDICTION WITH NOISE-CONTRASTIVE ESTIMATION FOR MARKED TEMPORAL POINT PROCESS
</title>

<abstract>
Embodiments for systems and methods of sequential event prediction with noise-contrastive estimation for marked temporal point process are disclosed.
</abstract>

<claims>
1. A method of improving the training of marked temporal point process models, comprising: utilizing a processor in communication with a tangible storage medium storing instructions that are executed by the processor to perform operations comprising: accessing a model that expresses sequential events using a marked temporal point process, the model configured to formulate a prediction of a future event and associated future event characteristics based on sequential event data applied to the model; training the model, by: identifying event samples Pd and initial noise samples Pn from a data distribution associated with the model, generating additional noise samples from the data distribution, and utilizing the additional noise samples to re-parameterize the model to adaptively push noise distribution associated with Pn towards Pd as the model accesses more information from the data distribution.
2. The method of claim 1, further comprising: generating the additional noise samples, by: computing values for a predicted time and a predicted mark by using as inputs at least a history sequence i and a parametric model pθ; and generating, iteratively, a predetermined number of times a noise sample inputting at least the predicted time and predicted mark x, and utilizing a probability density function inputting at least a value of variance and a value of mean which increase with respect to a number of iterations.
3. The method of claim 2, wherein the probability density function outputs an adaptive Gaussian noise distribution.
4. The method of claim 2, wherein a neural deep learning model maps an observed history of the history sequence i to a vector representation hi.
5. The method of claim 1, wherein the event samples pd are continuous joint distributions of a time value t and predicted mark x.
6. The method of claim 1, further comprising generating at least one noise event for an observed event.
7. The method of claim 1, wherein a plurality of dense layers are applied to project a raw input into a multi-dimensional space.
8. A processor, configured to: access a model that expresses sequential events using a marked temporal point process, the model configured to formulate a prediction of a future event and associated future event characteristics based on sequential event data applied to the model; train the model using adaptive noise sample generation, by: identifying event samples Pd and initial noise samples Pn from a data distribution associated with the model, generating additional noise samples from the data distribution, and utilizing the additional noise samples to re-parameterize the model to adaptively push noise distribution associated with Pn towards Pd as the model accesses more information from the data distribution.
9. The processor of claim 8 further configured to generate the additional noise samples by: computing values for a predicted time and a predicted mark by using as inputs at least a history sequence i and a parametric model pθ; and generating, iteratively, a predetermined number of times a noise sample inputting at least the predicted time and predicted mark x, and utilizing a probability density function inputting at least a value of variance and a value of mean which increase with respect to a number of iterations.
10. The processor of claim 9, wherein the probability density function outputs an adaptive Gaussian noise distribution.
11. The processor of claim 9, further configured to employ a neural deep learning model that maps an observed history of the history sequence i to a vector representation hi.
12. The processor of claim 8, wherein the event samples pd are continuous joint distributions of a time value t and predicted mark x.
13. The processor of claim 8, further configured to generate at least one noise event for an observed event.
14. The processor of claim 8, further configured to apply a plurality of dense layers to project a raw input into a multi-dimensional space.
</claims>
</document>
