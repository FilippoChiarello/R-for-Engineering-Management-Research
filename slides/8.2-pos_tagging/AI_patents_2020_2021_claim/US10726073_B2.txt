<document>

<filing_date>
2019-10-28
</filing_date>

<publication_date>
2020-07-28
</publication_date>

<priority_date>
2018-10-26
</priority_date>

<ipc_classes>
G06F12/0802,G06F16/00,G06F16/9035,G06F9/30,G06N3/04
</ipc_classes>

<assignee>
TENSIL AI COMPANY
</assignee>

<inventors>
ALCORN, THOMAS DAVID BAXTER
</inventors>

<docdb_family_id>
70325176
</docdb_family_id>

<title>
Method and apparatus for compiling computation graphs into an integrated circuit
</title>

<abstract>
Disclosed are systems and methods for a compiler, which can receive a computation workload, and a description of the computation graph of the workload and compile a circuit layout of the workload. In one embodiment, an RTL generator assigns the node operations of the computation graph to a first or second type. In the first type, the workload is loaded and processed in tiles equal to a compute filter width. In the second type, the workload is loaded in tiles larger in size than the width of the compute filter, allowing the compute filter to process more operations in parallel and reach the data needed for the underlying operations more efficiently.
</abstract>

<claims>
1. A method, implemented by at least one computing device, comprising: receiving and storing an artificial intelligence workload and a computation graph of the artificial intelligence workload, wherein the workload comprises a compute filter having a filter height and a filter width associated with a local input cache memory; normalizing the computation graph; parsing the normalized computation graph and determining node operations of the normalized computation graph; identifying the node operations as either a first or second type of hardware construct; and querying a hardware construct library to assign one or more hardware modules to perform the identified node operations, wherein each hardware construct in the hardware construct library describes at least a connection interface using a special domain-specific language associated with the one or more hardware modules, the identified node operations of the first type are performed at least in part by loading portions of the artificial intelligence workload equal in size to the height and width of the compute filter, and the identified node operations of the second type are performed at least in part by loading a portion of the artificial intelligence workload greater than or equal in size to the width of the compute filter.
2. The method of claim 1, further comprising outputting a layout of the one or more hardware modules in silicon.
3. The method of claim 1, wherein the one or more hardware modules assigned to the identified node operations of the first type, comprise a controller, wherein the controller is configured to: query an input row and input column counter, indicating a location of a tile of artificial intelligence workload to be processed; load, from a memory storing the artificial intelligence workload to the local input cache memory, the tile of the artificial intelligence workload, wherein the tile of the artificial intelligence workload is of size equal to the compute filter width and filter height; perform, in parallel, in the one or more hardware modules, the node operations on the tile of artificial intelligence workload; store result of performing the node operations in an output interface memory; increment the input column counter, and wherein if the input column counter overflows, increment the input row counter and reset the input column counter; increment an output column counter, and if the output column counter overflows, increment an output row counter, wherein the output row and output column counters indicate active output storage position; and iterate through querying, loading, performing, storing, and the incrementing until a final input position is reached.
4. The method of claim 3, wherein the controller further comprises a run signal and a done signal, wherein the run and done signals are complementary, and wherein a high run signal and a low done signal indicate the controller is operating and a low run signal and a high done signal indicate the controller is done operating.
5. The method of claim 1, wherein the one or more hardware modules assigned to the identified node operations of the second type, comprise a controller, and a plurality of local input cache memories, wherein the controller is configured to: query a position indicator, indicating a location of a block of artificial intelligence workload to be processed; load, from a memory storing the artificial intelligence workload to the plurality of local input cache memories, the block of the artificial intelligence workload; perform, in parallel, in the one or more hardware modules, the identified node operations of the second type on the block of the artificial intelligence workload, with the compute filter, wherein width of the block of the artificial intelligence workload is greater in size than the width of the compute filter; store results of performing the identified node operations of the second type in an output interface memory; increment an output position counter of the output interface memory; increment an input position indicator; and iterate through querying, loading, performing, storing and incrementing until a final input position is reached.
6. The method of claim 5, wherein the controller is further configured to: store partial results of performing the identified node operations of the second type in a stitching cache memory, wherein the partial results are due to the results depending on data from a next block of artificial intelligence workload to be processed; load the next block; and perform, in parallel, in the one or more hardware modules, the identified node operations of the second type of the next block, and wherein a stitching module is configured to generate an output in part by merging the partial results with results of processing the next block; and the stitching module is further configured to store the output in the output interface memory.
7. The method of claim 5, wherein the controller further comprises a run signal and a done signal, wherein the run and done signals are complementary, and wherein a high run signal and a low done signal indicate the controller is operating and a low run signal and high done signal indicates the controller is done performing.
8. The method of claim 1, wherein the normalizing the computation graph comprises merging one or more operations from the computation graph, wherein a corresponding hardware module from the hardware construct library performs the one or more operations.
9. The method of claim 1, wherein the identified node operations comprise convolution and max-pooling.
10. The method of claim 1, wherein the identifying of the node operations as the first or second type is at least partly based on chip area, speed, power consumption and number of the node operations.
11. A system comprising: at least one hardware processor; a graph normalizer module configured, by the at least one hardware processor, to receive and store an artificial intelligence workload and a computation graph of the artificial intelligence workload, wherein the workload comprises a compute filter, having a filter height and a filter width and the graph normalizer module is further configured, by the at least one hardware processor, to normalize the computation graph, parse the normalized computation graph and determine node operations of the normalized computation graph; a register-transfer level (RTL) generator configured, by the at least one hardware processor, to identify the node operations as either a first or second type of hardware construct; and wherein the RTL generator is further configured, by the at least one hardware processor, to query a hardware construct library to assign one or more hardware modules to perform the identified node operations, wherein each hardware construct in the hardware construct library describes at least a connection interface using a special domain-specific language associated with the one or more hardware modules, the identified node operations of the first type are performed at least in part by loading portions of the artificial intelligence workload equal in size to the height and width of the compute filter, and the identified node operations of the second type are performed at least in part by loading a portion of the artificial intelligence workload greater than or equal in size to the width of the compute filter.
12. The system of claim 11, further comprising a layout synthesizer configured to output a layout of the one or more hardware modules in silicon.
13. The system of claim 11, wherein the one or more hardware modules assigned to the identified node operations of the first type, comprise a controller, wherein the controller is configured to: query an input row and input column counter, indicating a location of a tile of artificial intelligence workload to be processed; load, from a memory storing the artificial intelligence workload to the local input cache memory, the tile of the artificial intelligence workload, wherein the tile of the artificial intelligence workload is of size equal to the compute filter width and filter height; perform, in parallel, in the one or more hardware modules, the node operations on the tile of artificial intelligence workload; store result of performing the node operations in an output interface memory; increment the input column counter, and wherein if the input column counter overflows, increment the input row counter and reset the input column counter; increment an output column counter, and if the output column counter overflows, increment an output row counter, wherein the output row and output column counters indicate active output storage position; and iterate through querying, loading, performing, storing, and the incrementing until a final input position is reached.
14. The system of claim 13, wherein the controller further comprises a run signal and a done signal, wherein the run and done signals are complementary, and wherein a high run signal and a low done signal indicate the controller is operating and a low run signal and a high done signal indicate the controller is done operating.
15. The system of claim 11, wherein the one or more hardware modules assigned to the identified node operations of the second type, comprise a controller, and a plurality of local input cache memories, wherein the controller is configured to: query a position indicator, indicating a location of a block of artificial intelligence workload to be processed; load, from a memory storing the artificial intelligence workload to the plurality of local input cache memories, the block of the artificial intelligence workload; perform, in parallel, in the one or more hardware modules, the identified node operations of the second type on the block of the artificial intelligence workload, with the compute filter, wherein width of the block of the artificial intelligence workload is greater in size than the width of the compute filter; store results of performing the identified node operations of the second type in an output interface memory; increment an output position counter of the output interface memory; increment an input position indicator; and iterate through querying, loading, performing, storing and incrementing until a final input position is reached.
16. The system of claim 15, wherein the controller is further configured to: store partial results of performing the identified node operations of the second type in a stitching cache memory, wherein the partial results are due to the results depending on data from a next block of artificial intelligence workload to be processed; load the next block; and perform, in parallel, in the one or more hardware modules, the identified node operations of the second type of the next block, and wherein a stitching module is configured to generate an output in part by merging the partial results with results of processing the next block; and the stitching module is further configured to store the output in the output interface memory.
17. The system of claim 15, wherein the controller further comprises a run signal and a done signal, wherein the run and done signals are complementary, and wherein a high run signal and a low done signal indicate the controller is operating and a low run signal and high done signal indicates the controller is done performing.
18. The system of claim 11, wherein the normalizing the computation graph comprises merging one or more operations from the computation graph, wherein a corresponding hardware module from the hardware construct library performs the one or more operations.
19. The system of claim 11, wherein the identified node operations comprise convolution and max-pooling.
20. The system of claim 11, wherein the RTL generator is further configured to identify the node operations as the first or second type at least partly based on chip area, speed, power consumption and number of the node operations.
</claims>
</document>
