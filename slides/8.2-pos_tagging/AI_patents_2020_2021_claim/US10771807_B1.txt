<document>

<filing_date>
2019-03-29
</filing_date>

<publication_date>
2020-09-08
</publication_date>

<priority_date>
2019-03-28
</priority_date>

<ipc_classes>
G06F17/15,G06N3/04,H04N19/159,H04N19/184,H04N19/186,H04N19/52
</ipc_classes>

<assignee>
WIPRO
</assignee>

<inventors>
RAMACHANDRA, MANJUNATH
ULAGANATHAN, SETHURAMAN
</inventors>

<docdb_family_id>
72289989
</docdb_family_id>

<title>
System and method for compressing video using deep learning
</title>

<abstract>
A method and system for compressing videos using deep learning is disclosed. The method includes segmenting each of a plurality of frames associated with a video into a plurality of super blocks. The method further includes determining a block size for partition of each of the plurality of super blocks into a plurality of sub blocks, based on a feature of each of the plurality of super blocks using a Convolutional Neural Network (CNN). The method further includes generating a prediction data for each of the plurality of sub blocks based on a motion vector predicted and learned by the CNN. The method further includes determining a residual data for each of the plurality of sub blocks by subtracting the prediction data from an associated original data. The method includes generating a transformed quantized residual data using each of a transformation algorithm and a quantization algorithm.
</abstract>

<claims>
1. A method of compressing videos using deep learning, the method comprising: segmenting, by a video compressing device, each of a plurality of frames associated with a video into a plurality of super blocks based on an element present in each of the plurality of frames and a motion associated with the element, wherein the segmentation of the plurality frames into the plurality of super blocks is of variable shape and size; determining, by the video compressing device, a block size for partition of each of the plurality of super blocks into a plurality of sub blocks, based on a feature of each of the plurality of super blocks using a Convolutional Neural Network (CNN), wherein the feature comprises at least one of a size of the super block and a motion related information; generating, by the video compression device, a prediction data for each of the plurality of sub blocks based on a motion vector predicted and learned by the CNN, wherein the CNN predicts the motion vector based on a co-located frames; determining, by the video compression device, a residual data for each of the plurality of sub blocks by subtracting the prediction data from an associated original data, wherein the associated original data is a bit stream of each of the plurality of sub blocks; and generating, by the video compressing device, a transformed quantized residual data using each of a transformation algorithm and a quantization algorithm based on a plurality of parameters associated with the residual data, wherein the plurality of parameters comprises the compression rate and signal to noise ratio.
2. The method of claim 1, further comprising: receiving the video from an external computing device through an interface; and performing pre-processing analytics on the video, wherein the pre-processing analytics comprises at least one of removal of noise or converting of Red Green Blue (RGB) color space to YCbCr color space.
3. The method of claim 1, further comprising training the CNN for each of the plurality of super block based on the feature of a plurality of set of frames associated with a plurality of video compression techniques and a user feedback to the CNN.
4. The method of claim 3, further comprising predicting, by the trained CNN, for each of the plurality of super blocks, at least one of a prediction data, the block size, or a motion related information.
5. The method of claim 1, further comprising selecting, by the CNN, a suitable prediction mode, wherein the suitable prediction mode is at least one of an inter mode or an intra mode.
6. The method of claim 5, wherein the inter mode comprises prediction between a frame and at least one adjacent frame within the plurality of frames, and wherein the intra mode comprises prediction within the frame.
7. The method of claim 1, wherein the transformation algorithm and the quantization algorithm is applied to compress the residual data.
8. The method of claim 7, wherein the transformation algorithm is based on at least one of the CNN or a gaussian pulse wavelet.
9. The method of claim 1, further comprising generating a plurality of compressed bit streams for the transformed quantized residual data based on an entropy coding.
10. The method of claim 1, wherein the element comprises at least one of an object present in a frame of the plurality of frames and texture associated with the object.
11. A video compressing device using deep learning, the video compressing device comprising: a processor; and a memory communicatively coupled to the processor, wherein the memory stores processor instructions, which, on execution, causes the processor to: segment each of a plurality of frames associated with a video into a plurality of super blocks based on an element present in each of the plurality of frames and a motion associated with the element, wherein the segmentation of the plurality of frames into the plurality of super blocks is of variable shape and size; determine a block size for partition of each of the plurality of super blocks into a plurality of sub blocks, based on a feature of each of the plurality of super blocks using a Convolutional Neural Network (CNN), wherein the feature comprises at least one of a size of the super block and a motion related information; generate a prediction data for each of the plurality of sub blocks based on a motion vector predicted and learned by the CNN, wherein the CNN predicts the motion vector based on a co-located frames; determine a residual data for each of the plurality of sub blocks by subtracting the prediction data from an associated original data, wherein the associated original data is a bit stream of each of the plurality of sub blocks; and generate a transformed quantized residual data using each of a transformation algorithm and a quantization algorithm based on a plurality of parameters associated with the residual data, wherein the plurality of parameters comprises the compression rate and Signal to noise ratio.
12. The video compressing device of claim 11, wherein the processor instructions further cause the processor to: receive the video from an external computing device through an interface; and perform pre-processing analytics on the video, wherein the pre-processing analytics comprises at least one of removal of noise or converting of Red Green Blue (RGB) color space to YCbCr color space.
13. The video compressing device of claim 11, further comprising training the CNN for each of the plurality of super block based on the feature of a plurality of set of frames associated with a plurality of video compression techniques and a user feedback to the CNN.
14. The video compressing device of claim 13, further comprising predicting, by the trained CNN, for each of the plurality of super blocks, at least one of a prediction data, the block size, or a motion related information.
15. The video compressing device of claim 11, further comprising selecting, by the CNN, a suitable prediction mode, wherein the suitable prediction mode is at least one of an inter mode or an intra mode.
16. The video compressing device of claim 15, wherein the inter mode comprises prediction between a frame and at least one adjacent frame within the plurality of frames, and wherein the intra mode comprises prediction within the frame.
17. The video compressing device of claim 11, wherein the transformation algorithm and the quantization algorithm is applied to compress the residual data.
18. The video compressing device of claim 17, wherein the transformation algorithm is based on at least one of the CNN or a gaussian pulse wavelet.
19. The video compressing device of claim 11, further comprising generating a plurality of compressed bit streams for the transformed quantized residual data based on an entropy coding.
20. A non-transitory computer-readable storage medium having stored thereon, a set of computer-executable instructions causing a computer comprising one or more processors to perform steps comprising: segmenting each of a plurality of frames associated with a video into a plurality of super blocks based on an element present in each of the plurality of frames and a motion associated with the element, wherein the segmentation of the plurality of frames into the plurality super blocks is of variable shape and size; determining a block size for partition of each of the plurality of super blocks into a plurality of sub blocks, based on a feature of each of the plurality of super blocks using a Convolutional Neural Network (CNN), wherein the feature comprises at least one of a size of the super block and a motion related information; generating a prediction data for each of the plurality of sub blocks based on a motion vector predicted and learned by the CNN, wherein the CNN predicts the motion vector based on a co-located frames; determining a residual data for each of the plurality of sub blocks by subtracting the prediction data from an associated original data, wherein the associated original data is a bit stream of each of the plurality of sub blocks; and generating a transformed quantized residual data using each of a transformation algorithm and a quantization algorithm based on a plurality of parameters associated with the residual data, wherein the plurality of parameters comprises the compression rate and Signal to noise ratio.
</claims>
</document>
