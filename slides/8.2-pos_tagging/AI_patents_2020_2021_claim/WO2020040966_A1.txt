<document>

<filing_date>
2019-08-05
</filing_date>

<publication_date>
2020-02-27
</publication_date>

<priority_date>
2018-08-24
</priority_date>

<ipc_classes>
G01S13/86,G01S13/88,G06F3/01
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
AMIHOOD, PATRICK M.
GIUSTI, LEONARDO
POUPYREV, IVAN
</inventors>

<docdb_family_id>
67660474
</docdb_family_id>

<title>
SMARTPHONE, SYSTEM AND METHOD COMPRISING A RADAR SYSTEM
</title>

<abstract>
This document describes techniques and systems that enable a smartphone, system and method comprising a radar system. The techniques and systems use a radar field to accurately determine three-dimensional (3D) gestures that can be used to interact with augmented-reality (AR) objects that are presented on a display of an electronic device, such as a smartphone. These techniques allow the user to make 3D gestures from a distanceâ€”the user does not have to hold the electronic device steady while touching the screen and the gestures do not obstruct the user's view of the AR objects presented on the display.
</abstract>

<claims>
What is claimed is:
1. A smartphone, comprising:
a display;
a radar system, implemented at least partially in hardware, configured to:
provide a radar field;
sense reflections from an object in the radar field;
analyze the reflections from the object in the radar field; and provide, based on the analysis of the reflections, radar data;
one or more computer processors; and
one or more computer-readable media having instructions stored thereon that, responsive to execution by the one or more computer processors, implement a radar-based application configured to:
present an augmented-reality (AR) element through the display of the smartphone, the AR element including a touch input control and related to a real object, an image of which is presented on the display; and
responsive to a determination, based on the radar data, that the object in the radar field is moving toward the display, maintain the touch input control at a fixed location on the display.
2. The smartphone of claim 1, wherein the object in the radar field is a user, and whereby ease and accuracy of the user's interaction with the touch input control are facilitated by maintaining the touch input control at the fixed location even as the image of the real object is moving unsteadily on the display.
3. The smartphone of claim 1 or 2, wherein the radar-based application is further configured to present the augmented-reality (AR) element, including the touch input control, while the user holds the smartphone with a first hand.
4. The smartphone of at least one of the preceding claims, wherein:
the determination that the object in the radar field is moving toward the display comprises determining that the user is reaching toward the smartphone with a second hand; the smartphone is positioned such that the image of the real object is no longer being presented on the display; and
the radar-based application is further configured to maintain the AR element and the touch input control at approximately a same location on the display as presented prior to the user moving the smartphone such that the image of the real object is no longer presented on the display.
5. The smartphone of at least one of the preceding claims, wherein: the touch input control has been activated via a prior touch input by the second hand; the radar-based application is further configured to present on the display, responsive to the activation of the touch input control, a two-dimensional (2D) interface that includes:
another image of the real object; and
another touch input control that provides the user with information about the real object.
6. The smartphone of at least one of the preceding claims, wherein the radarbased application is further configured to maintain the fixed location of the touch input control when the image of the real object is no longer presented on the display.
7. The smartphone of at least one of the preceding claims, wherein the radarbased application is further configured, responsive to the touch input control at the fixed location not being touched within a threshold time, to stop providing the touch input control at the fixed location.
8. The smartphone of at least one of the preceding claims , wherein the radarbased application is further configured to provide the touch input control at the fixed location if the object in the radar field is moving toward the display at a velocity that exceeds a threshold velocity.
9. The smartphone of at least one of the preceding claims, wherein the smartphone includes an image-capture device and the radar-based application presents, via the image-capture device, the image of the real object in real time or near-real time.
10. The smartphone of at least one of the preceding claims, wherein the radar system is further configured to determine, based on the radar data, that the object in the radar field is moving toward the display.
11. The smartphone of at least one of the preceding claims, wherein the radarbased application is further configured to determine, based on the radar data, that the object in the radar field is moving toward the display.
12. The smartphone of at least one of the preceding claims, wherein the object in the radar field is a body part of a user.
13. A system, comprising:
an electronic device that includes a display;
a radar system, implemented at least partially in hardware, configured to:
provide a radar field;
sense, at a first time, reflections from an object in the radar field;
analyze the reflections from the object in the radar field; and
provide, based on the analysis of the reflections, radar data;
one or more computer processors; and
one or more computer-readable media having instructions stored thereon that, responsive to execution by the one or more computer processors, implement a radar-based application configured to:
present an augmented-reality (AR) element through the display of the electronic device;
receive, at a second time, an input selecting the AR element, the second time later than the first time;
determine, based on the radar data, a gesture by the object in the radar field; and
perform an action related to the selected AR element, the action corresponding to the determined gesture.
14. The system of claim 13, wherein the input selecting the AR element is a touch input.
15. The system of claim 13 or 14, wherein the determined gesture is a threedimensional (3D) gesture.
16. The system of at least one of the claims 13 to 15, wherein the object is a body part of the user.
17. The system of at least one of the claims 13 to 16, wherein the determined gesture comprises changing a distance between the object and the radar system.
18. The system of claim 17, wherein:
the action corresponding to the changed distance being an increased distance between the object and the radar system is a movement of the selected AR element closer to the user; and
the action corresponding to the changed distance being a decreased distance between the object and the radar system is a movement of the selected AR element away from the user.
19. The system of claim 18, wherein:
the movement of the selected AR element closer to the user is proportional to the increased distance; and
the movement of the selected AR element away from the user is proportional to the decreased distance. 20 The system of at least one of the claims 13 to 19, wherein the determined gesture comprises changing a position of the object, relative to the radar system, while maintaining a substantially similar distance between the body part and a plane of the display of the electronic device.
21. The system of claim 20, wherein:
the action corresponding to changing the position in a first direction is a rotation of the selected AR element in a first rotational direction about an axis of the selected AR element; and
the action corresponding to changing the position in a second direction is a rotation of the selected AR element in a second rotational direction about the axis.
22. The system of at least one of the claims 13 to 21, wherein the radar system further comprises a digital beamformer and an angle estimator, and the radar system is configured to monitor angles in a field of view between approximately -90 degrees and approximately 90 degrees.
23. A method implemented in an electronic device that includes a display, a radar system, and a radar-based application, the method comprising:
providing, by the radar system, a radar field;
sensing, by the radar system, reflections from an object in the radar field; analyzing the reflections from the object in the radar field;
providing, based on the analysis of the reflections, radar data; presenting, by the radar-based application, an augmented-reality (AR) element through the display of the electronic device, the AR element including a touch input control and related to a real object, an image of which is presented on the display; and
responsive to a determination, based on the radar data, that the object in the radar field is moving toward the display, maintaining the touch input control at a fixed location on the display.
24. The method of claim 23, wherein the determination, based on the radar data, that the object in the radar field is moving toward the display is made by the radar system or by the radar-based application.
25. The method of claim 23 or 24, wherein the electronic device is a handheld device, the object in the radar field is a user, and the method further comprises:
maintaining the touch input control at the fixed location even as the image of the real object is moving unsteadily on the display, whereby ease and accuracy of the user's interaction with the touch input control are facilitated.
26. The method of at least one of claims 23 to 25, further comprising: presenting, by the radar-based application, the augmented-reality (AR) element, including the touch input control, while the user holds the electronic device with a first hand.
27. The method of at least one of the claims 23 to 26, wherein:
the determination that the object in the radar field is moving toward the display comprises determining that the user is reaching toward the electronic device with a second hand;
the electronic device is positioned such that the image of the real object is no longer being presented on the display; and
the method further comprises: maintaining, by the radar-based application, the AR element and the touch input control at approximately a same location on the display as presented prior to the user moving the electronic device such that the image of the real object is no longer presented on the display.
28. The method of at least one of the claims 23 to 27, wherein the touch input control has been activated via a prior touch input by the second hand, and the method further comprises:
responsive to the activation of the touch input control, presenting on the display, by the radar-based application, a two-dimensional (2D) user interface, the 2D user interface including:
another image of the real object; and
another touch input control that provides the user with information about the real object.
</claims>
</document>
