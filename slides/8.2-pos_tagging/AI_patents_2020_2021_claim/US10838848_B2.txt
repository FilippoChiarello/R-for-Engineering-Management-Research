<document>

<filing_date>
2018-06-01
</filing_date>

<publication_date>
2020-11-17
</publication_date>

<priority_date>
2017-06-01
</priority_date>

<ipc_classes>
G06F11/36,G06F17/27,G06F40/284,G06F40/30,G06F40/35,G06N3/00,G06N3/02,G06N3/04,G06N3/08,G06N5/02
</ipc_classes>

<assignee>
ROYAL BANK OF CANADA
</assignee>

<inventors>
FONG, CORY
</inventors>

<docdb_family_id>
64454174
</docdb_family_id>

<title>
System and method for test generation
</title>

<abstract>
Computer implemented methods and systems are provided for generating one or more test cases based on received one or more natural language strings. An example system comprises a natural language classification unit that utilizes a trained neural network in conjunction with a reinforcement learning model, the system receiving as inputs various natural language strings and providing as outputs mapped test actions, mapped by the neural network.
</abstract>

<claims>
1. A computer implemented system, implemented on one or more processors operating in conjunction with computer memory, for generating one or more test automation scripts for one or more test cases based on received one or more natural language strings representing a natural language description of the one or more test cases, the system comprising: a token extraction engine configured to receive the one or more natural language strings and parse the one or more natural language strings to extract one or more word vectors representing extracted features of the one or more natural language strings; a pre-trained natural language classification engine configured to provide the one or more word vectors into a neural network having a first layer, a second layer, and a third layer, the pre-trained natural language classification engine pre-trained to map a pre-defined action to the one or more word vectors; the first layer configured for embedding the one or more word vectors into a d-dimensional vector space, the d-dimensional vector space based on a pre-trained mapping to associate word vectors and pre-defined actions, where d is based upon a variable length of words of the one or more word vectors, generating a first intermediate output of n vectors, where n is a number of words; the second layer configured for mapping the one or more word vectors into a fixed-size vector and processing the fixed-size vector through a rectifier activation function to yield a second intermediate output representative of a granularity of the words; the third layer configured as a logic regression layer for receiving the first intermediate output and the second intermediate output and combining the first intermediate output and the second intermediate output to map the natural language description into a vector space indicative of whether the one or more word vectors of the natural language description are related with one another and output the pre-defined action most likely map to the one or more word vectors; a reinforcement learning engine configured to receive a list of available parameters associated with the pre-defined action, and select one or more parameter values for use with the pre-defined action, the reinforcement learning engine pre-trained through an optimization process to optimize receipt of one or more rewards based on performance, the one or more rewards associated with an accuracy score obtained through review of output accuracy of the one or more test automation scripts generated in response to one or more received training natural language strings and for the pre-defined action, the one or more parameter values that led to detection of one or more state changes to states that are associated with the one or more rewards; and a test script generation engine configured to generate the one or more test automation scripts based at least on the mapping of the vector space, the pre-defined action, and the one or more parameter values that led to detection of one or more state changes to the states that are associated with the one or more rewards.
2. The system of claim 1, further comprising a bias correction mechanism that is configured to perturb the reinforcement learning engine by introducing noise into the optimization process, the noise adapted to modify reward provisioning parameters of a highest reward encountered so far of the one or more rewards by reducing award provided for the highest reward encountered so far such that the reinforcement learning engine continues to explore further possible parameter inputs.
3. The system of claim 1, wherein the neural network of the pre-trained natural language classification engine is pre-trained to infer an intent metric based on at least one of: the pre-defined action being performed, a location of the pre-defined action in a test case, one or more pre-defined actions performed prior to a current step, and a composite sub-classification of a natural language description of the pre-defined action, the location of the pre-defined action in the test case, or the one or more pre-defined actions performed prior to the current step.
4. The system of claim 3, wherein the neural network is further configured to infer the intent metric based on the location of the pre-defined action in the test case by comparing against processed information associated with one or more other pre-defined actions of the test case.
5. The system of claim 4, wherein the neural network is further configured to identify one or more dependencies in the test case, and to select the one or more parameter values based at least on the identified one or more dependencies.
6. The system of claim 1, wherein the generation of the test automation scripts is conducted using the reinforcement learning engine configured to store a set of actions including at least strings and pre-defined values representative of one or more valid computational actions available within a computing application.
7. The system of claim 6, wherein the reinforcement learning engine is configured to track one or more states, each state associated with a corresponding reward and stored as a computational dictionary where string attributes are paired with an integer reward wherein the string attributes represent the one or more states of the application by denoting a presence or an absence of graphical qualities rendered on a graphical user interface, and wherein for each string attribute of the string attributes detected in a particular state, the reinforcement learning engine rewards a corresponding reward value from the computational dictionary.
8. The system of claim 7, wherein the set of actions taken and rewards awarded are mapped to a computational function to determine whether a softmax activation is triggered.
9. The system of claim 8, wherein, for each action of the set of actions, metadata representative of the state upon which the application was in during the action is recorded as a hashmap object.
10. The system of claim 1, wherein the one or more test automation scripts are used for controlling a graphical user interface.
11. The system of claim 10, wherein the one or more parameter values are provided as inputs representing selected account credentials into the graphical user interface when controlled by the one or more test automation scripts.
12. A computer implemented method, implemented on one or more processors operating in conjunction with computer memory, for generating one or more test automation scripts for one or more test cases based on received one or more natural language strings representing a natural language description of the one or more test cases, the method comprising: receiving the one or more natural language strings and parse the one or more natural language strings to extract one or more word vectors representing extracted features of the one or more natural language strings; providing a pre-trained natural language classification engine configured to provide the one or more word vectors into a pre-trained neural network having a first layer, a second layer, and a third layer, the first layer configured for embedding the one or more word vectors into a d-dimensional vector space, the d-dimensional vector space based on a pre-trained mapping to associate word vectors and pre-defined actions, where d is based upon a variable length of words of the one or more word vectors, generating a first intermediate output of n vectors, where n is a number of words; the second layer configured for mapping the one or more word vectors into a fixed-size vector and processing the fixed-size vector through a rectifier activation function to yield a second intermediate output representative of a granularity of the words, the third layer configured as a logic regression layer for receiving the first intermediate output and the second intermediate output and combining the first intermediate output and the second intermediate output to map the natural language description into a vector space indicative of whether the one or more word vectors of the natural language description are related with one another and output the pre-defined action most likely map to the one or more word vectors; providing a reinforcement learning engine configured to receive a list of available parameters associated with the pre-defined action, and select one or more parameter values for use with the pre-defined action, the reinforcement learning engine pre-trained through an optimization process to optimize receipt of one or more rewards based on performance, the one or more rewards associated with an accuracy score obtained through review of output accuracy of the one or more test automation scripts generated in response to one or more received training natural language strings and for the pre-defined action, the one or more parameter values that led to detection of one or more state changes to states that are associated with the one or more rewards; and generating the one or more test automation scripts based at least on the mapping of the vector space, the pre-defined action, and the one or more parameter values that led to detection of one or more state changes to the states that are associated with the one or more rewards.
13. The method of claim 12, further comprising a bias correction mechanism that is configured to perturb the reinforcement learning engine by introducing noise into the optimization process, the noise adapted to modify reward provisioning parameters of a highest reward encountered so far of the one or more rewards by reducing an award provided for the highest reward encountered so far such that the reinforcement learning engine continues to explore further possible parameter inputs.
14. The method of claim 12, further comprising pre-training the pre-trained natural language classification engine by training the neural network to infer an intent metric based on at least one of: the pre-defined action being performed, a location of the pre-defined action in a test case, one or more pre-defined actions performed prior to a current step, and a composite sub-classification of a natural language description of the pre-defined action, the location of the pre-defined action in the test case, or the one or more pre-defined actions performed prior to the current step.
15. The method of claim 14, wherein the neural network is further configured to infer the intent metric based on the location of the pre-defined action in the test case by comparing against processed information associated with one or more other pre-defined action of the test case.
16. The method of claim 15, wherein the neural network is further configured to identify one or more dependencies in the test case, and to select the one or more parameter values based at least on the identified one or more dependencies.
17. The method of claim 12, wherein the generation of the test automation scripts is conducted using the reinforcement learning engine configured to store a set of actions including at least strings and pre-defined values representative of one or more valid computational actions available within a computing application.
18. The method of claim 17, wherein the reinforcement learning engine is configured to track one or more states, each state associated with a corresponding reward and stored as a computational dictionary where string attributes are paired with an integer reward wherein the string attributes represent the one or more states of the application by denoting a presence or an absence of graphical qualities rendered on a graphical user interface, and wherein for each string attribute of the string attributes detected in a particular state, the reinforcement learning engine rewards a corresponding reward value from the computational dictionary.
19. The method of claim 18, wherein the set of actions taken and rewards awarded are mapped to a computational function to determine whether a softmax activation is triggered.
20. A non-transitory computer readable medium storing machine interpretable instructions, which when executed, cause one or more processors to perform one or more steps corresponding to a method comprising: receiving the one or more natural language strings and parse the one or more natural language strings to extract one or more word vectors representing extracted features of the one or more natural language strings; providing the one or more word vectors into a pre-trained neural network having a first layer, a second layer, and a third layer, the first layer configured for embedding the one or more word vectors into a d-dimensional vector space, the d-dimensional vector space based on a pre-trained mapping to associate word vectors and pre-defined actions, where d is based upon a variable length of words of the one or more word vectors, generating a first intermediate output of n vectors, where n is a number of words; the second layer configured for mapping the one or more word vectors into a fixed-size vector and processing the fixed-size vector through a rectifier activation function to yield a second intermediate output representative of a granularity of the words, the third layer configured as a logic regression layer for receiving the first intermediate output and the second intermediate output and combining the first intermediate output and the second intermediate output to map the natural language description into a vector space indicative of whether the one or more word vectors of the natural language description are related with one another and output the pre-defined action most likely map to the one or more word vectors; providing a reinforcement learning engine configured to receive a list of available parameters associated with the pre-defined action, and select one or more parameter values for use with the pre-defined action, the reinforcement learning engine pre-trained through an optimization process to optimize receipt of one or more rewards based on performance, the one or more rewards associated with an accuracy score obtained through review of output accuracy of the one or more test automation scripts generated in response to one or more received training natural language strings and for the pre-defined action, the one or more parameter values that led to detection of one or more state changes to states that are associated with the one or more rewards; and generating the one or more test automation scripts based at least on the mapping of the vector space, the pre-defined action, and the one or more parameter values that led to detection of one or more state changes to the states that are associated with the one or more rewards.
</claims>
</document>
