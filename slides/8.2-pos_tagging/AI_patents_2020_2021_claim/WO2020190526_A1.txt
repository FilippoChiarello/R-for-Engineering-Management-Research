<document>

<filing_date>
2020-03-05
</filing_date>

<publication_date>
2020-09-24
</publication_date>

<priority_date>
2019-03-18
</priority_date>

<ipc_classes>
G06N3/04,G06N3/063,G06N3/08
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
NA, TAESIK
LO, DANIEL
CHUNG, ERIC S.
ZHU, HAISHAN
</inventors>

<docdb_family_id>
70058514
</docdb_family_id>

<title>
MIXED PRECISION TRAINING OF AN ARTIFICIAL NEURAL NETWORK
</title>

<abstract>
The use of mixed precision values when training an artificial neural network (ANN) can increase performance while reducing cost. Certain portions and/or steps of an ANN may be selected to use higher or lower precision values when training. Additionally, or alternatively, early phases of training are accurate enough with lower levels of precision to quickly refine an ANN model, while higher levels of precision may be used to increase accuracy for later steps and epochs. Similarly, different gates of a long short-term memory (LSTM) may be supplied with values having different precisions.
</abstract>

<claims>
1. A computer-implemented method, comprising:
defining an artificial neural network (ANN) comprising a plurality of layers of nodes;
setting a first bit width for activation values associated with a first layer of the plurality of layers of nodes;
setting a second bit width for activation values associated with a second layer of the plurality of layers of nodes; and
during training of or inference from the ANN,
applying a first activation function to the first layer of the plurality of layers of nodes, thereby generating a plurality activation values having the first bit width; and
applying a second activation function to the second layer of the plurality of layers of nodes, thereby generating a second plurality of activation values having the second bit width.
2. The computer-implemented method of claim 1, further comprising:
setting a third bit width for weights associated with the first layer of the plurality of layers of nodes, wherein during training of or inference from the ANN, the first layer of the plurality of layers of nodes generates weights having the third bit width; and
setting a fourth bit width for weights associated with the second layer of the plurality of layers of nodes, wherein during training of or inference from the ANN, the second layer of the plurality of layers of nodes generates weights having the fourth bit width.
3. The computer-implemented method of any of claims 1 and 2, wherein the first layer of the plurality of layers of nodes comprises an input layer, wherein the second layer of the plurality of layers of nodes comprises an output layer, and wherein the first bit width and the second bit width are set to be different from bit widths associated with a set of remaining layers of nodes.
4. The computer-implemented method of any of claims 1-3, wherein the ANN is trained or an inference is made from the ANN over a plurality of steps, wherein the first bit width is used to train or infer from the first layer of the plurality of layers of nodes during a first of the plurality of steps, and wherein a fifth bit width is used to train or infer from the first layer of the plurality of layers of nodes during a second of the plurality of steps.
5. The computer-implemented method of any of claims 1-4, wherein an effective bit width is determined for the first layer during the first of the plurality of steps by averaging a bit width associated with the first layer and a bit width associated with the first of the plurality of steps.
6. The computer-implemented method of any of claims 1-5, further comprising: setting a sixth bit width for a first gate type of a long short-term memory (LSTM) component of the ANN; and
setting a seventh bit width for a second gate type of the LSTM component of the
ANN.
7. The computer-implemented method of any of claims 1-6, wherein the activation values are represented in a block floating-point format (BFP) having a mantissa comprising fewer bits than a mantissa in a normal-precision floating-point representation.
8. A computer-implemented method, comprising:
defining an artificial neural network (ANN) comprising a plurality of layers of nodes, wherein the ANN is trained over a plurality of steps;
setting a first bit width for activation values generated during a first step of the plurality of steps;
setting a second bit width for activation values generated during a second step of the plurality of steps;
training the ANN by applying a first activation function during the first step of the plurality of steps, thereby generating activation values having the first bit width; and
training the ANN by applying a second activation function during the second step of the plurality of steps, thereby generating activation values having the second bit width.
9. The computer-implemented method of claim 8, wherein the ANN is trained over a plurality of epochs, wherein the first bit width is set for values generated during a first epoch, and wherein the second bit width is set for values generated during a second epoch.
10. The computer-implemented method of any of claims 8 and 9, wherein a third bit width is associated with a first layer of a plurality of layers of nodes, wherein a fourth bit width is associated with a second layer of a plurality of layers of nodes, and wherein an effective bit width for nodes in the first layer and that are trained during the first step is based on a combination of the first bit width and the third bit width.
11. The computer-implemented method of any of claims 8-10, wherein the effective bit width for nodes in the first layer and that are trained during the first step is determined by increasing the first bit width when the third bit width is greater than the first bit width, and decreasing the first bit width when the third bit width is lower than the first bit width.
12. The computer-implemented method of any of claims 8-11, wherein the first bit width or the second bit width are dynamically updated during training when a quantization error exceeds or falls below a defined threshold.
13. The computer-implemented method of any of claims 8-12, further comprising:
inferring an output from the trained ANN over the plurality of steps, the inferring including:
applying the first activation function during the first step of the plurality of steps; and
applying the second activation function during the second step of the plurality of steps.
14. A computing device, comprising:
one or more processors; and
at least one computer storage media having computer-executable instructions stored thereupon which, when executed by the one or more processors, will cause the computing device to:
define an artificial neural network (ANN) comprising one or more components that comprise a plurality of gates;
set a first bit width for a first of the plurality of gates;
set a second bit width for a second of the plurality of gates; and infer an output from the ANN in part by processing inputs supplied to the plurality of gates, wherein inputs supplied to the first of the plurality of gates are processed using the first bit width and wherein inputs supplied to the second of the plurality of gates are processed using the second bit width.
15. The computing device of claim 14, wherein the one or more components comprise one or more long short-term memory components (LSTMs), wherein the LSTM comprises a j gate, an i gate, an f gate, and an o gate, and wherein the j gate is assigned different bit width than the other gates.
</claims>
</document>
