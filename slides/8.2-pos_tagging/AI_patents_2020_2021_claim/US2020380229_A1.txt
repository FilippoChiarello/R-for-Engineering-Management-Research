<document>

<filing_date>
2019-12-30
</filing_date>

<publication_date>
2020-12-03
</publication_date>

<priority_date>
2018-12-28
</priority_date>

<ipc_classes>
G06K7/14,G06T7/521,G06T7/593
</ipc_classes>

<assignee>
AQUIFI
</assignee>

<inventors>
DAL MUTTO, CARLO
PERUCH, FRANCESCO
TRACHEWSKY, JASON
</inventors>

<docdb_family_id>
73551580
</docdb_family_id>

<title>
SYSTEMS AND METHODS FOR TEXT AND BARCODE READING UNDER PERSPECTIVE DISTORTION
</title>

<abstract>
A method for automatically recognizing content of labels on objects includes: capturing visual information of an object using a scanning system including one or more cameras, the object having one or more labels on one or more exterior surfaces; detecting, by a computing system, one or more surfaces of the object having labels; rectifying, by the computing system, the visual information of the one or more surfaces of the object to compute one or more rectified images; and decoding, by the computing system, content of a label depicted in at least one of the one or more rectified images.
</abstract>

<claims>
1. A method for automatically recognizing content of labels on objects, comprising: capturing visual information of an object using a scanning system comprising one or more cameras, the object having one or more labels on one or more exterior surfaces; detecting, by a computing system, one or more surfaces of the object having labels; rectifying, by the computing system, the visual information of the one or more surfaces of the object to compute one or more rectified images; and decoding, by the computing system, content of a label depicted in at least one of the one or more rectified images.
2. The method of claim 1, wherein the label comprises a barcode, and wherein the decoding the content of the label comprises supplying the rectified image of the label to a barcode recognition algorithm.
3. The method of claim 2, wherein the barcode is a linear barcode.
4. The method of claim 2, wherein the barcode is a 2-D bar code.
5. The method of claim 1, wherein the label comprises text, and wherein the decoding the content of the label comprises supplying the rectified image of the label to an optical character recognition (OCR) algorithm.
6. The method of claim 1, wherein the label comprises an icon, and wherein the decoding the content of the label comprises supplying the rectified image of the label to an icon detection algorithm.
7. The method of claim 1, wherein the rectifying the visual information comprises: estimating, by the computing system, an orientation of each of the one or more surfaces with respect to the camera system based on the visual information; computing, by the computing system, for each surface of the one or more surfaces, a transformation matrix between the orientation of the surface and a fronto-parallel orientation with respect to the camera system; and transforming, by the computing system, at least a portion of the visual information of the object corresponding to each surface of the one or more surfaces with respect to the transformation matrix to compute a rectified image of each surface of the one or more surfaces.
8. The method of claim 7, wherein at least one camera of the one or more cameras is a depth camera, and wherein the visual information comprises a depth map of the object.
9. The method of claim 8, wherein the estimating the orientation comprises detecting a substantially planar shape of the label in the depth map, and wherein the computing the transformation matrix comprises computing a transformation between the substantially planar shape of the label in the depth map and the fronto-parallel orientation.
10. The method of claim 8, wherein each depth camera of the camera system comprises: a time-of-flight depth camera; a structured light depth camera; a stereo depth camera comprising: at least two color cameras; a stereo depth camera comprising: at least two color cameras; and a color projector; a stereo depth camera comprising: at least two infrared cameras; a stereo depth camera comprising: at least two infrared cameras; an infrared projector; and a color camera; or a stereo depth camera comprising: at least two RGB-IR cameras; and an infrared projector.
11. The method of claim 8, wherein the camera system further comprises one or more 2-D cameras.
12. The method of claim 7, wherein at least one camera of the one or more cameras is a depth camera, and wherein the visual information of the object comprises a 3-D model.
13. The method of claim 12, wherein the estimating the orientation of the label comprises detecting a substantially planar surface of the 3-D model, wherein the computing the transformation matrix comprises computing a transformation of a virtual camera to the fronto-parallel orientation with respect to the substantially planar surface, and wherein the rectified image of the label is computed by rendering a view of the substantially planar surface of the 3-D model from the fronto-parallel orientation.
14. The method of claim 12, wherein the camera system further comprises one or more 2-D cameras.
15. The method of claim 7, wherein at least one camera of the one or more cameras is a 2-D camera and the scanning system comprises an accelerometer rigidly attached to the 2-D camera, wherein the visual information comprises at least one 2-D image captured by the 2-D camera and accelerometer data associated with an orientation of the 2-D camera when the at least one 2-D image was captured, wherein the object is supported by a ground plane, and wherein the object has a cuboidal shape comprising a horizontal surface parallel to the ground plane and at least one vertical surface perpendicular to the ground plane.
16. The method of claim 15, wherein the estimating the orientation of the label comprises: detecting the horizontal surface and the at least one vertical surface of the object; measuring a direction normal to the ground plane based on the accelerometer data; and sampling an azimuth angle of the normal of the at least one vertical surface over a plurality of candidate azimuth values, and wherein the computing the transformation matrix comprises computing a plurality of transformation matrices corresponding to the plurality of candidate azimuth values.
17. The method of claim 15, wherein the camera system further comprises one or more depth cameras.
18. The method of claim 7, wherein the one or more cameras of the scanning system comprise a plurality of 2-D cameras, wherein the visual information comprises a first 2-D image captured by a first 2-D camera of the plurality of 2-D cameras, the first 2-D camera having a first pose with respect to the object, wherein the visual information comprises a second 2-D image captured by a second 2-D camera of the plurality of 2-D cameras, the second 2-D camera having a second pose with respect to the object, the second pose being different than the first pose of the first 2-D camera, the second 2-D camera being calibrated with the first 2-D camera, wherein one or more planar surfaces of the object are depicted by both the first 2-D image captured by the first 2-D camera and the second 2-D image captured by the second 2-D camera, and wherein the estimating the orientation of each of the one or more surfaces comprises triangulating a normal direction to each of the one or more surfaces of the object based on the first 2-D image captured by the first 2-D camera and the second 2-D image captured by the second 2-D camera.
19. A system for decoding content of labels on objects comprising: a scanning system comprising one or more cameras; a computing system connected to the scanning system, the computing system comprising a processor and memory storing instructions that, when executed by the processor, cause the processor to: control the scanning system to capture visual information of an object, the object having one or more labels on one or more exterior surfaces; detect one or more surfaces of the object depicting labels; rectify the visual information of the one or more surfaces of the object to compute one or more rectified images; and decode content of a label depicted in at least one of the one or more rectified images.
20. The system of claim 19, wherein the label comprises a barcode, and wherein the decoding the content of the label comprises supplying the rectified image of the label to a barcode recognition algorithm.
21. The system of claim 19, wherein the label comprises text, and wherein the decoding the content of the label comprises supplying the rectified image of the label to an optical character recognition (OCR) algorithm.
22. The system of claim 19 wherein the instructions to rectify the visual information comprises instructions that, when executed by the processor, cause the processor to: estimate an orientation of each of the one or more surfaces with respect to the camera system based on the visual information; compute, for each surface of the one or more surfaces, a transformation matrix between the orientation of the surface and a fronto-parallel orientation with respect to the camera system; and transform at least a portion of the visual information of the object corresponding to each surface of the one or more surface with respect to the transformation matrix to compute a rectified image of each surface of the one or more surfaces.
23. The system of claim 22, wherein the one or more cameras comprise at least one depth camera, wherein the visual information of the object comprises a depth map, wherein the instructions to estimate the orientation of each of the one or more surfaces comprise instructions that, when executed by the processor, cause the processor to detect a substantially planar shape of the label in the depth map, and wherein the instructions to compute the transformation matrix comprise instructions that, when executed by the processor, cause the processor to compute a transformation between the substantially planar shape of the label and the fronto-parallel orientation.
</claims>
</document>
