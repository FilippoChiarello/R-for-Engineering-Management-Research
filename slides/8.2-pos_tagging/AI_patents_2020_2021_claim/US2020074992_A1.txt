<document>

<filing_date>
2018-12-26
</filing_date>

<publication_date>
2020-03-05
</publication_date>

<priority_date>
2018-08-31
</priority_date>

<ipc_classes>
G10L15/06,G10L15/18,G10L25/78
</ipc_classes>

<assignee>
UBTECH ROBOTICS CORPORATION
</assignee>

<inventors>
XIONG, YOUJUN
XIA, YANHUI
LI, HAOMING
WEN, PINXIU
LI, LIYANG
</inventors>

<docdb_family_id>
69641460
</docdb_family_id>

<title>
METHOD AND APPARATUS FOR JUDGING TERMINATION OF SOUND RECEPTION AND TERMINAL DEVICE
</title>

<abstract>
The present disclosure discloses a method and an apparatus for judging termination of sound reception and a terminal device. The method including: performing a voice activity detection on a current sound clip to obtain a first value; performing a semantic relevance detection on the current sound clip and a next sound clip by deep learning to obtain a second value; performing a weighted calculation on the first value and the second value to obtain a third value; comparing the third value with a preset threshold; and determining whether sound reception of the current sound clip is terminated based on the comparison result.
</abstract>

<claims>
1. A method for judging termination of sound reception, comprising: performing a voice activity detection on a current sound clip to obtain a first value; performing a semantic relevance detection on the current sound clip and a next sound clip by deep learning to obtain a second value; performing a weighted calculation on the first value and the second value to obtain a third value: comparing the third value with a preset threshold; and determining whether sound reception of the current sound clip is terminated based on the comparison result.
2. The judgment method according to claim 1, wherein the performing the semantic relevance detection on the current sound clip and the next sound clip by deep learning to obtain the second value comprises: performing a semantic paraphrase on the current sound clip and the next sound clip and detecting the semantic relevance between the current sound clip and the next sound clip to obtain the second value for the current sound clip and the next sound clip.
3. The judgment method according to claim 2, wherein the detecting the semantic relevance between the current sound clip and the next sound clip comprises: determine whether voice in the current sound clip is semantically associated with voice in the next sound clip, and determine whether sentences are coherent and smooth.
4. The judgment method according to claim 2, further comprising: performing a semantic paraphrase or spell on an end word in the current sound clip and a beginning word in the next sound clip.
5. The judgment method according to claim 1, wherein the performing the weighted calculation on the first value and the second value to obtain the third value comprises: assigning a first preset weight to the first value to obtained a first weight value; assigning a second preset weight to the second value to obtained a second weight value; and performing the weighted calculation on the first weight value and the second weight value to obtain the third value.
6. The judgment method according to claim 1, wherein the performing the voice activity detection on the current sound clip to obtain the first value comprises: segmenting the current sound clip into a plurality of data frames, and processing the plurality of data frames to obtain the first value for the current sound clip.
7. The judgment method according to claim 6, wherein the processing the plurality of data frames to obtain the first value for the current sound clip comprises: determine whether there is a sudden drop in energy of the plurality of data frames; or determine whether there is a sudden change of frequency and amplitude.
8. The judgment method according to claim 1, wherein the first value is a voice end identifier of the sound clip, and the first value is used for identifying whether a cease phenomenon exists in the voice activity of the sound clip.
9. An apparatus for judging termination of sound reception, comprising a memory and a processor, wherein the memory is configured to store computer instructions, the processor connects to the memory and executes the computer instructions and is configured to: perform a voice activity detection on a current sound clip to obtain a first value; perform a semantic relevance detection on the current sound clip and a next sound clip by deep learning to obtain a second value; perform a weighted calculation on the first value and the second value to obtain a third value; compare the third value with a preset threshold; and determine whether sound reception of the current sound clip is terminated based on the comparison result.
10. The judgment apparatus according to claim 9, wherein the performing the semantic relevance detection on the current sound clip and the next sound clip by deep learning to obtain the second value comprises: performing a semantic paraphrase on the current sound clip and the next sound clip and detecting the semantic relevance between the current sound clip and the next sound clip to obtain the second value for the current sound clip and the next sound clip.
11. The judgment apparatus according to claim 10, wherein the detecting the semantic relevance between the current sound clip and the next sound clip comprises: determine whether voice in the current sound clip is semantically associated with voice in the next sound clip, and determine whether sentences are coherent and smooth.
12. The judgment apparatus according to claim 10, further comprising: performing a semantic paraphrase or spell on an end word in the current sound clip and a beginning word in the next sound clip.
13. The judgment apparatus according to claim 9, wherein the performing the weighted calculation on the first value and the second value to obtain the third value comprises: assigning a first preset weight to the first value to obtained a first weight value; assigning a second preset weight to the second value to obtained a second weight value; and performing the weighted calculation on the first weight value and the second weight value to obtain the third value.
14. The judgment apparatus according to claim 9, wherein the performing the voice activity detection on the current sound clip to obtain the first value comprises: segmenting the current sound clip into a plurality of data frames, and processing the plurality of data frames to obtain the first value for the current sound clip.
15. The judgment apparatus according to claim 14, wherein the processing the plurality of data frames to obtain the first value for the current sound clip comprises: determine whether there is a sudden drop in energy of the plurality of data frames; or determine whether there is a sudden change of frequency and amplitude.
16. A terminal device, comprising: a memory, a processor, and a computer program stored in the memory and executable by the processor; wherein the processor is configured to, when executes the computer program, implement following operations: performing a voice activity detection on a current sound clip to obtain a first value; performing a semantic relevance detection on the current sound clip and a next sound clip by deep learning to obtain a second value; performing a weighted calculation on the first value and the second value to obtain a third value; comparing the third value with a preset threshold; and determining whether sound reception of the current sound clip is terminated based on the comparison result.
17. The terminal device according to claim 16, wherein the performing the semantic relevance detection on the current sound clip and the next sound clip by deep learning to obtain the second value comprises: performing a semantic paraphrase on the current sound clip and the next sound clip and detecting the semantic relevance between the current sound clip and the next sound clip to obtain the second value for the current sound clip and the next sound clip.
18. The terminal device according to claim 17, further comprising: performing a semantic paraphrase or spell on an end word in the current sound clip and a beginning word in the next sound clip.
19. The terminal device according to claim 16, wherein the performing the weighted calculation on the first value and the second value to obtain the third value comprises: assigning a first preset weight to the first value to obtained a first weight value; assigning a second preset weight to the second value to obtained a second weight value; and performing the weighted calculation on the first weight value and the second weight value to obtain the third value.
20. The terminal device according to claim 16, wherein the performing the voice activity detection on the current sound clip to obtain the first value comprises: segmenting the current sound clip into a plurality of data frames, and processing the plurality of data frames to obtain the first value for the current clip.
</claims>
</document>
