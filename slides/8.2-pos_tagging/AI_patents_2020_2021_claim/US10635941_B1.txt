<document>

<filing_date>
2019-01-29
</filing_date>

<publication_date>
2020-04-28
</publication_date>

<priority_date>
2019-01-29
</priority_date>

<ipc_classes>
G06K9/62,G06N20/00,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
STRADVISION
</assignee>

<inventors>
BOO, SUKHOON
CHO, HOJIN
JANG, TAEWOONG
JE, HONGMO
JEONG, KYUNGJOONG
KIM, HAK-KYOUNG
KIM, INSU
KIM, KYE-HYEON
KIM, YONGJOONG
NAM, WOONHYUN
RYU, WOOJU
SUNG, MYUNGCHUL
YEO, DONGHUN
</inventors>

<docdb_family_id>
69177098
</docdb_family_id>

<title>
Method and device for on-device continual learning of neural network which analyzes input data by optimized sampling of training images, and method and device for testing the neural network for smartphones, drones, vessels, or military purpose
</title>

<abstract>
A method for on-device continual learning of a neural network which analyzes input data is provided for smartphones, drones, vessels, or a military purpose. The method includes steps of: a learning device, (a) uniform-sampling new data to have a first volume, instructing a boosting network to convert a k-dimension random vector into a k-dimension modified vector, instructing an original data generator network to repeat outputting synthetic previous data of a second volume corresponding to the k-dimension modified vector and previous data having been used for learning, and generating a batch for a current-learning; and (b) instructing the neural network to generate output information corresponding to the batch. The method can be used for preventing catastrophic forgetting and an invasion of privacy, and for optimizing resources such as storage and sampling processes for training images. Further the method can be performed through a learning for Generative adversarial networks (GANs).
</abstract>

<claims>
1. A method for on-device continual learning of a neural network which analyzes input data, comprising steps of: (a) a learning device, if new data acquired for learning reaches a preset base volume, uniform-sampling the new data of the preset base volume such that the new data has a preset first volume, inputting at least one k-dimension random vector into a boosting network, to thereby instruct the boosting network to convert the k-dimension random vector into at least one k-dimension modified vector having losses higher than those of the k-dimension random vector, inputting the k-dimension modified vector into an original data generator network which has been learned, to thereby instruct the original data generator network to repeat a process of outputting first synthetic previous data corresponding to the k-dimension modified vector and also corresponding to previous data having been used for learning the original data generator network, such that the first synthetic previous data has a preset second volume, and generating a first batch to be used for a first current-learning by referring to the new data of the preset first volume and the first synthetic previous data of the preset second volume; (b) the learning device instructing the neural network to generate output information corresponding to the first batch by inputting the first batch into the neural network, instructing a first loss layer to calculate one or more first losses by referring to the output information and its corresponding GT (Ground Truth), and performing the first current-learning of the neural network and the boosting network by backpropagating the first losses; (c) the learning device uniform-sampling the new data of the preset base volume such that the new data has the preset first volume, cloning the original data generator network to thereby generate a cloned data generator network, instructing the cloned data generator network to repeat a process of outputting second synthetic previous data corresponding to the k-dimension random vector and also corresponding to the previous data having been used for learning the original data generator network such that the second synthetic previous data has the preset second volume, instructing the original data generator network to repeat a process of outputting third synthetic previous data corresponding to the k-dimension random vector and also corresponding to the previous data having been used for learning the original data generator network such that the third synthetic previous data has a preset third volume which equals to a sum of the preset first volume and the preset second volume, and generating a second batch to be used for a second current-learning by referring to the new data of the preset first volume, the second synthetic previous data of the preset second volume, and the third synthetic previous data of the preset third volume; and (d) the learning device instructing a discriminator to generate score vectors corresponding to the second batch by inputting the second batch into the discriminator, instructing a second loss layer to calculate one or more second losses by referring to the score vectors and their corresponding GTs, and performing the second current-learning of the discriminator and the original data generator network by backpropagating the second losses.
2. The method of claim 1, wherein the learning device repeats the steps of (c) and (d) until losses of the discriminator and losses of the second data generator network respectively reach convergence states by backpropagating the second losses.
3. The method of claim 1, wherein, at the step of (d), the learning device performs a gradient ascent of at least one weight of the discriminator and at least one weight of the second data generator network by backpropagating the second losses.
4. The method of claim 1, wherein, at the step of (d), the learning device, in performing the second current-learning of the discriminator by backpropagating the second losses, regards the second synthetic previous data from the cloned data generator network as real data and performs the second current-learning of the discriminator.
5. The method of claim 1, wherein, at the step of (d), the learning device performs the second current-learning of the original data generator network such that third synthetic previous data score vectors, corresponding to the third synthetic previous data, among the score vectors are maximized.
6. The method of claim 1, wherein, if the second current-learning is a first learning, at the step of (a), the learning device generates the first batch using only the new data of the preset first volume, and wherein, at the step of (c), the learning device instructs the original data generator network to repeat a process of outputting third synthetic previous data corresponding to the k-dimension random vector such that the third synthetic previous data has the preset first volume, and generates the second batch by referring to the new data of the preset first volume and the third synthetic previous data of the preset first volume.
7. The method of claim 1, wherein the learning device repeats the steps of (a) and (b) until the first losses reaches a convergence state by backpropagating the first losses.
8. The method of claim 7, wherein, supposing that the steps of (a) and (b) are repeated as the first current-learning, the learning device (i) at a first iteration, initializes each of sampling probabilities corresponding to each piece of the new data of the preset base volume at the step of (a), and generates the new data of the preset first volume through the uniform-sampling of the new data of the preset base volume by referring to the initialized sampling probabilities, and if the first current-learning of the neural network is completed at the step of (b), updates each of the sampling probabilities corresponding to each piece of the new data of the preset first volume by referring to the first losses corresponding to the new data of the preset first volume, to thereby update each of the sampling probabilities corresponding to each piece of the new data of the preset base volume, and (ii) at a next iteration, generates the new data of the preset first volume through the uniform-sampling of the new data of the preset base volume by referring to each of the sampling probabilities updated at a previous iteration corresponding to each piece of the new data of the preset base volume at the step of (a), and if the first current-learning of the neural network is completed at the step of (b), updates each of the sampling probabilities updated at the previous iteration corresponding to each piece of the new data of the preset base volume by referring to the first losses corresponding to the new data of the preset first volume.
9. The method of claim 7, wherein, supposing that the steps of (a) and (b) are repeated as the first current-learning, at a first iteration, the learning device, at the step of (a), initializes the boosting network and instructs the initialized boosting network to convert the k-dimension random vector into the k-dimension modified vector, and at a next iteration, the learning device, at the step of (a), instructs the boosting network, which has completed the first current-learning at the step of (b) in a previous iteration, to convert the k-dimension random vector into the k-dimension modified vector.
10. The method of claim 1, wherein, at the step of (b), the learning device performs a gradient descent of at least one weight of the neural network such that losses of the neural network are minimized by backpropagating the first losses, and performs a gradient ascent of at least one weight of the boosting network such that losses corresponding to the first synthetic previous data among the first losses are maximized.
11. The method of claim 10, wherein the learning device clips the weight of the boosting network when performing the gradient ascent of the weight of the boosting network.
12. The method of claim 1, wherein the boosting network includes one or more fully connected layers of low dimension.
13. A method for testing a neural network which analyzes input data, comprising steps of: (a) a testing device acquiring test data, on condition that a learning device has completed processes of (I) if new data acquired for learning reaches a preset base volume, uniform-sampling the new data for training of the preset base volume such that the new data has a preset first volume, inputting at least one k-dimension random vector for training into a boosting network, to thereby instruct the boosting network to convert the k-dimension random vector for training into at least one k-dimension modified vector for training having losses higher than those of the k-dimension random vector for training, inputting the k-dimension modified vector for training into an original data generator network which has been learned, to thereby instruct the original data generator network to repeat a process of outputting first synthetic previous data for training corresponding to the k-dimension modified vector for training and also corresponding to previous data for training having been used for learning the original data generator network, such that the first synthetic previous data for training has a preset second volume, and generating a first batch for training to be used for a first current-learning by referring to the new data for training of the preset first volume and the first synthetic previous data for training of the preset second volume, (II) instructing the neural network to generate output information for training corresponding to the first batch for training by inputting the first batch for training into the neural network, instructing a first loss layer to calculate one or more first losses by referring to the output information for training and its corresponding GT (Ground Truth), and performing the first current-learning of the neural network and the boosting network by backpropagating the first losses, (III) uniform-sampling the new data for training of the preset base volume such that the new data for training has the preset first volume, cloning the original data generator network to thereby generate a cloned data generator network, instructing the cloned data generator network to repeat a process of outputting second synthetic previous data for training corresponding to the k-dimension random vector for training and also corresponding to the previous data for training having been used for learning the original data generator network such that the second synthetic previous data for training has the preset second volume, instructing the original data generator network to repeat a process of outputting third synthetic previous data for training corresponding to the k-dimension random vector for training and also corresponding to the previous data for training having been used for learning the original data generator network such that the third synthetic previous data for training has a preset third volume which equals to a sum of the preset first volume and the preset second volume, and generating a second batch for training to be used for a second current-learning by referring to the new data for training of the preset first volume, the second synthetic previous data for training of the preset second volume, and the third synthetic previous data for training of the preset third volume, and (IV) instructing a discriminator to generate score vectors for training corresponding to the second batch for training by inputting the second batch for training into the discriminator, instructing a second loss layer to calculate one or more second losses by referring to the score vectors for training and their corresponding GTs, and performing the second current-learning of the discriminator and the original data generator network by backpropagating the second losses; and (b) the testing device instructing the neural network to generate output information for testing corresponding to the test data by inputting the test data into the neural network.
14. A learning device for on-device continual learning of a neural network which analyzes input data, comprising: at least one memory that stores instructions; and at least one processor configured to execute the instructions to: perform processes of: (I) if new data acquired for learning reaches a preset base volume, uniform-sampling the new data of the preset base volume such that the new data has a preset first volume, inputting at least one k-dimension random vector into a boosting network, to thereby instruct the boosting network to convert the k-dimension random vector into at least one k-dimension modified vector having losses higher than those of the k-dimension random vector, inputting the k-dimension modified vector into an original data generator network which has been learned, to thereby instruct the original data generator network to repeat a process of outputting first synthetic previous data corresponding to the k-dimension modified vector and also corresponding to previous data having been used for learning the original data generator network, such that the first synthetic previous data has a preset second volume, and generating a first batch to be used for a first current-learning by referring to the new data of the preset first volume and the first synthetic previous data of the preset second volume, (II) instructing the neural network to generate output information corresponding to the first batch by inputting the first batch into the neural network, instructing a first loss layer to calculate one or more first losses by referring to the output information and its corresponding GT (Ground Truth), and performing the first current-learning of the neural network and the boosting network by backpropagating the first losses, (III) uniform-sampling the new data of the preset base volume such that the new data has the preset first volume, cloning the original data generator network to thereby generate a cloned data generator network, instructing the cloned data generator network to repeat a process of outputting second synthetic previous data corresponding to the k-dimension random vector and also corresponding to the previous data having been used for learning the original data generator network such that the second synthetic previous data has the preset second volume, instructing the original data generator network to repeat a process of outputting third synthetic previous data corresponding to the k-dimension random vector and also corresponding to the previous data having been used for learning the original data generator network such that the third synthetic previous data has a preset third volume which equals to a sum of the preset first volume and the preset second volume, and generating a second batch to be used for a second current-learning by referring to the new data of the preset first volume, the second synthetic previous data of the preset second volume, and the third synthetic previous data of the preset third volume, and (IV) instructing a discriminator to generate score vectors corresponding to the second batch by inputting the second batch into the discriminator, instructing a second loss layer to calculate one or more second losses by referring to the score vectors and their corresponding GTs, and performing the second current-learning of the discriminator and the original data generator network by backpropagating the second losses.
15. The learning device of claim 14, wherein the processor repeats the processes of (III) and (IV) until losses of the discriminator and losses of the second data generator network respectively reach convergence states by backpropagating the second losses.
16. The learning device of claim 14, wherein, at the process of (IV), the processor performs a gradient ascent of at least one weight of the discriminator and at least one weight of the second data generator network by backpropagating the second losses.
17. The learning device of claim 14, wherein, at the process of (IV), the processor, in performing the second current-learning of the discriminator by backpropagating the second losses, regards the second synthetic previous data from the cloned data generator network as real data and performs the second current-learning of the discriminator.
18. The learning device of claim 14, wherein, at the process of (IV), the processor performs the second current-learning of the original data generator network such that third synthetic previous data score vectors, corresponding to the third synthetic previous data, among the score vectors are maximized.
19. The learning device of claim 14, wherein, if the second current-learning is a first learning, at the process of (I), the processor generates the first batch using only the new data of the preset first volume, and wherein, at the process of (III), the processor instructs the original data generator network to repeat a process of outputting third synthetic previous data corresponding to the k-dimension random vector such that the third synthetic previous data has the preset first volume, and generates the second batch by referring to the new data of the preset first volume and the third synthetic previous data of the preset first volume.
20. The learning device of claim 14, wherein the processor repeats the processes of (I) and (II) until the first losses reaches a convergence state by backpropagating the first losses.
21. The learning device of claim 20, wherein, supposing that the processes of (I) and (II) are repeated as the first current-learning, the processor (i) at a first iteration, initializes each of sampling probabilities corresponding to each piece of the new data of the preset base volume at the process of (I), and generates the new data of the preset first volume through the uniform-sampling of the new data of the preset base volume by referring to the initialized sampling probabilities, and if the first current-learning of the neural network is completed at the process of (II), updates each of the sampling probabilities corresponding to each piece of the new data of the preset first volume by referring to the first losses corresponding to the new data of the preset first volume, to thereby update each of the sampling probabilities corresponding to each piece of the new data of the preset base volume, and (ii) at a next iteration, generates the new data of the preset first volume through the uniform-sampling of the new data of the preset base volume by referring to each of the sampling probabilities updated at a previous iteration corresponding to each piece of the new data of the preset base volume at the process of (I), and if the first current-learning of the neural network is completed at the process of (II), updates each of the sampling probabilities updated at the previous iteration corresponding to each piece of the new data of the preset base volume by referring to the first losses corresponding to the new data of the preset first volume.
22. The learning device of claim 20, wherein, supposing that the processes of (I) and (II) are repeated as the first current-learning, at a first iteration, the processor, at the process of (I), initializes the boosting network and instructs the initialized boosting network to convert the k-dimension random vector into the k-dimension modified vector, and at a next iteration, the processor, at the process of (I), instructs the boosting network, which has completed the first current-learning at the process of (II) in a previous iteration, to convert the k-dimension random vector into the k-dimension modified vector.
23. The learning device of claim 14, wherein, at the process of (II), the processor performs a gradient descent of at least one weight of the neural network such that losses of the neural network are minimized by backpropagating the first losses, and performs a gradient ascent of at least one weight of the boosting network such that losses corresponding to the first synthetic previous data among the first losses are maximized.
24. The learning device of claim 23, wherein the processor clips the weight of the boosting network when performing the gradient ascent of the weight of the boosting network.
25. The learning device of claim 14, wherein the boosting network includes one or more fully connected layers of low dimension.
26. A testing device for testing a neural network which analyzes input data, comprising: at least one memory that stores instructions; and at least one processor, on condition that a learning device has completed processes of (1) if new data acquired for learning reaches a preset base volume, uniform-sampling the new data for training of the preset base volume such that the new data has a preset first volume, inputting at least one k-dimension random vector for training into a boosting network, to thereby instruct the boosting network to convert the k-dimension random vector for training into at least one k-dimension modified vector for training having losses higher than those of the k-dimension random vector for training, inputting the k-dimension modified vector for training into an original data generator network which has been learned, to thereby instruct the original data generator network to repeat a process of outputting first synthetic previous data for training corresponding to the k-dimension modified vector for training and also corresponding to previous data for training having been used for learning the original data generator network, such that the first synthetic previous data for training has a preset second volume, and generating a first batch for training to be used for a first current-learning by referring to the new data for training of the preset first volume and the first synthetic previous data for training of the preset second volume, (2) instructing the neural network to generate output information for training corresponding to the first batch for training by inputting the first batch for training into the neural network, instructing a first loss layer to calculate one or more first losses by referring to the output information for training and its corresponding GT, and performing the first current-learning of the neural network and the boosting network by backpropagating the first losses, (3) uniform-sampling the new data for training of the preset base volume such that the new data for training has the preset first volume, cloning the original data generator network to thereby generate a cloned data generator network, instructing the cloned data generator network to repeat a process of outputting second synthetic previous data for training corresponding to the k-dimension random vector for training and also corresponding to the previous data for training having been used for learning the original data generator network such that the second synthetic previous data for training has the preset second volume, instructing the original data generator network to repeat a process of outputting third synthetic previous data for training corresponding to the k-dimension random vector for training and also corresponding to the previous data for training having been used for learning the original data generator network such that the third synthetic previous data for training has a preset third volume which equals to a sum of the preset first volume and the preset second volume, and generating a second batch for training to be used for a second current-learning by referring to the new data for training of the preset first volume, the second synthetic previous data for training of the preset second volume, and the third synthetic previous data for training of the preset third volume, and (4) instructing a discriminator to generate score vectors for training corresponding to the second batch for training by inputting the second batch for training into the discriminator, instructing a second loss layer to calculate one or more second losses by referring to the score vectors for training and their corresponding GTs, and performing the second current-learning of the discriminator and the original data generator network by backpropagating the second losses; configured to execute the instructions to: perform a process of instructing the neural network to generate output information for testing corresponding to acquired test data by inputting the test data into the neural network.
</claims>
</document>
