<document>

<filing_date>
2019-08-26
</filing_date>

<publication_date>
2020-03-05
</publication_date>

<priority_date>
2018-08-30
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
JOHNSON PREMKUMAR, MELVIN JOSE
ERIGUCHI, AKIKO
FIRAT, ORHAN
</inventors>

<docdb_family_id>
67902618
</docdb_family_id>

<title>
CROSS-LINGUAL CLASSIFICATION USING MULTILINGUAL NEURAL MACHINE TRANSLATION
</title>

<abstract>
Training and/or using a multilingual classification neural network model to perform a natural language processing classification task, where the model reuses an encoder portion of a multilingual neural machine translation model. In a variety of implementations, a client device can generate a natural language data stream from a spoken input from a user. The natural language data stream can be applied as input to an encoder portion of the multilingual classification model. The output generated by the encoder portion can be applied as input to a classifier portion of the multilingual classification model. The classifier portion can generate a predicted classification label of the natural language data stream. In many implementations, an output can be generated based on the predicted classification label, and a client device can present the output.
</abstract>

<claims>
What is claimed is:
1. A method implemented by one or more processors, comprising:
receiving a natural language data stream in a first language, the natural language data stream generated based on user interface input of a user via a client device;
generating a classification label prediction for the natural language data stream in the first language, wherein generating the classification label prediction comprises:
processing, using an encoder portion of a m ultilingual classification model, the natural language data strea m to generate encoder output, and
processing, using a classifier portion of the multilingual classification model, the encoder output to generate a classification label prediction for the natural language data stream in the first language;
wherein the multilingual classification model is a trained neural network model, wherein the encoder portion is a reused multilingual neural machine translation encoder that was trained using a plurality of languages that include the first language and at least a second language, and wherein the multilingual classification model is trained for a classification task using at least second la nguage supervised training examples that each have respective second la nguage training example input that is in the second language; generating output using the classification label prediction for the natural language data stream in the first language; and
causing the client device to render the output for presentation to the user.
2. The method of claim 1, wherein the multilingual classification model is trained for the classification task without using any first language supervised training example that has first language training example input that is in the first language.
3. The method of claim 1,
wherein the multilingual classification model is trained for the classification task using a first quantity of the second language supervised training examples that each have the respective second language training example input that is in the second language;
wherein the multilingual classification model is trained for the classification task using a second quantity of first language supervised training examples that each have respective first language training example input that is in the first language; and
wherein the second quantity is less than fifty percent of the first quantity.
4. The method of claim 3, wherein the second quantity is less than ten percent of the first quantity, or less than five percent of the first quantity.
5. The method of any one of the preceding claims, wherein the multilingual neural machine translation encoder was trained to translate from at least the first language to at least the second language and/or from at least the second language to at least the first language.
6. The method of any preceding claim, wherein training the multilingual classification model using the second language supervised training examples comprises processing the second language training examples inputs using the encoder portion and the classifier portion to generate predicted outputs, and updating weights of the classifier portion based on errors determined based on the predicted outputs and based on labels for the second language supervised training examples, without updating any weights of the reused encoder portion.
7. The method of any preceding claim, wherein processing, using the encoder portion, the natural language data stream to generate the encoder output comprises:
processing the natural language data stream, using a bi-directional recurrent network layer of the encoder portion, to generate bi-directional recurrent output; and processing the bi-directiona l recurrent output using one or more unidirectional recurrent network layers of the encoder portion, wherein the encoder output comprises one or more hidden states of a last unidirectional recurrent network layer in the one or more unidirectional recurrent network layers.
8. The method of any preceding claim, wherein processing, using the classifier portion, the encoder output to generate the classification label prediction comprises:
processing the encoder output, using pooling layers of the classifier portion, to generate pooling output; and
processing the pooling output, using one or more softmax layers of the classifier portion, to generate the classification label prediction.
9. The method of claim 8, wherein processing the encoder output, using the pooling layers to generate the classification label prediction comprises:
processing, using a pre-pooling feed-forwa rd layer of the pooling layers, the encoder output to generate a pre-pooling output;
processing, using a pooling layer of the pooling layers, the pre-pooling output to generate a pooling output;
processing, using a post-pooling feed-forward layer of the pooling layers, the pooling output to generate a post-pooling output; and
processing, using the one or more softmax layers, the post-pooling output to generate the classification label prediction for the natural language data stream in the first language.
10. The method of any preceding claim, wherein the plura lity of languages includes at least the first language, the second la nguage, and a third language.
11. The method of any preceding claim, wherein the user interface input is spoken input and further comprising:
BO generating the natural language data stream by converting the spoken input to text using a speech-to-text processor.
12. The method of any preceding claim, wherein the classification task is a natural language inference classification task and wherein the classification label prediction indicates whether the natural language data stream is an entailment, is neutral, or is a contradiction.
13. The method of claim 12, wherein the classification label prediction indicates that the natural language data stream is the entailment, and wherein generating the output using the classification label prediction comprises:
in response to the classification label prediction indicating that the natural language data stream is the entailment:
using an immediately preceding natural language data stream, in combination with the natural language data stream, in generating the output.
14. The method of claim 13, wherein the natural language data stream and the preceding natural language data stream are each generated based on respective spoken input.
15. The method of claim 14, further comprising:
determining, using the classification label prediction, that the natural language data stream includes a request to modify a state of a networked hardware device;
in response to determining that the natural language data stream includes the request to modify the state of the networked hardware device:
causing a control signal to be provided to the networked hardware device, wherein the control signal causes the state of the networked hardware device to be modified;
wherein the output perceptibly indicates the modification of the state of the networked hardware device.
16. The method of claim 15, wherein the networked hardware device is selected from the group consisting of a light, a thermostat, and a networked camera.
17. A computer program product comprising instructions, when executed by one or more processors, cause the one or more processors to carry out the method of any one of the preceding claims.
18. A computer-readable storage medium comprising instructions, which, when executed by one or more processors, cause the one or more processors to carry out the method of any one of claims 1 to 16.
19. A system comprising one or more processors for carrying out the method of any one of claims 1 to 16.
</claims>
</document>
