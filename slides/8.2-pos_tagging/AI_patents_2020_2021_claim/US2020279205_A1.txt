<document>

<filing_date>
2019-08-09
</filing_date>

<publication_date>
2020-09-03
</publication_date>

<priority_date>
2019-02-28
</priority_date>

<ipc_classes>
G06K9/62,G06N20/00,G06Q10/06
</ipc_classes>

<assignee>
AUTODESK
</assignee>

<inventors>
FITZMAURICE, GEORGE
GROSSMAN, TOVI
ANDERSON, FRASER
GONG, JUN
</inventors>

<docdb_family_id>
72236073
</docdb_family_id>

<title>
TECHNIQUES FOR TAILORING FABRICATION ENVIRONMENTS BASED ON USER, TASK, AND EXPERTISE
</title>

<abstract>
Techniques are disclosed for determining users, tasks, and levels of expertise in fabrication environments. Sensors may be placed in a fabrication environment, instrumented in tools, and/or worn on users. Given labeled features extracted from sensor data, machine learning techniques are used to train models to recognize users, tasks, and levels of expertise. The trained models may then be deployed to determine the users, tasks, and levels of expertise in a fabrication environment based on features extracted from new sensor data. In turn, actions such as displaying guidance or instructional materials, or locking a user out of using certain tools or parts of the fabrication environment, may be taken.
</abstract>

<claims>
1. A computer-implemented method for performing an action in a fabrication environment, the method comprising: receiving data from one or more sensors disposed in the fabrication environment; extracting one or more features from the data; processing the extracted features using a trained machine learning model to determine at least one of a user present in the fabrication environment, a task being performed by a user in the fabrication environment, and an expertise level of a user in performing a task in the fabrication environment; and performing at least one action in the fabrication environment based on at least one of the user, the task, and the expertise level.
2. The method of claim 1, wherein extracting the one or more features comprises: dividing the data received from each sensor included in the one or more sensors into a plurality of data instances using a temporal sliding window; and determining, for each data instance in the plurality of data instances, at least one of a spectral feature, a statistical feature, and a number of saccades.
3. The method of claim 1, wherein extracting the one or more features further comprises determining a single window that includes averages of data points in each window of the sliding window.
4. The method of claim 1, wherein the one or more sensors comprise at least one of a wearable sensor, a sensor instrumented in a tool, and a sensor placed in the fabrication environment.
5. The method of claim 1, wherein the one or more sensors comprise at least one of an eye tracker, an accelerometer, a gyroscope, a microphone, a heart rate sensor, an inertial measurement unit, an ambient light sensor, a humidity sensor, a temperature sensor, a barometric pressure sensor, and a magnetometer.
6. The method of claim 1, wherein the trained machine learning model comprises a random forest classifier or regressor.
7. The method of claim 1, wherein performing the at least one action includes: determining instructional content based on at least one of the user, the task, and the level of expertise; and displaying the instructional content.
8. The method of claim 1, wherein the one or more extracted features are processed to determine the expertise level of a user in performing a task in the fabrication environment, and the at least one action comprises preventing the user from accessing at least one tool needed to perform the task.
9. A non-transitory computer-readable storage medium including instructions that, when executed by a processor, cause the processor to perform steps of: receiving data from one or more sensors disposed in a fabrication environment; extracting one or more features from the data; and processing the extracted features using a trained machine learning model to determine at least one of a user present in the fabrication environment, a task being performed by a user in the fabrication environment, and an expertise level of a user in performing a task in the fabrication environment.
10. The computer-readable storage medium of claim 9, the steps further comprising: training the machine learning model based on training data collected as a plurality of users perform predefined tasks, wherein the plurality of users include users having at least one of different levels of expertise in performing the predefined tasks and different levels of overall expertise.
11. The computer-readable storage medium of claim 10, wherein the plurality of users further includes users having at least one of different handedness and different vision.
12. The computer-readable storage medium of claim 9, wherein extracting the features includes: dividing the received data from each of the one or more sensors into a plurality of data instances using a temporal sliding window; and determining, for each of the data instances, at least one of a spectral feature, a statistical feature, or a number of saccades.
13. The computer-readable storage medium of claim 9, wherein extracting the features further comprises determining a single window that includes averages of data points in each window of the sliding window.
14. The computer-readable storage medium of claim 9, wherein the one or more sensors comprise at least one of a sensor worn on a user, a sensor instrumented in a tool, and a sensor placed in a fabrication environment.
15. The computer-readable storage medium of claim 9, the steps further comprising: determining instructional content based on at least one of the user, the task, and the level of expertise; and displaying the instructional content.
16. The computer-readable storage medium of claim 9, wherein the extracted features are processed to determine the expertise level of the user in performing the task, and the steps further comprise: responsive to determining that the expertise level of the user in performing the task does not satisfy a predefined threshold, preventing the user from accessing at least one tool associated with the task.
17. A system comprising: one or more sensors comprising at least one of a sensor worn on a user, a sensor instrumented in a tool, and a sensor placed in a fabrication environment; and a computing device comprising: a memory storing a management application, and a processor that is coupled to the memory and, when executing the management application, is configured to: receive data from the one or more sensors; extract one or more features from the data; and process the extracted features using a trained machine learning model to determine at least one of a user present in the fabrication environment, a task being performed by a user in the fabrication environment, and an expertise level of a user in performing a task in the fabrication environment.
18. The system of claim 17, wherein: the one or more sensors comprise the sensor instrumented in the tool; the sensor instrumented in the tool is one of an accelerator, a gyroscope, an inertial measurement unit, an ambient light sensor, a humidity sensor, a temperature sensor, a barometric pressure sensor, and a magnetometer; and the sensor instrumented in the tool is attached to a handle, a side of a blade, an iron, a shaft, a drill press, or a base of the tool.
19. The system of claim 17, wherein: the one or more sensors comprise the sensor placed in the fabrication environment; and the sensor placed in the fabrication environment is an accelerometer or a microphone.
20. The system of claim 17, wherein: the one or more sensors comprise the sensor worn on the user; and the sensor worn on the user is one of an eye tracker, an accelerometer, a gyroscope, and a heart rate sensor.
</claims>
</document>
