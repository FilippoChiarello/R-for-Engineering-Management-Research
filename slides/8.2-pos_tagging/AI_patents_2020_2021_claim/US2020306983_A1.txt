<document>

<filing_date>
2020-03-26
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2019-03-27
</priority_date>

<ipc_classes>
B25J13/08,B25J19/02,B25J19/04,B25J9/16
</ipc_classes>

<assignee>
LG ELECTRONICS
</assignee>

<inventors>
LEE, JAE KWANG
NOH, DONGKI
LIM, SEUNGWOOK
CHOI, KAHYUNG
EOH, GYUHO
</inventors>

<docdb_family_id>
72604014
</docdb_family_id>

<title>
MOBILE ROBOT AND METHOD OF CONTROLLING THE SAME
</title>

<abstract>
A mobile robot includes a traveling unit configured to move a main body, a LiDAR sensor configured to acquire geometry information, a camera sensor configured to acquire an image of the outside of the main body, and a controller. The controller generates odometry information based on the geometry information acquired by the LiDAR sensor. The controller determines a current location of the mobile robot by performing feature matching between images acquired by the camera sensor based on the odometry information. The controller combines the information obtained by the camera sensor and the LiDAR sensor to accurately determine the current location of the mobile robot.
</abstract>

<claims>
1. A mobile robot comprising: a traveling unit configured to move a main body; a LiDAR sensor configured to acquire geometry information; a camera sensor configured to acquire images of an outside of the main body; and a controller configured to: generate odometry information based on the geometry information acquired by the LiDAR sensor; and perform feature matching between images acquired by the camera sensor based on the odometry information to estimate a current location of the mobile robot.
2. The mobile robot according to claim 1, further comprising: a traveling sensor configured to sense a traveling state based on movement of the main body, the traveling state including a distance traveled by the mobile robot, wherein the controller is configured to: perform an iterative closest point (ICP) matching on the geometry information acquired by the LiDAR sensor; and combine the traveling state sensed by the traveling sensor with a result of the ICP matching to generate the odometry information.
3. The mobile robot according to claim 1, wherein the controller comprises: a LiDAR service module configured to: receive the geometry information acquired by the LiDAR sensor; and determine an amount of location displacement using the geometry information and previous location information; and a vision service module configured to: receive the amount of location displacement from the LiDAR service module; receive an image from the camera sensor; determine a location of a feature point by matching a first feature point extracted from the image based on the amount of location displacement and a second feature point extracted from a previous location; and determine the current location of the mobile robot based on the determined location of the feature point.
4. The mobile robot according to claim 3, further comprising a storage configured to store node information comprising the determined current location.
5. The mobile robot according to claim 4, wherein the vision service module is configured to transmit the node information to the LiDAR service module, and the LiDAR service module is configured to: determine an additional amount of location displacement by which the mobile robot has moved during a time taken by the vision service module to determine the current location; and include the additional amount of location displacement in the node information.
6. The mobile robot according to claim 4, further comprising: a traveling sensor configured to sense a traveling state based on movement of the main body, the traveling state including a distance traveled by the mobile robot, wherein the controller further comprises a traveling service module configured to receive the traveling state sensed by the traveling sensor, the traveling service module is configured to transmit the traveling state to the LiDAR service module, and the LiDAR service module is configured to generate the odometry information by combining the traveling state sensed by the traveling sensor and an ICP result obtained by performing ICP on the geometry information acquired by the LiDAR sensor.
7. The mobile robot according to claim 1, wherein the controller is configured to: determine the current location of the mobile robot based on the geometry information acquired by the LiDAR sensor in an area having an illuminance less than a reference value; and perform a loop closing to correct an error when the mobile robot enters an area having an illuminance equal to or greater than the reference value.
8. The mobile robot according to claim 1, wherein the controller is configured to generate a correlation between nodes by performing an iterative closest point (ICP) matching between a current node and an adjacent node based on the geometry information acquired by the LiDAR sensor, when feature matching between images acquired by the camera sensor fails.
9. A method of controlling a mobile robot, the method comprising: acquiring geometry information using a LiDAR sensor; acquiring images of an outside of the main body using a camera sensor; creating odometry information based on the geometry information acquired by the LiDAR sensor; performing feature matching between images acquired by the camera sensor based on the odometry information; and determining a current location of the mobile robot based on a result of the feature matching.
10. The method according to claim 9, further comprising determining an uncertainty in the current location based on geometry information acquired by the LiDAR sensor.
11. The method according to claim 9, further comprising: sensing a traveling state using a traveling sensor, the traveling state including a distance traveled by the mobile robot; and matching the traveling state and the geometry information acquired by the LiDAR sensor based on an iterative closest point (ICP) algorithm.
12. The method according to claim 11, wherein generating the odometry information comprises combining the traveling state sensed by the traveling sensor and a result of the ICP algorithm.
13. The method according to claim 9, wherein generating the odometry information comprises: a LiDAR service module of a controller receiving the geometry data acquired by the LiDAR sensor; and the LiDAR service module determining an amount of location displacement using the geometry data and previous location information.
14. The method according to claim 13, wherein performing the feature matching comprises: a vision service module of the controller receiving the amount of location displacement from the LiDAR service module; the vision service module receiving an image from the camera sensor; and the vision service module determining a location of a feature point by matching a first feature point extracted from the image based on the amount of location displacement and a second feature point extracted from the previous location.
15. The method according to claim 14, further comprising storing node information comprising the calculated current location information in a storage.
16. The method according to claim 15, further comprising: the vision service module transmitting the node information to the LiDAR service module; the LiDAR service module determining an additional amount of location displacement by which the mobile robot has moved during a time period in which the vision service module determines the current location of the mobile robot; and the LiDAR service module including the additional amount of location displacement in the node information.
17. The method according to claim 16, further comprising: sensing a traveling state using a traveling sensor, the traveling state including a distance traveled by the mobile robot, wherein generating the odometry information comprises the LiDAR service module combining odometry information based on traveling state sensed by the traveling sensor and an ICP result obtained by performing ICP on the geometry information acquired by the LiDAR sensor.
18. The method according to claim 17, further comprising the traveling service module of the controller transmitting the travel state sensed by the traveling sensor to the LiDAR service module.
19. The method according to claim 9, further comprising: determining the current location based on the geometry information acquired by the LiDAR sensor in an area having an illuminance less than a reference value; and performing a loop closing to correct an error when the main body of the mobile robot moves and enters an area having an illuminance equal to or greater than the reference value.
20. The method according to claim 9, further comprising, performing iterative closest point (ICP) matching between a current node and an adjacent node based on the geometry information acquired by the LiDAR sensor to generate a correlation between nodes, when feature matching between images input from the camera sensor fails.
</claims>
</document>
