<document>

<filing_date>
2019-05-22
</filing_date>

<publication_date>
2020-11-26
</publication_date>

<priority_date>
2019-05-22
</priority_date>

<ipc_classes>
G06F16/901,G06K9/62,G06N20/00
</ipc_classes>

<assignee>
UNIVERSITY OF CALIFORNIA
</assignee>

<inventors>
CARREIRA-PERPIÑÁN, Miguel Á.
</inventors>

<docdb_family_id>
73456882
</docdb_family_id>

<title>
TREE ALTERNATING OPTIMIZATION FOR LEARNING CLASSIFICATION TREES
</title>

<abstract>
Computer-implemented methods for learning decision trees to optimize classification accuracy, comprising inputting an initial decision tree and an initial data training set and, for nodes not descendants of each other, if the node is a leaf, assigning a label based on a majority label of training points that reach the leaf, and if the node is a decision node, updating the parameters of the node's decision function based on solution of a reduced problem, iterating over the all nodes of the tree until parameters change less than a set threshold, or a number of iterations reaches a set limit, pruning the resulting tree to remove dead branches and pure subtrees, and using the resulting tree to make predictions from target data. In some embodiments, the TAO algorithm employs a sparsity penalty to learn sparse oblique trees where each decision function is a hyperplane involving only a small subset of features.
</abstract>

<claims>
1. A computer-implemented method for learning a decision tree to optimize classification accuracy, the method comprising: inputting an initial decision tree having a binary split at each node; inputting an initial data training set; for each node i of the decision tree: if the node is a leaf, assigning a label to the leaf based at least in part on a majority label of training points that reach the leaf; and if the node is a decision node, updating parameters of the node's decision function based on solution of a reduced problem: where ƒi (⋅; θi) is the decision function of the node i, γnϵ{left, right} is a child that leads to the correct classification for xn under i's current subtree, and L is the 0/1 loss; iterating over all nodes of the decision tree until the parameters change less than a set tolerance or a number of iterations reaches a set limit; where, for each iteration, a set of nodes at the same depth level are processed; pruning a resulting tree to remove dead branches and pure subtrees; and using the resulting tree on a client system to classify input from target data.
2. The computer-implemented method of claim 1, where pruning the resulting tree occurs only after a last iteration when the parameters change less than a set tolerance or a number of iterations reaches a set limit.
3. The computer-implemented method of claim 1, where each iteration is performed in reverse breadth-first search (BFS) order.
4. The computer-implemented method of claim 1, where the set of nodes at the same depth level are processed in parallel.
5. The computer-implemented method of claim 1, where the initial decision tree is an oblique tree and a penalty λ∥wi∥1 is added to the reduced problem for every decision node processed in the tree.
6. The computer-implemented method of claim 1, where the parameters of the node's decision function are updated only if the objective function decreases.
7. The computer-implemented method of claim 1, where the initial decision tree is an axis-aligned tree.
8. The computer-implemented method of claim 1, where iterating over all nodes of the tree continues until the parameters change less than a set tolerance.
9. The computer-implemented method of claim 1, where iterating over all nodes of the tree continues until a number of iterations reaches a set limit.
10. The computer-implemented method of claim 1, where the initial decision tree is not complete.
11. The computer-implemented method of claim 1, where the initial decision tree has random parameter values in the nodes.
12. A computer-implemented method for learning a decision tree to optimize classification accuracy, the method comprising: inputting an initial decision tree having a binary split at each node; inputting an initial data training set; for each node i of the tree: if the node is a leaf, assigning a label to the leaf based at least in part on a majority label of training points that reach the leaf; and if the node is a decision node, updating parameters of the node's decision function based on solution of a reduced problem: where ƒi(⋅; θi) is the decision function of the node i, γnϵ{left, right} is a child that leads to the correct classification for xn under i's current subtree, L is the 0/1 loss, and where wi is a weight vector and λ is a user set hyperparameter with a value ≥0; iterating over all nodes of the tree until the parameters change less than a set tolerance or a number of iterations reaches a set limit; where, for each iteration, all nodes at a same depth level are processed in parallel; pruning a resulting tree to remove dead branches and pure subtrees; and using the resulting tree on a client system to classify input from target data.
13. The computer-implemented method of claim 12, where pruning the resulting tree occurs only after a last iteration when the parameters change less than a set tolerance or a number of iterations reaches a set limit.
14. The computer-implemented method of claim 12, where the initial decision tree is an oblique tree.
15. The computer-implemented method of claim 12, where each iteration is performed in reverse breadth-first search (BFS) order.
16. The computer-implemented method of claim 12, where the initial decision tree has random parameter values in the nodes.
17. The computer implemented method of claim 12, where the parameters of the node's decision function are updated only if the objective function decreases.
18. A computer-implemented method for learning a sparse decision tree to optimize classification accuracy and sparsity, the method comprising: inputting an initial binary decision tree having oblique nodes; inputting an initial data training set; for each node i of the tree: if the node is a leaf, assigning a label to the leaf based at least in part on a majority label of training points that reach the leaf; and if the node is a decision node, updating parameters of the node's decision function based on solution of a reduced problem: where ƒi (⋅; θi) is the decision function of the node, ynϵ{left, right} is a child that leads to the correct classification for xn under i's current subtree, L is the 0/1 loss, and where wi is a weight vector and λ is a user set hyperparameter with a value ≥0, set at an initial value; iterating over all nodes of the tree until the parameters change less than a set tolerance or a number of iterations reaches a set limit; where, for each iteration, all nodes at the same depth level are processed in parallel; pruning a resulting tree to remove dead branches and pure subtrees; repeating the above steps of the computer-implemented method, where the initial binary decision tree input is a previous tree and each repeat has a user-chosen value of λ larger than a previous λ value to produce new resulting trees; choosing a best tree from the new resulting trees based on the accuracy and sparsity of each of the new resulting trees; and using the best tree on a client system to make predictions from target data.
19. The computer-implemented method of claim 18, where each iteration is performed in reverse breadth-first search (BFS) order.
20. The computer-implemented method of claim 18, where the initial decision tree has random parameter values in the nodes.
</claims>
</document>
