<document>

<filing_date>
2020-06-02
</filing_date>

<publication_date>
2020-12-03
</publication_date>

<priority_date>
2019-06-03
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
UNIST(ULSAN NATIONAL INSTITUTE OF SCIENCE AND TECHNOLOGY)
</assignee>

<inventors>
YU, HYEONGSEOK
LEE, Jongeun
SIM, Hyeonuk
</inventors>

<docdb_family_id>
73550859
</docdb_family_id>

<title>
METHOD AND APPARATUS WITH NEURAL NETWORK PARAMETER QUANTIZATION
</title>

<abstract>
A processor-implemented method includes determining a first quantization value by performing log quantization on a parameter from one of input activation values and weight values in a layer of a neural network, comparing a threshold value with an error between a first dequantization value obtained by dequantization of the first quantization value and the parameter, determining a second quantization value by performing log quantization on the error in response to the error being greater than the threshold value as a result of the comparing; and quantizing the parameter to a value in which the first quantization value and the second quantization value are grouped.
</abstract>

<claims>
1. A processor-implemented method, the method comprising: determining a first quantization value by performing log quantization on a parameter from one of input activation values and weight values in a layer of a neural network; comparing a threshold value with an error between a first dequantization value obtained by dequantization of the first quantization value and the parameter; determining a second quantization value by performing log quantization on the error in response to the error being greater than the threshold value as a result of the comparing; and quantizing the parameter to a value in which the first quantization value and the second quantization value are grouped.
2. The method of claim 1, wherein the determining of the first quantization value comprises: determining the first quantization value by performing log quantization on a value corresponding to a quantization level closest to the parameter, from among a plurality of quantization levels, and the determining of the second quantization value comprises: determining the second quantization value by performing log quantization on a value corresponding to a quantization level closest to the error, from among the plurality of quantization levels.
3. The method of claim 1, wherein the second quantization value is represented by a same number of bits as a number of bits representing the first quantization value.
4. The method of claim 1, wherein the threshold value is determined based on a predetermined trade-off relationship between a recognition rate of the neural network and a size of data according to the quantization of the parameter.
5. The method of claim 1, wherein the quantizing comprises: adding a tag bit to each of the first quantization value and the second quantization value.
6. The method of claim 5, wherein the adding comprises: adding a first tag bit, indicating that there is the second quantization value subsequent to the first quantization value, before a first bit of bits representing the first quantization value or after a last bit of the bits, and adding a second tag bit, indicating that there is no quantization value subsequent to the second quantization value, before a first bit of bits representing the second quantization value or after a last bit of the bits.
7. The method of claim 1, wherein the quantizing comprises: adding a code value, indicating that the first quantization value and the second quantization value are consecutive values, before a first bit of bits representing the first quantization value or after a last bit of bits representing the second quantization value.
8. The method of claim 1, further comprising: dequantizing the value in which the first quantization value and the second quantization value are grouped; and performing a convolution operation between a dequantization value obtained by dequantizing the value and the input activation values.
9. The method of claim 8, wherein the dequantizing of the value comprises: calculating each of a first dequantization value, which is a value obtained by dequantization of the first quantization value, and a second dequantization value, which is a value obtained by dequantization of the second quantization value; and obtaining the dequantization value by adding the first dequantization value and the second dequantization value.
10. A non-transitory computer-readable storage medium storing instructions that, when executed by a processor, cause the processor to perform the method of claim 1.
11. An apparatus, the apparatus comprising: one or more processors configured to: determine a first quantization value by performing log quantization on a parameter from one of input activation values and weight values in a layer of a neural network; compare a threshold value with an error between a first dequantization value obtained by dequantization of the first quantization value and the parameter; determine a second quantization value by performing log quantization on the error in response to the error being greater than the threshold value as a result of the comparing; and quantize the parameter to a value in which the first quantization value and the second quantization value are grouped.
12. The apparatus of claim 11, wherein the one or more processors are further configured to: determine the first quantization value by performing log quantization on a value corresponding to a quantization level closest to the parameter, from among a plurality of quantization levels; and determine the second quantization value by performing log quantization on a value corresponding to a quantization level closest to the error, from among the plurality of quantization levels.
13. The apparatus of claim 11, wherein the second quantization value is represented by a same number of bits as a number of bits representing the first quantization value.
14. The apparatus of claim 11, wherein the threshold value is determined based on a predetermined trade-off relationship between a recognition rate of the neural network and a size of data according to the quantization of the parameter.
15. The apparatus of claim 11, wherein the one or more processors are further configured to add a tag bit to each of the first quantization value and the second quantization value.
16. The apparatus of claim 15, wherein the one or more processors are further configured to: add a first tag bit, indicating that there is the second quantization value subsequent to the first quantization value, before a first bit of bits representing the first quantization value or after a last bit of the bits; and add a second tag bit, indicating that there is no quantization value subsequent to the second quantization value, before a first bit of bits representing the second quantization value or after a last bit of the bits.
17. The apparatus of claim 11, wherein the one or more processors are further configured to add a code value, indicating that the first quantization value and the second quantization value are consecutive values, before a first bit of bits representing the first quantization value or after a last bit of bits representing the second quantization value.
18. The apparatus of claim 11, wherein the one or more processors are further configured to: dequantize the value in which the first quantization value and the second quantization value are grouped; and perform a convolution operation between a dequantization value obtained by dequantizing the value and the input activation values.
19. The apparatus of claim 18, wherein the one or more processors are further configured to: calculate each of a first dequantization value, which is a value obtained by dequantization of the first quantization value, and a second dequantization value, which is a value obtained by dequantization of the second quantization value; and obtain the dequantization value by adding the first dequantization value and the second dequantization value.
</claims>
</document>
