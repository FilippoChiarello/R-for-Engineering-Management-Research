<document>

<filing_date>
2019-06-18
</filing_date>

<publication_date>
2020-11-17
</publication_date>

<priority_date>
2016-09-07
</priority_date>

<ipc_classes>
G06F17/27,G06F40/205,G06F40/30,G10L15/16,G10L15/18
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
CELIKYILMAZ, ASLI
CHEN, YUN-NUNG VIVIAN
DENG LI
GAO JIANFENG
HAKKANI-TUR, DILEK, Z.
TUR, GOKHAN
</inventors>

<docdb_family_id>
61280591
</docdb_family_id>

<title>
Knowledge-guided structural attention processing
</title>

<abstract>
Systems and methods for determining knowledge-guided information for a recurrent neural networks (RNN) to guide the RNN in semantic tagging of an input phrase are presented. A knowledge encoding module of a Knowledge-Guided Structural Attention Process (K-SAP) receives an input phrase and, in conjunction with additional sub-components or cooperative components generates a knowledge-guided vector that is provided with the input phrase to the RNN for linguistic semantic tagging. Generating the knowledge-guided vector comprises at least parsing the input phrase and generating a corresponding hierarchical linguistic structure comprising one or more discrete sub-structures. The sub-structures may be encoded into vectors along with attention weighting identifying those sub-structures that have greater importance in determining the semantic meaning of the input phrase.
</abstract>

<claims>
1. A computer implemented method for providing structural linguistic knowledge to a semantic tagging process, the method comprising: parsing an input phrase into one or more discrete sub-structures; generating a weighted sum vector comprising an encoded vector corresponding to a discrete sub-structure, wherein the encoded vector is weighted by an attention weight that identifies a linguistic importance of the discrete sub-structure, and the weighted sum vector is a combination of a plurality of encoded vectors that correspond to the one or more discrete sub-structures according to a respective attention weight of each of the one or more discrete sub-structures; combining the weighted sum vector with an input phrase vector to generate a knowledge-guided vector corresponding to the input phrase, wherein the input phrase vector is generated by encoding the input phrase; and providing the knowledge-guided vector and the input phrase to a recurrent neural network (RNN) for semantic tagging.
2. The method of claim 1, comprising, for each of the one or more discrete sub-structures: generating an encoded vector corresponding to each of the one or more discrete sub-structures according to a subject matter of each of the one or more discrete sub-structures; and incorporating the attention weight in the corresponding encoded vector.
3. The method of claim 1, wherein generating an encoded vector of the plurality of encoded vectors comprises generating an encoded vector corresponding to each of the one or more discrete sub-structures according to a subject matter of each of the one or more discrete sub-structure and according to a linguistic knowledge base.
4. The method of claim 3, wherein the linguistic knowledge base comprises information indicating dependency relationships in phrases.
5. The method of claim 4, wherein the linguistic knowledge base is an external linguistic knowledge base.
6. The method of claim 1, wherein the attention weight generated for each of the one or more discrete sub-structures is determined according to a similarity between a subject matter of each of the one or more discrete sub-structures and a subject matter of the input phrase.
7. The method of claim 1, wherein each of the one or more discrete sub-structures corresponds to a path from a root node and includes those nodes from the root node down a hierarchical structure to a leaf node.
8. A computer system for providing structural linguistic knowledge to the semantic tagging process, the system comprising a processor and a memory, wherein the processor executes instructions in the memory as part of or in conjunction with additional components in providing structural linguistic knowledge to a semantic tagging process, the additional components comprising: a knowledge encoding module configured to: parse an input phrase into one or more discrete sub-structures; generate a weighted sum vector comprising an encoded vector corresponding to a discrete sub-structure of the one or more discrete sub-structures, wherein the encoded vector is weighted by an attention weight that identifies a linguistic importance of the discrete sub-structure, and the weighted sum vector is a combination of a plurality of encoded vectors that correspond to the one or more discrete sub-structures according to a respective attention weight of each of the one or more discrete sub-structures; combine the weighted sum vector with an input phrase vector to generate a knowledge-guided vector corresponding to the input phrase, wherein the input phrase vector is generated by encoding the input phrase; and provide the knowledge-guided vector and the input phrase to a recurrent neural network (RNN) for semantic tagging.
9. The computer system of claim 8, wherein the knowledge encoding module determines a knowledge-guided vector corresponding to the input phrase in conjunction with a parsing module, and wherein the parsing module is configured to generate a linguistic hierarchical structure of the input phrase, the linguistic hierarchical structure comprising the one or more discrete sub-structures of the input phrase.
10. The computer system of claim 8, wherein the knowledge encoding module determines the knowledge-guided vector corresponding to the input phrase in conjunction with a vector encoding module; and wherein the vector encoding module is configured to generate an encoded vector of the plurality of encoded vectors corresponding to each of the one or more discrete sub-structures according to a subject matter of the corresponding one or more discrete sub-structures.
11. The computer system of claim 8, wherein the knowledge encoding module determines the knowledge-guided vector corresponding to the input phrase in conjunction with an attention weighting module; and wherein the attention weighting module is configured to determine an attention weight for each of the one or more discrete sub-structures according to the linguistic importance of the corresponding sub-structure, and associate the corresponding attention weight to the encoded vector of each of the one or more discrete sub-structures.
12. A computer implemented method for generating a linguistic hierarchical structure of an input phrase, the method comprising: receiving an input phrase to be semantically tagged; parsing the input phrase into one or more discrete sub-structures, wherein each discrete sub-structure represents a path from a root node of the linguistic hierarchical structure to a leaf node of the linguistic hierarchical structure; generating the linguistic hierarchical structure, wherein the linguistic hierarchical structure comprises the one or more discrete sub-structures encoded into a plurality of vectors, wherein each vector of the plurality of vectors is weighted according an importance of a corresponding discrete sub-structure and combined to form a weighted sum vector; and predicting sematic tags for the input phrase via a knowledge-guided vector input to a recurrent neural network, wherein the knowledge-guided vector is generated by combining an input vector and the weighted sum vector and the input vector is generated by encoding the input phrase.
13. The method of claim 12, wherein the root node indicates a general purpose of the input phrase.
14. The method of claim 12, wherein the leaf node indicates an individual term of the input phrase.
15. The method of claim 12, wherein a discrete substructure comprises a root node, a leaf node, and an intermediate node.
16. The method of claim 12, wherein the weighted sum vector is generated according to a content of the one or more discrete sub-structures.
17. The method of claim 12, wherein the input phrase is parsed according to at least one of a dependency relation or a knowledge graph specific relation.
18. The method of claim 12, wherein the one or more discrete sub-structures are encoded into the plurality of vectors via a fully-connected neural network (NN) with linear activation, a recurrent neural network (RNN), a convolutional neural network (CNN), or any combination thereof.
19. The method of claim 12, wherein the weighted sum vector represents the knowledge and attention weights to be applied to the input phrase.
</claims>
</document>
