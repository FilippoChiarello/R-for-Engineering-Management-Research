<document>

<filing_date>
2020-02-19
</filing_date>

<publication_date>
2020-06-18
</publication_date>

<priority_date>
2018-05-09
</priority_date>

<ipc_classes>
G01C21/20,G06N3/04,G06N3/08,G06T7/73
</ipc_classes>

<assignee>
DEEPMIND TECHNOLOGIES
</assignee>

<inventors>
HADSELL, RAIA THAIS
KUMARAN, SUDARSHAN
URIA-MART√çNEZ, BENIGNO
BANINO, ANDREA
</inventors>

<docdb_family_id>
66484052
</docdb_family_id>

<title>
PERFORMING NAVIGATION TASKS USING GRID CODES
</title>

<abstract>
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for selecting actions to be performed by an agent interacting with an environment. In one aspect, a system comprises a grid cell neural network and an action selection neural network. The grid cell network is configured to: receive an input comprising data characterizing a velocity of the agent; process the input to generate a grid cell representation; and process the grid cell representation to generate an estimate of a position of the agent in the environment; the action selection neural network is configured to: receive an input comprising a grid cell representation and an observation characterizing a state of the environment; and process the input to generate an action selection network output.
</abstract>

<claims>
1. A system for selecting actions to be performed by an agent interacting with an environment, the system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to implement: a grid cell neural network that is configured to: receive an input comprising data characterizing a velocity of the agent; process the input to generate a grid cell representation; and process the grid cell representation to generate an estimate of a position of the agent in the environment; an action selection neural network that is configured to: receive an input comprising a grid cell representation and an observation characterizing a state of the environment; and process the input to generate an action selection network output; a subsystem that is configured to: receive data characterizing a current velocity of the agent; provide the data characterizing the current velocity of the agent as input to the grid cell neural network to obtain a current grid cell representation; receive a current observation characterizing a current state of the environment; provide the current grid cell representation and the current observation as input to the action selection neural network to obtain an action selection network output; and select, using the action selection network output, an action to be performed by the agent in response to the current observation.
2. The system of claim 1, wherein: the action selection network output comprises a score distribution over actions in a set of possible actions, and selecting an action to be performed by the agent comprises sampling an action in the set of possible actions in accordance with the score distribution over the actions in the set of possible actions.
3. The system of claim 1, wherein the grid cell neural network is a recurrent neural network.
4. The system of claim 1, wherein the action selection neural network is a recurrent neural network.
5. The system of claim 1, wherein the data characterizing the velocity of the agent comprises data characterizing a translational velocity of the agent and data characterizing an angular velocity of the agent.
6. The system of claim 1, wherein the estimate of the position of the agent in the environment comprises a location of the agent and a head direction of the agent.
7. The system of claim 1, wherein: the instructions further cause the one or more computers to implement a vision neural network that is configured to: receive an input comprising an observation characterizing a state of the environment; and process the input to generate an estimate of the position of the agent in the environment; and wherein the grid cell neural network is configured to receive an input further comprising an estimate of the position of the agent in the environment.
8. The system of claim 7, wherein the subsystem is further configured to provide an estimate of the position of the agent in the environment generated by the vision neural network as input to the grid cell neural network.
9. The system of claim 7, wherein the subsystem is further configured to: with a first probability, provide an estimate of the position of the agent in the environment generated by the vision neural network as input to the grid cell neural network; with a second probability, process an estimate of the position of the agent in the environment generated by the vision neural network by a masking layer which zeros the estimate of the position of the agent in the environment to generate a masked estimate of the position of the agent, and provide the masked estimate of the position of the agent as input to the grid cell neural network.
10. The system of claim 7, wherein the vision neural network is a convolutional neural network.
11. The system of claim 1, wherein processing the input to generate a grid cell representation comprises: processing the input by a recurrent neural network layer to generate a recurrent layer output; and processing the recurrent layer output by a linear neural network layer to generate the grid cell representation.
12. The system of claim 11, wherein dropout is applied to the linear neural network layer.
13. The system of claim 11, wherein processing the grid cell representation to generate an estimate of a position of the agent in the environment comprises: generating a linear transformation of the grid cell representation; and processing the linear transformation of the grid cell representation by a softmax layer to generate the estimate of the position of the agent in the environment.
14. The system of claim 1, wherein the action selection network is trained by a reinforcement learning technique to perform a navigation task in the environment.
15. The system of claim 14, wherein the action selection network output further comprises a predicted expected return that is an estimate of a time-discounted return resulting from the environment being in the current state, and wherein the reinforcement learning technique is an actor-critic reinforcement learning technique.
16. The system of claim 14, wherein: the input received by the action selection neural network further comprises a goal grid cell representation, wherein the goal grid cell representation is a grid cell representation generated by the grid cell neural network at a time step when the position of the agent in the environment was a goal position; and the subsystem is further configured to maintain data specifying a goal grid cell representation and to provide the goal grid cell representation as input to the action selection neural network.
17. The system of claim 1, wherein the grid cell neural network is trained by a supervised learning technique.
18. The system of claim 7, wherein the vision neural network is trained by a supervised learning technique.
19. One or more non-transitory computer storage media storing instructions that when executed by one or more computers cause the one or more computers to perform operations for selecting actions to be performed by an agent interacting with an environment, the operations comprising: receiving data characterizing a current velocity of the agent; providing the data characterizing the current velocity of the agent as an input to a grid cell neural network to obtain a current grid cell representation, wherein the grid cell neural network is trained to: process an input characterizing a velocity of the agent to generate a grid cell representation; and process the grid cell representation to generate an estimate of a position of the agent in the environment; receiving a current observation characterizing a current state of the environment; processing the current grid cell representation and the current observation using an action selection neural network to generate an action selection network output; and selecting, using the action selection network output, an action to be performed by the agent in response to the current observation.
20. A method, performed by one or more data processing apparatus, for selecting actions to be performed by an agent interacting with an environment, the method comprising: receiving data characterizing a current velocity of the agent; providing the data characterizing the current velocity of the agent as an input to a grid cell neural network to obtain a current grid cell representation, wherein the grid cell neural network is trained to: process an input characterizing a velocity of the agent to generate a grid cell representation; and process the grid cell representation to generate an estimate of a position of the agent in the environment; receiving a current observation characterizing a current state of the environment; processing the current grid cell representation and the current observation using an action selection neural network to generate an action selection network output; and selecting, using the action selection network output, an action to be performed by the agent in response to the current observation.
</claims>
</document>
