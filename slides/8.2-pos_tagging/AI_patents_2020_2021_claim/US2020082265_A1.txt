<document>

<filing_date>
2019-11-11
</filing_date>

<publication_date>
2020-03-12
</publication_date>

<priority_date>
2017-11-02
</priority_date>

<ipc_classes>
G06F16/901,G06K9/46,G06K9/62,G06K9/66,G06N20/00,G06N3/08,G06N5/02
</ipc_classes>

<assignee>
PALO ALTO RESEARCH CENTER
</assignee>

<inventors>
ZHOU, RONG
ROSSI, RYAN
</inventors>

<docdb_family_id>
66244047
</docdb_family_id>

<title>
DEEP GRAPH REPRESENTATION LEARNING
</title>

<abstract>
A method of deep graph representation learning includes: deriving a set of base features; and automatically developing, by a processing device, a multi-layered hierarchical graph representation based on the set of base features, wherein each successive layer of the multi-layered hierarchical graph representation leverages an output from a previous layer to learn features of a higher-order.
</abstract>

<claims>
1. A method of deep graph representation learning, the method comprising: deriving a set of base features; and automatically developing, by a processing device, a multi-layered hierarchical graph representation based on the set of base features, wherein each successive layer of the multi-layered hierarchical graph representation leverages an output from a previous layer to learn features of a higher-order.
2. The method of claim 1, wherein automatically developing the multi-layered hierarchical graph representation comprises: adding the set of base features to a feature matrix, (i) generating, by a processing device, a current feature layer from the feature matrix and a set of relational feature operators, wherein the current feature layer corresponds to a set of current features; (ii) evaluating feature pairs associated with the current feature layer; (iii) selecting a subset of features from the set of current features based on the evaluated feature pairs; and (iv) adding the subset of features to the feature matrix to generate an updated feature matrix.
3. The method of claim 2, further comprising: incrementing the current feature layer to generate a new current feature layer; and repeating steps (i) through (iv) for the new current feature layer until no new features emerge or a max number of layers is reached.
4. The method of claim 2, further comprising: transforming the set of base features after generating a current feature layer to generate a plurality of transformed based features; and adding the plurality of transformed base features to the updated feature matrix to generate a new updated feature matrix.
5. The method of claim 2, further comprising: transforming the set of current features after generating a current feature layer.
6. The method of claim 2, wherein a plurality of features in the feature matrix are transfer learning features.
7. The method of claim 2, wherein selecting the subset of features further comprises: applying a set of relational feature operators to each feature of a previous feature layer.
8. A system comprising: a memory to store a set of base features; and a processing device, operatively coupled to the memory, to: derive the set of base features; and automatically develop a multi-layered hierarchical graph representation based on the set of base features, wherein each successive layer of the multi-layered hierarchical graph representation leverages an output from a previous layer to learn features of a higher-order.
9. The system of claim 8, wherein to automatically develop the multi-layered hierarchical graph representation the processing device is further to: add the set of base features to a feature matrix; (i) generate a current feature layer from the feature matrix and a set of relational feature operators, wherein the current feature layer corresponds to a set of current features; (ii) evaluate feature pairs associated with the current feature layer; (iii) select a subset of features from the set of current features based on the evaluated feature pairs; and (iv) add the subset of features to the feature matrix to generate an updated feature matrix.
10. The system of claim 9, wherein the processing device is further to: increment the current feature layer to generate a new current feature layer; and repeat steps (i) through (iv) for the new current feature layer until no new features emerge or a max number of layers is reached.
11. The system of claim 9, wherein the processing device is further to: transform the set of base features after generating a current feature layer to generate a plurality of transformed based features; and add the plurality of transformed base features to the updated feature matrix to generate a new updated feature matrix.
12. The system of claim 9, wherein the processing device is further to: transform the set of current features after generating a current feature layer.
13. The system of claim 9, wherein the processing device is one or more graphics processing units of one or more servers.
14. The system of claim 9, wherein to select the subset of features the processing device is to: apply a set of relational feature operators to each feature of a previous feature layer.
15. A non-transitory computer-readable storage medium having instructions stored thereon that, when executed by a processing device, cause the processing device to: derive a set of base features; and automatically develop, by the processing device, a multi-layered hierarchical graph representation based on the set of base features, wherein each successive layer of the multi-layered hierarchical graph representation leverages an output from a previous layer to learn features of a higher-order.
16. A non-transitory computer-readable storage medium of claim 15, the processing device further to: add the set of base features to a feature matrix; (i) generate, by the processing device, a current feature layer from the feature matrix and a set of relational feature operators, wherein the current feature layer corresponds to a set of current features, (ii) evaluate feature pairs associated with the current feature layer; (iii) select a subset of features from the set of current features based on the evaluated feature pairs; and (iv) add the subset of features to the feature matrix to generate an updated feature matrix.
17. The non-transitory computer-readable storage medium of claim 16, the processing device further to: increment the current feature layer to generate a new current feature layer; and repeat steps (i) through (iv) for the new current feature layer until no new features emerge or a max number of layers is reached.
18. The non-transitory computer-readable storage medium of claim 16, the processing device further to: transform the set of base features after generating a current feature layer to generate a plurality of transformed based features; and add the plurality of transformed base features to the updated feature matrix to generate a new updated feature matrix.
19. The non-transitory computer-readable storage medium of claim 16, the processing device further to: transform the set of current features after generating a current feature layer.
20. The non-transitory computer-readable storage medium of claim 16, wherein a plurality of features in the feature matrix are transfer learning features.
</claims>
</document>
