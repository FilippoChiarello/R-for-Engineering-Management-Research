<document>

<filing_date>
2018-04-17
</filing_date>

<publication_date>
2020-07-28
</publication_date>

<priority_date>
2017-04-17
</priority_date>

<ipc_classes>
G06F13/00,G06F17/16,G06F9/30,G06F9/32,G06F9/38,G06F9/455,G06F9/48,G06F9/52,G06N3/04,G06N3/063,G06N3/08
</ipc_classes>

<assignee>
CEREBRAS SYSTEMS
</assignee>

<inventors>
MORRISON, MICHAEL
LIE, SEAN
JAMES, MICHAEL EDWIN
LAUTERBACH, GARY R.
AREKAPUDI, SRIKANTH
</inventors>

<docdb_family_id>
63855635
</docdb_family_id>

<title>
Data structure descriptors for deep learning acceleration
</title>

<abstract>
Techniques in advanced deep learning provide improvements in one or more of accuracy, performance, and energy efficiency. An array of processing elements performs flow-based computations on wavelets of data. Each processing element has a respective compute element and a respective routing element. Instructions executed by the compute element include operand specifiers, some specifying a data structure register storing a data structure descriptor describing an operand as a fabric vector or a memory vector. The data structure descriptor further describes the memory vector as one of a one-dimensional vector, a four-dimensional vector, or a circular buffer vector. Optionally, the data structure descriptor specifies an extended data structure register storing an extended data structure descriptor. The extended data structure descriptor specifies parameters relating to a four-dimensional vector or a circular buffer vector.
</abstract>

<claims>
1. A compute element comprising: a memory; means for decoding an instruction, the instruction comprising an operand field; means for accessing an operand descriptor based at least in part on the operand field; means for decoding the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to; means for accessing an operand in accordance with the operand descriptor and the particular type; means for performing an iteration of the instruction via accessing, in accordance with an access pattern described by the operand descriptor, sufficient data elements of a vector for the iteration; wherein the types comprise a fabric type and a memory type; wherein the compute element is comprised in a processing element that comprises a fabric router, the processing element is one of a fabric of processing elements each comprising a respective compute element and a respective fabric router; wherein the processing elements are interconnected via a fabric coupled to the respective fabric routers; wherein the fabric of processing elements is enabled to perform dataflow-based and instruction-based processing; wherein the fabric of processing elements is implemented via wafer-scale integration; wherein when the particular type is the fabric type, the operand is accessed via the fabric; wherein when the particular type is the memory type, the operand is accessed via the memory; and wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
2. A method comprising: in a compute element, decoding an instruction, the instruction comprising an operand field; in the compute element, accessing an operand descriptor based at least in part on the operand field; in the compute element, decoding the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to; in the compute element, accessing an operand in accordance with the operand descriptor and the particular type; performing an iteration of the instruction via accessing, in accordance with an access pattern described by the operand descriptor, sufficient data elements of a vector for the iteration; wherein the types comprise a fabric type and a memory type; wherein the compute element is comprised in a processing element that comprises a fabric router, the processing element is one of a fabric of processing elements each comprising a respective compute element and a respective fabric router; wherein the processing elements are interconnected via a fabric coupled to the respective fabric routers; wherein the fabric of processing elements is enabled to perform dataflow-based and instruction-based processing; wherein the fabric of processing elements is implemented via wafer-scale integration; wherein when the particular type is the fabric type, the operand is accessed via the fabric; wherein when the particular type is the memory type, the operand is accessed via a memory of the compute element; and wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
3. A method comprising: in a compute element, decoding an instruction, the instruction comprising an operand field; in the compute element, accessing an operand descriptor based at least in part on the operand field; in the compute element, decoding the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to; in the compute element, accessing an operand in accordance with the operand descriptor and the particular type; performing an iteration of the instruction via accessing, in accordance with an access pattern described by the operand descriptor, sufficient data elements of a vector for the iteration; wherein the types comprise a fabric type and a memory type; wherein the compute element is comprised in a processing element that comprises a fabric router, the processing element is one of a fabric of processing elements each comprising a respective compute element and a respective fabric router; wherein the processing elements are interconnected via a fabric coupled to the respective fabric routers; wherein the fabric of processing elements is enabled to perform dataflow-based and instruction-based processing; wherein the fabric of processing elements is implemented via wafer-scale integration; wherein when the particular type is the fabric type, the operand is accessed via the fabric; wherein when the particular type is the memory type, the operand is accessed via a memory of the compute element; and wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
4. A system comprising: a fabric of processing elements, each processing element comprising a fabric router coupled to a compute element, the fabric of processing elements enabled to perform dataflow-based processing and instruction-based processing, the fabric of processing elements implemented via wafer-scale integration; wherein each processing element is enabled to selectively communicate fabric packets with others of the processing elements at least in part via the fabric router of the respective processing element; wherein each compute element comprises a memory and is enabled to decode an instruction, the instruction comprising an operand field, access an operand descriptor based at least in part on the operand field, decode the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to, the plurality of types comprising a fabric type and a memory type, access an operand in accordance with the operand descriptor and the particular type, wherein the access of the operand is via the respective fabric router coupled to the compute element when the particular type is the fabric type, and wherein the access of the operand is via the memory when the particular type is the memory type; wherein the operand descriptor identifies an access pattern as one of a one-dimensional memory vector access pattern, a four-dimensional memory vector access pattern, and a circular memory buffer access pattern; wherein the operand descriptor is enabled to specify one of a plurality of extended operand descriptors; and wherein the extended operand descriptors are enabled to specify one or more of stride information and dimension information of a four-dimensional memory vector.
5. A system comprising: a fabric of processing elements, each processing element comprising a fabric router coupled to a compute element, the fabric of processing elements enabled to perform dataflow-based processing and instruction-based processing, the fabric of processing elements implemented via wafer-scale integration; wherein each processing element is enabled to selectively communicate fabric packets with others of the processing elements at least in part via the fabric router of the respective processing element; wherein each compute element comprises a memory and is enabled to decode an instruction, the instruction comprising an operand field, access an operand descriptor based at least in part on the operand field, decode the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to, the plurality of types comprising a fabric type and a memory type, access an operand in accordance with the operand descriptor and the particular type, wherein the access of the operand is via the respective fabric router coupled to the compute element when the particular type is the fabric type, and wherein the access of the operand is via the memory when the particular type is the memory type; wherein the operand descriptor identifies an access pattern as one of a one-dimensional memory vector access pattern, a four-dimensional memory vector access pattern, and a circular memory buffer access pattern; wherein the operand descriptor is enabled to specify one of a plurality of extended operand descriptors; and wherein the extended operand descriptors are enabled to specify one or more of a start address and an end address of a circular memory buffer.
6. A system comprising: a fabric of processing elements, each processing element comprising a fabric router coupled to a compute element, the fabric of processing elements enabled to perform dataflow-based processing and instruction-based processing, the fabric of processing elements implemented via wafer-scale integration; wherein each processing element is enabled to selectively communicate fabric packets with others of the processing elements at least in part via the fabric router of the respective processing element; wherein each compute element comprises a memory and is enabled to decode an instruction, the instruction comprising an operand field, access an operand descriptor based at least in part on the operand field, decode the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to, the plurality of types comprising a fabric type and a memory type, access an operand in accordance with the operand descriptor and the particular type, wherein the access of the operand is via the respective fabric router coupled to the compute element when the particular type is the fabric type, and wherein the access of the operand is via the memory when the particular type is the memory type; wherein the operand descriptor identifies an access pattern as one of a one-dimensional memory vector access pattern, a four-dimensional memory vector access pattern, and a circular memory buffer access pattern; wherein the operand descriptor is enabled to specify one of a plurality of extended operand descriptors; and wherein the extended operand descriptors are enabled to specify FIFO or non-FIFO operation of a circular memory buffer.
7. A system comprising: a fabric of processing elements, each processing element comprising a fabric router coupled to a compute element, the fabric of processing elements enabled to perform dataflow-based processing and instruction-based processing, the fabric of processing elements implemented via wafer-scale integration; wherein each processing element is enabled to selectively communicate fabric packets with others of the processing elements at least in part via the fabric router of the respective processing element; wherein each compute element comprises a memory and is enabled to decode an instruction, the instruction comprising an operand field, access an operand descriptor based at least in part on the operand field, decode the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to, the plurality of types comprising a fabric type and a memory type, access an operand in accordance with the operand descriptor and the particular type, perform an iteration of the instruction via accessing, in accordance with an access pattern described by the operand descriptor, sufficient data elements of a vector for the iteration, wherein the access of the operand is via the fabric when the particular type is the fabric type, and wherein the access of the operand is via the memory when the particular type is the memory type; and wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
8. A system comprising: a fabric of processing elements, each processing element comprising a fabric router coupled to a compute element, the fabric of processing elements enabled to perform dataflow-based processing and instruction-based processing, the fabric of processing elements implemented via wafer-scale integration; wherein each processing element is enabled to selectively communicate fabric packets with others of the processing elements at least in part via the fabric router of the respective processing element; wherein each compute element comprises a memory and is enabled to decode an instruction, the instruction comprising an operand field, access an operand descriptor based at least in part on the operand field, decode the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to, the plurality of types comprising a fabric type and a memory type, access an operand in accordance with the operand descriptor and the particular type, perform an iteration of the instruction via accessing, in accordance with an access pattern described by the operand descriptor, sufficient data elements of a vector for the iteration, wherein the access of the operand is via the fabric when the particular type is the fabric type, and wherein the access of the operand is via the memory when the particular type is the memory type; and wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
9. A compute element comprising: a memory; means for decoding an instruction, the instruction comprising an operand field; means for accessing an operand descriptor based at least in part on the operand field; means for decoding the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to; means for accessing an operand in accordance with the operand descriptor and the particular type; means for performing an iteration of the instruction via accessing, in accordance with an access pattern described by the operand descriptor, sufficient data elements of a vector for the iteration; wherein the types comprise a fabric type and a memory type; wherein the compute element is comprised in a processing element that comprises a fabric router, the processing element is one of a fabric of processing elements each comprising a respective compute element and a respective fabric router; wherein the processing elements are interconnected via a fabric coupled to the respective fabric routers; wherein the fabric of processing elements is enabled to perform dataflow-based and instruction-based processing; wherein the fabric of processing elements is implemented via wafer-scale integration; wherein when the particular type is the fabric type, the operand is accessed via the fabric; wherein when the particular type is the memory type, the operand is accessed via the memory; and wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
10. A compute element comprising: a memory; means for decoding an instruction, the instruction comprising an operand field; means for accessing an operand descriptor based at least in part on the operand field; means for decoding the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to; means for accessing an operand in accordance with the operand descriptor and the particular type, and wherein the types comprise a fabric type and a memory type; wherein the compute element is comprised in a processing element that comprises a fabric router coupled to the compute element, the processing element is one of a fabric of processing elements each comprising a respective compute element and a respective fabric router coupled to the respective compute element; wherein the processing elements are interconnected via a fabric coupled to the respective fabric routers to operate as a fabric of processing elements, and each processing element is enabled to selectively communicate fabric packets with others of the processing elements at least in part via the respective fabric router of the respective processing element; wherein the fabric of processing elements is enabled to perform dataflow-based and instruction-based processing; wherein the fabric of processing elements is implemented via wafer-scale integration; wherein when the particular type is the fabric type, the operand is accessed via the fabric router coupled to the processing element; wherein when the particular type is the memory type, the operand is accessed via the memory; wherein the operand descriptor identifies an access pattern as one of a one-dimensional memory vector access pattern, a four-dimensional memory vector access pattern, and a circular memory buffer access pattern; wherein the operand descriptor is enabled to specify one of a plurality of extended operand descriptors; and wherein the extended operand descriptors are enabled to specify one or more of stride information and dimension information of a four-dimensional memory vector.
11. A compute element comprising: a memory; means for decoding an instruction, the instruction comprising an operand field; means for accessing an operand descriptor based at least in part on the operand field; means for decoding the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to; means for accessing an operand in accordance with the operand descriptor and the particular type, and wherein the types comprise a fabric type and a memory type; wherein the compute element is comprised in a processing element that comprises a fabric router coupled to the compute element, the processing element is one of a fabric of processing elements each comprising a respective compute element and a respective fabric router coupled to the respective compute element; wherein the processing elements are interconnected via a fabric coupled to the respective fabric routers to operate as a fabric of processing elements, and each processing element is enabled to selectively communicate fabric packets with others of the processing elements at least in part via the respective fabric router of the respective processing element; wherein the fabric of processing elements is enabled to perform dataflow-based and instruction-based processing; wherein the fabric of processing elements is implemented via wafer-scale integration; wherein when the particular type is the fabric type, the operand is accessed via the fabric router coupled to the processing element; wherein when the particular type is the memory type, the operand is accessed via the memory; wherein the operand descriptor identifies an access pattern as one of a one-dimensional memory vector access pattern, a four-dimensional memory vector access pattern, and a circular memory buffer access pattern; wherein the operand descriptor is enabled to specify one of a plurality of extended operand descriptors; and wherein the extended operand descriptors are enabled to specify one or more of a start address and an end address of a circular memory buffer.
12. A compute element comprising: a memory; means for decoding an instruction, the instruction comprising an operand field; means for accessing an operand descriptor based at least in part on the operand field; means for decoding the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to; means for accessing an operand in accordance with the operand descriptor and the particular type, and wherein the types comprise a fabric type and a memory type; wherein the compute element is comprised in a processing element that comprises a fabric router coupled to the compute element, the processing element is one of a fabric of processing elements each comprising a respective compute element and a respective fabric router coupled to the respective compute element; wherein the processing elements are interconnected via a fabric coupled to the respective fabric routers to operate as a fabric of processing elements, and each processing element is enabled to selectively communicate fabric packets with others of the processing elements at least in part via the respective fabric router of the respective processing element; wherein the fabric of processing elements is enabled to perform dataflow-based and instruction-based processing; wherein the fabric of processing elements is implemented via wafer-scale integration; wherein when the particular type is the fabric type, the operand is accessed via the fabric router coupled to the processing element; wherein when the particular type is the memory type, the operand is accessed via the memory; wherein the operand descriptor identifies an access pattern as one of a one-dimensional memory vector access pattern, a four-dimensional memory vector access pattern, and a circular memory buffer access pattern; wherein the operand descriptor is enabled to specify one of a plurality of extended operand descriptors; and wherein the extended operand descriptors are enabled to specify FIFO or non-FIFO operation of a circular memory buffer.
13. A method comprising: in a compute element, decoding an instruction, the instruction comprising an operand field; in the compute element, accessing an operand descriptor based at least in part on the operand field; in the compute element, decoding the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to; in the compute element, accessing an operand in accordance with the operand descriptor and the particular type, and wherein the types comprise a fabric type and a memory type; wherein the compute element is comprised in a processing element that comprises a fabric router coupled to the compute element, the processing element is one of a fabric of processing elements each comprising a respective compute element and a respective fabric router coupled to the respective compute element; wherein the processing elements are interconnected via a fabric coupled to the respective fabric routers to operate as a fabric of processing elements, and each processing element is enabled to selectively communicate fabric packets with others of the processing elements at least in part via the respective fabric router of the respective processing element; wherein the fabric of processing elements is enabled to perform dataflow-based and instruction-based processing; wherein the fabric of processing elements is implemented via wafer-scale integration; wherein when the particular type is the fabric type, the operand is accessed via the fabric router coupled to the processing element; wherein when the particular type is the memory type, the operand is accessed via a memory of the compute element; wherein the operand descriptor identifies an access pattern as one of a one-dimensional memory vector access pattern, a four-dimensional memory vector access pattern, and a circular memory buffer access pattern; wherein the operand descriptor is enabled to specify one of a plurality of extended operand descriptors; and wherein the extended operand descriptors are enabled to specify one or more of stride information and dimension information of a four-dimensional memory vector.
14. A method comprising: in a compute element, decoding an instruction, the instruction comprising an operand field; in the compute element, accessing an operand descriptor based at least in part on the operand field; in the compute element, decoding the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to; in the compute element, accessing an operand in accordance with the operand descriptor and the particular type, and wherein the types comprise a fabric type and a memory type; wherein the compute element is comprised in a processing element that comprises a fabric router coupled to the compute element, the processing element is one of a fabric of processing elements each comprising a respective compute element and a respective fabric router coupled to the respective compute element; wherein the processing elements are interconnected via a fabric coupled to the respective fabric routers to operate as a fabric of processing elements, and each processing element is enabled to selectively communicate fabric packets with others of the processing elements at least in part via the respective fabric router of the respective processing element; wherein the fabric of processing elements is enabled to perform dataflow-based and instruction-based processing; wherein the fabric of processing elements is implemented via wafer-scale integration; wherein when the particular type is the fabric type, the operand is accessed via the fabric router coupled to the processing element; wherein when the particular type is the memory type, the operand is accessed via a memory of the compute element; wherein the operand descriptor identifies an access pattern as one of a one-dimensional memory vector access pattern, a four-dimensional memory vector access pattern, and a circular memory buffer access pattern; wherein the operand descriptor is enabled to specify one of a plurality of extended operand descriptors; and wherein the extended operand descriptors are enabled to specify one or more of a start address and an end address of a circular memory buffer.
15. A method comprising: in a compute element, decoding an instruction, the instruction comprising an operand field; in the compute element, accessing an operand descriptor based at least in part on the operand field; in the compute element, decoding the operand descriptor to determine a particular one of a plurality of types the operand descriptor refers to; in the compute element, accessing an operand in accordance with the operand descriptor and the particular type, and wherein the types comprise a fabric type and a memory type; wherein the compute element is comprised in a processing element that comprises a fabric router coupled to the compute element, the processing element is one of a fabric of processing elements each comprising a respective compute element and a respective fabric router coupled to the respective compute element; wherein the processing elements are interconnected via a fabric coupled to the respective fabric routers to operate as a fabric of processing elements, and each processing element is enabled to selectively communicate fabric packets with others of the processing elements at least in part via the respective fabric router of the respective processing element; wherein the fabric of processing elements is enabled to perform dataflow-based and instruction-based processing; wherein the fabric of processing elements is implemented via wafer-scale integration; wherein when the particular type is the fabric type, the operand is accessed via the fabric router coupled to the processing element; wherein when the particular type is the memory type, the operand is accessed via a memory of the compute element; wherein the operand descriptor identifies an access pattern as one of a one-dimensional memory vector access pattern, a four-dimensional memory vector access pattern, and a circular memory buffer access pattern; wherein the operand descriptor is enabled to specify one of a plurality of extended operand descriptors; and wherein the extended operand descriptors are enabled to specify FIFO or non-FIFO operation of a circular memory buffer.
16. The compute element of claim 1, wherein when the type is the fabric type, the operand descriptor is associated with a fabric virtual channel of the fabric.
17. The compute element of claim 1, wherein the access pattern is one of a fabric vector, a one-dimensional memory vector, a four-dimensional memory vector, and a circular memory buffer.
18. The compute element of claim 1, wherein the means for accessing an operand is enabled to read data elements from an input queue coupled to the fabric when the type is the fabric type and the operand is a source.
19. The compute element of claim 1, wherein the means for accessing an operand is enabled to write data elements to an output queue coupled to the fabric when the type is the fabric type and the operand is a destination.
20. The compute element of claim 1, wherein the means for accessing an operand is enabled to read from the memory when the type is the memory type and the operand is a source.
21. The compute element of claim 1, wherein the means for accessing an operand is enabled to write to the memory when the type is the memory type and the operand is a destination.
22. The compute element of claim 1, wherein the operand is a vector and the operand descriptor comprises information describing a length of the vector.
23. The compute element of claim 1, wherein the compute element is enabled to execute the instruction, and the operand descriptor comprises microthreading information describing how the compute element is to operate when the operand is a vector and there is a stall accessing an element of the vector.
24. The compute element of claim 23, further comprising, responsive to the stall and the microthreading information indicating microthreading not enabled, means for the compute element to stall.
25. The compute element of claim 23, wherein the instruction is a first instruction; and further comprising, responsive to the stall and the microthreading information indicating microthreading is enabled, means for the compute element to suspend processing of the first instruction and to select a second instruction for processing.
26. The compute element of claim 1, wherein the operand is a vector and the operand descriptor indicates how many elements of the vector to process in parallel.
27. The compute element of claim 1, wherein the operand descriptor comprises an indicator of whether to terminate processing when the operand is a vector and a control fabric packet conveying an element of the vector is received.
28. The compute element of claim 1, wherein the operand descriptor comprises an indicator of a virtual channel to selectively activate responsive to completion of the instruction.
29. The compute element of claim 9, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
30. The compute element of claim 1, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
31. The compute element of claim 1, wherein the operand comprises at least a portion of one or more of: a vector, a matrix, and a tensor.
32. The method of claim 2, wherein when the type is the fabric type, the operand descriptor is associated with a fabric virtual channel of the fabric.
33. The method of claim 2, wherein the access pattern is one of a fabric vector, a one-dimensional memory vector, a four-dimensional memory vector, and a circular memory buffer.
34. The method of claim 2, wherein when the type is the fabric type and the operand is a source, the accessing the operand comprises reading data elements from an input queue coupled to the fabric.
35. The method of claim 2, wherein when the type is the fabric type and the operand is a destination, the accessing the operand comprises writing data elements to an output queue coupled to the fabric.
36. The method of claim 2, wherein when the type is the memory type and the operand is a source, the accessing the operand comprises reading from the memory.
37. The method of claim 2, wherein when the type is the memory type and the operand is a destination, the accessing the operand comprises writing to the memory.
38. The method of claim 2, wherein the operand is a vector and the operand descriptor comprises information describing a length of the vector.
39. The method of claim 2, wherein the operand descriptor comprises microthreading information describing how the compute element is to operate when the operand is a vector and there is a stall accessing an element of the vector.
40. The method of claim 39, further comprising, responsive to the stall and the microthreading information indicating microthreading not enabled, the compute element stalling.
41. The method of claim 39, wherein the instruction is a first instruction; and further comprising, responsive to the stall and the microthreading information indicating microthreading is enabled, the compute element suspends processing of the first instruction and selects a second instruction for processing.
42. The method of claim 41, wherein the first instruction is associated with a first task and the second instruction is associated with a second task.
43. The method of claim 2, wherein the operand is a vector and the operand descriptor indicates how many elements of the vector to process in parallel.
44. The method of claim 2, wherein the operand descriptor comprises an indicator of whether to terminate processing when the operand is a vector and a control fabric packet conveying an element of the vector is received.
45. The method of claim 2, wherein the operand descriptor comprises an indicator of a virtual channel to selectively activate responsive to completion of the instruction.
46. The method of claim 2, wherein the operand comprises at least a portion of one or more of: a vector, a matrix, and a tensor.
47. The system of claim 4, wherein the operand descriptors are enabled to specify a vector length of vector operands.
48. The method of claim 2, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
49. The method of claim 3, wherein when the type is the fabric type, the operand descriptor is associated with a fabric virtual channel of the fabric.
50. The method of claim 3, wherein the access pattern is one of a fabric vector, a one-dimensional memory vector, a four-dimensional memory vector, and a circular memory buffer.
51. The method of claim 3, wherein when the type is the fabric type and the operand is a source, the accessing the operand comprises reading data elements from an input queue coupled to the fabric.
52. The method of claim 3, wherein when the type is the fabric type and the operand is a destination, the accessing the operand comprises writing data elements to an output queue coupled to the fabric.
53. The method of claim 3, wherein when the type is the memory type and the operand is a source, the accessing the operand comprises reading from the memory.
54. The method of claim 3, wherein when the type is the memory type and the operand is a destination, the accessing the operand comprises writing to the memory.
55. The method of claim 3, wherein the operand is a vector and the operand descriptor comprises information describing a length of the vector.
56. The method of claim 3, wherein the operand descriptor comprises microthreading information describing how the compute element is to operate when the operand is a vector and there is a stall accessing an element of the vector.
57. The method of claim 56, further comprising, responsive to the stall and the microthreading information indicating microthreading not enabled, the compute element stalling.
58. The method of claim 56, wherein the instruction is a first instruction; and further comprising, responsive to the stall and the microthreading information indicating microthreading is enabled, the compute element suspends processing of the first instruction and selects a second instruction for processing.
59. The method of claim 58, wherein the first instruction is associated with a first task and the second instruction is associated with a second task.
60. The method of claim 3, wherein the operand is a vector and the operand descriptor indicates how many elements of the vector to process in parallel.
61. The method of claim 3, wherein the operand descriptor comprises an indicator of whether to terminate processing when the operand is a vector and a control fabric packet conveying an element of the vector is received.
62. The method of claim 3, wherein the operand descriptor comprises an indicator of a virtual channel to selectively activate responsive to completion of the instruction.
63. The method of claim 3, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
64. The method of claim 3, wherein the operand comprises at least a portion of one or more of: a vector, a matrix, and a tensor.
65. The system of claim 4, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
66. The system of claim 4, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
67. The system of claim 5, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
68. The system of claim 5, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
69. The system of claim 6, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
70. The system of claim 6, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
71. The system of claim 7, wherein when the type is the fabric type, the operand descriptor is associated with a fabric virtual channel of the fabric.
72. The system of claim 7, wherein the access pattern is one of a fabric vector, a one-dimensional memory vector, a four-dimensional memory vector, and a circular memory buffer.
73. The system of claim 7, wherein the operand is a vector and the operand descriptor comprises information describing a length of the vector.
74. The system of claim 7, wherein the operand is a vector and the operand descriptor indicates how many elements of the vector to process in parallel.
75. The system of claim 7, wherein the operand descriptor comprises an indicator of whether to terminate processing when the operand is a vector and a control fabric packet conveying an element of the vector is received.
76. The system of claim 7, wherein the operand descriptor comprises an indicator of a virtual channel to selectively activate responsive to completion of the instruction.
77. The system of claim 7, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
78. The system of claim 7, wherein the operand comprises at least a portion of one or more of: a vector, a matrix, and a tensor.
79. The system of claim 8, wherein when the type is the fabric type, the operand descriptor is associated with a fabric virtual channel of the fabric.
80. The system of claim 8, wherein the access pattern is one of a fabric vector, a one-dimensional memory vector, a four-dimensional memory vector, and a circular memory buffer.
81. The system of claim 8, wherein the operand is a vector and the operand descriptor comprises information describing a length of the vector.
82. The system of claim 8, wherein the operand is a vector and the operand descriptor indicates how many elements of the vector to process in parallel.
83. The system of claim 8, wherein the operand descriptor comprises an indicator of whether to terminate processing when the operand is a vector and a control fabric packet conveying an element of the vector is received.
84. The system of claim 8, wherein the operand descriptor comprises an indicator of a virtual channel to selectively activate responsive to completion of the instruction.
85. The system of claim 8, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
86. The system of claim 8, wherein the operand comprises at least a portion of one or more of: a vector, a matrix, and a tensor.
87. The compute element of claim 9, wherein when the type is the fabric type, the operand descriptor is associated with a fabric virtual channel of the fabric.
88. The compute element of claim 9, wherein the access pattern is one of a fabric vector, a one-dimensional memory vector, a four-dimensional memory vector, and a circular memory buffer.
89. The compute element of claim 9, wherein the operand is a vector and the operand descriptor comprises information describing a length of the vector.
90. The compute element of claim 9, wherein the operand is a vector and the operand descriptor indicates how many elements of the vector to process in parallel.
91. The compute element of claim 9, wherein the operand descriptor comprises an indicator of whether to terminate processing when the operand is a vector and a control fabric packet conveying an element of the vector is received.
92. The compute element of claim 9, wherein the operand descriptor comprises an indicator of a virtual channel to selectively activate responsive to completion of the instruction.
93. The compute element of claim 9, wherein the operand comprises at least a portion of one or more of: a vector, a matrix, and a tensor.
94. The compute element of claim 10, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
95. The compute element of claim 10, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
96. The compute element of claim 11, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
97. The compute element of claim 11, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
98. The compute element of claim 12, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
99. The compute element of claim 12, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
100. The method of claim 13, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
101. The method of claim 13, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
102. The method of claim 14, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
103. The method of claim 14, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
104. The method of claim 15, wherein execution of the instruction implements at least a portion of one or more of: computing an activation of a neural network, computing a partial sum of activations of a neural network, computing an error of a neural network, computing a gradient estimate of a neural network, and updating a weight of a neural network.
105. The method of claim 15, wherein the operand comprises at least a portion of one or more of: a weight of a neural network, an activation of a neural network, a partial sum of activations of a neural network, an error of a neural network, a gradient estimate of a neural network, and a weight update of a neural network.
</claims>
</document>
