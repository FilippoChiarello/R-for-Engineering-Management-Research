<document>

<filing_date>
2018-09-14
</filing_date>

<publication_date>
2020-09-01
</publication_date>

<priority_date>
2018-09-14
</priority_date>

<ipc_classes>
G06K9/00,G10L15/08,G10L25/63
</ipc_classes>

<assignee>
ADP COMPANY
</assignee>

<inventors>
REZNICEK, ROBERTO
</inventors>

<docdb_family_id>
68000163
</docdb_family_id>

<title>
Automatic emotion response detection
</title>

<abstract>
A computer-implemented method, system, and computer program product for determining a valence indication, the computer-implemented method comprising: selecting a number of areas, a number of thresholds, a number of points, a number of emotion models, a number of expression models, a number of algorithms; using the number of areas, the number of thresholds, the number of points, the number of emotion models, the number of expression models, and the number of algorithms to form a valence formula; retrieving a video stream from a camera in the particular area in the location; using the video stream, calculating a valence indication for each of a number of individuals having images in the video stream; and wherein the valence indication represents a predominant emotion of a number of individuals at a point in time in the particular area.
</abstract>

<claims>
1. A computer-implemented method for determining a valence indication representing a human reaction to a particular area in a location, the computer-implemented method comprising: selecting a number of areas in the location, a number of thresholds, a number of points, a number of emotion models, a number of expression models, a number of algorithms; using the number of areas, the number of thresholds, the number of points, the number of emotion models, the number of expression models, and the number of algorithms, forming a valence formula; retrieving a number of video streams from a number of cameras in the particular area in the location; and using the number of video streams, calculating a valence indication for each of a number of individuals having images in the number of video streams, wherein the valence indication represents a predominant emotion of an individual at a point in time in the particular area, and wherein the valence indication is calculated using the valence formula.
2. The computer-implemented method of claim 1, wherein retrieving a video stream from a camera in the particular area in the location further comprises: using a facial recognition program to determine if a number of human face images are in a video stream from the camera; responsive to determining that there are a number of human face images in the video stream, isolating data for each of the number of human face images; responsive to isolating the data for each of the number of human face images, using a facial monitoring algorithm to superimpose a number of points over each of the number of human face images to form a number of image plots; responsive to superimposing the number of points over each of the number of human face images, comparing each of the number of image plots to a number of expression models to determine, for each of the number of image plots, a first data set; and responsive to superimposing the number of points over each of the number of human face images, comparing each of the number of image plots to a number of video emotion models to determine, for each of the number of image plots, a second data set.
3. The computer-implemented method of claim 2, further comprising: retrieving an audio stream from an audio device in the particular area in the location; responsive to determining that there are a number of human voices in the audio stream, isolating sound data for each of the number of human voices; responsive to isolating the sound data for each of the number of human voices, using a voice analysis algorithm to identify key words and a tonal element associated with each of the key words; responsive to identifying the key words and the tonal element associated with each of the key words, comparing each of the key words and an associated tonal element to a number of audio emotion models; and responsive to comparing the key words and the associated tonal element to the number of audio emotion models, determining a third data set.
4. The computer-implemented method of claim 2, further comprising: responsive to determining the first data set and the second data set, applying the valence formula to the first data set and the second data set to calculate the valence indication.
5. The computer-implemented method of claim 3, further comprising: responsive to determining the first data set, the second data set, and the third data set, applying the valence formula to the first data set, the second data set, and the third data set to calculate the valence indication.
6. The computer-implemented method of claim 2, wherein the facial monitoring algorithm is a two-dimensional facial monitoring algorithm.
7. The computer-implemented method of claim 2, wherein the facial monitoring algorithm is a three-dimensional facial monitoring algorithm.
8. The computer-implemented method of claim 2, wherein the number of expression models are selected from one or both of eye position models and head orientation models.
9. The computer-implemented method of claim 2, wherein the number of video emotion models is selected from a list consisting of happiness, surprise, anger, disgust, fear, sadness, neutral, confusion, dislike, and drowsiness.
10. The computer-implemented method of claim 3, further comprising: applying the valence indication to one of a marketing model, a security model, and an atmosphere model; and responsive to applying the valence indication to one of the marketing model, the security model, and the atmosphere model, determining an action to be taken.
11. A computer system for determining a valence indication representing a human reaction to a particular area in a location, the computer system comprising: a storage device configured to store program instructions; and one or more processors operably connected to the storage device and configured to execute the program instructions to cause the computer system to: select a number of areas in the location, a number of thresholds, a number of points, a number of emotion models, a number of expression models, a number of algorithms; use the number of areas, the number of thresholds, the number of points, the number of emotion models, the number of expression models, and the number of algorithms, to form a valence formula; retrieve a number of video streams from a number of cameras in the particular area in the location; use the number of video streams to calculate a valence indication for each of a number of individuals having images in the number of video streams wherein the valence indication represents a predominant emotion of a number of individuals at a point in time in the particular area, and wherein the valence indication is calculated using the valence formula.
12. The apparatus of claim 11, wherein the processors further execute instructions to cause the system to: use a facial recognition program to determine if a number of human face images are in a video stream from a camera; responsive to determining that there are a number of human face images in the video stream, isolate data for each of the number of human face images; responsive to isolating the data for each of the number of human face images, use a facial monitoring algorithm to superimpose a number of points over each of the number of human face images to form a number of image plots; and responsive to superimposing the number of points over each of the number of human face images, compare each of the number of image plots to a number of expression models to determine, for each of the number of image plots, a first data set; and responsive to superimposing the number of points over each of the number of human face images, compare each of the number of image plots to a number of emotion models to determine, for each of the number of image plots, a second data set.
13. The apparatus of claim 12, wherein the processors further execute instructions to cause the system to: retrieve an audio stream from an audio device in the particular area in the location; responsive to determining that there are a number of human voices in the audio stream, isolate sound data for each of the number of human voices; responsive to isolating the sound data for each of the number of human voices, use a voice analysis algorithm to identify key words and a tonal element associated with each of the key words; and responsive to identifying the key words and the tonal element associated with each of the key words, compare each of the key words and an associated tonal element to a number of audio emotion models; and responsive to comparing the key words and the associated tonal element to the number of audio emotion models, determine a third data set.
14. The apparatus of claim 13, wherein the processors further execute instructions to cause the system to: responsive to determining the first data set and the second data set, apply the valence formula to the first data set and the second data set to calculate the valence indication.
15. The apparatus of claim 13, wherein the processors further execute instructions to cause the system to: responsive to determining the first data set, the second data set, and the third data set, apply the valence formula to the first data set, the second data set, and the third data set to calculate the valence indication.
16. A computer program product for determining a valence indication representing a human reaction to a particular area in a location, the computer program product comprising: a non-transitory computer-readable storage medium having program instructions embodied thereon, the program instructions executable by a number of processors to: select a number of areas in the location, a number of thresholds, a number of points, a number of emotion models, a number of expression models, a number of algorithms; use the number of areas, the number of thresholds, the number of points, the number of emotion models, the number of expression models, and the number of algorithms, to form a valence formula; retrieve a number of video streams from a number of cameras in the particular area in the location; use the number of video streams to calculate a valence indication for each of a number of individuals having images in the number of video streams; and wherein the valence indication represents a predominant emotion of an individual at a point in time in the particular area; and wherein the valence indication is calculated using the valence formula.
17. The computer program product of claim 16, further comprising instructions for: using a facial recognition program to determine if a number of human face images are in a video stream from a camera; responsive to determining that there are a number of human face images in the video stream, isolating the data for each of the number of human face images; responsive to isolating data for each of the number of human face images, using a facial monitoring algorithm to superimpose a number of points over each of the number of human face images to form a number of image plots; and responsive to superimposing the number of points over each of the number of human face images, comparing each of the number of image plots to a number of expression models to determine, for each of the number of image plots, a first data set; and responsive to superimposing the number of points over each of the number of human face images, comparing each of the number of image plots to a number of emotion models to determine, for each of the number of image plots, a second data set.
18. The computer program product of claim 17, further comprising instructions for: responsive to determining the valence indication of one or more emotions, retrieving an audio stream from an audio device in the particular area in the location; responsive to determining that there are a number of human voices in the audio stream, isolate sound data for each of the number of human voices; responsive to isolating the sound data for each of the number of human voices, use a voice analysis algorithm to identify key words and a tonal element associated with each of the key words; and responsive to identifying the key words and the tonal element associated with each of the key words, compare each of the key words and an associated tonal element to a number of audio emotion models; and responsive to comparing the key words and the associated tonal element to the number of audio emotion models, determine a third data set.
19. The computer program product of claim 18, further comprising instructions for: responsive to determining the first data set, the second data set, and the third data set, applying the valence formula to the first data set, the second data set, and the third data set to calculate the valence indication.
20. The computer program product of claim 19, further comprising instructions for: applying the valence indication to one of a marketing model, a security model, and an atmosphere model; and responsive to applying the valence indication to one of the marketing model, the security model, and the atmosphere model, to determine an action to be taken.
</claims>
</document>
