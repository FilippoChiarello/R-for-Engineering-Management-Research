<document>

<filing_date>
2019-04-05
</filing_date>

<publication_date>
2020-10-08
</publication_date>

<priority_date>
2019-04-05
</priority_date>

<ipc_classes>
G06N3/04,G06N3/063,G06N3/08
</ipc_classes>

<assignee>
GILLIAN, NICHOLAS
GOOGLE
POUPYREV, IVAN
PALLIPURAM, Gerard
</assignee>

<inventors>
GILLIAN, NICHOLAS
POUPYREV, IVAN
PALLIPURAM, Gerard
</inventors>

<docdb_family_id>
66448615
</docdb_family_id>

<title>
SELECTIVE INFERENCE GENERATION WITH DISTRIBUTED MACHINE-LEARNED MODELS
</title>

<abstract>
A computing system includes at least a first computing device and a second computing device that is physically separate from the first computing device. The computing devices comprise a plurality of processors and a plurality of non-transitory computer-readable media that collectively store a multi-headed machine-learned model that is distributed across the computing devices. The multi-headed machine-learned model comprises a first model head provisioned at the first computing device and configured to receive sensor data from one or more sensors. The first model head is configured to generate a first set of feature representations based at least in part on the sensor data. The multi-headed machine-learned model comprises a second model head provisioned at the second computing device and configured to generate a second set of feature representations in response to receiving data associated with the first set of feature representations from the first computing device.
</abstract>

<claims>
1. A computing system comprising a plurality of computing devices including at least a first computing device and a second computing device that is physically separate from the first computing device, the plurality of computing devices comprising:
a plurality of processors; and
a plurality of non-transitory computer-readable media that collectively store a multi headed machine-learned model that is distributed across the plurality of computing devices, the multi-headed machine-learned model comprising:
a first model head provisioned at the first computing device and configured to receive sensor data from one or more sensors, wherein the first model head is configured to generate a first set of feature representations based at least in part on the sensor data; and a second model head provisioned at the second computing device and configured to generate a second set of feature representations in response to receiving data associated with the first set of feature representations from the first computing device.
2. The computing system of claim 1, wherein the first model head is configured to selectively generate at least one inference based at least in part on the first set of feature representations and one or more machine-learned inference criteria.
3. The computing system of claim 2, wherein the first model head is configured to selectively generate the at least one inference by:
determining whether the first set of feature representations satisfies the one or more machine-learned inference criteria; and
in response to determining that the first set of feature representations satisfies the one or more machine-learned inference criteria, generate the at least one inference based at least in part on the first set of feature representations.
4. The computing system of claim 2 or 3, wherein the first model head is configured to:
in response to determining that the first set of feature representations fails to satisfy the one or more machine-learned inference criteria, initiate a transmission of data associated with the first set of feature representations from the first computing device to the second model head at the second computing device; and
in response to determining that the first set of feature representations satisfies the one or more machine-learned inference criteria, transmitting the at least one inference from the first computing device to the second computing device.
5. The computing system of claim 4, wherein the first model head is configured to:
in response to determining that the first set of feature representations fails to satisfy the one or more machine-learned inference criteria, generate a first set of compressed feature representations based at least in part on the first set of feature representations;
wherein the data associated with the first set of feature representations includes the first set of compressed feature representations.
6. The computing system of claim 5, wherein:
the first set of compressed feature representations is generated using one or more first machine-learned compression parameters; and
the multi-headed machine-learned model is trained to determine the one or more first machine-learned compression parameters based at least in part on one or more first training constraints that are representative of one or more computing parameters associated with at least one of the first computing device or the second computing device.
7. The computing system of claim 6, wherein the one or more machine-learned inference criteria are one or more first machine-learned inference criteria, wherein the second model head is configured to:
determine whether the second set of feature representations satisfies one or more second machine-learned inference criteria;
in response to determining that the second set of feature representations satisfies the one or more second machine-learned inference criteria, generate at least one inference based at least in part on the second set of feature representations; and
in response to determining that the second set of feature representations fails to satisfy the one or more second machine-learned inference criteria, generate a second set of compressed feature representations based at least in part on the second set of feature representations and initiate a data transmission associated with the second set of compressed feature representations from the second computing device to a third model head at a third computing device.
8. The computing system of claim 7, wherein:
the second set of compressed feature representations is generated using one or more second machine-learned compression parameters; and
the multi-headed machine-learned model is trained to determine the one or more second machine-learned compression parameters based at least in part on one or more second training constraints that are representative of one or more computing parameters associated with at least one of the second computing device or the third computing device.
9. The computing system of any of claims 6-8, wherein:
the one or more first training constraints and the one or more second training constraints include at least one of a bandwidth constraint, a memory constraint, or a processing capability constraint.
10. The computing system of any of claims 2-9, wherein the one or more machine-learned inference criteria includes a threshold amount of data for inference generation.
11. The computing system of any of the preceding claims, wherein:
the one or more sensors include a capacitive touch sensor comprising a set of conductive lines;
the first computing device is communicatively coupled to the capacitive touch sensor; and
the multi-headed machine-learned model is configured to generate inferences associated with detection of at least one gesture based on touch input to the capacitive touch sensor.
12. The computing system of any of the preceding claims, wherein:
the one or more sensors include an inertial measurement unit; the first computing device is communicatively coupled to the inertial measurement unit; and
the multi-headed machine-learned model is configured to generate inferences associated with a movement recognition based on movement of an interactive object including the inertial measurement unit.
13. A computer-implemented method to train a multi -headed machine-learned model, comprising:
obtaining, by at least a first computing device, data descriptive of the multi-headed machine-learned model, wherein the multi-headed machine-learned model is configured for distribution across a plurality of computing devices including a second computing device and a third computing device, the multi-headed machine-learned model comprising a first model head configured for provisioning at the second computing device and a second model head configured for provisioning at the third computing device;
obtaining, by at least the first computing device, one or more training constraints representative of one or more computing parameters associated with at least one of the second computing device or the third computing device; and
training, by at least the first computing device, the multi-headed machine-learned model based on a set of training data and the one or more training constraints, wherein training, by at least the first computing device, the multi-headed machine-learned model comprises:
determining, by at least the first computing device, one or more parameters of a loss function based on the one or more training constraints and the set of training data; and modifying, by at least the first computing device, at least a portion of the multi-headed machine-learned model based at least in part on the one or more parameters of the loss function.
14. The computer-implemented method of claim 13, wherein modifying, by at least the first computing device, at least the portion of the multi-headed machine-learned model based at least in part on the one or more parameters of the loss function, comprises: generating one or more inference criteria for determining, by the first model head, whether to transmit an inference generated by the first model head to the second computing device or whether to communicate data indicative of one or more feature representations to the second computing device.
15. The computer-implemented method of any of claims 13 or 14, wherein:
the one or more training constraints include a first set of training constraints associated with the second computing device and a second set of training constraints associated with the third computing device; and
training, by at least the first computing device, the multi-headed machine-learned model comprises jointly optimizing, by at least the first computing device, the first model head and the second model head based on a first set of training constraints associated with the second computing device and a second set of training constraints associated with the third computing device.
16. The computer-implemented method of claim 15, wherein jointly optimizing, by at least the first computing device, the first model head and the second model head, comprises:
determining a first set of compression parameters for generating first feature representations by the first model head in response to sensor data associated with one or more sensors; and
determining a second set of compression parameters for generating second feature representations by the second model head in response to data indicative of the first feature representations.
17. The computer-implemented method of claim 16, wherein:
the first model head is configured to selectively transmit one or more of the first feature representations to the second model head based on at least in part on one or more inference criteria associated with inference generation by the multi-headed machine-learned model.
18. The computer-implemented method of any of claims 13-17, wherein:
the first model head is configured to generate a first set of compressed feature representations based at least in part on the sensor data and a machine-learned compression; and training, by at least the first computing device, the multi-headed machine-learned model comprises determining the machine-learned lossy compression based at least in part on the one or more training constraints.
19. A computing device, comprising:
one or more processors; and
one or more non-transitory computer-readable media that collectively store a first head of a multi-headed machine-learned model that is configured for distribution across a plurality of computing devices including the computing device, wherein the multi-headed machine-learned model is configured to generate inferences associated with at least one of a gesture detection or a movement recognition, the multi-headed machine-learned model comprising a first model head provisioned at the computing device and configured to receive input data, wherein the first model head is configured to generate a first set of feature representations based at least in part on the input data and to selectively generate at least one inference based at least in part on the input data and one or more inference criteria.
20. The computing device of claim 19, wherein the one or more inference criteria are one or more machine-learned inference criteria, wherein the first model head is configured to selectively generate the at least one inference by:
determining whether the first set of feature representations satisfies the one or more machine-learned inference criteria; and
in response to determining that the first set of feature representations satisfies the one or more machine-learned inference criteria, generate the at least one inference based at least in part on the first set of feature representations.
</claims>
</document>
