<document>

<filing_date>
2019-07-29
</filing_date>

<publication_date>
2020-04-02
</publication_date>

<priority_date>
2018-09-27
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
NEC LABORATORIES EUROPE
</assignee>

<inventors>
NIEPERT, MATHIAS
WANG, CHENG
</inventors>

<docdb_family_id>
69947647
</docdb_family_id>

<title>
RECURRENT NEURAL NETWORKS AND STATE MACHINES
</title>

<abstract>
A computer-implemented method includes instantiating a neural network including a recurrent cell. The recurrent cell includes a probabilistic state component. The method further includes training the neural network with a sequence of data. In an embodiment, the method includes extracting a deterministic finite automaton from the trained recurrent neural network and classifying a sequence with the extracted automaton.
</abstract>

<claims>
We claim:
1. A computer-implemented method comprising: instantiating a neural network comprising a recurrent cell, the recurrent cell comprising a probabilistic state component; and training the neural network with a sequence of data.
2. The method of claim 1, further comprising: extracting a deterministic finite automaton from the trained recurrent neural network.
3. The method of claim 2, further comprising: classifying a sequence with the extracted automaton.
4. The method of claim 1, wherein the probabilistic state component comprises at least two discrete vectors.
5. The method of claim 4, wherein the probabilistic state component comprises a finite quantity of predetermined vectors and the recurrent cell is configured to express a hidden state of the recurrent cell as a function of the predetermined vectors.
6. The method of claim 5, wherein the hidden state of the recurrent cell is a second hidden state and the recurrent cell is configured to express the second hidden state as a weighted average of the predetermined vectors, the method further comprising: computing the weighted average to minimize a disparity between the second hidden state and a first hidden state of the recurrent cell.
7. The method of claim 5, wherein the neural network comprises a plurality of the recurrent cells, each of the recurrent cells comprising the probabilistic state component.
8. The method of claim 7, wherein each of the recurrent cells comprises a respective unique set of the predetermined vectors, each of the respective sets being constant over at least multiple consecutive time steps.
9. The method of claim 8, wherein training the neural network comprises adjusting each of the predetermined vector sets to minimize a loss function after the multiple consecutive time steps.
10. The method of claim 1, wherein the probabilistic state component comprises a plurality of vectors and the recurrent cell is configured to express a hidden state at time step T as a function of at least two of the plurality of vectors based on (i) a hidden state of the recurrent cell at time step Tâˆ’1 and (ii) a dynamic input for time step T.
11. The method of claim 11, wherein the recurrent cell is configured to express a hidden state at time step T+1 as a function of at least two of the plurality of vectors based on (i) a hidden state of the recurrent cell at time step T and (ii) a dynamic input for time step T+1.
12. A processing system comprising one or more processors configured to: instantiate a neural network comprising a recurrent cell, the recurrent cell comprising a probabilistic state component; and train the neural network with a sequence of data.
13. The processing system of claim 12, wherein the one or more processors are configured to: extract a deterministic finite automaton from the trained recurrent neural network; and classify a sequence with the extracted automaton.
14. A non-transitory computer-readable medium comprising code for causing one or more processors of a processing system to: instantiate a neural network comprising a recurrent cell, the recurrent cell comprising a probabilistic state component; and train the neural network with a sequence of data.
15. The medium of claim 14, further comprising code for causing the one or more processors to: extract a deterministic finite automaton from the trained recurrent neural network; and classify a sequence with the extracted automaton.
</claims>
</document>
