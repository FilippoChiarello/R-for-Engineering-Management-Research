<document>

<filing_date>
2020-05-21
</filing_date>

<publication_date>
2020-11-26
</publication_date>

<priority_date>
2019-05-23
</priority_date>

<ipc_classes>
G06K9/62,G06N3/04,G10L15/06,G10L15/16
</ipc_classes>

<assignee>
CAPITAL ONE SERVICES
</assignee>

<inventors>
MUELLER ERIK T.
OLABIYI, OLUWATOBI
</inventors>

<docdb_family_id>
73457336
</docdb_family_id>

<title>
Adversarial Bootstrapping for Multi-Turn Dialogue Model Training
</title>

<abstract>
Systems described herein may use machine classifiers to perform a variety of natural language understanding tasks including, but not limited to multi-turn dialogue generation. Machine classifiers in accordance with aspects of the disclosure may model multi-turn dialogue as a one-to-many prediction task. The machine classifier may be trained using adversarial bootstrapping between a generator and a discriminator with multi-turn capabilities. The machine classifiers may be trained in both auto-regressive and traditional teacher-forcing modes, with the maximum likelihood loss of the auto-regressive outputs being weighted by the score from a metric-based discriminator model. The discriminators input may include a mixture of ground truth labels, the teacher-forcing outputs of the generator, and/or negative examples from the dataset. This mixture of input may allow for richer feedback on the autoregressive outputs of the generator. Additionally, dual sampling may improve response relevance and coherence by overcoming the problem of exposure bias.
</abstract>

<claims>
1. A computing device, comprising: one or more processors; and memory storing instructions that, when executed by the one or more processors, cause the computing device to: initialize a machine classifier having a deep neural network architecture and a plurality of machine classifier parameters, wherein the deep neural network architecture comprises an encoder, a generator, a discriminator, and an output layer; train the machine classifier, based on a training set comprising a plurality of examples, to refine the plurality of machine classifier parameters, wherein training the machine classifier using an example comprises: generating, by the encoder, an encoded input based on the example; generating, by the generator, a generator response based on the encoded input; generating, by the discriminator, discriminator feedback based on the encoded input and the generator response; and updating the plurality of machine classifier parameters based on minimizing an average gradient of a loss function calculated based on a first weight determined based on the discriminator feedback and a second weight determined based on the generated response; and generate, by the output layer, one or more class labels based on an input data set using the trained machine classifier.
2. The computing device of claim 1, wherein the instructions, when executed by the one or more processors, further cause the computing device to generate the discriminator feedback based on a ground truth label associated with the encoded input and at least one previous response generated by the generator based on the encoded input.
3. The computing device of claim 1, wherein the instructions, when executed by the one or more processors, further cause the computing device to generate the generator response for an example by using the discriminator feedback to weight a cross-entropy loss for each of a set of candidate responses generated by the generator and selecting the candidate response with the lowest loss as the generator response.
4. The computing device of claim 1, wherein the encoded input comprises a single vector representation of the example.
5. The computing device of claim 1, wherein the discriminator comprises a metric encoder.
6. The computing device of claim 1, wherein the generator comprises a maximum likelihood estimator classifier.
7. The computing device of claim 1, wherein the deep neural network architecture comprises a feed-forward neural network.
8. The computing device of claim 1, wherein the deep neural network architecture comprises a convolutional neural network.
9. The computing device of claim 1, wherein the deep neural network architecture comprises a recurrent neural network.
10. The computing device of claim 1, wherein the input data comprises a multi-turn dialogue data set.
11. A method, comprising: initializing, by a computing device, a machine classifier having a deep neural network architecture and a plurality of machine classifier parameters, wherein the deep neural network architecture comprises an encoder, a generator, and a discriminator; training, by the computing device, the machine classifier, based on a training set comprising a plurality of examples, to refine the plurality of machine classifier parameters, wherein training the machine classifier using an example comprises: generating, by the encoder, an encoded input based on the example, the encoded input comprising a single vector; generating, by the generator, a generator response based on the encoded input; generating, by the discriminator, discriminator feedback based on the generator output, a ground truth label associated with the encoded input, and at least one previous response generated by the generator based on the encoded input; updating the plurality of machine classifier parameters based on minimizing an average gradient of a loss function calculated based on a first weight determined based on the discriminator feedback and a second weight determined based on the generator response; and generating, by the computing device and using the trained machine classifier, at least one dialogue response based on a multi-turn dialogue data set.
12. The method of claim 11, further comprising generating the generator response by: generating, by the generator, a plurality of candidate responses, each candidate response comprising a loss calculated based on the discriminator feedback; and selecting the generator response from the plurality of candidate response based on the loss for each candidate response.
13. The method of claim 11, further comprising generating the generator response based on a ground truth label associated with the encoded input and at least one previous response generated by the generator based on the encoded input.
14. The method of claim 11, wherein the discriminator comprises a metric encoder.
15. The method of claim 14, wherein the generator comprises a maximum likelihood estimator classifier.
16. The method of claim 11, wherein the deep neural network architecture comprises a recurrent neural network.
17. A non-transitory machine-readable medium storing instructions that, when executed by one or more processors, cause the one or more processors to perform steps comprising: initializing a machine classifier having a deep neural network architecture and a plurality of machine classifier parameters, wherein the deep neural network architecture comprises an encoder, a generator, and a discriminator; training the machine classifier, based on a training set comprising a plurality of examples, to refine the plurality of machine classifier parameters, wherein training the machine classifier using an example comprises: generating, by the encoder, an encoded input based on the example, the encoded input comprising a single vector; generating, by the generator, a generator response based on the encoded input by weighting a cross-entropy loss for each of a set of candidate responses, generated by the generator for the encoded input, and selecting the candidate response in the set of candidate responses with the lowest loss as the generator response; generating, by the discriminator, discriminator feedback based on the generator response, a ground truth label associated with the encoded input, and at least one previous response generated by the generator based on the encoded input; and updating the plurality of machine classifier parameters based on minimizing an average gradient of a loss function calculated based on a first weight determined based on the discriminator feedback and a second weight determined based on the generator response; and generating, by the output layer, one or more class labels based on an input data set using the trained machine classifier.
18. The non-transitory machine-readable medium of claim 17, wherein the deep neural network architecture comprises a recurrent neural network.
19. The non-transitory machine-readable medium of claim 17, wherein the discriminator comprises a metric encoder.
20. The non-transitory machine-readable medium of claim 17, wherein the generator comprises a maximum likelihood estimator classifier.
</claims>
</document>
