<document>

<filing_date>
2019-09-06
</filing_date>

<publication_date>
2020-12-10
</publication_date>

<priority_date>
2019-06-07
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
TATA CONSULTANCY SERVICES
</assignee>

<inventors>
HEBBALAGUPPE, RAMYA
HEGDE, SRINIDHI
PRASAD, Ranjitha
</inventors>

<docdb_family_id>
67809244
</docdb_family_id>

<title>
SPARSITY CONSTRAINTS AND KNOWLEDGE DISTILLATION BASED LEARNING OF SPARSER AND COMPRESSED NEURAL NETWORKS
</title>

<abstract>
Implementations of the present disclosure build a Bayesian student network using the knowledge learnt by an accurate but complex pre-trained teacher network, and sparsity induced by the variational parameters in a student network. Further, the sparsity inducing capability of the teacher on the student network is learnt by employing a Block Sparse Regularizer on a concatenated tensor of teacher and student network weights. Specifically, the student network is trained using the variational lower bound based loss function, constrained on the hint from the teacher, and block-sparsity of weights.
</abstract>

<claims>
1. A processor implemented method, comprising: initializing, by one or more hardware processors, a first neural network with a plurality of weights; and training, by the one or more hardware processors, the first neural network by iteratively performing until a final loss function converges within a predefined threshold to obtain a trained compressed and sparser neural network: passing through the first neural network, (i) a subset of an input data received corresponding to a specific domain and (ii) ground truth information corresponding to the subset of the input data; dynamically updating, by the one or more hardware processors, the plurality of weights of the first neural network based on a first difference in an output generated by the first neural network and the corresponding ground truth information of the subset of an input data; dynamically updating, by the one or more hardware processors, the plurality of weights of the first network based on a second difference in an output generated by (i) the first neural network and (ii) a second neural network for the subset; and applying, by the one or more hardware processors, one or more sparsity constraints by utilizing block sparse regularization and a variational dropout techniques, on the plurality of weights of the first neural network with reference to a set of weights of the second neural network to determine one or more weights to be dropped or retained, from or in, the plurality of weights of the first neural network.
2. The processor implemented method of claim 1, wherein the first difference in an output and the corresponding ground truth information of the subset of an input data is estimated using a cross-entropy loss function.
3. The processor implemented method of claim 1, wherein the second difference in an output generated by (i) the first neural network and (ii) a second neural network for the subset is estimated using a Kullback-Leibler (KL) divergence function.
4. The processor implemented method of claim 1, wherein the one or more weights to be dropped or retained are determined by solving the final loss function.
5. The processor implemented method of claim 1, wherein the final loss function is optimized to obtain the trained compressed and sparser neural network comprising the determined one or more weights being less than the plurality of weights in the second neural network, and wherein selection of the first neural network is based on number of parameters in one or more layers in a neural network.
6. The processor implemented method of claim 1, wherein the second neural network is a pre-trained neural network.
7. A system, comprising: a memory storing instructions; one or more communication interfaces; and one or more hardware processors coupled to the memory via the one or more communication interfaces, wherein the one or more hardware processors are configured by the instructions to: initialize a first neural network with a plurality of weights, wherein the first neural network is comprised in the memory and executed by the one or more hardware processors; and train the first neural network by iteratively performing until a final loss function converges within a predefined threshold to obtain a trained compressed and sparser neural network: passing through the first neural network, (i) a subset of an input data received corresponding to a specific domain and (ii) ground truth information corresponding to the subset of the input data; dynamically updating the plurality of weights of the first neural network based on a first difference in an output generated by the first neural network and the corresponding ground truth information of the subset of an input data; dynamically updating the plurality of weights of the first network based on a second difference in an output generated by (i) the first neural network and (ii) a second neural network for the subset, wherein the second neural network is comprised in the memory and executed by the one or more hardware processors; and applying, by the one or more hardware processors, one or more sparsity constraints by utilizing block sparse regularization and a variational dropout techniques, on the plurality of weights of the first neural network with reference to a set of weights of the second neural network to determine one or more weights to be dropped or retained, from or in, the plurality of weights of the first neural network.
8. The system of claim 7, wherein the first difference in an output and the corresponding ground truth information of the subset of an input data is estimated using a cross-entropy loss function.
9. The system of claim 7, wherein the second difference in an output generated by (i) the first neural network and (ii) a second neural network for the subset is estimated using a Kullback-Leibler (KL) divergence function.
10. The system of claim 7, wherein the one or more weights to be dropped or retained are determined by solving the final loss function.
11. The system of claim 7, wherein the final loss function is optimized to obtain the trained compressed and sparser neural network comprising the determined one or more weights being less than the plurality of weights in the second neural network, and wherein selection of the first neural network is based on number of parameters in one or more layers in a neural network.
12. The system of claim 7, wherein the second neural network is a pre-trained neural network.
13. One or more non-transitory machine readable information storage media storing instructions which, when executed by one or more hardware processors, cause the one or more hardware processors to perform a method comprising: initializing, by the one or more hardware processors, a first neural network with a plurality of weights; and training, by the one or more hardware processors, the first neural network by iteratively performing until a final loss function converges within a predefined threshold to obtain a trained compressed and sparser neural network: passing through the first neural network, (i) a subset of an input data received corresponding to a specific domain and (ii) ground truth information corresponding to the subset of the input data; dynamically updating, by the one or more hardware processors, the plurality of weights of the first neural network based on a first difference in an output generated by the first neural network and the corresponding ground truth information of the subset of an input data; dynamically updating, by the one or more hardware processors, the plurality of weights of the first network based on a second difference in an output generated by (i) the first neural network and (ii) a second neural network for the subset; and applying, by the one or more hardware processors, one or more sparsity constraints by utilizing block sparse regularization and a variational dropout techniques, on the plurality of weights of the first neural network with reference to a set of weights of the second neural network to determine one or more weights to be dropped or retained, from or in, the plurality of weights of the first neural network.
14. The one or more non-transitory machine readable information storage media of claim 13, wherein the first difference in an output and the corresponding ground truth information of the subset of an input data is estimated using a cross-entropy loss function.
15. The one or more non-transitory machine readable information storage media of claim 13, wherein the second difference in an output generated by (i) the first neural network and (ii) a second neural network for the subset is estimated using a Kullback-Leibler (KL) divergence function.
16. The one or more non-transitory machine readable information storage media of claim 13, wherein the one or more weights to be dropped or retained are determined by solving the final loss function.
17. The one or more non-transitory machine readable information storage media of claim 13, wherein the final loss function is optimized to obtain the trained compressed and sparser neural network comprising the determined one or more weights being less than the plurality of weights in the second neural network, and wherein selection of the first neural network is based on number of parameters in one or more layers in a neural network.
18. The one or more non-transitory machine readable information storage media of claim 13, wherein the second neural network is a pre-trained neural network.
</claims>
</document>
