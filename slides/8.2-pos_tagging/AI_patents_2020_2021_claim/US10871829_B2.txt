<document>

<filing_date>
2018-12-04
</filing_date>

<publication_date>
2020-12-22
</publication_date>

<priority_date>
2017-12-05
</priority_date>

<ipc_classes>
G06F3/01,G06F3/0346,G06F3/041,G06F3/0488
</ipc_classes>

<assignee>
TACTAI
</assignee>

<inventors>
DOMENIKOS; STEVEN D.
DERCHE, ILANN
KAMADOLLI, SHYAM SHANKAR
</inventors>

<docdb_family_id>
66659143
</docdb_family_id>

<title>
Touch enabling process, haptic accessory, and core haptic engine to enable creation and delivery of tactile-enabled experiences with virtual objects
</title>

<abstract>
Various systems, methods and computer program products are disclosed which provide tactile feedback from virtual objects. In some particular embodiments, a touch-enabled platform (TEP) is configured to: receive data indicating a user is contacting a touch interface on a device; analyze the data to determine a characteristic of the contact between the user and the touch interface; and provide a waveform to actuate vibration at the touch interface based upon the characteristic of the contact and a display characteristic at the touch interface.
</abstract>

<claims>
1. A touch-enabled platform (TEP) comprising: program code running on a cloud-based server and in communication with a database including a haptic object properties (HOPs) library having a haptic object profile for at least one object, wherein the TEP is configured to actuate haptic feedback at a device by performing actions comprising: receive data indicating a user is contacting a touch interface on the device; analyze the data to determine a characteristic of the contact between the user and the touch interface; and provide a waveform to actuate vibration at the touch interface based upon the characteristic of the contact and a display characteristic at the touch interface, wherein the display characteristic includes a depiction of an object not stored in the HOPs library, and wherein the TEP includes an engine configured to perform the following: analyze the object not stored in the HOPs library; create a haptic object profile for the object not stored in the HOPs library, the haptic object profile comprising the waveform; and provide the waveform to actuate the vibration at the touch interface, wherein the program code is configured to provide the waveform to an actuator at the device for delivering the haptic content as a dynamic time-varying waveform in response to the user contact with the touch interface at the device, wherein the device further comprises a filter for modifying the haptic content based upon a vibrational characteristic of the device, wherein the filter is configured to vary the haptic content for distinct types of device based on at least one of a resonance frequency of the device or a strength of an actuator at the device, by: decreasing an amplitude of frequencies of the dynamic time-varying waveform that are around the resonance frequency of the device, and increasing an amplitude of frequencies of the dynamic time-varying waveform in response to detecting that that the actuator has insufficient strength to render the haptic content.
2. The TEP of claim 1, wherein the engine in the TEP functions in real time in response to receiving the data indicating that the user is contacting the touch interface.
3. The TEP of claim 1, wherein the TEP is configured to provide the waveform based upon a haptic object profile for the object contained in the HOPs library, or the haptic object profile for the object not stored in the HOPs library and created by the engine.
4. The TEP of claim 1, further comprising vibrating the actuator at the touch device based on the waveform, wherein the actuator is internal to the device, coupled to the device, or part of a haptic accessory used with the device, wherein the device includes a smartphone, a tablet, a touch-screen device, a stylus, or a keyboard.
5. The TEP of claim 1, wherein the waveform is created from an image of a physical object or an image of a derivative object.
6. The TEP of claim 5, further comprising a storage system for storing the created waveform.
7. A system comprising: at least one computing device comprising a cloud-based platform, the at least one computing device being configured to: obtain raw content capable of haptic representation, wherein the raw content comprises at least one still frame image or video file; convert the raw content into haptic content and embed the haptic content into a data file with the at least one still frame image or video file; and provide the haptic content to a client application in real time in response to user interaction with a touch interface device; and a device driver coupled with the at least one computing device for delivering the haptic content, wherein the device driver is configured to deliver the haptic content as a dynamic time-varying waveform in response to the user interaction with the touch interface device, wherein the device driver is a component in a haptic rendering device having a filter for modifying the haptic content based upon a vibrational characteristic of the haptic rendering device, wherein the filter is configured to vary the haptic content for distinct types of haptic rendering device based on at least one of a resonance frequency of the touch interface device or a strength of an actuator at the touch interface device, by: decreasing an amplitude of frequencies of the dynamic time-varying waveform that are around the resonance frequency of the touch interface device, and increasing an amplitude of frequencies of the dynamic time-varying waveform in response to detecting that that the actuator has insufficient strength to render the haptic content.
8. The system of claim 7, wherein the haptic rendering device is coupled to the touch interface device.
9. The system of claim 8, wherein the touch interface device includes a smartphone, a tablet, a touch-screen device, a stylus, or a keyboard.
10. The system of claim 7, wherein the dynamic time-varying waveform is created from an image of a physical object or an image of a derivative object.
11. The system of claim 7, wherein the raw content comprises the video file, and wherein the converting and embedding processes comprise converting the raw content into haptic content and embedding the haptic content into a data file with the video file.
</claims>
</document>
