<document>

<filing_date>
2019-12-10
</filing_date>

<publication_date>
2020-09-17
</publication_date>

<priority_date>
2015-10-28
</priority_date>

<ipc_classes>
G06F16/901,G06N20/00,G06N5/04
</ipc_classes>

<assignee>
QOMPLX
</assignee>

<inventors>
CHUNG, JEFFREY
CRABTREE, JASON
JURUKOVSKI, LUKA
SELLERS, ANDREW
Parikh, Bhashit
</inventors>

<docdb_family_id>
72424835
</docdb_family_id>

<title>
RAPID PREDICTIVE ANALYSIS OF VERY LARGE DATA SETS USING THE DISTRIBUTED COMPUTATIONAL GRAPH USING CONFIGURABLE ARRANGEMENT OF PROCESSING COMPONENTS
</title>

<abstract>
A system for predictive analysis of very large data sets using a distributed computational graph that intelligently combines processing of a current data stream with the ability to retrieve relevant stored data in such a way that conclusions or actions may be drawn in a predictive manner. The system has a pipeline construction module that allows a user to construct a streaming analytic workflow using modular building blocks, each of which represents either an environmental orchestration stage or a data processing stage of a streaming analytic workflow, and has a pipeline processing module that receives a data stream and constructs a directed computational graph by processing the data stream through the streaming analytic workflow. The directed computational graph is used to analyze the data stream.
</abstract>

<claims>
1. A system for predictive analysis of very large data sets using a directed computational graph comprising: a processor, a memory, a non-volatile data storage, and a first plurality of programming instructions stored in the memory and operable on the processor of a computing device; a pipeline construction module comprising a second plurality of programming instructions stored in the memory of the computing device, wherein the second plurality of programming instructions, when operating on the processor of the computing device, cause the computing device to: present a graphical user interface to a user comprising modular building blocks, each comprising modular building blocks comprising either a declarative definition of an environmental orchestration stage of a streaming analytics workflow or a declarative definition of a data processing stage of a streaming analytics workflow; and receive and store input from the user through the graphical user interface, the input comprising a streaming analytics workflow constructed by the user using the modular building blocks; and a pipeline processing module comprising a third plurality of programming instructions stored in the memory of the computing device, wherein the third plurality of programming instructions, when operating on the processor of the computing device, cause the computing device to: retrieve the stored streaming analytics workflow; receive a first data stream for analysis using the streaming analytics workflow; construct a directed computational graph by processing the first data stream through the streaming analytics workflow; wherein the directed computational graph comprises nodes representing workflow stages and edges representing message outputs between the workflow stages; wherein the workflow stages comprise: one or more environmental orchestration stages, each configured to: set up data processing stages and data paths; and teardown data processing stages; and one or more data processing stages each comprising one or more data source stages, one or more data sink stages, and a plurality of transformation stages; and wherein the directed computational graph is used to produce a result of analysis of the first data stream.
2. The system of claim 1, wherein the directed computational graph further comprises one or more cyclic workflows.
3. The system of claim 1, wherein a workflow stage in the directed computational graph is constructed using a different workflow stage in the directed computational graph.
4. The system of claim 1, wherein the pipeline processing module is configured to employ exactly-once semantics, wherein a datapoint is the data stream impacts the construction of the directed computational graph the first time that it is received, and subsequent instances of an identical or semantically-similar datapoint in the data stream do not impact the construction of the directed computational graph.
5. The system of claim 1, wherein the streaming analytics workflow comprises analysis of the data stream in a sliding time window.
6. The system of claim 1, wherein the modular building blocks are domain-agnostic.
7. The system of claim 1, wherein the modular building blocks are domain-specific.
8. The system of claim 1, wherein the pipeline processing module receives a second data stream comprising a data context that is preserved from the first stream into a node of the directed computational graph, the data context shared at the node allowing the first data stream and the second data stream to share common meaning of data associated with the data context.
9. A method for predictive analysis of very large data sets using a directed computational graph comprising the steps of: presenting a graphical user interface to a user comprising modular building blocks, each comprising modular building blocks comprising either a declarative definition of an environmental orchestration stage of a streaming analytics workflow or a declarative definition of a data processing stage of a streaming analytics workflow; and receiving and storing input from the user through the graphical user interface, the input comprising a streaming analytics workflow constructed by the user using the modular building blocks; retrieving the stored streaming analytics workflow; receiving a first data stream for analysis using the streaming analytics workflow; and constructing a directed computational graph by processing the first data stream through the streaming analytics workflow; wherein the directed computational graph comprises nodes representing workflow stages and edges representing message outputs between the workflow stages; wherein the workflow stages comprise: one or more environmental orchestration stages, each configured to: set up data processing stages and data paths; and teardown data processing stages; and one or more data processing stages each comprising one or more data source stages, one or more data sink stages, and a plurality of transformation stages; and wherein the directed computational graph is used to produce a result of analysis of the first data stream.
10. The method of claim 9, wherein the directed computational graph further comprises one or more cyclic workflows.
11. The method of claim 9, wherein a workflow stage in the directed computational graph is constructed using a different workflow stage in the directed computational graph.
12. The method of claim 9, wherein the construction of the directed computational graph employs exactly-once semantics, wherein a datapoint is the data stream impacts the construction of the directed computational graph the first time that it is received, and subsequent instances of an identical or semantically-similar datapoint in the data stream do not impact the construction of the directed computational graph.
13. The method of claim 9, wherein the streaming analytics workflow comprises analysis of the data stream in a sliding time window.
14. The method of claim 9, wherein the modular building blocks are domain-agnostic.
15. The method of claim 9, wherein the modular building blocks are domain-specific.
16. The method of claim 9, further comprising the step of receiving a second data stream comprising a data context that is preserved from the first stream into a node of the directed computational graph, the data context shared at the node allowing the first data stream and the second data stream to share common meaning of data associated with the data context.
</claims>
</document>
