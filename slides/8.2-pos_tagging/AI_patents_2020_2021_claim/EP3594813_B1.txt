<document>

<filing_date>
2018-03-26
</filing_date>

<publication_date>
2021-01-13
</publication_date>

<priority_date>
2017-04-28
</priority_date>

<ipc_classes>
G06F9/30,G06F9/38,G06F9/50,G06N3/04,G06N3/063,G06N3/08,G06T1/20,G06T15/00
</ipc_classes>

<assignee>
INTEL CORPORATION
</assignee>

<inventors>
OULD-AHMED-VALL, ELMOUSTAPHA
MACPHERSON, MIKE B.
CHEN, XIAOMING
HURD, LINDA L.
BAGHSORKHI, SARA S.
KIM, DUKHWAN
WEAST, JOHN C.
APPU, ABHISHEK R.
RAY, JOYDEEP
KOKER, ALTUG
YAO, ANBANG
ASHBAUGH, BEN J.
MA, LIWEI
STRICKLAND, MICHAEL S.
NEALIS, KEVIN
TANG, PING T.
LAKSHMANAN, BARATH
</inventors>

<docdb_family_id>
61965680
</docdb_family_id>

<title>
COMPUTE OPTIMIZATIONS FOR LOW PRECISION MACHINE LEARNING OPERATIONS
</title>

<abstract>
An accelerator on a multi-chip module, a method of accelerating a machine-learning operation and a data processing system are provided. In one embodiment, the accelerator comprises: a memory stack including multiple memory dies; and a graphics processing unit (GPU) coupled with the memory stack via one or more memory controllers. The GPU includes a plurality of multiprocessors having a single instruction, multiple thread (SIMT) architecture, the multiprocessors to execute at least one single instruction, the at least one single instruction to accelerate a linear algebra subprogram associated with a machine learning framework. The at least one single instruction to cause at least a portion of the GPU to perform a floating-point operation on input having differing precisions, the floating-point operation a two-dimensional matrix multiply and accumulate operation. At least a portion of the plurality of multiprocessors include a mixed precision core, the mixed precision core to execute a thread of the at least one single instruction, the mixed precision core including a floating-point unit to perform a first operation of the thread at a first precision and a second operation of the thread at a second precision. The first operation is a multiply having at least one 16-bit floating-point input and the second operation is an accumulate having a 32-bit floating-point input.
</abstract>

<claims>
1. An accelerator (446) on a multi-chip module, the accelerator comprising: a memory stack including multiple memory dies; and a graphics processing unit, GPU (410-413), coupled with the memory stack via one or more memory controllers, the GPU including a plurality of multiprocessors (234) having a single instruction, multiple thread, SIMT, architecture, the multiprocessors to execute at least one single instruction, the at least one single instruction to accelerate a linear algebra subprogram associated with a machine learning framework; the at least one single instruction to cause at least a portion of the GPU to perform a floating-point operation on input having differing precisions; wherein at least a portion of the plurality of multiprocessors is to execute a thread of the at least one single instruction, the portion of the plurality of multiprocessors including a floating-point unit to perform, as a mixed precision FP16/FP32 dual operation, a first operation of the thread at a first precision and a second operation of the thread at a second precision; and wherein the first operation is an operation having two or more 16-bit floating-point inputs and the second operation is an operation having two or more 32-bit floating-point inputs.
2. The accelerator as in claim 1, the memory stack including high bandwidth memory, or the memory stack being located on a same physical package as the GPU.
3. The accelerator as in claim 1, the portion of the plurality of multiprocessors to perform the first operation at a 16-bit precision and the second operation at a 32-bit precision.
4. The accelerator as in claim 1, the mixed precision core configurable to output a 16-bit floating-point value from the floating-point operation.
5. A method of accelerating a machine-learning operation, the method comprising: decoding a single instruction on a graphics processing unit, GPU (410-413), the GPU having a single instruction, multiple thread, SIMT, architecture, the GPU coupled with a memory stack via one or more memory controllers; and executing the single instruction via one or more multiprocessors (234) within the GPU, the single instruction to cause at least a portion of the GPU to perform a floating-point operation on input having differing precisions to accelerate a linear algebra subprogram associated with a machine learning framework, wherein executing the single instruction includes executing a thread of the single instruction on at least a portion of the one or more multiprocessors, the portion of the one or more multiprocessors including a floating-point unit to perform, as mixed precision FP16/FP32 dual operation, a first operation of the thread at a first precision and a second operation of the thread at a second precision, wherein the first operation is an operation having two or more 16-bit floating-point inputs and the second operation is an operation having two or more 32-bit floating-point inputs.
6. The method as in claim 5, additionally comprising performing multiple operations on input using the the portion of the one or more multiprocessors to generate a two-dimensional output matrix and storing the two-dimensional output matrix to the memory stack via the one or more memory controllers.
7. The method as in claim 6, wherein the memory stack includes high bandwidth memory and is located on a same physical package as the GPU.
8. The method as in claim 5, wherein the first precision is a 16-bit precision and the second precision is a 32-bit precision.
9. The method as in claim 5, additionally comprising generating a 16-bit floating-point output of the floating-point operation.
</claims>
</document>
