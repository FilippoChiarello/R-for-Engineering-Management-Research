<document>

<filing_date>
2019-12-27
</filing_date>

<publication_date>
2020-07-02
</publication_date>

<priority_date>
2018-12-27
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,G06N99/00
</ipc_classes>

<assignee>
PAYPAL
</assignee>

<inventors>
DONG, YANFEI
</inventors>

<docdb_family_id>
71124096
</docdb_family_id>

<title>
DATA AUGMENTATION IN TRANSACTION CLASSIFICATION USING A NEURAL NETWORK
</title>

<abstract>
Systems and methods for data augmentation in a neural network system includes performing a first training process, using a first training dataset on a neural network system including an autoencoder including an encoder and a decoder to generate a trained autoencoder. A trained encoder is configured to receive a first plurality of input data in an N-dimensional data space and generate a first plurality of latent variables in an M-dimensional latent space, wherein M is an integer less than N. A sampling process is performed on the first plurality of latent variables to generate a first plurality of latent variable samples. A trained decoder is used to generate a second training dataset using the first plurality of latent variable samples. The second training dataset is used to train a first classifier including a first classifier neural network model to generate a trained classifier for providing transaction classification.
</abstract>

<claims>
1. A system for training a neural network classifier, comprising:
a non-transitory memory; and
one or more hardware processors coupled to the non-transitory memory and configured to read instructions from the non-transitory memory to cause the system to perform operations comprising:
performing a first training process, using a first training dataset including a plurality of transactions, on a neural network system including an autoencoder including an encoder and a decoder to generate a trained autoencoder,
wherein a trained encoder of the trained autoencoder is configured to receive a first plurality of input data in an N-dimensional data space and generate a first plurality of latent variables in an M-dimensional latent space, wherein M is an integer less thanN;
performing a sampling process to the first plurality of latent variables to generate a first plurality of latent variable samples;
generating, using a trained decoder of the trained autoencoder, a second training dataset in the N-dimensional data space using the first plurality of latent variable samples; and
performing a second training process, using the second training dataset, on a first classifier including a first classifier neural network model to generate a trained classifier for providing transaction classification of a first transaction.
2. The system of claim 1, wherein the neural network system includes a second classifier including a second classifier neural network model for transaction classification, wherein the performing the first training process includes training the second classifier, using the first training dataset, to generate a trained second classifier; and
wherein the performing the sampling process to the first plurality of latent variables to generate a plurality of latent variable samples includes:
performing a first sub-sampling process, using a first sampler, to generate a second plurality of latent variable samples from the first plurality of latent variables; providing, using the trained second classifier, a plurality of class probabilities corresponding to the second plurality of latent variable samples respectively; and performing a second sub-sampling process, using a second sampler, to generate the first plurality of latent variable samples from the second plurality of latent variable samples based on the plurality of class probabilities.
3. The system of claim 2, wherein the second sub-sampling process includes an adaptive bootstrap sampling process.
4. The system of claim 2, wherein the neural network system includes a fraudulent transaction generative adversarial network (GAN) including a fraudulent transaction generator and a fraud discriminator,
wherein the fraud transaction generator including the decoder, and
wherein the performing the first training process includes training the fraudulent transaction GAN using a fraud-sensitive weighted adversarial loss function based on class probabilities of transactions provided by the second classifier.
5. The system of claim 2, wherein the second classifier is trained using a cross-entropy loss function.
6. The system of claim 2, wherein the second classifier is different from the first classifier.
7. The system of claim 1 , wherein the neural network system includes a prior distribution generative adversarial network (GAN) including a generator and a prior distribution discriminator,
wherein the generator including the encoder, and
wherein the performing the first training process includes training the prior distribution GAN using a predetermined prior distribution.
8. A method, comprising:
performing a first training process, using a first training dataset, on a neural network system including an autoencoder including an encoder and a decoder to generate a trained autoencoder,
wherein a trained encoder of the trained autoencoder is configured to receive a first plurality of input data in an N-dimensional data space and generate a first plurality of latent variables in an M-dimensional latent space, wherein M is an integer less than N;
performing a sampling process to the first plurality of latent variables to generate a first plurality of latent variable samples;
generating, using a trained decoder of the trained autoencoder, a second training dataset in the N-dimensional data space using the first plurality of latent variable samples; and
performing a second training process, using the second training dataset, on a first classifier including a first classifier neural network model to generate a trained classifier for providing data classification.
9. The method of claim 8, wherein the neural network system includes a second classifier including a second classifier neural network model for data classification,
wherein the performing the first training process includes:
training the second classifier, using the first training dataset, to generate a trained second classifier; and
wherein the performing the sampling process to the first plurality of latent variables to generate a plurality of latent variable samples includes:
performing a first sub-sampling process, using a first sampler, to generate a second plurality of latent variable samples from the first plurality of latent variables; providing, using the trained second classifier, a plurality of class probabilities corresponding to the second plurality of latent variable samples respectively; and performing a second sub-sampling process, using a second sampler, to generate the first plurality of latent variable samples from the second plurality of latent variable samples based on the plurality of class probabilities.
10. The method of claim 9, wherein the second sub-sampling process includes an adaptive bootstrap sampling process.
11. The method of claim 9, wherein the neural network system includes a fraudulent transaction generative adversarial network (GAN) including a fraudulent transaction generator and a fraud discriminator,
wherein the fraud transaction generator including the decoder, and
wherein the performing the first training process includes training the fraudulent transaction GAN using a fraud-sensitive weighted adversarial loss function based on class probabilities of transactions provided by the second classifier.
12. The method of claim 9, wherein the second classifier is trained using a cross-entropy loss function.
13. The method of claim 9, wherein the second classifier is different from the first classifier.
14. The method of claim 8, wherein the neural network system includes a prior distribution generative adversarial network (GAN) including a generator and a prior distribution discriminator,
wherein the generator including the encoder, and
wherein the performing the first training process includes training the prior distribution GAN using a predetermined prior distribution.
15. A non-transitory machine-readable medium having stored thereon machine-readable instructions executable to cause a machine to perform operations comprising: performing a first training process, using a first training dataset including a plurality of transactions, on a neural network system including an autoencoder including an encoder and a decoder to generate a trained autoencoder,
wherein a trained encoder of the trained autoencoder is configured to receive a first plurality of input data in an N-dimensional data space and generate a first plurality of latent variables in an M-dimensional latent space, wherein M is an integer less than N;
performing a sampling process to the first plurality of latent variables to generate a first plurality of latent variable samples;
generating, using a trained decoder of the trained autoencoder, a second training dataset in the N-dimensional data space using the first plurality of latent variable samples; and
performing a second training process, using the second training dataset, on a first classifier including a first classifier neural network model to generate a trained classifier for providing transaction classification of a first transaction.
16. The non-transitory machine-readable medium of claim 15, wherein the neural network system includes a second classifier including a second classifier neural network model for transaction classification,
wherein the performing the first training process includes:
training the second classifier, using the first training dataset, to generate a trained second classifier; and
wherein the performing the sampling process to the first plurality of latent variables to generate a plurality of latent variable samples includes:
performing a first sub-sampling process, using a first sampler, to generate a second plurality of latent variable samples from the first plurality of latent variables; providing, using the trained second classifier, a plurality of class probabilities corresponding to the second plurality of latent variable samples respectively; and performing a second sub-sampling process, using a second sampler, to generate the first plurality of latent variable samples from the second plurality of latent variable samples based on the plurality of class probabilities.
17. The non-transitory machine-readable medium of claim 16, wherein the second subsampling process includes an adaptive bootstrap sampling process. 18. The non-transitory machine-readable medium of claim 16, wherein the neural network system includes a fraudulent transaction generative adversarial network (GAN) including a fraudulent transaction generator and a fraud discriminator,
wherein the fraud transaction generator including the decoder, and
wherein the performing the first training process includes training the fraudulent transaction GAN using a fraud-sensitive weighted adversarial loss function based on class probabilities of transactions provided by the second classifier.
19. The non-transitory machine-readable medium of claim 16, wherein the second classifier is trained using a cross-entropy loss function.
20. The non-transitory machine-readable medium of claim 16, wherein the second classifier is different from the first classifier.
</claims>
</document>
