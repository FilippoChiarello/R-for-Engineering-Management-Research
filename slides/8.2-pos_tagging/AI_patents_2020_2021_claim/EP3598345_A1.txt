<document>

<filing_date>
2019-07-09
</filing_date>

<publication_date>
2020-01-22
</publication_date>

<priority_date>
2018-07-20
</priority_date>

<ipc_classes>
G06N3/04,G06N3/063,G06N3/08
</ipc_classes>

<assignee>
NXP SEMICONDUCTORS
</assignee>

<inventors>
MICHIELS, WILHELMUS PETRUS ADRIANUS JOHANNUS
DERKS, GERARDUS ANTONIUS FRANCISCUS
</inventors>

<docdb_family_id>
67220687
</docdb_family_id>

<title>
METHOD FOR MAKING A MACHINE LEARNING MODEL MORE DIFFICULT TO COPY
</title>

<abstract>
A method for protecting a machine learning model from copying is provided. The method includes providing a neural network architecture having an input layer, a plurality of hidden layers, and an output layer. Each of the plurality of hidden layers has a plurality of nodes. A neural network application is provided to run on the neural network architecture. First and second types of activation functions are provided. Activation functions including a combination of the first and second types of activation functions are provided to the plurality of nodes of the plurality of hidden layers. The neural network application is trained with a training set to generate a machine learning model. Using the combination of first and second types of activation functions makes it more difficult for an attacker to copy the machine learning model. Also, the neural network application may be implemented in hardware to prevent easy illegitimate upgrading of the neural network application.
</abstract>

<claims>
1. A method comprising: providing a neural network architecture having an input layer, a plurality of hidden layers, and an output layer, each of the plurality of hidden layers having a plurality of nodes; providing a neural network application to run on the neural network architecture; providing a first type of activation function; providing a second type of activation function; assigning a combination of the activation functions of the first and second types to the plurality of nodes; and training the neural network application with a training set to generate a machine learning model.
2. The method of claim 1, wherein the neural network application is implemented in a secure hardware element on an integrated circuit.
3. The method of claim 2, wherein the integrated circuit is characterized as being a field programmable gate array.
4. The method of any preceding claim, further comprising providing a different neural network architecture for implementation in each of a plurality of machine learning devices, wherein the different neural network architectures differ in the plurality of machine learning devices by changing a ratio of the combination of the first and second types of activation functions in the plurality of hidden layers.
5. The method of any preceding claim, wherein the activation function of the first type includes functions providing a plot having at most one increasing slope segment and/or at most one decreasing slope segment.
6. The method of any preceding claim, wherein the activation function of the second type includes functions providing a plot having at least two increasing slope segments and/or at least two decreasing slope segments.
7. The method of any preceding claim, wherein assigning a combination of the activation functions further comprises randomly assigning the mix of the activation functions.
8. The method of any preceding claim, wherein the combination of activation functions is implemented in a secure hardware element in an integrated circuit with the neural network application and wherein the neural network application is not changeable.
9. The method of any preceding claim, wherein the assigning the combination of activation functions further comprises the first and second types of the activation functions being assigned from a set of activation functions comprising tanh(x), rectified linear unit ReLU(x), step(x), Gaussian(x), sigmoid(x) sin(x), and sinc(x), wherein x is an input to a node of the plurality of nodes.
10. The method of any preceding claim, wherein assigning a combination of the activation functions of the first and second types to the plurality of nodes further comprises assigning a sine function as one of the activation functions of the first or the second types, and wherein fewer than fifty percent of the plurality of nodes comprises the sine function.
11. The method of any preceding claim, wherein the activation function of the second type is a sinusoid, and the activation function of the first type is a monotone.
12. A method of protecting a machine learning model from copying comprising the method of any preceding claim.
</claims>
</document>
