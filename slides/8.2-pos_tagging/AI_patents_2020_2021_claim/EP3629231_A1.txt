<document>

<filing_date>
2019-08-27
</filing_date>

<publication_date>
2020-04-01
</publication_date>

<priority_date>
2018-09-28
</priority_date>

<ipc_classes>
G06K9/00
</ipc_classes>

<assignee>
APTIV TECHNOLOGIES
</assignee>

<inventors>
IZZAT, IZZAT H.
ZHENG, YANG
</inventors>

<docdb_family_id>
67770449
</docdb_family_id>

<title>
OBJECT DETECTION SYSTEM OF A VEHICLE
</title>

<abstract>
An object detection system (20) includes color and infrared cameras (28, 30), a controller-circuit (26), and instructions (40). The color and infrared cameras (28, 30) are configured to output respective color image and infrared image signals (34, 36). The controller-circuit (26) is in communication with the cameras (28, 30), and includes a processor (33) and a storage medium (35). The processor (33) is configured to receive and transform the color image and infrared image signals (34, 36) into classification and location data (41, 43) associated with a detected object (39). The instructions (40) are stored in the at least one storage medium (35) and executed by the at least one processor (33), and are configured to utilize the color image and infrared image signals (34, 36) to form respective first and second maps (50A, 50B). The first map (50A) has a first plurality of layers (54A-64A), and the second map (50B) has a second plurality of layers (54B-64B). Selected layers from each are paired and fused to form a feature pyramid (48) that facilitates formulation of the classification and location data (41, 43).
</abstract>

<claims>
1. An object detection system (20) comprising: a color camera (28) configured to render a color image of an area, the color image characterized as indicative of visible light from the area, and outputted by the color camera (28) as a color image signal (34); an infrared camera (30) configured to render an infrared image of the area, the infrared image characterized as indicative infrared light from the area, and outputted by the infrared camera (30) as an infrared image signal (36); and a controller-circuit (26) in communication with the color camera (28) and the infrared camera (30), the controller-circuit (26) including at least one processor (33) and at least one storage medium (35), the at least one processor (33) configured to receive and transform the color image and infrared image signals (34, 36) into classification data (41) and location data (43) associated with a detected object (39); and instructions (40) stored in the at least one storage medium (35) and executed by the at least one processor (33), the instructions (40) configured to utilize the color image and infrared image signals (34, 36) to form respective first and second maps (50A, 50B), the first map (50A) having a first plurality of feature layers (54A-64A) and the second map (50B) having a second plurality of feature layers (54B-64B), and selected feature layers from the first and second maps (50A, 50B) are paired and fused using gated fusion to at least in part form a feature pyramid (48) facilitating the formulation of the classification and location data (41, 43).
2. The object detection system set forth in claim 1, wherein the feature pyramid (48) is of equal dimension to at least one of the color image and the infrared image.
3. The object detection system set forth in claim 2, wherein the object detection system (20) is a pedestrian detection system.
4. The object detection system set forth in claim 2, wherein the infrared camera (30) is a thermal imaging sensor.
5. The object detection system set forth in claim 2, wherein the color and infrared cameras (28, 30) are each at least a part of Deep Neural Network Detectors (DNNDs).
6. The object detection system set forth in claim 5, wherein the first and second maps (50A, 50B) are Single Shot Detectors (SSDs).
7. The object detection system set forth in claim 5, wherein the instructions (40) are associated with at least one gated fusion unit (68-76) configured to fuse selected layers from each one of the first and second maps (50A, 50B) to at least in-part form the feature pyramid (48).
8. The object detection system set forth in claim 7, wherein the gated fusion unit (68-76) applies the following formulas:
9. The object detection system set forth in claim 7, wherein the gated fusion unit (68-76) applies the following formulas:
10. A method of detecting a pedestrian (39), the method comprising: receiving from a visual image sensor (28), a visual image signal (34) indicative of a visual image of an area; receiving from a thermal image sensor (30), a thermal image signal (36) indicative of a thermal image of the area; the transforming the visual image signal (34) and the thermal image signal (36) into classification and localization data (41, 43) by a processor (33) executing instructions (40) that applies at least one gated fusion unit (66-76) to detect the pedestrian (39) in the area.
11. The method set forth in claim 10, further comprising: transforming the visual image signal (34) and thermal image signal (36) into respective first and second plurality of feature layers (54A-64A, 54B-64B) using convolution layers by the instructions (40); pairing at least a portion of the feature layers (54A-64A, 54B-64B) between the first and second plurality of convolution layers; and applying the at least one gated fusion unit (66-76) to the paired layers to generate at least one fused layer pair (78-88).
12. The method set forth in claim 11, further comprising:
outputting the at least one fused layer pairs (78-88) by the at least one gated fusion unit (66-76) to a feature pyramid (48).
13. A controller-circuit (26) adapted to facilitate detection of an object (39), the controller-circuit comprising: at least one processor (33) configured to receive a color image signal (34) and a second image signal (36) respectively received from a color camera (28) and an image device (30), execute executable instructions (40) to transform the color image signal (34) and the second image signal (36) into respective first and second maps (50A, 50B), the first map (50A) having a first plurality of feature layers (54A-64A) and the second map (50B) having a second plurality of feature layers (54B-64B), and selected feature layers from the first and second maps (50A, 50B) are paired and fused using gated fusion to at least in part form a feature pyramid (48) to facilitate the transformation of the color image signal (34) and the second image signal (36) into classification data (41) and location data (43) associated with the detected object (39); and at least one storage medium (35) configured to store the executable instructions (40) retrieved by the at least one processor (33).
14. The controller-circuit set forth in claim 13, wherein the feature pyramid (48) is of equal dimension to at least one of a color image indicative of the color image signal (34) and a second image indicative of the second image signal (36).
15. The controller-circuit set forth in claim 14, wherein the second image is a LiDAR image taken by a LiDAR device as the image device (30).
</claims>
</document>
