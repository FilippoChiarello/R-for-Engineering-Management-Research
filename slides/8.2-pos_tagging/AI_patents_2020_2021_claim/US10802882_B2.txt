<document>

<filing_date>
2018-12-13
</filing_date>

<publication_date>
2020-10-13
</publication_date>

<priority_date>
2018-12-13
</priority_date>

<ipc_classes>
G06F12/0815,G06F12/0875,G06F15/78,G06F9/38,G06F9/48,G06F9/50,G06F9/52,G06F9/54
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
LIU SU
HWANG, INSEOK
LEE, JINHO
ROZNER, ERIC
</inventors>

<docdb_family_id>
71071603
</docdb_family_id>

<title>
Accelerating memory access in a network using thread progress based arbitration
</title>

<abstract>
A method accelerates memory access in a network using thread progress based arbitration. A memory controller identifies a prioritized thread from multiple threads in an application. The prioritized thread reaches a synchronization barrier after the other threads due to the thread encountering more events than the other threads before reaching the barrier, where the events are from a group consisting of instruction executions, cache misses, and load/store operations in a core. The memory controller detects a cache miss by the prioritized thread during execution of the prioritized thread after the barrier is reached by the multiple threads. The memory controller then retrieves and returns data from the memory that cures the cache miss for the prioritized thread before retrieving data that cures cache misses for the other threads by applying thread progress based arbitration in the network.
</abstract>

<claims>
1. A method comprising: identifying, by a memory controller, a first prioritized thread from multiple threads in an application, wherein the first prioritized thread is executing in a first core in a network, wherein the first prioritized thread reaches a first barrier after other threads from the multiple threads, wherein a barrier is a stage at which the multiple threads synchronize, wherein the first prioritized thread reaches the first barrier after the other threads due to the first prioritized thread encountering more events than the other threads before reaching the first barrier, and wherein the events are from a group consisting of instruction executions, cache misses, and load/store operations in a core; detecting, by the memory controller, a cache miss by the first prioritized thread during execution of the first prioritized thread after the first barrier is reached by the multiple threads; retrieving, by the memory controller, data from a memory that cures the cache miss for the first prioritized thread before retrieving data that cures cache misses for the other threads by applying thread progress based arbitration in the network; and transmitting, by the memory controller, data from the memory that cures the cache miss for the first prioritized thread to the first core, wherein retrieving and transmitting the data from the memory that cures the cache miss for the first prioritized thread before retrieving data that cures cache misses for the other threads accelerates memory access for the first prioritized thread by applying the thread progress based arbitration.
2. The method of claim 1, wherein each of the multiple threads uses a separate core in the network.
3. The method of claim 1, wherein the network is a Network On Chip (NOC).
4. The method of claim 3, wherein each node in the NOC comprises an arbiter, and wherein the arbiter prioritizes requests for memory accesses to a memory controller that is associated with the NOC.
5. The method of claim 3, wherein the memory that cures the cache miss for the first prioritized thread is a system memory that supports the NOC.
6. The method of claim 3, wherein the memory that cures the cache miss for the first prioritized thread is a cache memory associated with a second core in the NOC.
7. The method of claim 1, further comprising: counting, by the memory controller, the events that are encountered by each of the multiple threads when reaching the first barrier; storing, by the memory controller, a first event count of the events that are encountered by each of the multiple threads when reaching the first barrier in an event counter; in response to the multiple threads reaching the first barrier, storing, by the memory controller, the first event count, resetting the event counter, and continuing to execute the multiple threads after the first barrier; counting, by the memory controller, the events that are encountered by each of the multiple threads before reaching a second barrier that is after the first barrier; storing a second event count of the events that are encountered by each of the multiple threads before reaching the second barrier in the event counter; calculating, by the memory controller, an event ratio between the first event count and the second event count for each of the multiple threads; identifying, by the memory controller, a second prioritized thread whose event ratio is greater than event ratios of the other threads; detecting, by the memory controller, a cache miss by the second prioritized thread during execution of the second prioritized thread after the first barrier is reached; retrieving, by the memory controller, data from the memory that cures the cache miss for the second prioritized thread after the first barrier is reached before retrieving data that cures cache misses for the other threads after the first barrier is reached; and transmitting, by the memory controller, data from the memory that cures the cache miss for the first prioritized thread to the first core.
8. The method of claim 1, wherein a header in a packet requesting the memory that cures the cache miss for the first prioritized thread includes a thread progress field that describes a quantity of the events that are encountered by the first prioritized thread before reaching the first barrier.
9. A computer program product comprising a computer readable storage medium having program code embodied therewith, wherein the computer readable storage medium is not a transitory signal per se, and wherein the program code is readable and executable by a processor to perform a method comprising: identifying a first prioritized thread from multiple threads in an application, wherein the first prioritized thread is executing in a first core in a network, wherein the first prioritized thread reaches a first barrier after other threads from the multiple threads, wherein a barrier is a stage at which the multiple threads synchronize, wherein the first prioritized thread reaches the first barrier after the other threads due to the first prioritized thread encountering more events than the other threads before reaching the first barrier, and wherein the events are from a group consisting of instruction executions, cache misses, and load/store operations in a core; detecting a cache miss by the first prioritized thread during execution of the first prioritized thread after the first barrier is reached by the multiple threads; retrieving data from a memory that cures the cache miss for the first prioritized thread before retrieving data that cures cache misses for the other threads by applying thread progress based arbitration in the network; and transmitting data from the memory that cures the cache miss for the first prioritized thread to the first core, wherein retrieving and transmitting the data from the memory that cures the cache miss for the first prioritized thread before retrieving data that cures cache misses for the other threads accelerates memory access for the first prioritized thread by applying the thread progress based arbitration.
10. The computer program product of claim 9, wherein the network is a Network On Chip (NOC), and wherein the memory that cures the cache miss for the first prioritized thread is a system memory that supports the NOC.
11. The computer program product of claim 9, wherein the method further comprises: counting the events that are encountered by each of the multiple threads when reaching the first barrier; storing a first event count of the events that are encountered by each of the multiple threads when reaching the first barrier in an event counter; in response to the multiple threads reaching the first barrier, storing the first event count, resetting the event counter, and continuing to execute the multiple threads after the first barrier; counting the events that are encountered by each of the multiple threads before reaching a second barrier that is after the first barrier; storing a second event count of the events that are encountered by each of the multiple threads before reaching the second barrier in the event counter; calculating an event ratio between the first event count and the second event count for each of the multiple threads; identifying a second prioritized thread whose event ratio is greater than event ratios of the other threads; detecting a cache miss by the second prioritized thread during execution of the second prioritized thread after the first barrier is reached; and retrieving data from the memory that cures the cache miss for the second prioritized thread after the first barrier is reached before retrieving data that cures cache misses for the other threads after the first barrier is reached.
12. The computer program product of claim 9, wherein a header in a packet requesting the memory that cures the cache miss for the first prioritized thread includes a thread progress field that describes a quantity of the events that are encountered by the first prioritized thread before reaching the first barrier.
13. The computer program product of claim 9, wherein the program code is provided as a service in a cloud environment.
14. A computer system comprising one or more processors, one or more computer readable memories, and one or more computer readable non-transitory storage mediums, and program instructions stored on at least one of the one or more computer readable non-transitory storage mediums for execution by at least one of the one or more processors via at least one of the one or more computer readable memories, the stored program instructions executed to perform a method comprising: identifying a first prioritized thread from multiple threads in an application, wherein the first prioritized thread is executing in a first core in a network, wherein the first prioritized thread reaches a first barrier after other threads from the multiple threads, wherein a barrier is a stage at which the multiple threads synchronize, wherein the first prioritized thread reaches the first barrier after the other threads due to the first prioritized thread encountering more events than the other threads before reaching the first barrier, and wherein the events are from a group consisting of instruction executions, cache misses, and load/store operations in a core; detecting a cache miss by the first prioritized thread during execution of the first prioritized thread after the first barrier is reached by the multiple threads; retrieving data from a memory that cures the cache miss for the first prioritized thread before retrieving data that cures cache misses for the other threads by applying thread progress based arbitration in the network; and transmitting data from the memory that cures the cache miss for the first prioritized thread to the first core, wherein retrieving and transmitting the data from the memory that cures the cache miss for the first prioritized thread before retrieving data that cures cache misses for the other threads accelerates memory access for the first prioritized thread by applying the thread progress based arbitration.
15. The computer system of claim 14, wherein the network is a Network On Chip (NOC).
16. The computer system of claim 15, wherein each node in the NOC comprises an arbiter, and wherein the arbiter prioritizes requests for memory accesses to a memory controller that is associated with the NOC.
17. The computer system of claim 15, wherein the memory that cures the cache miss for the first prioritized thread is a system memory that supports the NOC.
18. The computer system of claim 14, wherein the method further comprises: counting the events that are encountered by each of the multiple threads when reaching the first barrier; storing a first event count of the events that are encountered by each of the multiple threads when reaching the first barrier in an event counter; in response to the multiple threads reaching the first barrier, storing the first event count, resetting the event counter, and continuing to execute the multiple threads after the first barrier; counting the events that are encountered by each of the multiple threads before reaching a second barrier that is after the first barrier; storing a second event count of the events that are encountered by each of the multiple threads before reaching the second barrier in the event counter; calculating an event ratio between the first event count and the second event count for each of the multiple threads; identifying a second prioritized thread whose event ratio is greater than event ratios of the other threads; detecting a cache miss by the second prioritized thread during execution of the second prioritized thread after the first barrier is reached; and retrieving data from the memory that cures the cache miss for the second prioritized thread after the first barrier is reached before retrieving data that cures cache misses for the other threads after the first barrier is reached.
19. The computer system of claim 14, wherein a header in a packet requesting the memory that cures the cache miss for the first prioritized thread includes a thread progress field that describes a quantity of the events that are encountered by the first prioritized thread before reaching the first barrier.
20. The computer system of claim 14, wherein the stored program instructions are provided as a service in a cloud environment.
</claims>
</document>
