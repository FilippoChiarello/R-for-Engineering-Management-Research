<document>

<filing_date>
2019-03-25
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2019-03-25
</priority_date>

<ipc_classes>
G06F3/0484,G06N20/00,G10L15/16,G10L15/18,G10L15/22,G10L21/10,H04L29/06,H04L29/08
</ipc_classes>

<assignee>
CISCO TECHNOLOGY
</assignee>

<inventors>
NUCCI, ANTONIO
PIGNATARO, CARLOS M.
GOLOUBEW, DMITRY
DASH, GYANA R.
Shao, Qihong
Daga, Pranjal
Feygina, Anastasia
</inventors>

<docdb_family_id>
72606365
</docdb_family_id>

<title>
EXTRACTING KNOWLEDGE FROM COLLABORATIVE SUPPORT SESSIONS
</title>

<abstract>
At a communication server, a first computer device and a second computer device are connected to a collaborative support session configured to support audio communications, screen sharing, and control of the first computer device by the second computer device. Screen sharing video image content is converted to a text sequence with timestamps. A text log with timestamps is generated from the text sequence. Using a command-based machine learning model, a sequence of commands and associated parameters, with timestamps, are determined from the text log. Audio is analyzed to produce speech-based information with timestamps. The command sequence is time-synchronized with the speech-based information based on the timestamps of the command sequence and the timestamps of the speech-based information. A knowledge report for the collaborative support session is generated. The knowledge report includes entries each including a timestamp, commands and associated parameters, and speech-based information that are time-synchronized to the timestamp.
</abstract>

<claims>
1. A method comprising: at a communication server, connecting a first computer device and a second computer device to a collaborative support session configured to support audio communications, screen sharing, and control of the first computer device by the second computer device: converting screen sharing video image content to a text sequence with timestamps; generating from the text sequence a text log with timestamps; using a command-based machine learning model, determining from the text log a command sequence including commands and associated parameters, with command sequence timestamps, that were entered at either the first computer device or the second computer device; analyzing audio associated with the collaborative support session to produce speech-based information with speech-based information timestamps; time-synchronizing the command sequence with the speech-based information based on the command sequence timestamps and the speech-based information timestamps; and generating for the collaborative support session a knowledge report including entries each respectively including a timestamp, one or more of the commands and the associated parameters, and the speech-based information that are time-synchronized to the timestamp.
2. The method of claim 1, further comprising: determining identification information for the first computer device, wherein the using the command-based machine learning model includes determining the command sequence based on the identification information and the text log.
3. The method of claim 2, further comprising: using a mapping database, mapping abbreviations of commands generated by the command-based machine learning model to at least some the commands of the command sequence.
4. The method of claim 2, further comprising: using a domain specific machine learning model on the text log or on intermediate results output by the command-based machine learning model, disambiguating and de-parameterizing variations and abbreviations of commands and associated parameters to produce at least some of the commands and the associated parameters of the command sequence.
5. The method of claim 1, wherein: the analyzing the audio includes: performing speech-to-text conversion of speech in the audio; and using machine learning on the speech to classify the speech into emotion classifications; and the generating includes generating each entry to further include, respectively, a snippet of converted speech-to-text and an emotion classification that are time-synchronized to the timestamp.
6. The method of claim 1, further comprising: recording selections of screen content with screen content timestamps, wherein the generating includes generating each entry to further include, respectively, one of the selections of screen content that is time-synchronized to the timestamp.
7. The method of claim 6, further comprising: identifying edit operations performed on the screen content and associating the edit operations with edit timestamps that indicate when the edit operations occurred, wherein the generating further includes generating each entry to further include, respectively, one of the edit operations that is time-synchronized to the timestamp.
8. The method of claim 1, wherein the converting the screen sharing video content to the text sequence with timestamps includes: periodically extracting image frames from the screen sharing video image content; de-duplicating the image frames to produce de-duplicated image frames; excluding non-text objects from the de-duplicated image frames; performing image segmentation on the image frames resulting from the excluding, to produce segmented image frames; and performing optical character recognition on the segmented image frames to produce the text sequence with the timestamps.
9. The method of claim 1, wherein the generating from the text sequence the text log with timestamps includes: joining multiple frame text segments of the text sequence into multiple text lines using a distance-based algorithm; sliding the multiple text lines across each other to find a common text overlay; and generating a combined text line that combines the multiple text lines without repeating the common text overlay.
10. The method of claim 1, wherein the first computer device is a customer computer device and the second computer device is a support specialist computer device, and the collaborative support session includes a troubleshooting session to troubleshoot the customer computer device or customer equipment accessible through the customer computer device via the support specialist computer device.
11. The method of claim 1, wherein the text log includes the commands, the associated parameters, and command output responsive to the commands, and the generating includes generating the knowledge report including the entries such that the entries further include the command output.
12. An apparatus comprising: a network interface to communicate with a network; and a processor of a communication server coupled to the network interface and configured to perform operations including: connecting a first computer device and a second computer device to a collaborative support session configured to support audio communications, screen sharing, and control of the first computer device by the second computer device; converting screen sharing video image content to a text sequence with timestamps; generating from the text sequence a text log with timestamps; using a command-based machine learning model, determining from the text log a command sequence including commands and associated parameters, with command sequence timestamps, that were entered at either the first computer device or the second computer device; analyzing audio associated with the collaborative support session to produce speech-based information with speech-based information timestamps; time-synchronizing the command sequence with the speech-based information based on the command sequence timestamps and the speech-based information timestamps; and generating for the collaborative support session a knowledge report including entries each respectively including a timestamp, one or more of the commands and the associated parameters, and the speech-based information that are time-synchronized to the timestamp.
13. The apparatus of claim 12, wherein the operations further include: determining identification information for the first computer device, wherein the using the command-based machine learning model includes determining the command sequence based on the identification information and the text log.
14. The apparatus of claim 13, wherein the operations further include: using a domain specific machine learning model on the text log or on intermediate results output by the command-based machine learning model, disambiguating and de-parameterizing variations and abbreviations of commands and associated parameters to produce at least some of the commands and the associated parameters of the command sequence.
15. The apparatus of claim 12, wherein: the analyzing the audio includes: performing speech-to-text conversion of speech in the audio; and using machine learning on the speech to classify the speech into emotion classifications; and the generating includes generating each entry to further include, respectively, a snippet of the converted speech-to-text and an emotion classification that are time-synchronized to the timestamp.
16. The apparatus of claim 12, wherein the operations further include: recording selections of screen content with screen content timestamps, wherein the generating includes generating each entry to further include, respectively, one of the selections of screen content that is time-synchronized to the timestamp.
17. The apparatus of claim 16, wherein the operations further include: identifying edit operations performed on the screen content and associating the edit operations with edit timestamps that indicate when the edit operations occurred, wherein the generating further includes generating each entry to further include, respectively, one of the edit operations that is time-synchronized to the timestamp.
18. A non-transitory computer readable medium encoded with instructions that, when executed by a processor, cause the processor to perform: connecting a first computer device and a second computer device to a collaborative support session configured to support audio communications, screen sharing, and control of the first computer device by the second computer device; converting screen sharing video image content to a text sequence with timestamps; generating from the text sequence a text log with timestamps; using a command-based machine learning model, determining from the text log a command sequence including commands and associated parameters, with command sequence timestamps, that were entered at either the first computer device or the second computer device; analyzing audio associated with the collaborative support session to produce speech-based information with speech-based information timestamps; time-synchronizing the command sequence with the speech-based information based on the command sequence timestamps and the speech-based information timestamps; and generating for the collaborative support session a knowledge report including entries each respectively including a timestamp, one or more of the commands and the associated parameters, and the speech-based information that are time-synchronized to the timestamp.
19. The non-transitory computer readable medium of claim 18, further comprising instructions to cause the processor to perform: determining identification information for the first computer device, wherein the instructions to cause the processor to perform the using the command-based machine learning model include instructions to cause the processor to perform determining the command sequence based on the identification information and the text log.
20. The non-transitory computer readable medium of claim 19, further comprising instructions to cause the processor to perform: using a domain specific machine learning model on the text log or on intermediate results output by the command-based machine learning model, disambiguating and de-parameterizing variations and abbreviations of commands and associated parameters to produce at least some of the commands and the associated parameters of the command sequence.
</claims>
</document>
