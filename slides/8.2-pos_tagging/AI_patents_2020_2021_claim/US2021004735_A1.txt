<document>

<filing_date>
2019-03-20
</filing_date>

<publication_date>
2021-01-07
</publication_date>

<priority_date>
2018-03-22
</priority_date>

<ipc_classes>
G06K9/62,G06N20/00,G06Q10/06
</ipc_classes>

<assignee>
SIEMENS
</assignee>

<inventors>
SRIVASTAVA, SANJEEV
Chalupka, Krzysztof
</inventors>

<docdb_family_id>
66049709
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR COLLABORATIVE DECENTRALIZED PLANNING USING DEEP REINFORCEMENT LEARNING AGENTS IN AN ASYNCHRONOUS ENVIRONMENT
</title>

<abstract>
A method, and corresponding systems and computer-readable mediums, for implementing a hierarchical multi-agent control system for an environment. A method includes generating an observation of an environment by a first agent process and sending a first message that includes the observation to a meta-agent process. The method includes receiving a second message that includes a goal, by the first agent process and from the meta-agent process. The method includes evaluating a plurality of actions, by the first agent process and based on the goal, to determine a selected action. The method includes applying the selected action to the environment by the first agent process.
</abstract>

<claims>
1. A method executed by one or more data processing systems, comprising: generating an observation of an environment by a first agent process; sending a first message that includes the observation, by the first agent process and to a meta-agent process; receiving a second message that includes a goal, by the first agent process and from the meta-agent process; evaluating a plurality of actions, by the first agent process and based on the goal, to determine a selected action; and applying the selected action to the environment by the first agent process.
2. The method of claim 1, wherein the meta-agent process executes on a higher hierarchical level than the first agent process and is configured to communicate with and direct a plurality of agent processes including the first agent process.
3. The method of claim 1, wherein the observation is a partial observation that is associated with only a portion of the environment.
4. The method of claim 1, wherein the first agent process is a reinforcement-learning agent.
5. The method of claim 1, wherein the meta-agent process is a reinforcement-learning agent.
6. The method of claim 1, wherein the meta-agent process defines the goal based on the observation and a global policy.
7. The method of claim 1, wherein the evaluation is also based on one or more local policies.
8. The method of claim 1, wherein the evaluation includes determining a predicted result and associated reward for each of the plurality of actions, and the selected action is the action with the greatest associated reward.
9. The method of claim 1, wherein the evaluation is performed by using a controller process to formulate the plurality of actions and using a critic process to identify a reward value associated with each of the plurality of actions.
10. The method of claim 1, wherein the environment is physical hardware being monitored and controlled by at least the first agent process.
11. The method of claim 1, wherein the environment is one of a computer system, an electrical, plumbing, or air system, a heating, ventilation, and air conditioning system, a manufacturing system, a mail processing system, or a product transportation, sorting, or processing system.
12. The method of claim 1, wherein the first agent process is one of a plurality of agent processes each configured to communicate with and be assigned goals by the meta-agent process.
13. The method of claim 1, wherein the first agent process is one of a plurality of agent processes each configured to communicate with the meta-agent process and each of the other agent processes.
14. A data processing system comprising at least a processor and accessible memory, configured to perform a method as in claim 1.
15. A non-transitory computer-readable medium encoded with executable instructions that, when executed, cause a data processing system to perform a method as in claim 1.
</claims>
</document>
