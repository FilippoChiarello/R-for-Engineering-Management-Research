<document>

<filing_date>
2019-12-16
</filing_date>

<publication_date>
2020-06-25
</publication_date>

<priority_date>
2018-12-19
</priority_date>

<ipc_classes>
G06K9/00,G06K9/62
</ipc_classes>

<assignee>
FCA US
</assignee>

<inventors>
HORTON, STEPHEN
WANG, ZIJIAN
</inventors>

<docdb_family_id>
69173412
</docdb_family_id>

<title>
TECHNIQUES FOR PRECISELY LOCATING LANDMARKS IN MONOCULAR CAMERA IMAGES WITH DEEP LEARNING
</title>

<abstract>
Advanced driver assistance (ADAS) systems and methods for a vehicle comprise capturing an image using a monocular camera system of the vehicle and detecting a landmark in the image using a deep neural network (DNN) trained with labeled training data including generating a bounding box for the detected landmark, predicting a depth of pixels in the image using a convolutional neural network (CNN) trained with unlabeled training data captured by a stereo camera system, filtering noise by averaging predicted pixel depths for pixels in the region of the bounding box to obtain an average depth value for the detected landmark, determining a coordinate position of the detected landmark using its average depth value, and performing at least one ADAS feature using the determined coordinate position of the detected landmark.
</abstract>

<claims>
What is claimed is:
1. An advanced driver assistance system (ADAS) for a vehicle, the ADAS comprising:
a monocular camera system configured to capture an image; and a controller configured to:
receive the image;
detect a landmark in the image using a deep neural network (DNN) trained with labeled training data including generating a bounding box for the detected landmark;
predict a depth of pixels in the image using a convolutional neural network (CNN) trained with unlabeled training data captured by a stereo camera system;
filter noise by averaging predicted pixel depths for pixels in the region of the bounding box to obtain an average depth value for the detected landmark;
determine a coordinate position of the detected landmark using its average depth value; and
perform at least one ADAS feature using the determined coordinate position of the detected landmark.
2. The ADAS of claim 1 , wherein the determined coordinate position of the detected landmark is a polar coordinate position comprising the average depth value and an angle from the monocular camera system to a center of the detected landmark.
3. The ADAS of claim 2, wherein the ADAS feature comprises localizing a position of the vehicle on a global positioning system (GPS) map to the determined polar coordinate position.
4. The ADAS of claim 1 , wherein the ADAS feature comprises collision avoidance during an autonomous driving mode.
5. The ADAS of claim 1 , wherein the averaging of the predicted pixel depths in the region of the bounding box comprises applying an averaging filter.
6. The ADAS of claim 1 , wherein the CNN is configured to predict a depth of every pixel of the image.
7. The ADAS of claim 1 , wherein the unlabeled training data comprises pairs of simultaneously captured left and right images.
8. The ADAS of claim 1 , wherein the controller is further configured to perform vehicle-to-vehicle transfer learning to further train the DNN.
9. The ADAS of claim 1 , wherein the labeled training data comprises a plurality of images of different types of landmarks that are manually labeled by a human annotator.
10. The ADAS of claim 1 , wherein none of a stereo camera system, a kinetic camera system, and a light detection and ranging (LIDAR) system are used by the controller to predict the depth of the detected landmark.
11. A method of detecting a landmark from an image captured by a monocular camera system of a vehicle, the method comprising:
capturing, by the monocular camera system, the image;
receiving, by a controller of the vehicle, the image;
detecting, by the controller, a landmark in the image using a deep neural network (DNN) trained with labeled training data including generating a bounding box for the detected landmark;
predicting, by the controller, a depth of pixels in the image using a convolutional neural network (CNN) trained with unlabeled training data captured by a stereo camera system;
filtering, by the controller, noise by averaging predicted pixel depths for pixels in the region of the bounding box to obtain an average depth value for the detected landmark;
determining, by the controller, a coordinate position of the detected landmark using its average depth value; and
performing, by the controller, at least one advanced driver assistance system (ADAS) feature using the determined coordinate position of the detected landmark.
12. The method of claim 11 , wherein the determined coordinate position of the detected landmark is a polar coordinate position comprising the average depth value and an angle from the monocular camera system to a center of the detected landmark.
13. The method of claim 12, wherein the ADAS feature comprises localizing a position of the vehicle on a global positioning system (GPS) map to the determined polar coordinate position.
14. The method of claim 1 1 , wherein the ADAS feature comprises collision avoidance during an autonomous driving mode.
15. The method of claim 1 1 , wherein the averaging of the predicted pixel depths in the region of the bounding box comprises applying an averaging filter.
16. The method of claim 1 1 , wherein the CNN is configured to predict a depth of every pixel of the image.
17. The method of claim 1 1 , wherein the unlabeled training data comprises pairs of simultaneously captured left and right images.
18. The method of claim 11 , further comprising performing, by the controller, vehicle-to-vehicle transfer learning to further train the DNN.
19. The method of claim 11 , wherein the labeled training data comprises a plurality of images of different types of landmarks that are manually labeled by a human annotator.
20. The method of claim 11 , wherein none of a stereo camera system, a kinetic camera system, and a light detection and ranging (LIDAR) system are used by the controller to predict the depth of the detected landmark.
</claims>
</document>
