<document>

<filing_date>
2019-09-27
</filing_date>

<publication_date>
2020-04-02
</publication_date>

<priority_date>
2018-09-27
</priority_date>

<ipc_classes>
G06N3/00,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
DEEPMIND TECHNOLOGIES
</assignee>

<inventors>
POOLE, BENJAMIN
RAZAVI-NEMATOLLAHI, ALI
VAN DEN OORD, AÄRON GERARD ANTONIUS
VINYALS, ORIOL
</inventors>

<docdb_family_id>
68136365
</docdb_family_id>

<title>
COMMITTED INFORMATION RATE VARIATIONAL AUTOENCODERS
</title>

<abstract>
A variational autoencoder (VAE) neural network system, comprising an encoder neural network to encode an input data item to define a posterior distribution for a set of latent variables, and a decoder neural network to generate an output data item representing values of a set of latent variables sampled from the posterior distribution. The system is configured for training with an objective function including a term dependent on a difference between the posterior distribution and a prior distribution. The prior and posterior distributions are arranged so that they cannot be matched to one another. The VAE system may be used for compressing and decompressing data.
</abstract>

<claims>
1. A variational autoencoder neural network system, comprising:
an input to receive an input data item;
an encoder neural network configured to encode the input data item to determine a set of parameters defining a first, posterior distribution of a set of latent variables;
a subsystem to sample from the posterior distribution to determine values of the set of latent variables;
a decoder neural network configured to receive the values of the set of latent variables and to generate an output data item representing the values of the set of latent variables; wherein the variational autoencoder neural network system is configured for training with an objective function which has a first term dependent upon a difference between the input data item and the output data item and a second term dependent upon a difference between the posterior distribution and a second, prior distribution of the set of latent variables, and wherein a structure of the prior distribution is different to a structure of the posterior distribution such that the posterior distribution cannot be matched to the prior distribution.
2. A variational autoencoder neural network system as claimed in claim 1 wherein the posterior distribution and the prior distribution each comprise a multivariate Gaussian distribution and wherein a variance of the posterior distribution is a factor of a different to a variance of the prior distribution, where a ¹ 1.
3. A variational autoencoder neural network system as claimed in claim 1 or 2 wherein the encoder is configured to determine a sequence of sets of parameters defining a sequence of distributions for a sequence of sets of latent variables, one for each of a plurality of time steps.
4. A variational autoencoder neural network system as claimed in claim 3 wherein the prior distribution comprises an autoregressive distribution such that at each time step the prior distribution depends on the prior distribution at a previous time step.
5. A variational autoencoder neural network system as claimed in claim 4 wherein the values of the set of latent variables at a time step t, are defined by a sum of a times the values of the set of latent variables at a previous time step and a noise component, where \ a \ < 1.
6. A variational autoencoder neural network system as claimed in claim 3, 4 or 5 wherein the decoder neural network is an autoregressive neural network configured to generate a sequence of output data item values each conditional upon previously generated output data item values; and further comprising a system to restrict the values of the set of latent variables passed to the decoder at each time step to those which encode information about values in the sequence of output data values yet to be generated.
7. A variational autoencoder neural network system as claimed in any one of claims 3-6 further comprising an auxiliary neural network configured to learn the sequence of distributions for the sequence of sets of latent variables.
8. The trained decoder neural network of any one of claims 1-7.
9. The trained encoder neural network of any one of claims 1-7.
10. A method of training a variational autoencoder neural network system as recited in any one of claims 1-7, comprising:
receiving training data, the training data comprising training data items;
providing each training data item to the input of the variational autoencoder neural network system to generate a corresponding output data item;
determining a gradient of the objective function from a difference between the training data item and the corresponding output data item and from a difference between the posterior distribution and the prior distribution of the set of latent variables; and
backpropagating the gradient through the variational autoencoder neural network system to adjust parameters of the encoder neural network and of the decoder neural network to optimize the objective function.
11. A method as claimed in claim 10 when dependent upon claim 7 wherein providing each training data item to the input of the variational autoencoder neural network system to generate a corresponding output data item comprises sampling from the posterior distribution to determine sampled values of the set of latent variables; the method further comprising training the auxiliary neural network concurrently with the encoder neural network and decoder neural network using the sampled values of the set of latent variables.
12. A method of obtaining a decoder, the method comprising training a variational autoencoder neural network system as recited in claim 10 or 11 and then using the decoder neural network of the trained variational autoencoder neural network system to obtain the decoder.
13. A method of obtaining an encoder, the method comprising training a variational autoencoder neural network system as recited in claim 10 or 11 and then using the encoder neural network of the trained variational autoencoder neural network system to obtain the encoder.
14. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to perform the operations of the respective method of any one of claims 10-13.
15. A system or method as recited in any preceding claim wherein the data item comprises one or more of: a still or moving image, an audio signal, and a representation of a text string.
16. One or more computer storage media storing instructions that when executed by one or more computers cause the one or more computers to implement the system of any one of claims 1-9 or to perform the operations of the method of any one of claims 10-13.
</claims>
</document>
