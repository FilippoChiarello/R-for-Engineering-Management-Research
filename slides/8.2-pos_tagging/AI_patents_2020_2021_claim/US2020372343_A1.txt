<document>

<filing_date>
2018-11-21
</filing_date>

<publication_date>
2020-11-26
</publication_date>

<priority_date>
2018-09-11
</priority_date>

<ipc_classes>
G06K9/62,G06N3/08,G06T1/20
</ipc_classes>

<assignee>
SONY CORPORATION
</assignee>

<inventors>
MIKAMI, Hiroaki
</inventors>

<docdb_family_id>
65442963
</docdb_family_id>

<title>
INFORMATION PROCESSING APPARATUS AND INFORMATION PROCESSING METHOD
</title>

<abstract>
[Solution] An information processing apparatus is provided, which includes a learning unit that executes learning using a neural network, and the learning unit dynamically changes the value of the batch size during learning based on the gap value with the ideal state for learning output from the neural network. Furthermore, an information processing method is provided, which includes executing, by a processor, learning using a neural network, and the learning further includes dynamically changing the value of the batch size during learning based on the gap value with the ideal state for learning output from the neural network.
</abstract>

<claims>
1. An information processing apparatus comprising a learning unit that executes learning using a neural network, wherein the learning unit dynamically changes a value of a batch size during learning based on a gap value with an ideal state for learning output from the neural network.
2. The information processing apparatus according to claim 1, wherein the gap value with the ideal state includes at least a loss.
3. The information processing apparatus according to claim 2, wherein the learning unit increases the value of the batch size during learning when convergence of learning is expected based on the loss.
4. The information processing apparatus according to claim 3, wherein the learning unit increases the value of the batch size during learning based on an n-th differential value of the loss.
5. The information processing apparatus according to claim 4, wherein the learning unit increases the value of the batch size during learning based on whether at least any of the value of the loss and a gradient of the loss falls below a threshold.
6. The information processing apparatus according to claim 2, wherein the learning unit decreases the value of the batch size during learning when divergence of learning is expected based on the loss.
7. The information processing apparatus according to claim 1, wherein the learning unit dynamically changes the value of the batch size based on an epoch.
8. The information processing apparatus according to claim 7, wherein the learning unit increases the value of the batch size due to progress of epochs.
9. The information processing apparatus according to claim 8, wherein, when divergence of learning is expected based on the gap value with the ideal state, the learning unit reloads a network model in a previous epoch.
10. The information processing apparatus according to claim 9, wherein, when the network model in the previous epoch is reloaded, the learning unit sets a value of the batch size smaller than a value set in the previous epoch.
11. The information processing apparatus according to claim 7, wherein the learning unit increases the value of the batch size in each epoch.
12. The information processing apparatus according to claim 1, further comprising a batch size change unit that controls an increase or decrease in the batch size based on a value set by the learning unit.
13. The information processing apparatus according to claim 12, wherein the batch size change unit reconstructs a model in a GPU to control an increase or decrease in the batch size.
14. The information processing apparatus according to claim 12, wherein the batch size change unit increases or decreases a number of calculation loops for learning to control an increase or decrease in the batch size.
15. The information processing apparatus according to claim 12, wherein the batch size change unit increases or decreases a number of GPUs used for learning to control an increase or decrease in the batch size.
16. The information processing apparatus according to claim 12, wherein, when there is an additionally available GPU, the batch size change unit allocates the GPU to learning to control an increase in the batch size.
17. The information processing apparatus according to claim 12, wherein, when there is no additionally available GPU and there is a free space in a memory of a currently used GPU, the batch size change unit reconstructs a model in the currently used GPU to control an increase in the batch size.
18. The information processing apparatus according to claim 12, wherein, when there is no free space in a memory of a currently used GPU, the batch size change unit increases a number of calculation loops for learning to control an increase in the batch size.
19. The information processing apparatus according to claim 1, wherein the gap value with the ideal state includes at least any of a training error and a validation error.
20. An information processing method comprising executing, by a processor, learning using a neural network, wherein the learning further includes dynamically changing a value of a batch size during learning based on a gap value with an ideal state for learning output from the neural network.
</claims>
</document>
