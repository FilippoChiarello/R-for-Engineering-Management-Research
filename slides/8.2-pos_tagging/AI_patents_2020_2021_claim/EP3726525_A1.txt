<document>

<filing_date>
2018-11-07
</filing_date>

<publication_date>
2020-10-21
</publication_date>

<priority_date>
2017-12-14
</priority_date>

<ipc_classes>
G06F40/30,G06F40/35,G06N3/04,G06N3/08,G10L15/04,G10L15/16,G10L15/18
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
KIM, Jun Seong
</inventors>

<docdb_family_id>
66819408
</docdb_family_id>

<title>
ELECTRONIC DEVICE FOR ANALYZING MEANING OF SPEECH, AND OPERATION METHOD THEREFOR
</title>

<abstract>
An electronic device using an artificial neural network model including an attention mechanism, according to various embodiments, can comprise: a memory configured to store information including a plurality of recurrent neural network (RNN) layers; and at least one processor connected with the memory and configured to set, as a first key and a value, at least one first hidden representation acquired through at least one layer among the plurality of RNN layers, set, as a second key, at least one second hidden representation acquired through at least one second layer among the plurality of RNN layers, and acquire an attention included in an attention structure at least on the basis of data on the first key, data on the second key, or data on the value.
</abstract>

<claims>
1. An electronic device using an artificial neural network model comprising an attention mechanism, the electronic device comprising: a memory configured to store information comprising a plurality of recurrent neural network (RNN) layers; and at least one processor connected with the memory, wherein the at least one processor is configured to: set, as a first key and a value, at least one first hidden representation obtained through at least one first layer from among the plurality of RNN layers; set, as a second key, at least one second hidden representation obtained through at least one second layer from among the plurality of RNN layers; and based at least on data on the first key, data on the second key, or data on the value, obtain an attention included in the attention mechanism.
2. The electronic device of claim 1, wherein the at least one first layer is positioned at a lower stage than the at least one second layer in the artificial neural network, and
wherein the at least one processor is configured to obtain the at least one second hidden representation from the at least one first hidden representation through the at least one second layer.
3. The electronic device of claim 2, wherein the at least one processor is configured to obtain, from the attention, data on a slot or data on an intent, through at least one third layer positioned at a higher stage than the at least one second layer from among the plurality of RNN layers.
4. The electronic device of claim 1, wherein the at least one processor is configured to obtain data on a weight of each of the first key and the second key, based on the data on the first key and the data on the second key, and to obtain the attention based on a weighted sum of the data on the weight and the data on the value.
5. The electronic device of claim 1, wherein the at least one processor is configured to obtain the attention based on the following equation: where i indicates a step of decoding, j indicate an index of an input regarding the RNN, Tx indicates a length of an input sequence, si-1 indicates a decoder state at the i-th step of decoding, LAi indicates an attention regarding the i-th step of decoding, a indicates a neural network, αij indicates a weight regarding si-1, which is obtained by a, indicates the at least one first hidden presentation, and indicates the at least one second hidden representation.
6. The electronic device of claim 5, wherein the at least one processor is configured to: obtain data on a third key in which positional encoding is applied to the at least one first hidden representation; obtain data on a fourth key in which the positional encoding is applied to the at least one second hidden representation; and obtain the attention based at least on the third key and the fourth key.
7. The electronic device of claim 1, wherein training a language model regarding the RNN, training a slot filling model regarding the RNN, and training an intent detection model regarding the RNN share at least one of at least one layer of the plurality of RNN layers, the at least one first hidden representation, or the at least one second hidden representation, and
wherein a cost for the RNN is obtained based at least on multiplication of data indicating a cost regarding the language model by a weight-decay term.
8. A non-transitory computer-readable storage medium, which stores one or more programs for storing information comprising a plurality of recurrent neural network (RNN) layers, and for executing: setting, as a first key and a value, at least one first hidden representation obtained through at least one first layer from among the plurality of RNN layers; setting, as a second key, at least one second hidden representation obtained through at least one second layer from among the plurality of RNN layers; and, based at least on data on the first key, data on the second key, or data on the value, obtaining an attention included in an attention mechanism.
9. The non-transitory computer-readable storage medium of claim 8, wherein the at least one first layer is positioned at a lower stage than the at least one second layer in the artificial neural network, and
wherein the non-transitory computer-readable storage medium further stores one or more programs for executing obtaining the at least one second hidden representation from the at least one first hidden representation through the at least one second layer.
10. The non-transitory computer-readable storage medium of claim 9, further storing one or more programs for executing obtaining, from the attention, data on a slot or data on an intent, through at least one third layer positioned at a higher stage than the at least one second layer from among the plurality of RNN layers.
11. The non-transitory computer-readable storage medium of claim 8, further storing one or more programs for executing obtaining data on a weight of each of the first key and the second key, based on the data on the first key and the data on the second key, and obtaining the attention based on a weighted sum of the data on the weight and the data on the value.
12. The non-transitory computer-readable storage medium of claim 8, further storing one or more programs for executing obtaining the attention based on the following equation: where i indicates a step of decoding, j indicate an index of an input regarding the RNN, Tx indicates a length of an input sequence, si-1 indicates a decoder state at the i-th step of decoding, LAi indicates an attention regarding the i-th step of decoding, a indicates a neural network, αij indicates a weight regarding si-1, which is obtained by a, indicates the at least one first hidden presentation, and indicates the at least one second hidden representation.
13. The non-transitory computer-readable storage medium of claim 12, further storing one or more programs for executing: obtaining data on a third key in which positional encoding is applied to the at least one first hidden representation; obtaining data on a fourth key in which the positional encoding is applied to the at least one second hidden representation; and obtaining obtain the attention based at least on the third key and the fourth key.
14. The non-transitory computer-readable storage medium of claim 8, wherein training a language model regarding the RNN, training a slot filling model regarding the RNN, and training an intent detection model regarding the RNN share at least one of at least one layer of the plurality of RNN layers, the at least one first hidden representation, or the at least one second hidden representation, and
wherein a cost for the RNN is obtained based at least on multiplication of data indicating a cost regarding the language model by a weight-decay term.
15. An operation method of an electronic device using an artificial neural network model comprising an attention mechanism, the electronic device storing information comprising a plurality of recurrent neural network (RNN) layers, the method comprising: setting, as a first key and a value, at least one first hidden representation obtained through at least one first layer from among the plurality of RNN layers; setting, as a second key, at least one second hidden representation obtained through at least one second layer from among the plurality of RNN layers; and based at least on data on the first key, data on the second key, or data on the value, obtaining an attention included in the attention mechanism.
</claims>
</document>
