<document>

<filing_date>
2018-12-10
</filing_date>

<publication_date>
2020-06-11
</publication_date>

<priority_date>
2018-12-10
</priority_date>

<ipc_classes>
G01C21/36,G05D1/02,G06K9/00,G06K9/62,G06K9/72,G06T7/174,G06T7/70
</ipc_classes>

<assignee>
HERE GLOBAL
</assignee>

<inventors>
VISWANATHAN, ANIRUDH
</inventors>

<docdb_family_id>
68835112
</docdb_family_id>

<title>
METHOD AND APPARATUS FOR CREATING A VISUAL MAP WITHOUT DYNAMIC CONTENT
</title>

<abstract>
Methods described herein relate to creating a visual map of an environment free of scene clutter. Methods may include: receiving sensor data from at least one image sensor, where the sensor data is representative of a plurality of images, each image representative of a scene at a scene location; processing each image using semantic scene segmentation to identify segments of the image of the scene; classifying the segments of each of the images of the scene into one of static elements or dynamic elements; generating a decluttered image of the scene, where the decluttered image includes only elements classified as static elements; providing for storage of the decluttered image of the scene in a database; and identifying a location of a device as the scene location in response to sensor data from the device corresponding to the decluttered image of the scene.
</abstract>

<claims>
That which is claimed:
1. An apparatus comprising at least one processor and at least one non-transitory memory including computer program code instructions, the computer program code instructions configured to, when executed, cause the apparatus to at least: receive sensor data from at least one image sensor, wherein the sensor data is representative of a plurality of images, each image representative of a scene at a scene location; process each image using semantic scene segmentation to identify segments of the image of the scene; classify the segments of each of the images of the scene into one of static elements or dynamic elements; generate a decluttered image of the scene, wherein the decluttered image comprises only elements classified as static elements; provide for storage of the decluttered image of the scene in a database; and identify a location of a device as the scene location in response to sensor data from the device corresponding to the decluttered image of the scene.
2. The apparatus of claim 1, wherein causing the apparatus to process each image using semantic scene segmentation to identify segments of the image of the scene comprises causing the apparatus to process each image using a neural network that performs semantic scene segmentation to identify segments of the image of the scene.
3. The apparatus of claim 1, wherein causing the apparatus to generate a decluttered image of the scene comprises causing the apparatus to: process segmented images using a general adversarial network to hallucinate image contents of the scene; and constrain a geometry of the decluttered image of the scene based on the static elements of the scene.
4. The apparatus of claim 1, where causing the apparatus to identify a location of a device as the scene location comprises causing the apparatus to: receive sensor data from the device representative of a scene; correlate the received sensor data with the stored decluttered image of the scene; and identify a location of the device as the scene location.
5. The apparatus of claim 4, wherein the device comprises an autonomous vehicle, and wherein, in response to identifying the location of the device as the scene location, causing the apparatus to provide for autonomous control of the autonomous vehicle based on the identified location of the autonomous vehicle.
6. The apparatus of claim 4, wherein the device comprises a vehicle, and wherein, in response to identifying the location of the device as the scene location, causing the apparatus to provide for navigational assistance to the vehicle based on the identified location of the vehicle.
7. The apparatus of claim 1, wherein causing the apparatus to classify the segments of each of the images of the scene into one of static elements or dynamic elements comprises causing the apparatus to analyze distinctions between images of the scene and classifying elements that change between images as dynamic elements.
8. A computer program product comprising at least one non-transitory computer-readable storage medium having computer-executable program code instructions stored therein, the computer-executable program code instructions comprising program code instructions to: receive sensor data from at least one image sensor, wherein the sensor data is representative of a plurality of images, each image representative of a scene at a scene location; process each image using semantic scene segmentation to identify segments of the image of the scene; classify the segments of each of the images of the scene into one of static elements or dynamic elements; generate a decluttered image of the scene, wherein the decluttered image comprises only elements classified as static elements; provide for storage of the decluttered image of the scene in a database; and identify a location of a device as the scene location in response to sensor data from the device corresponding to the decluttered image of the scene.
9. The computer program product of claim 8, wherein the program code instructions to process each image using semantic scene segmentation to identify segments of the image of the scene comprise program code instructions to process each image using a neural network that performs semantic scene segmentation to identify segments of the image of the scene.
10. The computer program product of claim 8, wherein the program code instructions to generate a decluttered image of the scene comprise program code instructions to: process segmented images using a general adversarial network to hallucinate image contents of the scene; and constrain a geometry of the decluttered image of the scene based on the static elements of the scene.
11. The computer program product of claim 8, where the program code instructions to identify a location of a device as the scene location comprise the program code instructions to: receive sensor data from the device representative of a scene; correlate the received sensor data with the stored decluttered image of the scene; and identify a location of the device as the scene location.
12. The computer program product of claim 11, wherein the device comprises an autonomous vehicle, and wherein, in response to identifying the location of the device as the scene location, providing for autonomous control of the autonomous vehicle based on the identified location of the autonomous vehicle.
13. The computer program product of claim 11, wherein the device comprises a vehicle, and wherein, in response to identifying the location of the device as the scene location, providing for navigational assistance to the vehicle based on the identified location of the vehicle.
14. The computer program product of claim 8, wherein the program code instructions to classify the segments of each of the images of the scene into one of static elements or dynamic elements comprise program code instructions to analyze distinctions between images of the scene and classify elements that change between images as dynamic elements.
15. A method comprising: receiving sensor data from at least one image sensor, wherein the sensor data is representative of a plurality of images, each image representative of a scene at a scene location; processing each image using semantic scene segmentation to identify segments of the image of the scene; classifying the segments of each of the images of the scene into one of static elements or dynamic elements; generating a decluttered image of the scene, wherein the decluttered image comprises only elements classified as static elements; providing for storage of the decluttered image of the scene in a database; and identifying a location of a device as the scene location in response to sensor data from the device corresponding to the decluttered image of the scene.
16. The method of claim 15, wherein processing each image using semantic scene segmentation to identify segments of the image of the scene comprises processing each image using a neural network that performs semantic scene segmentation to identify segments of the image of the scene.
17. The method of claim 15, wherein generating a decluttered image of the scene comprises: processing segmented images using a general adversarial network to hallucinate image contents of the scene; and constraining a geometry of the decluttered image of the scene based on the static elements of the scene.
18. The method of claim 15, wherein identifying a location of a device as the scene location comprises: receiving sensor data from the device representative of a scene; correlating the received sensor data with the stored decluttered image of the scene; and identifying a location of the device as the scene location.
19. The method of claim 18, wherein the device comprises an autonomous vehicle, and wherein, in response to identifying the location of the device as the scene location, providing for autonomous control of the autonomous vehicle based on the identified location of the autonomous vehicle.
20. The method of claim 18, wherein the device comprises a vehicle, and wherein, in response to identifying the location of the device as the scene location, providing for navigational assistance to the vehicle based on the identified location of the vehicle.
</claims>
</document>
