<document>

<filing_date>
2019-07-17
</filing_date>

<publication_date>
2021-01-20
</publication_date>

<priority_date>
2019-07-17
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
ROBERT BOSCH
</assignee>

<inventors>
Straehle, Christoph-Nikolas
Bhattacharyya, Apratim
</inventors>

<docdb_family_id>
67402856
</docdb_family_id>

<title>
A MACHINE LEARNABLE SYSTEM WITH CONDITIONAL NORMALIZING FLOW
</title>

<abstract>
Some embodiments are directed to a machine learnable system (110). A conditional normalizing flow function maps a latent representation (z) to a base point (e=f(z,c)) in a base space (E) conditional on conditioning data (c). The conditional normalizing flow function is a machine learnable function and trained on a set of training pairs.
</abstract>

<claims>
1. A machine learnable system, the system comprising - a training storage comprising a set of training pairs, a training pair comprising conditioning data (c), and a prediction target (x), - a processor system configured for - an encoder function mapping a prediction target (x) in a target space (X) to a latent representation (z = ENC(x)) in a latent space (Z), - a decoderfunction mapping a latent representation (z) in the latent space (Z) to a target representation (x = DEC(z)) in the target space (X), - a conditional normalizing flow function mapping a latent representation (z) to a base point (e = f(z,c)) in a base space (E) conditional on conditioning data (c), the encoder function, decoder function and conditional normalizing flow function being machine learnable functions, the conditional normalizing flow function being invertible, the processor system being further configured to - training the encoder function, decoder function and conditional normalizing flow on the set of training pairs, the training comprising minimizing a reconstruction loss of the concatenation of the encoder function and the decoder function, and to minimize a difference between a probability distribution on the base space and the concatenation of encoder and the conditional normalizing flow function applied to the set of training pairs.
2. A machine learnable system as in Claim 1, wherein - conditioning data (c) in a training pair comprises past trajectory information of a traffic participant, and wherein the prediction target (x) comprises future trajectory information of the traffic participant, or - conditioning data (c) in a training pair comprises sensor information and wherein the prediction target (x) comprises a classification.
3. A machine learnable system as in any one of claims 1-2, wherein - the probability distribution on the base space is a predetermined probability distribution, or - the probability distribution on the base space is a probability distribution conditional on the conditioning data (c).
4. A machine learnable system as in any one of claim 1-3, wherein the encoder function and decoder function are non-deterministic functions, the encoder function and decoder function being arranged to generate a probability distribution from which a function output is determined.
5. A machine learnable system as in claim 4, wherein at least one of the encoder function and decoder function are arranged to generate a mean value, a function output is determined by sampling a Gaussian distribution having the mean value and a predetermined variance.
6. A machine learnable system as in any one of claims 1-5, wherein the training comprises maximizing an evidence lower bound (ELBO) being a lower bound on the conditional probability (p(x|c)) of a training target (x) given a conditioning data (c), the ELBO being defined as wherein, KL(q(z|x,c)||p(z|c)) is the Kullback-Leibler divergence of the probability distributions q(z|x,c) and p(z|c), the probability distributions p(z|c) being defined by the base distribution and the conditional normalizing flow.
7. A machine learnable system as in claim 6, wherein Kullback-Leibler divergence of KL(q(z|x,c)||p(z|c)) is computed by wherein NF is the conditional normalizing flow and J(z|c) the Jacobian of the conditional normalizing flow.
8. A machine learnable system as in any one of the preceding claims, wherein conditional normalizing flow function comprises a sequence of multiple invertible normalizing flow sub-functions, one or more parameters of the multiple invertible normalizing flow sub-functions being generated by a neural network depending on conditioning data.
9. A machine learnable prediction system, the system comprising - an input interface for obtaining conditional data (c) - a processor system configured for - an inverse conditional normalizing flow function mapping a base point (e) to a latent representation in the latent space (Z), (z = f-1(z,c)), conditional on conditioning data (c), and - a decoder function mapping the latent representation (z) in the latent space (Z) to a target representation (x = DEC(z)) in the target space (X), wherein the decoder function and conditional normalizing flow function having been learned according to a machine learnable system as in Claim 1, - determining a prediction target by - obtaining a base point (e) in a base space, - applying the inverse conditional normalizing flow function to the base point conditional on the conditional data (c) to obtain a latent representation, - applying the decoder function to the latent representation to obtain the prediction target.
10. A machine learnable system as in Claim 9, wherein - the base point (e) is sampled from a base space (E) according to a predetermined probability distribution, or - the base point (e) is sampled from a base space (E) according to a probability distribution conditional on the conditioning data (c).
11. A machine learnable system as in Claim 10, wherein the base point is sampled from the base space multiple times, and wherein at least a part of corresponding multiple prediction targets averaged.
12. An autonomous device controller having a machine learnable prediction system as in any one of claims 9-11, wherein the conditioning data comprises sensor data of an autonomous device, the machine learnable prediction system being configured to classify objects in the sensor data and/or to predict future sensor data, an autonomous device controller being configured for decision-making depending on the classification.
13. A computer-implemented machine learning method (500), the method comprising - accessing (505) set of training pairs, a training pair comprising conditioning data (c), and a prediction target (x), - mapping (510) a prediction target (x) in a target space (X) to a latent representation (z = ENC(x)) in a latent space (Z) with an encoder function, - mapping (515) a latent representation (z) in the latent space (Z) to a target representation (x = DEC(z)) in the target space (X) with a decoder function, - mapping (520) a latent representation (z) to a base point (e = f(z, c)) in a base space (E) conditional on conditioning data (c) with a conditional normalizing flow function, the encoder function, decoderfunction and conditional normalizing flow function being machine learnable functions, the conditional normalizing flow function being invertible, - training (525) the encoder function, decoder function and conditional normalizing flow on the set of training pairs, the training comprising minimizing a reconstruction loss of the concatenation of the encoder function and the decoder function, and to minimize a difference between a probability distribution on the base space and the concatenation of encoder and the conditional normalizing flow function applied to the set of training pairs.
14. A computer-implemented machine learnable prediction method (550), the method comprising - obtaining (555) conditional data (c), - determining (560) a prediction target by - obtaining (565) a base point (e) in a base space, - mapping (570) the base point to a latent representation conditional on the conditional data (c) using an inverse conditional normalizing flow function mapping the base point (e) to a latent representation in the latent space (Z), (z = f-1(z,c)), conditional on conditioning data (c), and - mapping (575) latent representation to obtain the prediction target using a decoder function mapping the latent representation (z) in the latent space (Z) to a target representation (x = DEC(z)) in the target space (X), wherein the decoder function and conditional normalizing flow function having been learned according to a machine learning method as in Claim 13.
15. A transitory or non-transitory computer readable medium (1000) comprising data (1020) representing instructions, which when executed by a processor system, cause the processor system to perform the method according to claim 13 or 14.
</claims>
</document>
