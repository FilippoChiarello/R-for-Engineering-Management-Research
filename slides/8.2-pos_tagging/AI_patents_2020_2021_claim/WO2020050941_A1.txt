<document>

<filing_date>
2019-08-06
</filing_date>

<publication_date>
2020-03-12
</publication_date>

<priority_date>
2018-09-04
</priority_date>

<ipc_classes>
G03B15/05,G03B42/00,G06T5/00,H04N5/235,H04N5/33
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
BARRON, JONATHAN
CHEN, JIAWEN
WANG JIAN
XUE, TIAFAN
</inventors>

<docdb_family_id>
67660828
</docdb_family_id>

<title>
DARK FLASH PHOTOGRAPHY WITH A STEREO CAMERA
</title>

<abstract>
Scenes can be imaged under low-light conditions using flash photography. However, the flash can be irritating to individuals being photographed, especially when those individuals' eyes have adapted to the dark. Additionally, portions of images generated using a flash can appear washed-out or otherwise negatively affected by the flash. These issues can he addressed by using a flash at an invisible wavelength, e.g., an infrared and/or ultraviolet flash. At the same time a scene is being imaged, at the in visible wavelength of the invisible flash, the scene can also be imaged at visible wavelengths. This can include simultaneously using both a standard RGB camera and a modified visible-plus-invisible-wavelengths camera (e.g., an 'IR- G-UV' camera). The visible and invisible image data can then be combined to generate an improved visible-light image of the scene, e.g., that approximates a visible light image of the scene, had the scene been illuminated during daytime light conditions.
</abstract>

<claims>
We claim:
1. A device comprising:
a first camera, wherein the first camera is operable to generate green image data based on green light received by the first camera, blue image data based on blue light received by the first camera, and red image data based on red light received by the first camera;
a second camera, wherein the second camera is operable to generate first visible image data based on light at a first visible wavelength that is received by the second camera and first invisible image data based on light at a first invisible wavelength received by the second camera;
a flash, wherein the flash is operable to emit light at the first invisible wavelength; and a controller, wherein the controller is operably coupled to the first camera, the second camera, and the flash, and wherein the controller comprises at least one processor programmed to perform controller operations comprising:
operating the flash, during a first period of time, to illuminate a scene with light at the first invisible wavelength;
operating the first camera, during the first period of time, to generate a first image of the scene, wherein the first image comprises information indicative of red, green, and blue light received from the scene; and
operating the second camera, during the first period of time, to generate a second image of the scene, wiierem the second image comprises information indicative of light received from the scene at the first visible wavelength and at the first invisible wavelength.
2. The device of claim 1, further comprising:
an additional flash, wherein the additional flash is operable to emit light at a second invisible wavelength, wherein the second camera is farther operable to generate second invisible image data based on light at the second invisible wavelength that is received by the second camera, and wherein the controller operations further comprise:
operating the additional flash, during the first period of time, to illuminate the scene with light at the second invisible wavelength, wherein the second image additionally comprises information indicative of light received from the scene at the second invisible wavelength.
3. The device of claim 2, wherein the first invisible wavelength is a first infrared wavelength and wherein the second invisible wavelength is a second infrared wavelength that differs from the first infrared wavelength.
4. The device of claim 2, wherein the first invisible wavelength is a first infrared wavelength and wherein the second invisible wavelength is a first ultraviolet wavelength.
5. The device of claim 1 , wherein the controller operations further comprise: using the first and second images to generate an improved visible-light image of the scene.
6. The device of claim 5, wherein using the first and second images to generate an improved visible-light image of the scene comprises:
determining a correspondence between the information, of the second image, indicative of light received from the scene at the first visible wavelength and the information, of the first image, indicative of at least one of the red, green, or blue light received from the scene; and
based on the determined correspondence, registering the first image to the second image to generate a registered visible-light image.
7. The device of claim 6, wherein using the first and second images to generate an improved visible-light image of the scene further comprises:
generating a scaled registered visible-light image by scaling at least one of a magnitude or a gradient of the information, of the second image, indicative of light received from the scene at the first invisible wavelength based on the registered visible-light image.
8. The device of claim 7, wherein using the first and second images to generate an improved visible-light image of the scene further comprises:
using a convolutional neural network to filter the scaled registered visible-light image.
9. The device of claim 1, wherein the first visible wavelength is a green wavelength.
10. The device of claim 1 , wherein the controller operations further comprise: generating, based on the first and second images, a depth map for the scene.
11. A method comprising:
illuminating a scene with light at a first invisible wavelength during a first period of time;
during the first period of time, obtaining image data for the scene, wherein the obtained image data comprises information indicative of red, green, and blue light received from the scene and information indicative of light received from the scene at the first invisible wavelength; and
using the obtained image data to generate an improved visible-light image of the scene.
12. The method of claim 11, wherein using the obtained image data to generate an improved visible-light image of the scene further comprises:
generating a scaled visible-light image by scaling at least one of a magnitude or a gradient of the information indicative of light received from the scene at the first invisible wavelength based on the information indicati ve of red, green, and blue light received from the scene; and
using a convolutional neural network to filter the scaled visible-light image.
13. The method of claim 1 1 , wherein obtaining image data for the scene during the first period of time comprises:
operating a first camera, during the first period of time, to generate a first image of the scene, wherein the first image comprises information indicative of red, green, and blue light received from the scene; and
operating a second camera, during the first period of time, to generate a second image of the scene, wherein the second image comprises information indicative of light received from the scene at at least one visible wavelength and at the first invisible wavelength; wherein using the obtained image data to generate an improved visible-light image of the scene comprises:
determining a correspondence between the information indicative of light received from the scene at the at least one first visible wavelength and the information indicative of at least one of the red, green, or blue light received from the scene; and
based on the determined correspondence, registering the first image to tire second image to generate a registered visible-light image.
14. The method of claim 13, wherein using the obtained image data to generate an improved visible-light image of the scene further comprises:
generating a scaled registered visible-light image by scaling at least one of a magnitude or a gradient of the information indicative of light received from the scene at the first invisible wavelength based on tire registered visible-light image.
15. The method of claim 14, wherein using the obtained image data to generate an improved visible-light image of the scene further comprises:
using a convolutional neural network to filter the scaled registered visible-light image.
16. The method of claim 14, wherein generating a scaled registered visible-light image by scaling at least one of a magnitude or a gradient of the information indicative of light received from the scene at the first invisible wavelength based on the registered visible-light image comprises using a bilateral filter to combine the registered visible-light image and the at least one of a magnitude or a gradient of the information indicative of light received from the scene at the first invisible wavelength.
17. The method of claim 11, further comprising:
operating an additional flash, during the first period of time, to ill uminate the scene with light at a second invisible wavelength, wherein the obtained image data additionally comprises information indicative of light received from the scene at the second invisible wavelength.
18. A device comprising:
a flash, wherein die flash is operable to emit light at a first invisible wavelength; one or more image sensors, wherein the one or more image sensors are operable to generate green image data based on green light recei ved by the image sensor, blue image data based on blue light received by the image sensor, red image data based on red light received by the image sensor, and first invisible image data based on light at the first invisible wavelength received by the image sensor; and
a controller, wherein the controller is operably coupled to the flash and the one or more image sensors, and wherein the controller comprises at least one processor programmed to perform controller operations comprising:
operating the flash, during a first period of time, to illuminate a scene with light at the first invisible wavelength; and
operating the one or more image sensors, during the first period of time, to generate image data for the scene, wherein the image data comprises information indicative of red, green, and blue light received from the scene and information indicative of light received from the scene at the first invisible wavelength.
19. The de vice of claim 18, wherein the image sensor comprises a first camera that is operable to generate green image data based on green light received by the first camera, blue image data based on blue light received by the first camera, red image data based on red light received by the first camera, and first invisible image data based on light at die first invisible wavelength received by the first camera.
20. The device of claim 18, further comprising:
an additional flash, wherein the additional flash is operable to emit light at a second invisible wavelength, wherein the image sensor is further operable to generate second invisible image data based on light at the second invisible wavelength that is received by the image sensor, and wherein the controller operations further comprise:
operating the additional flash, during the first period of time, to illuminate the scene with light at the second invisible wavelength, wherein the generated image data additionally comprises information indicative of light received from the scene at the second invisible wavelength.
9
</claims>
</document>
