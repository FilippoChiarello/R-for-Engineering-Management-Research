<document>

<filing_date>
2017-07-26
</filing_date>

<publication_date>
2020-02-25
</publication_date>

<priority_date>
2017-05-05
</priority_date>

<ipc_classes>
G06K9/00,G06K9/62,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
INTEL CORPORATION
</assignee>

<inventors>
YEHEZKEL ROHEKAR, RAANAN YONATAN
</inventors>

<docdb_family_id>
64014186
</docdb_family_id>

<title>
On the fly deep learning in machine learning for autonomous machines
</title>

<abstract>
A mechanism is described for facilitating on-the-fly deep learning in machine learning for autonomous machines. A method of embodiments, as described herein, includes detecting an output associated with a first deep network serving as a user-independent model associated with learning of one or more neural networks at a computing device having a processor coupled to memory. The method may further include automatically generating training data for a second deep network serving as a user-dependent model, where the training data is generated based on the output. The method may further include merging the user-independent model with the user-dependent model into a single joint model.
</abstract>

<claims>
1. An apparatus comprising: one or more processors to: detect an output associated with a first deep network serving as a user-independent model associated with learning of one or more neural networks at the apparatus having the processor; automatically generate training data for a second deep network serving as a user-dependent model, wherein the training data is generated based on the output; and merge the user-independent model with the user-dependent model into a single joint model, wherein merging includes converging one or more layers associated with the user-independent model with one or more layers associated with the user-dependent model in the joint model such that the joint model is trained on-the-fly.
2. The apparatus of claim 1, wherein the one or more processors are further to perform offline training of the user-independent model associated with one or more of the neural networks, wherein the user-independent model represents a context-independent model.
3. The apparatus of claim 1, wherein the joint model is trained, on-the-fly, to serve as a user-dependent classifier, wherein the user-dependent classifier represents a context-dependent classifier.
4. The apparatus of claim 3, wherein training is performed in runtime and during normal operations of the apparatus, wherein the normal operations include one or more of launching or running of an application.
5. The apparatus of claim 3, wherein training the joint model comprises mediating between the one or more layers of the user-independent model and the one or more layers of the user-dependent model that are converged within the joint model.
6. The apparatus of claim 1, wherein the one or more processors are further to: perform tuning of one or more parameters of the joint model, wherein the joint model tuning is performed using small labeled data; and generate one or more mediation parameters to define mediation for learning or training of the joint model.
7. The apparatus of claim 1, wherein the processor comprises a graphics processor co-located with an application processor on a common semiconductor package.
8. A method comprising: detecting an output associated with a first deep network serving as a user-independent model associated with learning of one or more neural networks at a computing device having a processor coupled to memory; automatically generating training data for a second deep network serving as a user-dependent model, wherein the training data is generated based on the output; and merging the user-independent model with the user-dependent model into a single joint model, wherein merging includes converging one or more layer associated with the user-independent model with one or more layers associated with the user-dependent model in the joint model such that the joint model is trained on-the-fly.
9. The method of claim 8, further comprising performing offline training of the user-independent model associated with one or more of the neural networks, wherein the user-independent model represents a context-independent model.
10. The method of claim 8, wherein the joint model is trained, on-the-fly, to serve as a user-dependent classifier, wherein the user-dependent classifier represents a context-dependent classifier.
11. The method of claim 10, wherein training is performed in runtime and during normal operations of the apparatus, wherein the normal operations include one or more of launching or running of an application.
12. The method of claim 10, wherein training the joint model comprises mediating between the one or more layers of the user-independent model and the one or more layers of the user-dependent model that are converged within the joint model.
13. The method of claim 8, further comprising: performing tuning of one or more parameters of the joint model, wherein the joint model tuning is performed using small labeled data; and generating one or more mediation parameters to define mediation for learning or training of the joint model.
14. The method of claim 8, wherein the processor comprises a graphics processor co-located with an application processor on a common semiconductor package.
15. At least one non-transitory machine-readable medium comprising instructions that when executed by a computing device, cause the computing device to perform operations comprising: detecting an output associated with a first deep network serving as a user-independent model associated with learning of one or more neural networks, wherein the computing device includes a processor coupled to memory; automatically generating training data for a second deep network serving as a user-dependent model, wherein the training data is generated based on the output; and merging the user-independent model with the user-dependent model into a single joint model, wherein merging includes converging one or more layers associated with the user-independent model with one or more layers associated with the user-dependent model in the joint model such that the joint model is trained on-the-fly.
16. The non-transitory machine-readable medium of claim 15, wherein the operations further comprise performing offline training of the user-independent model associated with one or more of the neural networks, wherein the user-independent model represents a context-independent model.
17. The non-transitory machine-readable medium of claim 15, wherein the joint model is trained, on-the-fly, to serve as a user-dependent classifier, wherein the user-dependent classifier represents a context-dependent classifier.
18. The non-transitory machine-readable medium of claim 17, wherein training is performed in runtime and during normal operations of the apparatus, wherein the normal operations include one or more of launching or running of an application.
19. The non-transitory machine-readable medium of claim 17, wherein training the joint model comprises mediating between the one or more layers of the user-independent model and the one or more layers of the user-dependent model that are converged within the joint model.
20. The non-transitory machine-readable medium of claim 15, wherein the operations further comprise: performing tuning of one or more parameters of the joint model, wherein the joint model tuning is performed using small labeled data; and generating one or more mediation parameters to define mediation for learning or training of the joint model, wherein the processor comprises a graphics processor co-located with an application processor on a common semiconductor package.
</claims>
</document>
