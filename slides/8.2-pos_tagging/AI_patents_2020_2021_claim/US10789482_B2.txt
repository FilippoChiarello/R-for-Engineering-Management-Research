<document>

<filing_date>
2017-03-28
</filing_date>

<publication_date>
2020-09-29
</publication_date>

<priority_date>
2016-04-08
</priority_date>

<ipc_classes>
G06K9/00,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
ZENG, WENJUN
LAN, CUILING
XING, JUNLIANG
LI, YANGHAO
</inventors>

<docdb_family_id>
58632580
</docdb_family_id>

<title>
On-line action detection using recurrent neural network
</title>

<abstract>
In implementations of the subject matter described herein, an action detection scheme using a recurrent neural network (RNN) is proposed. Representation information of an incoming frame of a video and a predefined action label for the frame are obtained to train a learning network including RNN elements and a classification element. The representation information represents an observed entity in the frame. Specifically, parameters for the RNN elements are determined based on the representation information and the predefined action label. With the determined parameters, the RNN elements are caused to extract features for the frame based on the representation information and features for a preceding frame. Parameters for the classification element are determined based on the extracted features and the predefined action label. The classification element with the determined parameters generates a probability of the frame being associated with the predefined action label. The parameters for the RNN elements are updated according to the probability.
</abstract>

<claims>
We claim:
1. A device comprising: a processing unit; a memory coupled to the processing unit and storing instructions thereon, the instructions, when executed by the processing unit, performing acts including: obtaining representation information of an incoming frame of a video and a predefined action label for the frame, the representation information representing an observed entity in the frame; determining first parameters for recurrent neural network (RNN) elements included in a learning network based on the representation information of the frame and the predefined action label; causing the RNN elements with the first parameters to extract features for the frame based on the representation information and features for a preceding frame of the video; determining second parameters for a classification element included in the learning network based on the features for the frame and the predefined action label; and updating the first parameters for the RNN elements according to a probability of the frame being associated with the predefined action label generated by the classification element with the second parameters.
2. The device of claim 1, wherein the predefined action label indicates a predefined action of the observed entity, and the learning network further includes a feature processing element and a forecast element, the acts further comprising: obtaining time information about a special frame in which the action starts or ends; causing the feature processing element to process the features for the frame based on the probability; determining third parameters for the forecast element based on the processed features and the time information; and updating the third parameters according to a confidence of the frame being the special frame generated by the forecast element with the third parameters.
3. The device of claim 2, wherein if the frame is prior to the special frame in the video, a time gap between the frame and the special frame is within a predetermined period of time.
4. The device of claim 2, wherein updating the first parameters further comprises: updating the first parameters for the RNN elements based on the confidence.
5. The device of claim 1, wherein the predefined action label indicates that the frame includes no action, and the learning network further includes a further classification element, the acts further comprising: determining fourth parameters for the further classification element based on the features and the predefined action label; causing the further classification element with the fourth parameters to generate a further probability of the frame being associated with the predefined action label; and in response to the further probability being below a threshold, causing the determining of the second parameters based on the features.
6. The device of claim 1, wherein the learning network further includes feature fusion elements, the acts further comprising: determining fifth parameters for the feature fusion elements based on the features extracted by the RNN elements and the predefined action label; causing the feature fusion elements with the fifth parameters to update the features based on a non-linear function; and causing the classification element to determine the probability based on the updated features.
7. The device of claim 1, wherein the RNN elements include long short-term memory (LSTM) elements.
8. The device of claim 1, wherein the representation information includes skeleton representation associated with the observed entity.
9. A device comprising: a processing unit; a memory coupled to the processing unit and storing instructions thereon, the instructions, when executed by the processing unit, performing acts including: obtaining representation information of an incoming frame of a video, the representation information representing an observed entity in the frame; causing recurrent neural network (RNN) elements with first predetermined parameters included in a learning network to extract features for the frame based on the representation information and features for a preceding frame of the video; and causing a classification element with second predetermined parameters included in the learning network to generate a probability of the frame being associated with a predefined action label based on the features.
10. The device of claim 9, wherein the predefined action label indicates a predefined action of the observed entity, and the learning network further includes a feature processing element and a forecast element, the acts further comprising: causing the feature processing element to process the features based on the probability; causing the forecast element with third predetermined parameters to generate a confidence of the frame being a special frame in which the action starts or ends based on the processed features; and in response to the confidence exceeds a threshold, determining a forecast for the special frame.
11. The device of claim 9, wherein the predefined action label indicates that the frame includes no action, and the learning network further includes a further classification element, the acts further comprising: causing the further classification element with fourth predetermined parameters to generate a further probability of the frame being associated with the predefined action label based on the features; and in response to the further probability being below a threshold, causing the classification element to generate the probability based on the features.
12. The device of claim 9, wherein the RNN elements include long short-term memory (LSTM) elements.
13. A method comprising: obtaining representation information of an incoming frame of a video and a predefined action label for the frame, the representation information representing an observed entity in the frame; determining first parameters for recurrent neural network (RNN) elements included in a learning network based on the representation information of the frame and the predefined action label; causing the RNN elements with the first parameters to extract features for the frame based on the representation information and features for a preceding frame of the video; determining second parameters for a classification element included in the learning network based on the features for the frame and the predefined action label; and updating the first parameters for the RNN elements according to a probability of the frame being associated with the predefined action label generated by the classification element with the second parameters.
14. The method of claim 13, wherein the predefined action label indicates a predefined action of the observed entity, and the learning network further includes a feature processing element and a forecast element, the method further comprising: obtaining time information about a special frame in which the action starts or ends; causing the feature processing element to process the features for the frame based on the probability; determining third parameters for the forecast element based on the processed features and the time information; and updating the third parameters according to a confidence of the frame being the special frame generated by the forecast element with the third parameters.
15. The method of claim 13, wherein the predefined action label indicates that the frame includes no action and the learning network further includes a further classification element, the method further comprising: determining fourth parameters for the further classification element based on the features and the predefined action label; causing the further classification element with the fourth parameters to generate a further probability of the frame being associated with the predefined action label; and in response to the further probability being below a threshold, causing the determining of the second parameters based on the features.
</claims>
</document>
