<document>

<filing_date>
2020-04-10
</filing_date>

<publication_date>
2020-07-30
</publication_date>

<priority_date>
2017-11-27
</priority_date>

<ipc_classes>
B60R11/04,G01C21/32,G01S17/42,G01S17/89,G06K9/00,G06N3/02,G06N3/04,G06N3/08,G06T5/50,G06T7/10,G06T7/246,G07C5/00
</ipc_classes>

<assignee>
TUSIMPLE
</assignee>

<inventors>
GUO, DAZHOU
HOU, XIAODI
MEI, XUE
WE, YUJIE
</inventors>

<docdb_family_id>
66634523
</docdb_family_id>

<title>
SYSTEM AND METHOD FOR LARGE-SCALE LANE MARKING DETECTION USING MULTIMODAL SENSOR DATA
</title>

<abstract>
A system and method for large-scale lane marking detection using multimodal sensor data are disclosed. A particular embodiment includes: receiving image data from an image generating device mounted on a vehicle; receiving point cloud data from a distance and intensity measuring device mounted on the vehicle; fusing the image data and the point cloud data to produce a set of lane marking points in three-dimensional (3D) space that correlate to the image data and the point cloud data; and generating a lane marking map from the set of lane marking points.
</abstract>

<claims>
1. A system comprising: a data processor; and a multimodal lane detection module, executable by the data processor, the multimodal lane detection module being configured to perform a multimodal lane detection operation configured to: receive image data from an image generating device mounted on a vehicle; fit piecewise lines for each lane marking object detected in the received image data; receive point cloud data from a distance and intensity measuring device mounted on the vehicle; fuse the image data and the point cloud data to produce a set of lane marking points in three-dimensional (3D) space that correlate to the image data and the point cloud data; and generate a lane marking map from the set of lane marking points.
2. The system of claim 1 wherein a neural network is used for identifying and labeling objects in the image data with object category labels on a per-pixel basis.
3. The system of claim 2 wherein the neural network is configured to categorize each of pixels of the image data into classes corresponding to each of the lane marking object.
4. The system of claim 1 wherein the image generating device includes an image camera or a motion video camera.
5. The system of claim 1 wherein the distance and intensity measuring device includes a laser range finder.
6. The system of claim 1 wherein the multimodal lane detection operation is further configured to receive vehicle metrics related to an environment or a condition of the vehicle from a vehicle subsystem.
7. The system of claim 1 wherein the multimodal lane detection operation is further configured to align and orient the image data with a terrain map corresponding to a location and use a terrain map elevation data to transform the image data to the 3D space, wherein the location is a geographical location where the vehicle is located.
8. The system of claim 1 wherein lane markings formed from lane marking points are produced from each frame of the received image data and the corresponding point cloud data.
9. A method comprising: receiving image data from an image generating device mounted on a vehicle; fitting piecewise lines for each lane marking object detected in the received image data; receiving point cloud data from a distance and intensity measuring device mounted on the vehicle; fusing the image data and the point cloud data to produce a set of lane marking points in three-dimensional (3D) space that correlate to the image data and the point cloud data; and generating a lane marking map from the set of lane marking points.
10. The method of claim 9 comprising tracking lane markings formed from the lane marking points across a plurality of frames of the received image data.
11. The method of claim 10 comprising associating the same lane markings across the plurality of frames to form the lane marking map.
12. The method of claim 11 wherein a smoothing technique is used to fit smooth new curves for each lane marking across the plurality of frames.
13. The method of claim 9 wherein the fusing includes projecting 3D point cloud data on to two-dimensional (2D) image data, and adding a 3D point cloud point to the set of lane marking points if a distance between a position of the projected 3D point cloud point in 2D space and a position of at least one of the piecewise lines is within a pre-determined threshold.
14. The method of claim 9 comprising receiving vehicle metrics via a Global Positioning System (GPS), an inertial measurement unit (IMU), or a radar, to determine at least one of a location, an orientation, or a speed of the vehicle.
15. The method of claim 9 comprising registering the point cloud data within a time range to a common coordinate space.
16. The method of claim 15 comprising generating an accumulated point cloud representing a collection of the point cloud data over time, wherein the point cloud data are aligned.
17. A non-transitory machine-useable storage medium embodying instructions which, when executed by a machine, cause the machine to: receive image data from an image generating device mounted on a vehicle; fit piecewise lines for each lane marking object detected in the received image data; receive point cloud data from a distance and intensity measuring device mounted on the vehicle; fuse the image data and the point cloud data to produce a set of lane marking points in three-dimensional (3D) space that correlate to the image data and the point cloud data; and generate a lane marking map from the set of lane marking points.
18. The non-transitory machine-useable storage medium of claim 17 wherein a neural network is used for identifying and labeling objects in the image data with object category labels.
19. The non-transitory machine-useable storage medium of claim 18 wherein the neural network is trained by using training images, wherein the training images include contexts of at least one of environments, locations, weather conditions, and lighting conditions.
20. The non-transitory machine-useable storage medium of claim 19 wherein the training images include an object labeling created manually or automated processes.
</claims>
</document>
