<document>

<filing_date>
2020-01-14
</filing_date>

<publication_date>
2020-09-17
</publication_date>

<priority_date>
2019-03-08
</priority_date>

<ipc_classes>
G06F3/01
</ipc_classes>

<assignee>
FAURECIA INDIA PRIVATE
INDIAN INSTITUTE OF SCIENCE
</assignee>

<inventors>
BISWAS, PRADIPTA
RAMAKRISHNAN, Aparna
SHARMA, Vinay Krishna
DESHMUKH, Sachin
PRABHAKAR, Gowdham
MADAN, Modiksha
</inventors>

<docdb_family_id>
72427788
</docdb_family_id>

<title>
A SYSTEM FOR MAN-MACHINE INTERACTION IN VEHICLES
</title>

<abstract>
A system for man-machine interaction for a vehicle is disclosed. The proposed system includes an interactive reflected display on a windscreen of the vehicle; an eye tracking module including an eye gaze tracker for tracking eye gaze direction of a user, and a training unit to train the tracking module using a neural network to predict a real-time set of coordinates for eye gaze direction of the user based on a created reference data set of coordinates of the display; a finger tracking module to detect presence of a finger of the user and track movement of the finger; a cursor module configured to, on the basis of a received input, move a cursor on the display to an area of interest on the display; and a wireless switch module operatively coupled to the display for selecting a target in the area of interest on the display.
</abstract>

<claims>
We Claim:
1. A system for manmachine interaction, the system comprising:
an interactive display;
an eye gaze tracking module, the tracking module comprising:
an eye gaze tracker for tracking eye gaze direction of a user;
wherein a training unit to train the tracking module using a neural network to predict a real-time set of coordinates for eye gaze direction of the user based on a created reference data set of coordinates of the display.
2. The system as claimed in claim 1, wherein the system comprises a finger tracking module to detect presence of a finger of the user and track movement of the finger, a cursor module configured to, on the basis of a received input, move a cursor on the display to an area of interest on the display, and a wireless switch module operatively coupled to the interactive display, wherein the wireless switch module is configured for selecting a target in the area of interest on the display based on position of the cursor on the display.
3. The system as claimed in claim 2, wherein the input of the cursor module is any of an eye gaze direction of user from the eye gaze tracking module and the presence of finger from the finger tracking module.
4. The system as claimed in claim 2, wherein a precedence is given to signal from the finger tracker over a signal from eye gaze tracker and position of the cursor on the display is corrected based on detected movement of the finger by moving the cursor in direction of detected movement of the finger, and wherein when the finger tracker does not locate the finger of the user within its field, the system resumes to move the cursor based on the detected direction of the eye gaze of the user.
5. The system as claimed in claim 1, wherein the reference data set of coordinates of the display is created based on a set of coordinates for the display determined by the eye gaze tracker for a corresponding eye gaze direction for each of a plurality of test users.
6. The system as claimed in claim 1, the system comprises a processor operatively coupled to eye gaze tracking module, the finger tracking module, the interactive reflected display, cursor module and the wireless switch module.
7. The system as claimed in claim 1, wherein the interactive display is reflection from a screen of a display device on an inclined and/or a curved surface such as a windscreen of a vehicle, and wherein the display device is selected form a group consisting of a tablet computer, personal digital assistance, and a mobile smart phone.
8. The system as claimed in claim 1, wherein the system further includes a median filter configured to take median of pixels of continuously recorded eye gaze positions on the display by the eye gaze tracker.
9. The system as claimed in claim 1, wherein the system incorporates a target prediction unit that highlights a clickable object nearest to the cursor position on the display to the area of interest on the display.
10. The system as claimed in claim 9, wherein the highlighting of the clickable object is based on hotspots associated with each of the clickable objects.
11. The system as claimed in claim 7, wherein the hotspots associated with different clickable objects are selected based on minimizing value of cost function:
where dy is distance between new hotspots on clickable objects i and j.
12. The system as claimed in claim 8, wherein a new hotspot is selected even if increasing value of the cost function, based on the following condition: random number between 0 and I . where T runs from 5000 to 1 and reduced by 1 in each iteration.
</claims>
</document>
