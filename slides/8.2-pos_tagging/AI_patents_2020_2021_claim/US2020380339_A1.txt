<document>

<filing_date>
2020-05-27
</filing_date>

<publication_date>
2020-12-03
</publication_date>

<priority_date>
2019-05-29
</priority_date>

<ipc_classes>
G06K9/62,G06N3/04,G06N3/063
</ipc_classes>

<assignee>
GENENTECH
</assignee>

<inventors>
Aiello, Katherine Ann
Ray, Shonket
Magana, Ramsey
Pettet, Alexandra
Branson, Kim Matthew
</inventors>

<docdb_family_id>
71094843
</docdb_family_id>

<title>
INTEGRATED NEURAL NETWORKS FOR DETERMINING PROTOCOL CONFIGURATIONS
</title>

<abstract>
Methods and systems disclosed herein relate generally to systems and methods for integrating neural networks, which are of different types and process different types of data. The different types of data may include static data and dynamic data, and the integrated neural networks can include feedforward and recurrent neural networks. Results of the integrated neural networks can be used to configure or modify protocol configurations.
</abstract>

<claims>
1. A method comprising: accessing a multi-structure data set corresponding to an entity, the multi-structure data set including a temporally sequential data subset and a static data subset, the temporally sequential data subset having a temporally sequential structure in that the temporally sequential data subset includes multiple data elements corresponding to multiple time points, and the static data subset having a static structure; executing a recurrent neural network (RNN) to transform the temporally sequential data subset into an RNN output; executing a feedforward neural network (FFNN) to transform the static data subset into a FFNN output, wherein the FFNN was trained without using the RNN and without using training data having the temporally sequential structure; determining an integrated output based on the RNN output, wherein at least one of the RNN output and the integrated output depend on the FFNN output, and wherein the integrated output corresponds to a prediction of an efficacy of treating the entity with a particular treatment; and outputting the integrated output.
2. The method of claim 1, further comprising: inputting, by a user, the multi-structure data or identifier of the entity; and receiving, by the user, the integrated output.
3. The method of claim 1, further comprising: determining, based on the integrated output, to treat the subject with the particular treatment; and prescribing the particular treatment for the subject.
4. The method of claim 1, wherein: the static data subset includes image data and non-image data; the FFNN executed to transform the image data includes a convolutional neural network; and the FFNN executed to transform the non-image data includes a multi-layer perceptron neural network.
5. The method of claim 1, wherein: the temporally sequential data subset includes image data; the recurrent neural network executed to transform the image data includes a LSTM convolutional neural network; the multi-structure data set includes another temporally sequential data subset that includes non-image data; the method further includes executing a LSTM neural network to transform the non-image data into another RNN output; and the integrated output is further based on the other RNN output.
6. The method of claim 1, wherein: the RNN output includes at least one hidden state of an intermediate recurrent layer in the RNN; the multi-structure data set includes another static data subset that includes non-image data; the method further includes executing another FFNN to transform the other static data subset into another FFNN output; and the other FFNN output including a set of intermediate values generated at an intermediate hidden layer in the other FFNN.
7. The method of claim 1, wherein determining the integrated output includes executing an integration FFNN to transform the FFNN output and the RNN output to the integrated output, wherein each of the FFNN and the RNN were trained without using the integrated FFNN.
8. The method of claim 1, further comprising: concatenating the FFNN output and a data element of the multiple data elements from the temporally sequential data subset, the data element corresponding to an earliest time point of the multiple time points to produce concatenated data; wherein executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process an input that includes: the concatenated data; and for each other data element of the multiple data elements that correspond to time points of the multiple time points subsequent to the earliest time points, the other data element; and wherein the integrated output includes the RNN output.
9. The method of claim 1, further comprising: generating an input that includes, for each data element of the multiple data elements from the temporally sequential data subset, a concatenation of the data element and the FFNN output; wherein executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process the input; and wherein the integrated output includes the RNN output.
10. The method of claim 1, wherein: the multi-structure data set includes: another temporally sequential data subset that includes other multiple data elements corresponding to the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the method further includes: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; and generating an input that includes, for each time point of the multiple time points, a concatenated data element that includes the data element of the multiple data elements that corresponds to the time point and the other data element of the other multiple data elements that corresponds to the time point; executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process the input; the RNN output corresponds to a single hidden state of an intermediate recurrent layer in the RNN, the single hidden state corresponding to a single time point of the multiple time points; and determining the integrated output includes processing the static-data integrated output and the RNN output using a second integration neural network.
11. The method of claim 1, wherein: the multi-structure data set includes: another temporally sequential data subset that includes other multiple data elements corresponding to the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the method further includes: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; generating an input that includes, for each time point of the multiple time points, a concatenated data element that includes the data element of the multiple data elements that corresponds to the time point and the other data element of the other multiple data elements that corresponds to the time point; executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process the input; the RNN output corresponds to multiple hidden states in the RNN, each of the multiple time points corresponding to a hidden state of the multiple hidden states; and determining the integrated output includes processing the static-data integration output and the RNN output using a second integration neural network.
12. The method of claim 1, wherein: the multi-structure data set includes: another temporally sequential data subset having another temporally sequential structure in that the other temporally sequential data subset includes other multiple data elements corresponding to other multiple time points, the other multiple time points being different than the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the method further includes: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; executing another RNN to transform the other temporally sequential data subset into another RNN output, the RNN having been trained independently from and executed independently from the other RNN, the RNN output including a single hidden state of an intermediate recurrent layer in the RNN, the single hidden state corresponding to a single time point of the multiple time points, the other RNN output including another single hidden state of another intermediate recurrent layer in the other RNN, the other single hidden state corresponding to another single time point of the other multiple time points; and concatenating the RNN output and the other RNN output; wherein determining the integrated output includes processing the static-data integrated output and the concatenated outputs using a second integration neural network.
13. The method of claim 1, wherein: the multi-structure data set includes: another temporally sequential data subset having another temporally sequential structure in that the other temporally sequential data subset includes other multiple data elements corresponding to other multiple time points, the other multiple time points being different than the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the method further includes: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; executing another RNN to transform the other temporally sequential data subset into another RNN output, the RNN having been trained independently from and executed independently from the other RNN, the RNN output including multiple hidden states of an intermediate recurrent layer in the RNN, the multiple hidden states corresponding to the multiple time points, the other RNN output including other multiple hidden states of another intermediate recurrent layer in the other RNN, the other multiple hidden states corresponding to the other multiple time points; and concatenating the RNN output and the other RNN output; wherein determining the integrated output includes processing the static-data integrated output and the concatenated outputs using a second integration neural network.
14. The method of claim 1, further comprising: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; and concatenating the RNN output and the static-data integrated output; wherein determining the integrated output includes executing a second integration neural network to transform the concatenated outputs into the integrated output.
15. The method of claim 14, further comprising: concurrently training the first integration neural network, the second integration neural network and the RNN using an optimization technique, wherein executing the RNN includes executing the trained RNN, wherein executing the first integration neural network includes executing the trained first integration neural network, and wherein executing the second integration neural network includes executing the trained second integration neural network.
16. The method of claim 1, further comprising: accessing domain-specific data that includes a set of training data elements and a set of labels, wherein each training data element of the set of training data elements corresponds to a label of the set of labels; and training the FFNN using the domain-specific data.
17. A method comprising: transmitting, at a user device and to a remote computing system, an identifier corresponding to an entity, wherein, upon receipt of the identifier, the remote computing system being configured to, upon receipt of the identifier: access a multi-structure data set corresponding to an entity, the multi-structure data set including a temporally sequential data subset and a static data subset, the temporally sequential data subset having a temporally sequential structure in that the temporally sequential data subset includes multiple data elements corresponding to multiple time points, and the static data subset having a static structure; execute a recurrent neural network (RNN) to transform the temporally sequential data subset into an RNN output; execute a feedforward neural network (FFNN) to transform the static data subset into a FFNN output, wherein the FFNN was trained without using the RNN and without using training data having the temporally sequential structure; determine an integrated output based on the RNN output, wherein at least one of the RNN output and the integrated output depend on the FFNN output, and wherein the integrated output corresponds to a prediction of an efficacy of treating the entity with a particular treatment; and transmit the integrated output; and receiving, at the user device and from the remote computing system, the integrated output.
18. The method of claim 17, further comprising: collecting at least part of multi-structure data using a medical imaging device or laboratory equipment.
19. Use of an integrated output in the treatment of a subject, wherein the integrated output is provided by a computing device executing a computational model based on a multi-structure data set corresponding to the subject to provide the integrated output, wherein: the multi-structure data set includes a temporally sequential data subset and a static data subset, the temporally sequential data subset having a temporally sequential structure in that the temporally sequential data subset includes multiple data elements corresponding to multiple time points, and the static data subset having a static structure; and executing the computational model includes: executing a recurrent neural network (RNN) to transform the temporally sequential data subset into an RNN output; and executing a feedforward neural network (FFNN) to transform the static data subset into a FFNN output, wherein the FFNN was trained without using the RNN and without using training data having the temporally sequential structure; and wherein the integrated output corresponds to a prediction of an efficacy of treating the subject with a particular treatment.
20. A system comprising: one or more data processors; and a non-transitory computer readable storage medium containing instructions which, when executed on the one or more data processors, cause the one or more data processors to perform actions including: accessing a multi-structure data set corresponding to an entity, the multi-structure data set including a temporally sequential data subset and a static data subset, the temporally sequential data subset having a temporally sequential structure in that the temporally sequential data subset includes multiple data elements corresponding to multiple time points, and the static data subset having a static structure; executing a RNN to transform the temporally sequential data subset into an RNN output; executing a FFNN to transform the static data subset into a FFNN output, wherein the FFNN was trained without using the RNN and without using training data having the temporally sequential structure; determining an integrated output based on the RNN output, wherein at least one of the RNN output and the integrated output depend on the FFNN output, and wherein the integrated output corresponds to a prediction of an efficacy of treating the entity with a particular treatment; and outputting the integrated output.
21. The system of claim 20, wherein: the static data subset includes image data and non-image data; the FFNN executed to transform the image data includes a convolutional neural network; and the FFNN executed to transform the non-image data includes a multi-layer perceptron neural network.
22. The system of claim 20, wherein: the temporally sequential data subset includes image data; the RNN executed to transform the image data includes a LSTM convolutional neural network; the multi-structure data set includes another temporally sequential data subset that includes non-image data; the actions further include executing a LSTM neural network to transform the non-image data into another RNN output; and the integrated output is further based on the other RNN output.
23. The system of claim 20, wherein: the RNN output includes at least one hidden state of an intermediate recurrent layer in the RNN; the multi-structure data set includes another static data subset that includes non-image data; the actions further include executing another FFNN to transform the other static data subset into another FFNN output; and the other FFNN output including a set of intermediate values generated at an intermediate hidden layer in the other FFNN.
24. The system of claim 20, wherein determining the integrated output includes executing an integration FFNN to transform the FFNN output and the RNN output to the integrated output, wherein each of the FFNN and the RNN were trained without using the integrated FFNN.
25. The system of claim 20, wherein the actions further include: concatenating the FFNN output and a data element of the multiple data elements from the temporally sequential data subset, the data element corresponding to an earliest time point of the multiple time points to produce concatenated data; wherein executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process an input that includes: the concatenated data; and for each other data element of the multiple data elements that correspond to time points of the multiple time points subsequent to the earliest time points, the other data element; and wherein the integrated output includes the RNN output.
26. The system of claim 20, wherein the actions further include: generating an input that includes, for each data element of the multiple data elements from the temporally sequential data subset, a concatenation of the data element and the FFNN output; wherein executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process the input; and wherein the integrated output includes the RNN output.
27. The system of claim 20, wherein: the multi-structure data set includes: another temporally sequential data subset that includes other multiple data elements corresponding to the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the actions further include: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; and generating an input that includes, for each time point of the multiple time points, a concatenated data element that includes the data element of the multiple data elements that corresponds to the time point and the other data element of the other multiple data elements that corresponds to the time point; executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process the input; the RNN output corresponds to a single hidden state of an intermediate recurrent layer in the RNN, the single hidden state corresponding to a single time point of the multiple time points; and determining the integrated output includes processing the static-data integrated output and the RNN output using a second integration neural network.
28. The system of claim 20, wherein: the multi-structure data set includes: another temporally sequential data subset that includes other multiple data elements corresponding to the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the actions further include: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; generating an input that includes, for each time point of the multiple time points, a concatenated data element that includes the data element of the multiple data elements that corresponds to the time point and the other data element of the other multiple data elements that corresponds to the time point; executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process the input; the RNN output corresponds to multiple hidden states in the RNN, each of the multiple time points corresponding to a hidden state of the multiple hidden states; and determining the integrated output includes processing the static-data integration output and the RNN output using a second integration neural network.
29. The system of claim 20, wherein: the multi-structure data set includes: another temporally sequential data subset having another temporally sequential structure in that the other temporally sequential data subset includes other multiple data elements corresponding to other multiple time points, the other multiple time points being different than the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the actions further include: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; executing another RNN to transform the other temporally sequential data subset into another RNN output, the RNN having been trained independently from and executed independently from the other RNN, the RNN output including a single hidden state of an intermediate recurrent layer in the RNN, the single hidden state corresponding to a single time point of the multiple time points, the other RNN output including another single hidden state of another intermediate recurrent layer in the other RNN, the other single hidden state corresponding to another single time point of the other multiple time points; and concatenating the RNN output and the other RNN output; wherein determining the integrated output includes processing the static-data integrated output and the concatenated outputs using a second integration neural network.
30. The system of claim 20, wherein: the multi-structure data set includes: another temporally sequential data subset having another temporally sequential structure in that the other temporally sequential data subset includes other multiple data elements corresponding to other multiple time points, the other multiple time points being different than the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the actions further include: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; executing another RNN to transform the other temporally sequential data subset into another RNN output, the RNN having been trained independently from and executed independently from the other RNN, the RNN output including multiple hidden states of an intermediate recurrent layer in the RNN, the multiple hidden states corresponding to the multiple time points, the other RNN output including other multiple hidden states of another intermediate recurrent layer in the other RNN, the other multiple hidden states corresponding to the other multiple time points; and concatenating the RNN output and the other RNN output; wherein determining the integrated output includes processing the static-data integrated output and the concatenated outputs using a second integration neural network.
31. The system of claim 20, wherein the actions further include: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; and concatenating the RNN output and the static-data integrated output; wherein determining the integrated output includes executing a second integration neural network to transform the concatenated outputs into the integrated output.
32. The system of claim 31, wherein the actions further include: concurrently training the first integration neural network, the second integration neural network and the RNN using an optimization technique, wherein executing the RNN includes executing the trained RNN, wherein executing the first integration neural network includes executing the trained first integration neural network, and wherein executing the second integration neural network includes executing the trained second integration neural network.
33. The system of claim 20, wherein the actions further include: accessing domain-specific data that includes a set of training data elements and a set of labels, wherein each training data element of the set of training data elements corresponds to a label of the set of labels; and training the FFNN using the domain-specific data.
34. A computer-program product tangibly embodied in a non-transitory machine-readable storage medium, including instructions configured to cause one or more data processors to perform actions including: accessing a multi-structure data set corresponding to an entity, the multi-structure data set including a temporally sequential data subset and a static data subset, the temporally sequential data subset having a temporally sequential structure in that the temporally sequential data subset includes multiple data elements corresponding to multiple time points, and the static data subset having a static structure; executing a RNN to transform the temporally sequential data subset into an RNN output; executing a FFNN to transform the static data subset into a FFNN output, wherein the FFNN was trained without using the RNN and without using training data having the temporally sequential structure; determining an integrated output based on the RNN output, wherein at least one of the RNN output and the integrated output depend on the FFNN output, and wherein the integrated output corresponds to a prediction of an efficacy of treating the entity with a particular treatment; and outputting the integrated output.
35. The computer-program product of claim 34, wherein: the static data subset includes image data and non-image data; the FFNN executed to transform the image data includes a convolutional neural network; and the FFNN executed to transform the non-image data includes a multi-layer perceptron neural network.
36. The computer-program product of claim 34, wherein: the temporally sequential data subset includes image data; the RNN executed to transform the image data includes a LSTM convolutional neural network; the multi-structure data set includes another temporally sequential data subset that includes non-image data; the actions further include executing a LSTM neural network to transform the non-image data into another RNN output; and the integrated output is further based on the other RNN output.
37. The computer-program product of claim 34, wherein: the RNN output includes at least one hidden state of an intermediate recurrent layer in the RNN; the multi-structure data set includes another static data subset that includes non-image data; the actions further include executing another FFNN to transform the other static data subset into another FFNN output; and the other FFNN output including a set of intermediate values generated at an intermediate hidden layer in the other FFNN.
38. The computer-program product of claim 34, wherein determining the integrated output includes executing an integration FFNN to transform the FFNN output and the RNN output to the integrated output, wherein each of the FFNN and the RNN were trained without using the integrated FFNN.
39. The computer-program product of claim 34, wherein the actions further include: concatenating the FFNN output and a data element of the multiple data elements from the temporally sequential data subset, the data element corresponding to an earliest time point of the multiple time points to produce concatenated data; wherein executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process an input that includes: the concatenated data; and for each other data element of the multiple data elements that correspond to time points of the multiple time points subsequent to the earliest time points, the other data element; and wherein the integrated output includes the RNN output.
40. The computer-program product of claim 34, wherein the actions further include: generating an input that includes, for each data element of the multiple data elements from the temporally sequential data subset, a concatenation of the data element and the FFNN output; wherein executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process the input; and wherein the integrated output includes the RNN output.
41. The computer-program product of claim 34, wherein: the multi-structure data set includes: another temporally sequential data subset that includes other multiple data elements corresponding to the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the actions further include: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; and generating an input that includes, for each time point of the multiple time points, a concatenated data element that includes the data element of the multiple data elements that corresponds to the time point and the other data element of the other multiple data elements that corresponds to the time point; executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process the input; the RNN output corresponds to a single hidden state of an intermediate recurrent layer in the RNN, the single hidden state corresponding to a single time point of the multiple time points; and determining the integrated output includes processing the static-data integrated output and the RNN output using a second integration neural network.
42. The computer-program product of claim 34, wherein: the multi-structure data set includes: another temporally sequential data subset that includes other multiple data elements corresponding to the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the actions further include: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; generating an input that includes, for each time point of the multiple time points, a concatenated data element that includes the data element of the multiple data elements that corresponds to the time point and the other data element of the other multiple data elements that corresponds to the time point; executing the RNN to transform the temporally sequential data subset into the RNN output includes using the RNN to process the input; the RNN output corresponds to multiple hidden states in the RNN, each of the multiple time points corresponding to a hidden state of the multiple hidden states; and determining the integrated output includes processing the static-data integration output and the RNN output using a second integration neural network.
43. The computer-program product of claim 34, wherein: the multi-structure data set includes: another temporally sequential data subset having another temporally sequential structure in that the other temporally sequential data subset includes other multiple data elements corresponding to other multiple time points, the other multiple time points being different than the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the actions further include: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; executing another RNN to transform the other temporally sequential data subset into another RNN output, the RNN having been trained independently from and executed independently from the other RNN, the RNN output including a single hidden state of an intermediate recurrent layer in the RNN, the single hidden state corresponding to a single time point of the multiple time points, the other RNN output including another single hidden state of another intermediate recurrent layer in the other RNN, the other single hidden state corresponding to another single time point of the other multiple time points; and concatenating the RNN output and the other RNN output; wherein determining the integrated output includes processing the static-data integrated output and the concatenated outputs using a second integration neural network.
44. The computer-program product of claim 34, wherein: the multi-structure data set includes: another temporally sequential data subset having another temporally sequential structure in that the other temporally sequential data subset includes other multiple data elements corresponding to other multiple time points, the other multiple time points being different than the multiple time points; and another static data subset of a different data type or data structure than the static data subset; the actions further include: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; executing another RNN to transform the other temporally sequential data subset into another RNN output, the RNN having been trained independently from and executed independently from the other RNN, the RNN output including multiple hidden states of an intermediate recurrent layer in the RNN, the multiple hidden states corresponding to the multiple time points, the other RNN output including other multiple hidden states of another intermediate recurrent layer in the other RNN, the other multiple hidden states corresponding to the other multiple time points; and concatenating the RNN output and the other RNN output; wherein determining the integrated output includes processing the static-data integrated output and the concatenated outputs using a second integration neural network.
45. The computer-program product of claim 34, wherein the actions further include: executing another FFNN to transform the other static data subset into another FFNN output; executing a first integration neural network to transform the FFNN output and the other FFNN output to a static-data integrated output; and concatenating the RNN output and the static-data integrated output; wherein determining the integrated output includes executing a second integration neural network to transform the concatenated outputs into the integrated output.
46. The computer-program product of claim 45, wherein the actions further include: concurrently training the first integration neural network, the second integration neural network and the RNN using an optimization technique, wherein executing the RNN includes executing the trained RNN, wherein executing the first integration neural network includes executing the trained first integration neural network, and wherein executing the second integration neural network includes executing the trained second integration neural network.
47. The computer-program product of claim 34, wherein the actions further include: accessing domain-specific data that includes a set of training data elements and a set of labels, wherein each training data element of the set of training data elements corresponds to a label of the set of labels; and training the FFNN using the domain-specific data.
48. A method comprising: receiving, a multi-structure data set corresponding to an entity, the multi-structure data set including a temporally sequential data subset and a static data subset, the temporally sequential data subset having a temporally sequential structure in that the temporally sequential data subset includes multiple data elements corresponding to multiple time points, and the static data subset having a static structure; executing a recurrent neural network (RNN) to transform the temporally sequential data subset into an RNN output; executing a feedforward neural network (FFNN) to transform the static data subset into a FFNN output, wherein the FFNN was trained without using the RNN and without using training data having the temporally sequential structure; determining an integrated output based on the RNN output, wherein at least one of the RNN output and the integrated output depend on the FFNN output, and wherein the integrated output corresponds to a prediction of an efficacy of treating the entity with a particular treatment; and outputting the integrated output; determining, based on the integrated output, to treat the subject with the particular treatment; and prescribing the particular treatment for the subject.
</claims>
</document>
