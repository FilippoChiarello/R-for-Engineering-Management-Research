<document>

<filing_date>
2019-09-17
</filing_date>

<publication_date>
2020-04-02
</publication_date>

<priority_date>
2018-09-26
</priority_date>

<ipc_classes>
G06T9/00
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
HEMMER, MICHAEL
MAKADIA, AMEESH
</inventors>

<docdb_family_id>
68104764
</docdb_family_id>

<title>
VIDEO ENCODING BY PROVIDING GEOMETRIC PROXIES
</title>

<abstract>
Compressing a frame of video includes receiving a frame of a video, identifying a three dimensional (3D) object in the frame, matching the 3D object to a stored 3D object, compressing the frame of the video using a color prediction scheme based on the 3D object and the stored 3D object, and storing the compressed frame with metadata, the metadata identifying the 3D object, indicating a position of the 3D object in the frame of the video and indicating an orientation of the 3D object in the frame of the video.
</abstract>

<claims>
1. A method comprising:
receiving a frame of a video;
identifying a three dimensional (3D) object in the frame;
matching the 3D object to a stored 3D object;
compressing the frame of the video using a color prediction scheme based on the 3D object and the stored 3D object; and
storing the compressed frame with metadata, the metadata identifying the 3D object, indicating a position of the 3D object in the frame of the video and indicating an orientation of the 3D object in the frame of the video.
2. The method of claim 1, wherein the compressing of the frame of the video using the color prediction scheme based on the 3D object and the stored 3D object includes:
generating a first 3D object proxy based on the stored 3D object;
transforming the first 3D object proxy based on the 3D object identified in the frame; generating a second 3D object proxy based on the stored 3D object;
identifying the 3D object in a key frame of the video;
transforming the second 3D object proxy based on the 3D object identified in the key frame;
mapping color attributes from the 3D object to the transformed first 3D object proxy; mapping color attributes from the 3D object identified in the key frame to the transformed second 3D object proxy; and
generating residuals for the 3D object based on the color attributes for the transformed first 3D object proxy and the color attributes for the transformed second 3D object proxy.
3. The method of claim 1, wherein the compressing of the frame of the video using the color prediction scheme based on the 3D object and the stored 3D object includes:
generating a first 3D object proxy based on the stored 3D object;
transforming the first 3D object proxy based on the 3D object identified in the frame; generating a second 3D object proxy based on the stored 3D object;
identifying the 3D object in a key frame of the video; transforming the second 3D object proxy based on the 3D object identified in the key frame;
mapping color atributes from the 3D object to the transformed first 3D object proxy; and
generating residuals for the 3D object based on the color attributes for the transformed first 3D object proxy and default color atributes for the transformed second 3D object proxy.
4. The method of claim 1, wherein the compressing of the frame of the video using the color prediction scheme based on the 3D object and the stored 3D object includes:
generating a first 3D object proxy based on the stored 3D object;
encoding the first 3D object proxy using an auto encoder;
transforming the encoded first 3D object proxy based on the 3D object identified in the frame;
generating a second 3D object proxy based on the stored 3D object;
encoding the second 3D object proxy using an autoencoder;
identifying the 3D object in a key frame of the video;
transforming the encoded second 3D object proxy based on the 3D object identified in the key frame;
mapping color atributes from the 3D object to the transformed first 3D object proxy; mapping color atributes from the 3D object identified in the key frame to the transformed second 3D object proxy; and
generating residuals for the 3D object based on the color attributes for the transformed first 3D object proxy and the color atributes for the transformed second 3D object proxy.
5. The method of claim 1, wherein the compressing of the frame of the video using the color prediction scheme based on the 3D object and the stored 3D object includes:
generating a first 3D object proxy based on the stored 3D object;
encoding the first 3D object proxy using an auto encoder;
transforming the encoded first 3D object proxy based on the 3D object identified in the frame;
generating a second 3D object proxy based on the stored 3D object;
encoding the second 3D object proxy using an autoencoder;
identifying the 3D object in a key frame of the video; transforming the encoded second 3D object proxy based on the 3D object identified in the key frame;
mapping color atributes from the 3D object to the transformed first 3D object proxy; and
generating residuals for the 3D object based on the color attributes for the transformed first 3D object proxy and default color atributes for the transformed second 3D object proxy.
6. The method of claim 1, further comprising:
before storing the 3D object:
identifying at least one 3D object of interest associated with the video;
determining a plurality of mesh atributes associated with the 3D object of interest; determining a position associated with the 3D object of interest;
determining an orientation associated with the 3D object of interest;
determining a plurality of color atributes associated with the 3D object of interest; and reducing a number of variables associated with the mesh attributes for the 3D object of interest using an autoencoder.
7. The method of claim 1, wherein compressing the frame of the video includes determining position coordinates of the 3D object relative to an origin coordinate of a background 3D object in a key frame.
8. The method of claim 1 , wherein
the stored 3D object includes default color atributes, and
the color prediction scheme uses the default color atributes.
9. The method of claim 1, further comprising:
identifying at least one 3D object of interest associated with the video;
generating at least one stored 3D object based on the at least one 3D object of interest, each of the at least one stored 3D object being defined by a mesh including a collection of points connected by faces, each point storing at least one atribute, the at least one atribute including a position coordinate for the respective point; and
storing the at least one stored 3D object in association with the video.
10. A method comprising:
receiving a frame of a video;
identifying a three dimensional (3D) object in the frame;
matching the 3D object to a stored 3D object;
decompressing the frame of the video using a color prediction scheme based on the 3D object and the stored 3D object; and
rendering the frame of the video.
11. The method of claim 10, wherein the decompressing of the frame of the video using the color prediction scheme based on the 3D object and the stored 3D object includes:
generating a first 3D object proxy based on the stored 3D object;
transforming the first 3D object proxy based on the 3D object identified in the frame; identifying the 3D object in a key frame of the video;
transforming the second 3D object proxy based on the 3D object identified in the key frame;
mapping color attributes from the 3D object to the transformed first 3D object proxy; mapping color attributes from the 3D object identified in the key frame to the transformed second 3D object proxy; and
generating color attributes for the 3D object based on the color attributes for the transformed first 3D object proxy and the color attributes for the transformed second 3D object proxy.
12. The method of claim 10, wherein the decompressing of the frame of the video using the color prediction scheme based on the 3D object and the stored 3D object includes:
generating a first 3D object proxy based on the stored 3D object;
transforming the first 3D object proxy based on the 3D object identified in the frame; identifying the 3D object in a key frame of the video;
transforming the second 3D object proxy based on the 3D object identified in the key frame;
mapping color attributes from the 3D object to the transformed first 3D object proxy; and generating color attributes for the 3D object based on the color attributes for the transformed first 3D object proxy and default color attributes for the transformed second 3D object proxy.
13. The method of claim 10, wherein the decompressing of the frame of the video using the color prediction scheme based on the 3D object and the stored 3D object includes:
generating a first 3D object proxy based on the stored 3D object;
decoding the first 3D object proxy using an autoencoder;
transforming the decoded first 3D object proxy based on metadata associated with the 3D object;
generating a second 3D object proxy based on the stored 3D object;
decoding the second 3D object proxy using an autoencoder;
identifying the 3D object in a key frame of the video;
transforming the decoded second 3D object proxy based on metadata associated with the 3D object identified in the key frame;
mapping color attributes from the 3D object to the transformed first 3D object proxy; mapping color attributes from the 3D object identified in the key frame to the transformed second 3D object proxy; and
generating color attributes for the 3D object based on the color attributes for the transformed first 3D object proxy and the color attributes for the transformed second 3D object proxy.
14. The method of claim 10, wherein the decompressing of the frame of the video using the color prediction scheme based on the 3D object and the stored 3D object includes:
generating a first 3D object proxy based on the stored 3D object;
decoding the first 3D object proxy using an autoencoder;
transforming the decoded first 3D object proxy based on metadata associated with the 3D object;
generating a second 3D object proxy based on the stored 3D object;
decoding the second 3D object proxy using an autoencoder;
identifying the 3D object in a key frame of the video;
transforming the decoded second 3D object proxy based on metadata associated with the 3D object identified in the key frame; mapping color attributes from the 3D object to the transformed first 3D object proxy; and
generating color attributes for the 3D object based on the color attributes for the transformed first 3D object proxy and default attributes for the transformed second 3D object proxy.
15. The method of claim 10, further comprising:
receiving at least one latent representation for a 3D shape;
using a machine trained generative modeling technique to:
determine a plurality of mesh attributes associated with the 3D shape;
determining a position associated with the 3D shape;
determining an orientation associated with the 3D shape; and determining a plurality of color attributes associated with the 3D shape; and storing the 3D shape as the stored 3D object.
16. The method of claim 10, wherein rendering the frame of the video includes:
receiving position coordinates of the 3D object relative to an origin coordinate of a background 3D object in a key frame; and
positioning the 3D object in the frame using the position coordinates.
17. The method of claim 10, further comprising:
receiving a neural network used by an encoder of an autoencoder to reduce a number of variables associated with mesh attributes, position, orientation and color attributes for at least one 3D object of interest;
regenerating points associated with a mesh for the at least one 3D object of interest using the neural network in a decoder of the autoencoder, the regeneration of the points including regenerating position attributes, orientation attributes and color attributes; and storing the at least one 3D object of interest as the stored 3D object.
18. A method for predicting color variance using a proxy:
generating a first 3D object proxy based on a stored 3D object;
generating a second 3D object proxy based on the stored 3D object; transforming the first 3D object proxy based on a 3D object identified in a frame of a video;
transforming the second 3D object proxy based on the 3D object identified in a key frame of the video;
mapping color attributes from the 3D object identified in the frame of the video to the transformed first 3D object proxy;
mapping color attributes from the 3D object identified in the key frame to the transformed second 3D object proxy; and
generating color data for the 3D object based on the color attributes for the transformed first 3D object proxy and the color attributes for the transformed second 3D object proxy.
19. The method of claim 18, further comprising:
before transforming the first 3D object proxy, encoding the first 3D object proxy using an autoencoder; and
before transforming the second 3D object proxy, encoding the second 3D object proxy using the autoencoder.
20. The method of claim 18, further comprising:
after transforming the first 3D object proxy, decoding the first 3D object proxy using an autoencoder; and
after transforming the second 3D object proxy, decoding the second 3D object proxy using the autoencoder.
21. The method of claim 18, wherein the generating of the color data for the 3D object includes subtracting the color attributes for the transformed first 3D object proxy from the color attributes for the transformed second 3D object proxy.
22. The method of claim 18, wherein the generating of the color data for the 3D object includes adding the color attributes for the transformed first 3D object proxy to the color attributes for the transformed second 3D object proxy.
</claims>
</document>
