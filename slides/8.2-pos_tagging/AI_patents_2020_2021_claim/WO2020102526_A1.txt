<document>

<filing_date>
2019-11-14
</filing_date>

<publication_date>
2020-05-22
</publication_date>

<priority_date>
2018-11-14
</priority_date>

<ipc_classes>
G06N3/02,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
NORTH CAROLINA STATE UNIVERSITY
</assignee>

<inventors>
LI, XILAI
WU TIANFU
</inventors>

<docdb_family_id>
70731689
</docdb_family_id>

<title>
DEEP NEURAL NETWORK WITH COMPOSITIONAL GRAMMATICAL ARCHITECTURES
</title>

<abstract>
The exemplified methods and systems provides deep neural network configured with a deep compositional grammatical architecture (e.g., to facilitate end-to-end representation learning). The instant deep compositional grammatical architecture beneficially integrates the compositionality and reconfigurability of grammar models with the capability of learning rich features of the deep neural networks in a principled way (e.g., for a convolutional neural network or a recombinant neural network). The instant deep compositional grammatical architecture utilizes AND-OR grammars to form an AND-OR grammar network.
</abstract>

<claims>
1. A computer-implemented method comprising:
instantiating one or more compositional grammatical neural network node layers, wherein at least one of the one or more compositional grammatical neural network node layer comprises a AND-OR grammar building block,
wherein the AND-OR grammar building block comprises an input that maps N groups of input-able features from one or more feature channels, and
wherein the AND-OR grammar building block comprises a graph of stacked and interconnected plurality of AND nodes and plurality of OR nodes that connects in a set of combinations of AND nodes and OR nodes to the N groups of inputted features of each of the one or more feature channels.
2. The computerimplemented method of claim 1, wherein the graph of
interconnected plurality of AND nodes and plurality of OR nodes are configured in a plurality of stacked stages, including a first stage followed by a second stage, wherein the first stage comprises at least one AND-node, and wherein the second stage comprises at least one OR-node.
3. The computerimplemented method of claim 1, wherein the graph of
interconnected plurality of AND nodes and plurality of OR nodes are configured in a plurality of stacked stages, including a first stage followed by a second stage, wherein the first stage comprises at least one OR-node, and wherein the second stage comprises at least one AND-node.
4. The computerimplemented method of claim 3, wherein the first stage comprises a first OR-node and a second OR-node, wherein the first OR-node is connected to a portion of the input, and wherein the second OR-node is connected to another portion of the input and to the first OR-node.
5. The computerimplemented method of claim 3, wherein the first stage comprises a first OR-node and a second OR-node, wherein the first OR-node is connected to a portion of the input, and wherein the second OR-node is connected to another portion of the input.
6. The computerimplemented method of claim 3, wherein the first stage comprises a first OR-node and a second OR-node, wherein the first OR-node is connected to a portion of the input, and wherein the second OR-node is connected to another portion of the input.
7. The computer-implemented method of claims 1-6, wherein the AND-OR grammar building block comprises a first hyper-parameter associated with a number of N groups of input-able features.
8. The computer-implemented method of claims 1-7, wherein the AND-OR grammar building block comprises a second hyper-parameter associated with a branching factor for each AND-nodes in the AND-OR grammar building block.
9. The computer-implemented method of claims 1-8, wherein the AND-OR grammar building block comprises a third hyper-parameter associated with i) phase structure grammar only and ii) a combination of phase structure grammar and dependency grammar.
10. The computer-implemented method of claims 1-9, wherein the AND-OR grammar building block comprises a fourth hyper-parameter associated with i) full phrase structure and ii) a partial phrase structure that do not include syntactically symmetric child nodes for ORnodes.
11. The computerimplemented method of claims 1-10, wherein the one or more compositional grammatical neural network node layers are instantiated in a convolutional neural network selected from the group consisting of GoogLeNets, ResNets, ResNeXts, DenseNets, and DualPathNets.
12. The computer-implemented method of claims 1-11, wherein the generated deep neural network structure comprises a second compositional grammatical neural network node layer, wherein the second compositional grammatical neural network node layer comprises a AND-OR grammar building block, wherein the AND-OR grammar building block comprises an input that maps N groups of input-able features from one or more feature channels, and wherein the AND-OR grammar building block comprises a graph of stacked and interconnected plurality of AND nodes (e.g., node configured to concatenate features from connected child nodes) and plurality of OR nodes (e.g., node configured to elementwise sum features from connected child nodes) that connects in a set of combinations of AND nodes and OR nodes to the N groups of inputted features of each of the one or more feature channels
13. The computerimplemented method of claims 1-12, wherein the generated deep neural network structure comprises one or more Conv-BatchNorm-ReLu stage that connects to a first instantiated compositional grammatical neural network node layer.
14. The computer-implemented method of claims 1-13, wherein the one or more compositional grammatical neural network nodes comprises a second AND-OR grammar building block.
15. The computer-implemented method of claims 1-14, further comprising:
classifying an image using the instantiated one or more neural network nodes.
16. The computer-implemented method of claims 1-14, further comprising:
classifying a linguistic text body using the instantiated one or more neural network nodes.
17. The computer-implemented system of any one of claims 1-16, wherein an N group of inputted features of at least one of the one or more feature channels includes at least 2 groups.
18. A computer-implemented method comprising:
a processor; and
a memory having instructions stored thereon, wherein execution of the instructions by the processor causes the processor to perform any of the method of claims 1-17.
19. A non-transitory computer readable medium comprising instructions stored thereon, wherein execution of the instructions by a processor causes the processor to perform any of the method of claims 1-17.
20. A computer-implemented method comprising:
a processor; and
a memory having instructions stored thereon, wherein execution of the instructions by the processor causes the processor to instantiate a compositional grammatical neural network node layer one or more ANDOR grammar building block means.
</claims>
</document>
