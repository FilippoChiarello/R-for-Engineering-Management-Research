<document>

<filing_date>
2019-02-01
</filing_date>

<publication_date>
2020-08-06
</publication_date>

<priority_date>
2019-02-01
</priority_date>

<ipc_classes>
G06T13/20,G06T19/00,G06T7/70
</ipc_classes>

<assignee>
IBM (INTERNATIONAL BUSINESS MACHINES CORPORATION)
</assignee>

<inventors>
TAO SHU
GUVEN KAYA, SINEM
ZHOU, BING
</inventors>

<docdb_family_id>
71836474
</docdb_family_id>

<title>
ACTIVE VISUAL RECOGNITION IN MOBILE AUGMENTED REALITY
</title>

<abstract>
A method, computer system, and a computer program product for intelligently generating an augmented reality (AR)-assisted repair guidance for a user is provided. The present invention may include detecting a scan of an object. The present invention may then include filtering one or more pose-controlled video frames. The present invention may also include extracting at least one new image feature by utilizing a convolutional neural network (CNN) based on the filtered one or more pose-controlled video frames. The present invention may also include aggregating a plurality of visual information associated with the object by utilizing the CNN, wherein the extracted at least one new image feature is included in the aggregated plurality of visual information. The present invention may further include presenting three dimensional (3D) animated instructions to the user, wherein the 3D animated instructions include an AR-assisted repair guidance for the object.
</abstract>

<claims>
1. A method for intelligently generating an augmented reality (AR)-assisted repair guidance for a user, the method comprising: detecting a scan of an object on a user mobile device; filtering one or more pose-controlled video frames, wherein the filtered one or pose-controlled video frames include the object; extracting at least one new image feature by utilizing a convolutional neural network (CNN) based on the filtered one or more pose-controlled video frames; aggregating a plurality of visual information associated with the object by utilizing the CNN, wherein the extracted at least one new image feature is included in the aggregated plurality of visual information; and presenting a three dimensional (3D) animated instructions to the user, wherein the 3D animated instructions include an AR-assisted repair guidance for the object.
2. The method of claim 1, wherein filtering one or more pose-controlled video frames, wherein the filtered one or pose-controlled video frames include the object, further comprises: calibrating an initial pose of the object, wherein a plurality of video frames are captured by one or more cameras associated with the user mobile device; automatically clustering one or more informative poses associated with the captured plurality of video frames; aggregating the one or more clustered informative poses associated with the captured plurality of video frames by the CNN, wherein a plurality of most informative poses are selected; and filtering the selected plurality of most informative poses based on a plurality of pose data associated with each of the most informative poses in the selected plurality of most informative poses.
3. The method of claim 2, wherein calibrating the initial pose of the object, wherein the plurality of video frames are captured by one or more cameras associated with the user mobile device, further comprises: instructing, the user, on a plurality of positions and a plurality of directions to point the one or more cameras associated with the user mobile device toward the object, wherein the object and the user mobile device are in the same coordinate for a consistent initial relative pose.
4. The method of claim 3, further comprising: identifying one or more points of interest (POIs) associated with the object; and focusing the one or more cameras associated with the user mobile device to capture a plurality of poses to include the identified one or more POIs associated with the object.
5. The method of claim 1, wherein presenting the three dimensional (3D) animated instructions to the user, wherein the 3D animated instructions include the AR-assisted repair guidance for the object, further comprises: verifying that one or more pre-requisites associated with each step of the 3D animated instructions is satisfied by utilizing one or more cameras, wherein the one or more pre-requisites include the utilization of a plurality of proper tools; in response to verifying that the one or more pre-requisites has been satisfied, proceeding to the next step of the 3D animated instructions.
6. The method of claim 2, wherein filtering the selected plurality of most informative poses based on the plurality of pose data associated with each of the most informative poses in the selected plurality of most informative poses, further comprises: retrieving the selected plurality of most informative poses associated with the captured plurality of video frames; and selecting a plurality of most stable video frames from the retrieved plurality of most informative poses based on the pose data associated with each captured video frame by utilizing one or more inertial sensors.
7. The method of claim 1, wherein extracting at least one new image feature by utilizing the convolutional neural network (CNN) based on the filtered one or more pose-controlled video frames, further comprising: receiving the filtered one or more pose-controlled video frames; dividing the received one or more pose-controlled video frames into a plurality of individual video frames; dividing the plurality of individual video frames into a plurality of images associated with each of the individual video frames in the plurality of individual video frames; and identifying a plurality of image features associated with the plurality of images.
8. The method of claim 7, further comprising: in response to determining the at least one new image feature is associated with the identified plurality of image features associated with the object, extracting the determined at least one new image feature.
9. The method of claim 8, further comprising: in response to determining no new image feature is associated with the identified plurality of image features associated with the object, receiving a plurality of previously used extracted image features associated with the object.
10. The method of claim 1, wherein aggregating the plurality of visual information associated with the object by utilizing the CNN, wherein the extracted at least one new image features is included in the aggregated plurality of visual information, further comprises: concatenating the plurality of visual information associated with the object; receiving the concatenated plurality of visual information by a deep neural network; and producing, as an output, a recognized state associated with the object.
11. A computer system for intelligently generating an augmented reality (AR)-assisted repair guidance for a user, comprising: one or more processors, one or more computer-readable memories, one or more computer-readable tangible storage medium, and program instructions stored on at least one of the one or more tangible storage medium for execution by at least one of the one or more processors via at least one of the one or more memories, wherein the computer system is capable of performing a method comprising: detecting a scan of an object on a user mobile device; filtering one or more pose-controlled video frames, wherein the filtered one or pose-controlled video frames include the object; extracting at least one new image feature by utilizing a convolutional neural network (CNN) based on the filtered one or more pose-controlled video frames; aggregating a plurality of visual information associated with the object by utilizing the CNN, wherein the extracted at least one new image feature is included in the aggregated plurality of visual information; and presenting a three dimensional (3D) animated instructions to the user, wherein the 3D animated instructions include an AR-assisted repair guidance for the object.
12. The computer system of claim 11, wherein filtering one or more pose-controlled video frames, wherein the filtered one or pose-controlled video frames include the object, further comprises: calibrating an initial pose of the object, wherein a plurality of video frames are captured by one or more cameras associated with the user mobile device; automatically clustering one or more informative poses associated with the captured plurality of video frames; aggregating the one or more clustered informative poses associated with the captured plurality of video frames by the CNN, wherein a plurality of most informative poses are selected; and filtering the selected plurality of most informative poses based on a plurality of pose data associated with each of the most informative poses in the selected plurality of most informative poses.
13. The computer system of claim 12, wherein calibrating the initial pose of the object, wherein the plurality of video frames are captured by one or more cameras associated with the user mobile device, further comprises: instructing, the user, on a plurality of positions and a plurality of directions to point the one or more cameras associated with the user mobile device toward the object, wherein the object and the user mobile device are in the same coordinate for a consistent initial relative pose.
14. The computer system of claim 11, wherein presenting the three dimensional (3D) animated instructions to the user, wherein the 3D animated instructions include the AR-assisted repair guidance for the object, further comprises: verifying that one or more pre-requisites associated with each step of the 3D animated instructions is satisfied by utilizing one or more cameras, wherein the one or more pre-requisites include the utilization of a plurality of proper tools; in response to verifying that the one or more pre-requisites has been satisfied, proceeding to the next step of the 3D animated instructions.
15. The computer system of claim 12, wherein filtering the selected plurality of most informative poses based on the plurality of pose data associated with each of the most informative poses in the selected plurality of most informative poses, further comprises: retrieving the selected plurality of most informative poses associated with the captured plurality of video frames; and selecting a plurality of most stable video frames from the retrieved plurality of most informative poses based on the pose data associated with each captured video frame by utilizing one or more inertial sensors.
16. The computer system of claim 11, wherein extracting at least one new image feature by utilizing the convolutional neural network (CNN) based on the filtered one or more pose-controlled video frames, further comprising: receiving the filtered one or more pose-controlled video frames; dividing the received one or more pose-controlled video frames into a plurality of individual video frames; dividing the plurality of individual video frames into a plurality of images associated with each of the individual video frames in the plurality of individual video frames; and identifying a plurality of image features associated with the plurality of images.
17. A computer program product for intelligently generating an augmented reality (AR)-assisted repair guidance for a user, comprising: one or more computer-readable storage media and program instructions stored on at least one of the one or more tangible storage media, the program instructions executable by a processor to cause the processor to perform a method comprising: detecting a scan of an object on a user mobile device; filtering one or more pose-controlled video frames, wherein the filtered one or pose-controlled video frames include the object; extracting at least one new image feature by utilizing a convolutional neural network (CNN) based on the filtered one or more pose-controlled video frames; aggregating a plurality of visual information associated with the object by utilizing the CNN, wherein the extracted at least one new image feature is included in the aggregated plurality of visual information; and presenting a three dimensional (3D) animated instructions to the user, wherein the 3D animated instructions include an AR-assisted repair guidance for the object.
18. The computer program product of claim 17, wherein filtering one or more pose-controlled video frames, wherein the filtered one or pose-controlled video frames include the object, further comprises: calibrating an initial pose of the object, wherein a plurality of video frames are captured by one or more cameras associated with the user mobile device; automatically clustering one or more informative poses associated with the captured plurality of video frames; aggregating the one or more clustered informative poses associated with the captured plurality of video frames by the CNN, wherein a plurality of most informative poses are selected; and filtering the selected plurality of most informative poses based on a plurality of pose data associated with each of the most informative poses in the selected plurality of most informative poses.
19. The computer program product of claim 18, wherein calibrating the initial pose of the object, wherein the plurality of video frames are captured by one or more cameras associated with the user mobile device, further comprises: instructing, the user, on a plurality of positions and a plurality of directions to point the one or more cameras associated with the user mobile device toward the object, wherein the object and the user mobile device are in the same coordinate for a consistent initial relative pose.
20. The computer program product of claim 19, further comprising: identifying one or more points of interest (POIs) associated with the object; and focusing the one or more cameras associated with the user mobile device to capture a plurality of poses to include the identified one or more POIs associated with the object.
</claims>
</document>
