<document>

<filing_date>
2020-05-27
</filing_date>

<publication_date>
2020-09-10
</publication_date>

<priority_date>
2018-10-19
</priority_date>

<ipc_classes>
G06K9/00,G06K9/62,G06N3/08
</ipc_classes>

<assignee>
BEIJING SENSETIME TECHNOLOGY DEVELOPMENT COMPANY
</assignee>

<inventors>
CHEN, KEYU
QIAN CHEN
WANG FEI
</inventors>

<docdb_family_id>
70283563
</docdb_family_id>

<title>
METHOD AND APPARATUS FOR CHILD STATE ANALYSIS, VEHICLE, ELECTRONIC DEVICE, AND STORAGE MEDIUM
</title>

<abstract>
A method and for child state analysis, a vehicle, an electronic device, and a storage medium are provided. The method includes: performing face feature extraction on at least one image frame in an obtained video stream; classifying whether a person in the image is a child and at least one state of the person according to face features to obtain a first classification result of whether the person in the image is a child, and a second classification result of the at least one state of the person; outputting the first classification result and the second classification result; and/or outputting prompt information according to the first classification result and the second classification result.
</abstract>

<claims>
1. A method for child state analysis, comprising: performing face feature extraction on at least one image frame in an obtained video stream; classifying whether a person in the at least one image frame is a child and at least one state of the person according to face features to obtain a first classification result of whether the person in the at least one image frame is a child, and a second classification result of the at least one state of the person; and at least one of: outputting the first classification result and the second classification result; or outputting prompt information according to the first classification result and the second classification result.
2. The method according to claim 1, further comprising: in response to the first classification result indicating that the person in the at least one image frame is a child, determining whether the second classification result satisfies a predetermined condition; and in response to the second classification result satisfying the predetermined condition, outputting the prompt information.
3. The method according to claim 1, wherein the state of the person comprises at least one of the following: a normal state, an abnormal state, a sleep state, a wake-up state, or a state of leaving a child seat; and the second classification result comprises at least one of the following: whether the state of the person is the normal state, whether the state of the person is the abnormal state, whether the state of the person is the sleep state, whether the state of the person is the wake-up state, or whether the state of the person is the state of leaving the child seat.
4. The method according to claim 3, wherein the abnormal state comprises at least one of the following: a crying state, irritable state, a vomiting state, a choking state, or a pain state.
5. The method according to claim 2, wherein the predetermined condition comprises at least one of the following: the state of the person is an abnormal state, the state of the person is a wake-up state, or the state of the person is a state of leaving a child seat.
6. The method according to claim 4, wherein in response to the second classification result satisfying that the state of the person is the abnormal state, outputting the prompt information comprises: in response to at least one of the following conditions: the person being in the crying state for a first preset duration, the person being in the irritable state for a second preset duration, or the person being in the pain state for a third preset duration, outputting the prompt information.
7. The method according to claim 1, wherein performing face feature extraction on the at least one image frame in the obtained video stream comprises: performing face feature extraction on the at least one image frame in the obtained video stream by using a feature extraction branch of a neural network; and classifying whether the person in the at least one image frame is a child and at least one state of the person according to the face features comprises: respectively determining whether the person in the at least one image frame is a child and classification of the at least one state of the person based on the face features extracted by the feature extraction branch by using at least two classification branches connected to the feature extraction branch in the neural network.
8. The method according to claim 1, wherein performing face feature extraction on the at least one image frame in the obtained video stream comprises: performing face feature extraction on the at least one image frame in the obtained video stream by using a feature extraction branch of a neural network; classifying whether the person in the at least one image frame is a child and at least one state of the person according to the face features comprises: respectively determining whether the person in the at least one image frame is a child and classification of at least one face state of the person based on the face features extracted by the feature extraction branch by using at least two classification branches connected to the feature extraction branch in the neural network; and performing statistics on classification results of the at least one face state of the person corresponding to the at least one image frame in the obtained video stream by using a statistical classification branch connected to the at least two classification branches in the neural network, and determining the classification of the at least one state of the person based on a statistical result.
9. The method according to claim 8, wherein the face state of the person comprises at least one of the following: an eye open state or an eye closed state.
10. The method according to claim 7, wherein the classification branch for determining whether the person in the at least one image frame is a child is pre-trained based on annotation data for children of distinguishing genders.
11. The method according to claim 7, wherein before performing face feature extraction on the at least one image frame in the obtained video stream, the method further comprises: training the neural network by using a sample image which is provided with child annotation information and state annotation information.
12. The method according to claim 11, wherein the child annotation information is used to indicate whether the person in the sample image is a child; in response to an age or estimated age of the person in the sample image being greater than an age or estimated age of a person in a reference image, the child annotation information of the sample image is used to indicate that the person is not a child; and in response to the age or estimated age of the person in the sample image being less than or equal to the age or estimated age of the person in the reference image, the child annotation information of the sample image is used to indicate that the person is a child.
13. The method according to claim 12, wherein the reference image comprises a boy reference image and a girl reference image.
14. The method according to claim 11, wherein training the neural network by using the sample image comprises: performing face feature extraction on the sample image by using the feature extraction branch; classifying whether the person in the sample image is a child and at least one state of the person by sharing face features extracted by the at least two classification branches, to obtain a first predicted classification result of whether the person in the sample image is a child and a second predicted classification result of the at least one state of the person; obtaining a first loss based on the first predicted classification result and the child annotation information, and obtaining a second loss based on the second predicted classification result and the state annotation information; and adjusting parameters of the neural network based on the first loss and the second loss.
15. The method according to claim 14, wherein adjusting the parameters of the neural network based on the first loss and the second loss comprises: performing weighted summation on the first loss and the second loss to obtain a network loss; and adjusting parameters of the feature extraction branch and the at least two classification branches based on the network loss.
16. The method according to claim 1, further comprising: displaying at least one piece of the following information by using at least one of a vehicle-mounted device or a terminal device: the obtained video stream, the first classification result, the second classification result, or the prompt information.
17. The method according to claim 1, wherein before performing face feature extraction on the at least one image frame in the obtained video stream, the method further comprises: obtaining the video stream by using at least one camera provided in a vehicle.
18. An apparatus for child state analysis, comprising: a processor; and a memory configured to store instructions executable by the processor, wherein the processor, upon execution of the instructions, is configured to: perform face feature extraction on at least one image frame in an obtained video stream; classify whether a person in the at least one image frame is a child and at least one state of the person according to face features to obtain a first classification result of whether the person in the at least one image frame is a child, and a second classification result of the at least one state of the person; and at least one of: output the first classification result and the second classification result; or output prompt information according to the first classification result and the second classification result.
19. A vehicle, comprising: the apparatus for child state analysis according to claim 18.
20. A non-transitory computer storage medium, configured to store computer readable instructions that, when being executed by a computer, cause the computer to implement the following: performing face feature extraction on at least one image frame in an obtained video stream; classifying whether a person in the at least one image frame is a child and at least one state of the person according to face features to obtain a first classification result of whether the person in the at least one image frame is a child, and a second classification result of the at least one state of the person; and at least one of: outputting the first classification result and the second classification result; or outputting prompt information according to the first classification result and the second classification result.
</claims>
</document>
