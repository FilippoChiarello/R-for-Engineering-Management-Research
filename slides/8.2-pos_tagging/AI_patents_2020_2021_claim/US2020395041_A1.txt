<document>

<filing_date>
2019-02-19
</filing_date>

<publication_date>
2020-12-17
</publication_date>

<priority_date>
2018-02-20
</priority_date>

<ipc_classes>
G10L15/14,G10L15/16,G10L25/30,G10L25/69,G10L25/75
</ipc_classes>

<assignee>
NTT (NIPPON TELEGRAPH AND TELEPHONE CORPORATION)
</assignee>

<inventors>
KAMEOKA, HIROKAZU
TANAKA, Ko
</inventors>

<docdb_family_id>
67687781
</docdb_family_id>

<title>
DEVICE, METHOD, AND PROGRAM FOR ANALYZING SPEECH SIGNAL
</title>

<abstract>
A parameter included in a fundamental frequency pattern of a voice can be estimated from the fundamental frequency pattern with high accuracy and the fundamental frequency pattern of the voice can be reconstructed from the parameter included in the fundamental frequency pattern. A learning unit 30 learns a deep generation model including an encoder which regards a parameter included in a fundamental frequency pattern in a voice signal as a latent variable of the deep generation model and estimates the latent variable from the fundamental frequency pattern in the voice signal on the basis of parallel data of the fundamental frequency pattern in the voice signal and the parameter included in the fundamental frequency pattern in the voice signal, and a decoder which reconstructs the fundamental frequency pattern in the voice signal from the latent variable.
</abstract>

<claims>
1. 1.-7. (canceled)
8. A computer-implemented method for estimating aspects of voice data, the method comprising: learning a deep generation model, wherein the deep generation model comprises: an encoder estimating a first parameter included in a first fundamental frequency pattern in a first voice data as a latent variable of the deep generation model using parallel data between the first fundamental frequency pattern in the first voice data and the first parameter included in the first fundamental frequency pattern in the first voice data, and a decoder reconstructing the first fundamental frequency pattern in the first voice data based on the latent variable of the deep generation model; estimating, based on a second fundamental frequency pattern in a second voice data, a second parameter included in the second fundamental frequency pattern using the encoder of the deep generation model; and estimating, based on a third parameter included in a third fundamental frequency pattern in a third voice data, the third fundamental frequency pattern using the decoder of the deep generation model.
9. The computer-implemented method of claim 8, the method further comprising: maximizing an output of an objective function for learning the deep generation model, wherein the objective function is based at least on: a distance between an output of the decoder having the first fundamental frequency pattern of the first voice data as an input and a prior distribution of the first parameter represented using a state sequence of a path-restricted hidden Markov model (HMM), and an output of the encoder having the latent variable as an input of the decoder.
10. The computer-implemented method of claim 9, wherein each of the encoder and the decoder is configured using a convolutional neural network.
11. The computer-implemented method of claim 9, wherein the first voice data is a learning data, wherein the first voice data and the second voice data are distinct, and wherein the first voice data and the third voice data are distinct.
12. The computer-implemented method of claim 9, wherein the first fundamental frequency pattern of the first voice data relates to one or more of: an interrogative sentence based on the ending of an utterance sentence, an intention of a speaker represented by the first voice data, a melody of a singer represented by the first voice data, and an emotion of the singer represented by the first voice data.
13. The computer-implemented method of claim 9, wherein the first parameter in the first fundamental frequency pattern of the first voice data represents one or more of: an accent of voice in the first voice data, and vibrato and overshoot in the first voice data as a singing voice.
14. The computer-implemented method of claim 9, the method further comprising: receiving the first voice data; receiving the first fundamental frequency pattern based on the received singing voice data; generating the first parameter included in the first fundamental frequency pattern in the received singing voice data, wherein the first parameter relates to vibrato and overshoot; learning the deep generation model; and synthesizing voice data of the singer based on the learnt deep generation model; and providing the synthesized voice data.
15. A system for estimating aspects of voice data, the system comprises: a processor; and a memory storing computer-executable instructions that when executed by the processor cause the system to: learn a deep generation model, wherein the deep generation model comprises: an encoder estimating a first parameter included in a first fundamental frequency pattern in a first voice data as a latent variable of the deep generation model using parallel data between the first fundamental frequency pattern in the first voice data and the first parameter included in the first fundamental frequency pattern in the first voice data, and a decoder reconstructing the first fundamental frequency pattern in the first voice data based on the latent variable of the deep generation model; estimate, based on a second fundamental frequency pattern in a second voice data, a second parameter included in the second fundamental frequency pattern using the encoder of the deep generation model; and estimate, based on a third parameter included in a third fundamental frequency pattern in a third voice data, the third fundamental frequency pattern using the decoder of the deep generation model.
16. The system of claim 15, the computer-executable instructions when executed further causing the system to: maximize an output of an objective function for learning the deep generation model, wherein the objective function is based at least on: a distance between an output of the decoder having the first fundamental frequency pattern of the first voice data as an input and a prior distribution of the first parameter represented using a state sequence of a path-restricted hidden Markov model (HMM), and an output of the encoder having the latent variable as an input of the decoder.
17. The system of claim 15, wherein each of the encoder and the decoder is configured using a convolutional neural network.
18. The system of claim 15, wherein the first voice data is a learning data, wherein the first voice data and the second voice data are distinct, and wherein the first voice data and the third voice data are distinct.
19. The system of claim 15, wherein the first fundamental frequency pattern of the first voice data relates to one or more of: an interrogative sentence based on the ending of an utterance sentence, an intention of a speaker represented by the first voice data, a melody of a singer represented by the first voice data, and an emotion of the singer represented by the first voice data.
20. The system of claim 15, wherein the first parameter in the first fundamental frequency pattern of the first voice data represents one or more of: an accent of voice in the first voice data, and vibrato and overshoot in the first voice data as a singing voice.
21. The system of claim 15, the computer-executable instructions when executed further causing the system to: receive the first voice data; generate the first fundamental frequency pattern based on the received singing voice data; generate the first parameter included in the first fundamental frequency pattern in the received singing voice data, wherein the first parameter relates to vibrato and overshoot; learn the deep generation model; and synthesize voice data of the singer based on the learnt deep generation model; and provide the synthesized voice data.
22. A computer-readable non-transitory recording medium storing computer-executable instructions that when executed by a processor cause a computer system to: learn a deep generation model, wherein the deep generation model comprises: an encoder estimating a first parameter included in a first fundamental frequency pattern in a first voice data as a latent variable of the deep generation model using parallel data between the first fundamental frequency pattern in the first voice data and the first parameter included in the first fundamental frequency pattern in the first voice data, and a decoder reconstructing the first fundamental frequency pattern in the first voice data based on the latent variable of the deep generation model; estimate, based on a second fundamental frequency pattern in a second voice data, a second parameter included in the second fundamental frequency pattern using the encoder of the deep generation model; and estimate, based on a third parameter included in a third fundamental frequency pattern in a third voice data, the third fundamental frequency pattern using the decoder of the deep generation model.
23. The computer-readable non-transitory recording medium of claim 22, the computer-executable instructions when executed further causing the system to: maximize an output of an objective function for learning the deep generation model, wherein the objective function is based at least on: a distance between an output of the decoder having the first fundamental frequency pattern of the first voice data as an input and a prior distribution of the first parameter represented using a state sequence of a path-restricted hidden Markov model (HMM), and an output of the encoder having the latent variable as an input of the decoder.
24. The computer-readable non-transitory recording medium of claim 22, wherein each of the encoder and the decoder is configured using a convolutional neural network.
25. The computer-readable non-transitory recording medium of claim 22, wherein the first fundamental frequency pattern of the first voice data relates to one or more of: an interrogative sentence based on the ending of an utterance sentence, an intention of a speaker represented by the first voice data, a melody of a singer represented by the first voice data, and an emotion of the singer represented by the first voice data.
26. The computer-readable non-transitory recording medium of claim 23, wherein the first parameter in the first fundamental frequency pattern of the first voice data represents one or more of: an accent of voice in the first voice data, and vibrato and overshoot in the first voice data as a singing voice.
27. The computer-readable non-transitory recording medium of claim 23, the computer-executable instructions when executed further causing the system to: receive the first voice data; generate the first fundamental frequency pattern based on the received singing voice data; generate the first parameter included in the first fundamental frequency pattern in the received singing voice data, wherein the first parameter relates to vibrato and overshoot; learn the deep generation model; and synthesize voice data of the singer based on the learnt deep generation model; and provide the synthesized voice data.
</claims>
</document>
