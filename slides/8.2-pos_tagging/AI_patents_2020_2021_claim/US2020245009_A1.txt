<document>

<filing_date>
2019-01-25
</filing_date>

<publication_date>
2020-07-30
</publication_date>

<priority_date>
2019-01-25
</priority_date>

<ipc_classes>
G06N20/20,G06N7/00,H04N21/25,H04N21/6379,H04N21/81
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
CHAVAN, PRITHVIRAJ ABASAHEB
DHAMNANI, SUNNY
IBRAHIM, AS AKIL ARIF
SAINI, SHIV KUMAR
SRINIVASAN, AAKASH
</inventors>

<docdb_family_id>
71732964
</docdb_family_id>

<title>
UTILIZING A DEEP GENERATIVE MODEL WITH TASK EMBEDDING FOR PERSONALIZED TARGETING OF DIGITAL CONTENT THROUGH MULTIPLE CHANNELS ACROSS CLIENT DEVICES
</title>

<abstract>
The present disclosure relates to systems, methods, and non-transitory computer readable media for training and utilizing a generative machine learning model to select one or more treatments for a client device from a set of treatments based on digital characteristics corresponding to the client device. In particular, the disclosed systems can train and apply a variational autoencoder with a task embedding layer that generates estimated effects for treatment combinations. For example, the disclosed systems receive, as input, digital characteristics corresponding to the client device and various treatment combinations. The disclosed systems apply the trained generative machine learning model with the task embedding layer to the digital characteristics to generate effect estimations for the various treatment combinations. Based on the effect estimations for the treatment combinations, the disclosed systems select one or more treatments to provide to the client device.
</abstract>

<claims>
1. In a digital medium environment for selecting and transmitting individualized digital content to client devices across multiple distribution channels, a computer-implemented method for utilizing a deep generative model to efficiently and accurately generate treatment effect estimations, the computer-implemented method comprising: identifying a digital repository of historical training data comprising training characteristic sets corresponding to training client devices, training treatments, and training effects; a step for training a generative machine learning model with a task embedding layer utilizing the digital repository of historical training data; identifying a client device, digital characteristics corresponding to the client device, and a set of treatments; and a step for generating a multi-treatment effect estimation for the client device utilizing the digital characteristics via the trained generative machine learning model with the task embedding layer.
2. The method of claim 1, wherein the generative machine learning model with the task embedding layer comprises a variational autoencoder having the task embedding layer.
3. The method of claim 2, wherein the variational autoencoder comprises an encoder comprising the task embedding layer and a decoder comprising a second task embedding layer.
4. The method of claim 1, wherein the multi-treatment effect estimation comprises a predicted response from the client device upon utilizing a combination of at least two treatments from the set of treatments.
5. The method of claim 4, wherein the set of treatments comprise at least two of: providing digital content to the client device via e-mail, providing digital content to the client device via social media, providing digital content to the client device via a text message, providing digital content to the client device via a digital notification via a software application on the client device, providing digital content to the client device via paid search, or providing digital content to the client device via a website.
6. A non-transitory computer readable medium comprising instructions that, when executed by at least one processor, cause a computing device to: identify a client device and digital characteristics corresponding to the client device; apply a trained generative machine learning model with a task embedding layer to the digital characteristics corresponding to the client device to generate a multi-treatment effect estimation of a first subset of treatments from a set of treatments by: utilizing the task embedding layer of the trained generative machine learning model to generate a first task embedding of the first subset of treatments from the set of treatments; analyzing the first task embedding of the first subset of treatments and the digital characteristics corresponding to the client device via the trained generative machine learning model to generate a first multi-treatment effect estimation of the first subset of treatments; and based on the first multi-treatment effect estimation of the first subset of treatments, select one or more treatments from the set of treatments to provide to the client device, the one or more treatments comprising one or more distribution channels for providing digital content to the client device.
7. The non-transitory computer readable medium of claim 6, further comprising instructions that, when executed by the at least one processor, cause the computing device to: utilize the task embedding layer of the trained generative machine learning model to generate a second task embedding of a second subset of treatments from the set of treatments; and analyze the second task embedding of the second subset of treatments to generate a second multi-treatment effect estimation of the second subset of treatments.
8. The non-transitory computer readable medium of claim 7, further comprising instructions that, when executed by the at least one processor, cause the computing device to select the one or more treatments from the set of treatments by comparing the multi-treatment effect estimation and the second multi-treatment effect estimation.
9. The non-transitory computer readable medium of claim 7, further comprising instructions that, when executed by the at least one processor, cause the computing device to: generate the first task embedding and the second task embedding by applying an embedding weight matrix to the first subset of treatments and the second subset of treatments.
10. The non-transitory computer readable medium of claim 6, wherein the trained generative machine learning model comprises a variational autoencoder comprising an encoder having the task embedding layer and a decoder.
11. The non-transitory computer readable medium of claim 10, further comprising instructions that, when executed by the at least one processor, cause the computing device to: utilize the encoder to analyze the first task embedding of the first subset of treatments and the digital characteristics corresponding to the client device to generate latent features of the client device; and utilize the decoder to analyze the latent features of the client device and the first task embedding of the first subset of treatments to generate the first multi-treatment effect estimation.
12. The non-transitory computer readable medium of claim 6, wherein: the set of treatments comprises at least three of: providing digital content to the client device via e-mail, providing digital content to the client device via social media, providing digital content to the client device via a text message, providing digital content to the client device via a digital notification via a software application on the client device, providing digital content to the client device via paid search, or providing digital content to the client device via a website; the first subset of treatments comprises at least two treatments from the set of treatments; and the selected one or more treatments consist of the first subset of treatments.
13. The non-transitory computer readable medium of claim 6, wherein the digital characteristics comprise online activity of a user associated with the client device.
14. A system comprising: at least one processor; a memory comprising: a variational autoencoder having a task embedding layer; a digital repository of historical training data comprising a training characteristic sets corresponding to training client devices, training treatments, and training effects; and instructions that, when executed by the at least one processor, cause the system to train the variational autoencoder by: applying the variational autoencoder to a training characteristic set of the training characteristic sets to generate a predicted distribution of treatments in light of the training characteristic set; utilizing the task embedding layer of the variational autoencoder to generate a task embedding of the treatments based on the predicted distribution of the treatments; analyzing the task embedding and the training characteristic set via the variational autoencoder to generate a predicted multi-treatment effect; and modifying parameters of the variational autoencoder by comparing the predicted multi-treatment effect with a training effect from the training effects.
15. The system of claim 14, wherein the variational autoencoder comprises an encoder having the task embedding layer and a decoder.
16. The system of claim 15 further comprising instructions that, when executed by the at least one processor, cause the system to: utilize the encoder to analyze the task embedding of the treatments generated by the task embedding layer to generate latent features of the training client devices; and utilize the decoder to analyze the latent features of the training client devices and the multi-task embedding of the treatments to generate the predicted multi-treatment effect.
17. The system of claim 16 further comprising instructions that, when executed by the at least one processor, cause the system to generate the task embedding of the treatments by: generating the predicted distribution of treatments in light of the training characteristic set; sampling the predicted distribution of treatments to generate a treatment vector; and generating the task embedding by applying a weight matrix to the treatment vector.
18. The system of claim 17 further comprising instructions that, when executed by the at least one processor, cause the system to modify parameters of the variational autoencoder by determining a measure of loss between the predicted distribution of treatments in light of the training characteristic set and a training distribution of treatments in light of the training characteristic set.
19. The system of claim 16 further comprising instructions that, when executed by the at least one processor, cause the system to generate the predicted multi-treatment effect by: generating a predicted distribution of treatments in light of the latent features of the training client devices; sampling the predicted distribution of treatments in light of the latent features of the training client devices to generate a decoder treatment vector; and generating the predicted multi-treatment effect by applying a weight matrix to the decoder treatment vector.
20. The system of claim 19, further comprising instructions that, when executed by the at least one processor, cause the system to modify parameters of the variational autoencoder by determining a measure of loss between the predicted distribution of treatments in light of the latent features of the training client devices with a training distribution of treatments in light of the latent features of the training client devices.
</claims>
</document>
