<document>

<filing_date>
2019-12-17
</filing_date>

<publication_date>
2020-06-18
</publication_date>

<priority_date>
2018-12-17
</priority_date>

<ipc_classes>
G06K9/46,G06K9/62,G06K9/66,G06K9/72,G06N3/08
</ipc_classes>

<assignee>
SRI INTERNATIONAL
</assignee>

<inventors>
AHUJA, KARUNA
DIVAKARAN AJAY
ROY, ANIRBAN
SIKKA, KARAN
</inventors>

<docdb_family_id>
71072703
</docdb_family_id>

<title>
ALIGNING SYMBOLS AND OBJECTS USING CO-ATTENTION FOR UNDERSTANDING VISUAL CONTENT
</title>

<abstract>
A method, apparatus and system for understanding visual content includes determining at least one region proposal for an image, attending at least one symbol of the proposed image region, attending a portion of the proposed image region using information regarding the attended symbol, extracting appearance features of the attended portion of the proposed image region, fusing the appearance features of the attended image region and features of the attended symbol, projecting the fused features into a semantic embedding space having been trained using fused attended appearance features and attended symbol features of images having known descriptive messages, computing a similarity measure between the projected, fused features and fused attended appearance features and attended symbol features embedded in the semantic embedding space having at least one associated descriptive message and predicting a descriptive message for an image associated with the projected, fused features.
</abstract>

<claims>
1. A method for understanding visual content, comprising: determining at least one region proposal for an image of the visual content; attending at least one corresponding symbol of the proposed image region of the visual content; attending at least one image portion of the proposed image region using information regarding the attended, at least one corresponding symbol; extracting visual features of the attended, at least one portion of the proposed image region using a neural network; fusing the visual features of the attended, at least one portion of the proposed image region and features of the attended, at least one corresponding symbol; projecting the fused features into a semantic embedding space having been trained using fused attended visual features and attended symbol features of images having known descriptive messages; computing a similarity measure between the projected, fused features and fused attended visual features and attended symbol features embedded in the semantic embedding space having at least one associated descriptive message; and predicting a descriptive message for an image associated with the projected, fused features by determining a nearest embedded, fused attended visual features and attended symbol features to the projected, fused features of the image in the semantic embedding space based on the similarity measures computed for the projected, fused features of the image.
2. The method of claim 1, wherein fusing the visual features of the attended, at least one portion of the proposed image region and the features of the attended, at least one corresponding symbol comprises: determining a summary vector for the extracted visual features; determining a summary vector for the symbol features; and fusing the summary vector of the extracted visual features and the summary vector of the symbol features.
3. The method of claim 2, wherein determining the summary vector for the visual features comprises normalizing attention scores calculated for the visual features using an initialized summary vector for the symbol features, and wherein determining the summary vector for the symbol features comprises normalizing attention scores calculated for the symbol features using a previously determined summary vector for the visual features.
4. The method of claim 1, wherein attending a symbol of the proposed image region and attending at least one image portion of the proposed image region for each proposed image region is performed in a multi-hop iteration process comprising using at least one of symbol attention information and image portion attention information from a previous iteration as input for attending at least one image portion and symbol in a subsequent iteration.
5. The method of claim 1, wherein visual features and symbol features are extracted using at least one neural network.
6. The method of claim 5, wherein the at least one neural network comprises at least one of a convolutional neural network or a recurrent neural network.
7. The method of claim 1, further comprising assigning a score for each of the symbol features.
8. The method of claim 7, wherein the score is based on at least one of information input by a user or information predicted using a neural network.
9. The method of claim 1, wherein the symbol comprises at least one of text and audio.
10. A method for creating a semantic embedding space for determining a descriptive message for understanding images of visual content, comprising: for each of a plurality of images of the visual content having at least one respective, known descriptive message; determining at least one region proposal for the image, and for each of the proposed image regions; attending at least one corresponding symbol of the proposed image region of the visual content; attending at least one portion of the image region using information regarding the attended, at least one corresponding symbol; extracting appearance features of the attended, at least one portion of the proposed image region; fusing the appearance features of the attended, at least one portion of the proposed image region and features of the attended, at least one corresponding symbol; and creating a first feature vector representation of the fused features; encoding words of the known descriptive message into a sentence; creating a second feature vector representation of the sentence; and semantically embedding the first feature vector representation and the second feature vector representation in a semantic embedding space such that embedded feature vector representations that are related are closer together in the semantic embedding space than unrelated feature vector representations.
11. The method of claim 10, wherein the semantic embedding space is created using a max-margin based triplet ranking loss function.
12. An apparatus for visual content understanding of an image, comprising: a region proposal module determining at least one region proposal for an image of the visual content; an attention module attending at least one corresponding symbol of the proposed image region of the visual content and attending at least one portion of the proposed image region using information regarding the attended, at least one corresponding symbol; a feature detection module extracting appearance features of the attended, at least one portion of the proposed image region using a neural network; a modality fusion module fusing the appearance features of the attended, at least one portion of the proposed image region and features of the attended, at least one corresponding symbol; an embedding module projecting the fused features into a semantic embedding space having been trained using fused attended appearance features and attended symbol features of images having known descriptive messages; computing a similarity measure between the projected, fused features and fused attended appearance features and attended symbol features embedded in the semantic embedding space having at least one associated descriptive message; predicting a descriptive message for an image associated with the projected, fused features by determining a nearest embedded, fused attended appearance features and attended symbol features to the projected, fused features of the image in the semantic embedding space based on the similarity measures computed for the projected, fused features of the image.
13. The apparatus of claim 12, wherein for fusing the visual features of the attended, at least one portion of the proposed image region and the features of the attended, at least one corresponding symbol, the modality fusion module is configured to: determine a summary vector for the extracted visual features; determine a summary vector for the symbol features; and fuse the summary vector of the extracted visual features and the summary vector of the symbol features.
14. The apparatus of claim 13, wherein for determining the summary vector for the visual features, the modality fusion module is configured to normalize attention scores calculated for the visual features using an initialized summary vector for the symbol features, and wherein for determining the summary vector for the symbol features the modality fusion module is configured to normalize attention scores calculated for the symbol features using a previously determined summary vector for the visual features.
15. The apparatus of claim 12, wherein attending a symbol of the proposed image region and attending at least one image portion of the proposed image region for each proposed image region is performed by the attention module using a multi-hop iteration process comprising using at least one of symbol attention information and image portion attention information from a previous iteration as input for attending at least one image portion and symbol in a subsequent iteration.
16. The apparatus of claim 12, wherein the feature detection module extracts the visual features and the symbol features using at least one neural network.
17. The apparatus of claim 16, wherein the at least one neural network comprises at least one convolutional neural network.
18. The apparatus of claim 12, the feature detection module assigns a score for each of the symbol features.
19. The apparatus of claim 18, wherein the score is based on at least one of information input by a user or predicted using a neural network.
20. The apparatus of claim 12, wherein the symbol comprises at least one of text and audio.
</claims>
</document>
