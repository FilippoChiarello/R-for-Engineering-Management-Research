<document>

<filing_date>
2018-08-23
</filing_date>

<publication_date>
2020-02-27
</publication_date>

<priority_date>
2018-08-23
</priority_date>

<ipc_classes>
G06F9/445,G06N99/00
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
CHAUDHURI, SURAJIT
DING, BOLIN
WANG, CHI
HUANG, SILU
</inventors>

<docdb_family_id>
67297415
</docdb_family_id>

<title>
EFFICIENT CONFIGURATION SELECTION FOR AUTOMATED MACHINE LEARNING
</title>

<abstract>
In automated machine learning, an approximate best configuration can be selected among multiple candidate machine-learning configurations by progressively sampling training and test datasets for the iterative training and testing of the configurations while progressively pruning the set of candidate configurations based on associated estimated confidence intervals for their respective performance.
</abstract>

<claims>
1. One or more machine-readable media storing instructions for execution by one or more hardware processors, execution of the instructions causing the one or more hardware processors to determine an approximate best machine-learning configuration among a set of machine-learning configurations by performing operations comprising: selecting a machine-learning configuration within the set for training and determining an associated sample size; causing the selected machine-learning configuration to be trained on a sampled training dataset having the associated sample size; estimating a confidence interval of a quality metric for the trained machine-learning configuration; and pruning the set based on comparisons between the estimated confidence interval of the trained machine-learning configuration and estimated confidence intervals of other machine-learning configurations within the set.
2. The one or more machine-readable media of claim 1, wherein the selecting, causing, estimating, and pruning operations are performed iteratively.
3. The one or more machine-readable media of claim 2, wherein the approximate best machine-learning configuration is a last machine-learning configuration remaining within the set upon iterative pruning.
4. The one or more machine-readable media of claim 2, wherein, for each of the machine-learning configurations, the associated sample sizes increase progressively over repeated training iterations.
5. The one or more machine-readable media of claim 4, wherein the associated sample sizes increase geometrically over repeated training iterations.
6. The one or more machine-readable media of claim 1, wherein the confidence interval is estimated based at least in part on a training value of the quality metric as determined for the trained machine-learning configuration on the sampled training dataset and a test value of the quality metric determined for the trained machine-learning configuration based on a sampled test dataset.
7. The one or more machine-readable media of claim 6, wherein an upper bound of the estimated confidence interval is greater than the training value and a lower bound of the confidence interval is smaller than the test value.
8. The one or more machine-readable media of claim 1, wherein the quality metric measures an accuracy of predictions made by the trained machine-learning configuration.
9. The one or more machine-readable media of claim 1, wherein pruning the set of machine-learning configurations comprises determining, among lower bounds of the confidence intervals of the machine-learning configurations within the set, a highest lower bound, and removing any machine-learning configuration from the set whose confidence interval has an upper bound that exceeds the highest lower bound by no more than a prescribed loss tolerance.
10. The one or more machine-readable media of claim 1, wherein selection of a machine-learning configuration for training is based at least in part on training costs associated with reducing the confidence intervals of the machine-learning configurations within the set.
11. The one or more machine-readable media of claim 1, wherein the approximate best machine-learning configuration is one of one or more machine-learning configurations remaining within the pruned set when a time limit has been reached.
12. A method comprising: iteratively pruning a set of machine-learning configurations based on a training dataset and a test dataset by using one or more hardware processors to perform operations comprising, in each of a plurality of iterations: sampling the training and test datasets in accordance with a sampling schedule associated with a machine-learning configuration selected from the set; training the selected machine-learning configuration based on the sampled training dataset and determining a training accuracy associated with the trained selected machine-learning configuration; evaluating the trained selected machine-learning configuration based on the sampled test dataset to determine a test accuracy associated with the trained selected machine-learning configuration; determining a confidence interval associated with the trained selected machine-learning configuration based at least in part on the training and test accuracies; pruning the set of machine-learning configurations based on comparisons between the determined confidence interval and confidence intervals associated with other machine-learning configurations within the set; and selecting one of the machine-learning configurations remaining within the pruned set for a next iteration.
13. The method of claim 12, wherein pruning the set of machine-learning configurations comprises comparing, among the confidence intervals associated with the machine-learning configurations within the set, a confidence interval having a highest lower bound against all other confidence intervals, and removing from the set of machine-learning configurations any machine-learning configuration whose associated confidence interval overlaps by no more than a prescribed loss tolerance with the confidence interval having the highest lower bound.
14. The method of claim 12, wherein the sampling schedules associated with the machine-learning configurations increase at least a sample size of the sampled training dataset over repeated training of a same machine-learning configuration.
15. The method of claim 12, wherein selecting the machine-learning configuration comprises, for at least some iterations, identifying a machine-learning configuration having a highest upper bound of its associated confidence interval, and, if a training cost gradient of the identified machine-learning configuration is below a sum of training cost gradients of the other machine-learning configurations, selecting the identified machine-learning configuration, and otherwise selecting a machine-learning configuration having a second-highest upper bound of its associated confidence interval.
16. The method of claim 12, wherein the confidence interval is determined based further on sample sizes of the sampled training dataset and the sampled test dataset.
17. The method of claim 12, wherein the set of machine-learning configurations is iteratively pruned until it consists of only one remaining machine-learning configuration.
18. A system comprising: one or more hardware processors configured to implement a plurality of processing components for determining an approximate best machine-learning configuration among a set of machine-learning configurations, the processing components comprising: a training and test component configured to: train, upon selection of one of the machine-learning configurations within the set, the selected machine learning configuration on a sampled training dataset, and compute training and test quality metrics associated with the trained machine-learning configuration; and a sampling and scheduling component configured to: compute confidence intervals for the machine-learning configurations from the training and test quality metrics, and iteratively prune the set of machine-learning configurations based on the confidence intervals, select machine-learning configurations for training by the training and test component, and determine, for the selected machine-learning configurations, associated sample sizes for the sampled training dataset.
19. The system of claim 18, wherein the processing components further comprise: a data sampler configured to sample a training dataset based on the sample sizes determined by the sampling and scheduling component for the selected machine-learning configurations.
20. The system of claim 18, wherein the sampling and scheduling component determines the sample sizes for each of the machine-learning configurations based on a predetermined progressive sampling schedule associated with that machine-learning configuration.
</claims>
</document>
