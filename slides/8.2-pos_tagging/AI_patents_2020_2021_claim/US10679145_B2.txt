<document>

<filing_date>
2016-07-12
</filing_date>

<publication_date>
2020-06-09
</publication_date>

<priority_date>
2015-08-07
</priority_date>

<ipc_classes>
G06F9/50,G06N20/00,G06N99/00
</ipc_classes>

<assignee>
NEC CORPORATION
NEC LABORATORIES AMERICA
</assignee>

<inventors>
KADAV, ASIM
</inventors>

<docdb_family_id>
57983593
</docdb_family_id>

<title>
System and method for balancing computation with communication in parallel learning
</title>

<abstract>
A machine learning method includes installing a plurality of model replicas for training on a plurality of computer learning nodes; receiving training data at a each model replica and updating parameters for the model replica after trailing; sending the parameters to other model replicas with a communication batch size; evaluating received parameters from other model replicas; and dynamically adjusting the communication batch size to balance computation and communication overhead and ensuring convergence even with a mismatch in processing abilities on different computer learning nodes.
</abstract>

<claims>
1. A machine learning method, comprising: installing a plurality of model replicas for training on a plurality of computer learning nodes; receiving training data at each model replica and updating parameters for the model replica after training, wherein a number of training iterations is adjusted prior to sending any model updates; sending the parameters to other model replicas with a communication batch size; evaluating received parameters from other model replicas; and dynamically adjusting the communication batch size to balance computation and communication overhead and ensuring convergence even with a mismatch in processing abilities on different computer learning nodes, wherein: a new model generated based on the updated parameters is sent to each of the other model replicas after a predetermined number of iterations, a total number of receiving replicas are read, counted, and merged, wherein a communication batch size is increased when the total number of received replicas is below a predetermined threshold, and communication batch size is decreased when the total number of received models exceeds an iteration count threshold.
2. The method of claim 1, comprising sending iteration counts and counting the model updates that arrive at each model replica, and if a model replica notices too few replicas, the model replica increases the communication batch size in increments, and if the model replica notices other model replicas have higher iteration counts as compared to its iteration count, the model replica increases model update transmission frequency as other computer learning nodes merge their updates.
3. The method of claim 1, comprising balancing computation and communication in distributed machine learning.
4. The method of claim 1, comprising adjusting the communication batch sizes to automatically balance processor and network loads.
5. The method of claim 1, comprising ensuring accurate convergence and high accuracy machine learning models by adjusting training sizes with communication batch sizes.
6. The method of claim 1, comprising training a plurality of model replicas trained in parallel using parameter updates.
7. The method of claim 1, wherein the model replicas train and compute new model weights.
8. The method of claim 1, comprising sending or receiving parameters from all other model replicas and applying the parameters to the current model replica model.
9. The method of claim 1, comprising training on data captured by sensors coupled to an actuator.
10. The method of claim 9, wherein the actuator comprises a motor or engine to move a physical object.
11. A machine learning system, comprising: a plurality of computer learning nodes running a plurality of model replicas for training, each including a processor and a data storage device, the processors being configured for: receiving training data at a first model replica and updating parameters for the model replica after training, wherein a number of training iterations is adjusted prior to sending any model updates; sending the parameters to other model replicas with a communication batch size; evaluating received parameters from other model replicas; and adjusting the communication batch size to balance computation and communication overhead and ensuring convergence even with a mismatch in processing abilities on different computer learning nodes, wherein: a new model generated based on the updated parameters is sent to each of the other model replicas after a predetermined number of iterations, a total number of receiving replicas are read, counted, and merged, wherein a communication batch size is increased when the total number of received replicas is below a predetermined threshold, and communication batch size is decreased when the total number of received models exceeds an iteration count threshold.
12. The system of claim 11, wherein the processor is further configured for sending iteration counts and counting the model updates that arrive at each model replica, and if a model replica notices too few replicas, the model replica increases the communication batch size in increments, and if the model replica notices other model replicas have higher iteration counts as compared to its iteration count, the model replica increases model update transmission frequency as other computer learning nodes merge their updates.
13. The system of claim 11, wherein the processor is further configured for balancing computation and communication in distributed machine learning.
14. The system of claim 11, wherein the processor is further configured for adjusting the communication batch sizes to automatically balance processor and network loads.
15. The system of claim 11, wherein the processor is further configured for ensuring accurate convergence and high accuracy machine learning models by adjusting training sizes with communication batch sizes.
16. The system of claim 11, wherein the processor is further configured for training a plurality of model replicas trained in parallel using parameter updates.
17. The system of claim 11, wherein the model replicas train and compute new model weights.
18. The system of claim 11, comprising sending or receiving parameters from all other model replicas and applying the parameters to the current model replica model.
</claims>
</document>
