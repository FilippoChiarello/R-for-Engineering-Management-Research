<document>

<filing_date>
2018-01-26
</filing_date>

<publication_date>
2020-11-25
</publication_date>

<priority_date>
2017-01-27
</priority_date>

<ipc_classes>
G06T7/33
</ipc_classes>

<assignee>
Correvate Limited
</assignee>

<inventors>
SELVIAH, DAVID R.
WILLMAN, EERO
</inventors>

<docdb_family_id>
58462764
</docdb_family_id>

<title>
APPARATUS, METHOD, AND SYSTEM FOR ALIGNMENT OF 3D DATASETS
</title>

<abstract>
The present invention lies in the field of 3D surveying, mapping, and imaging. In particular, the present invention relates to the rotational alignment of 3D datasets. Embodiments include an apparatus method and program for rotations! and optionally also transnational alignment of 3D datasets. The 3D datasets being stored as point clouds, transformed into vector sets, and the vector sets being represented as a unit sphere or Gaussian sphere and compared for best alignment. The found best alignment is used to rotate the two 3D datasets into rotational and translational alignment with one another.
</abstract>

<claims>
1. An apparatus (10) comprising: a 3D dataset acquisition unit (12), configured to: obtain from an imaging apparatus (20) at a first position and orientation, a first 3D dataset of a first space at a first time, the first 3D dataset being a first point cloud in three dimensions, each point in the first point cloud representing a reading within the first space by the imaging apparatus; and obtain from an imaging apparatus (20) at a second position and orientation, a second 3D dataset of a second space at a second time, the second space and the first space overlapping, the second 3D dataset being a second point cloud in three dimensions, each point in the second point cloud representing a reading within the second space by the imaging apparatus; a storage unit (14) configured to store the first 3D dataset and the second 3D dataset as respective clouds of points in a common coordinate system; and a rotational alignment processor (16) configured to: transform the stored first point cloud into a first set of vectors, and transform the stored second point cloud into a second set of vectors, wherein each member of the first set of vectors and each member of the second set of vectors represents the respective point and neighbouring points; find at which angle of rotation of the second set of vectors relative to the first set in one or more of three axes of rotation, defined in the common coordinate system, the greatest degree of matching between the angular distribution of the first set of vectors and the angular distribution of the second set of vectors is obtained, the degree of matching being measured to compare the angular distributions of the first and second sets of vectors; and either: output the angle of rotation about each of the one or more axis or axes of rotation for which the calculated degree of matching is greatest; or rotate the second point cloud by the found angle of rotation about the respective one or more axes of rotation in the common coordinate system and output the rotated second point cloud in the common coordinate system; characterised in that the apparatus (10) further comprises:
a translational alignment processor (18) configured to: for a line and a plane in the common coordinates system, wherein the line is at an angle to or normal to the plane: record the position, relative to an arbitrary origin, of a projection onto the line of each point among the first point cloud and store the recorded positions as a first 1-dimensional array, and/or store one or more properties or readings of each point at the respective recorded position as the first 1-dimensional array; record the position, relative to an arbitrary origin, of a projection onto the plane of each point among the first point cloud, and store the recorded positions as a first 2-dimensional array, and/or store one or more properties or readings of each point at the respective recorded position as the first 2-dimensional array; record the position, relative to the arbitrary origin, of the projection onto the line of each point among the rotated second point cloud, and store the recorded positions as a second 1-dimensional array, and/or store one or more properties or readings of each point at the respective recorded position as the second 1-dimensional array; record the position, relative to an arbitrary origin, of a projection onto the plane of each point among the rotated second point cloud, and store the recorded positions as a second 2-dimensional array, and/or store one or more properties or readings of each point at the respective recorded position as the second 2-dimensional array; find a translation along the line of the second 1-dimensional array relative to the first 1-dimensional array at which a greatest degree of matching between the first 1-dimensional array and the second 1-dimensional array is computed, and record the translation at which the greatest degree of matching is computed; and find a translation, in the plane, of the second 2-dimensional array relative to the first 2-dimensional array at which a greatest degree of matching between the first 2-dimensional array and the second 2-dimentional array is computed, and record said translation; output either: the first point cloud and the rotated second point cloud as a merged point cloud in the common coordinate system, with the rotated second point cloud translated along the line and in the plane in the common coordinates system by the respective recorded translation; and/or a vector representation of the recorded translation along the line and in the plane, and a vector representation of the three axes of rotation and the respective found angles of rotation; and/or the rotated second point cloud translated along the line and in the plane in the common coordinates system by the respective recorded translation.
2. The apparatus (10) according to claim 1, wherein
a 3D pattern matching algorithm is executed on the first point cloud and the second point cloud to recognise sets of points describing instances of features identifiable by said algorithm, to label the members of the sets of points as members of an instance of the respective identified feature; wherein
in finding a greatest degree of matching between the first 1-dimensional array and the second 1-dimensional array, and/or between the first 2-dimensional array and the second 2-dimensional array, degrees of matching for instances of the same feature are calculated on a feature-by-feature basis, and the degree of matching is a weighted combination of the feature-by-feature degrees of matching, weighted according to the number of labelled points labelled as members of an instance of the respective feature.
3. The apparatus (10) according to claim 1 or 2, wherein: the imaging apparatus (20) from which one or both of the first 3D dataset and the second 3D dataset is obtained is a LIDAR scanner, optical 3D scanning apparatus including photogrammetry, and/or a sonar scanner; and/or the imaging apparatus (20) from which one or both of the first 3D dataset and the second 3D dataset is obtained is an electrical resistivity tomography scanner inserted into a ground surface and the imaged space is below the ground surface; and/or the imaging apparatus (20) from which one or both of the first 3D dataset and the second 3D dataset is obtained is an electrical impedance tomography scanner comprising electrodes for placement on a skin of a subject animal, and the imaged space is interior of the subject animal; and/or the imaging apparatus (20) from which one or both of the first 3D dataset and the second 3D dataset is obtained is an MRI scanner or a scanner detecting nuclear quadrupole resonance, and the imaged space is the interior of a subject animal.
4. The apparatus (10) according to claim 1, wherein
edge detection is performed using sum vectors, wherein
obtaining the sum vectors, comprises, for each point cloud individually, for each point in the respective point cloud as an inquiry point: form a search radius sphere of predetermined radius around the inquiry point or find a specific number of nearest neighbour points; calculate vectors from the inquiry point to all other points within the search radius sphere or within the near neighbourhood; sum the calculated vectors to obtain the sum vector; wherein
the length of a sum vector indicates the likelihood that the point of the point cloud from which it originates is located on an edge or at a corner and the angle of a sum vector relative to nearby surface normal vectors indicates whether there is an edge or a corner, and if so what kind of edge it is, such as an edge where two planes meet or an occlusion or a hole in a plane,
wherein the angle relative to nearby surface normal vectors is found by performing a vector dot product, dividing by the product of the magnitudes of the two vectors and finding the angle whose cosine is this number.
5. An apparatus (10) according to claim 1, wherein transforming the stored first and second point clouds into first and second sets of vectors includes applying a non-linear filter to the point clouds or sets of vectors, wherein applying the non-linear filter comprises excluding from further processing any point from the point clouds, or any vectors from the sets of vectors, not recognised as representing edges by an edge detection algorithm.
6. An apparatus (10) according to claim 1, wherein transforming the stored first and second point clouds into first and second sets of vectors includes: finding planes by one or more from among: RANSAC plane detection, Hough Transform plane detection, and region growing based on surface curvature, surface normal angle deviation, colour, reflectivity or roughness and distance from the plane; finding the area of each plane; wherein the respective set of vectors includes a vector for each found plane, the vector being a plane vector, which is a surface normal vector having a length equal to the area of the plane.
7. An apparatus (10) according to claim 1, wherein,
the vectors of the first set of vectors include cylinder vectors, and the vectors of the second set include cylinder vectors; and transforming the first point cloud into a first set of cylinder vectors and transforming the second point cloud into a second set of cylinder vectors, respectively, comprises: using a method for recognition of cylinders in the respective 3D dataset and finding the radius, position and orientation of each recognised cylinder; including in the respective set of vectors, a cylinder vector for each recognised cylinder, the cylinder vector for the recognised cylinder lying along, and parallel to, the orientation of the axis of the recognised cylinder, and having a length related to the radius of the recognised cylinder.
8. The apparatus (10) according to any of the preceding claims, wherein
finding at which angle of rotation of the second set of vectors relative to the first set in one or more of three axes of rotation defined in the common coordinate system the greatest degree of matching between the angular distribution of the first set of vectors and the angular distribution of the second set of vectors is obtained, comprises: attributing a common origin to the vectors in the first and second sets of vectors; dividing a notional sphere centred on the common origin into surface regions or volume regions; finding at which angle of rotation of the second set of vectors relative to the first set in each of the one or more of the three axes of rotation defined in the common coordinate system the best match between the vector point distribution of vectors from the first set of vectors and the vector point distribution of vectors from the second set of vectors among the surface regions or volume regions is obtained.
9. The apparatus (10) according to any of the preceding claims, wherein
the reading represented by each point in the first and second point clouds is a reading of a surface or interface between two materials;
the rotational alignment processor (16) is further configured, before finding the best match between the angular distributions of the first and second sets of vectors, to:
filter the first and second sets of vectors by recognising, in both sets, surface normal unit vectors representing one of template shapes, and removing from the respective sets surface normal unit vectors not recognised as representing one of the template shapes.
10. The apparatus (10) according to claim 1, wherein
rotational movement of the imaging apparatus (20) is tracked with the angles of rotation output by the rotational alignment processor; and
movement of the imaging apparatus (20) between obtaining the two point clouds is tracked with the recorded translations along the line and plane recorded by the translational alignment processor; and wherein
the imaging apparatus (20) is mounted to a vehicle, and a vehicle track is obtained from the recorded translations along the line and plane recorded by the translational alignment processor.
11. The apparatus according to claim 1, wherein the vectors of the first set of vectors and the vectors of the second set of vectors are in each case vectors of one of the following vector types: - surface normal unit vectors, wherein the reading represented by each point in the first and second point clouds is a reading of a surface or interface between two materials, - sum vectors, wherein obtaining the sum vectors, comprises, for each point cloud individually, for each point in the respective point cloud as an inquiry point: form a search radius sphere of predetermined radius around the inquiry point or find a specific number of nearest neighbour points; calculate vectors from the inquiry point to all other points within the search radius sphere or within the near neighbourhood; sum the calculated vectors to obtain the sum vector; - plane vectors, a plane vector representing a plane area; - point density gradient vectors, a point density gradient vector representing the direction of increase of point density; - point density gradient divergence vectors, a point density gradient vector representing where the point density gradient vectors diverge, and - gradient vectors, wherein gradient vectors represent the directions of increase of a parameter of the reading at a point, said parameter being from among: colour, brightness, curvature, and roughness.
12. A system (10, 20) comprising: a first imaging apparatus (20) configured to generate, at a first position and orientation a first 3D dataset, the first 3D dataset of a first space at a first time; a second imaging apparatus (20) configured to generate, at a second position and orientation, a second 3D dataset, the second 3D dataset of a second space at a second time, the second space and the first space overlapping, wherein, either the second imaging apparatus is the first imaging apparatus and the second time is different from the first time, or the second imaging apparatus is different from the first imaging apparatus; an apparatus (10) according to claim 1, wherein the imaging apparatus (20) at the first position and orientation is the first imaging apparatus (20), and the imaging apparatus (20) at the second position and orientation is the second imaging apparatus (20).
13. A method comprising: obtaining (S101a) a first 3D dataset of a first space at a first time, the first 3D dataset being a first point cloud in three dimensions, each point in the first point cloud representing a reading within the first space by an imaging apparatus; and obtaining (S101b) a second 3D dataset of a second space at a second time, the second space and the first space overlapping, the second 3D dataset being a second point cloud in three dimensions, each point in the second point cloud representing a reading within the second space by an imaging apparatus; storing (S103a, S103b) the first 3D dataset and the second 3D dataset as respective clouds of points in a common coordinate system; transforming (S104a) the stored first point cloud into a first set of vectors, and transforming (S104b) the stored second point cloud into a second set of vectors, wherein each member of the first set of vectors and each member of the second set of vectors represents the respective point and neighbouring points; finding (S105) at which angle of rotation of the second set of vectors relative to the first set in one or more of three axes of rotation, defined in the common coordinate system, the greatest degree of matching between the angular distribution of the first set of vectors and the angular distribution of the second set of vectors is obtained, the degree of matching being measured to compare the angular distributions of the first and second sets of vectors; and either: storing the angle of rotation about each of the one or more axis or axes of rotation for which the calculated degree of matching is greatest; or rotating (S106) the second point cloud by the stored angle of rotation about the respective one or more axes of rotation in the common coordinate system, and outputting the rotated second point cloud in the common coordinate system, characterised in that
the method further comprises: for a line and a plane in the common coordinates system, wherein the line is at an angle to or normal to the plane: recording the position, relative to an arbitrary origin, of a projection onto the line of each point among the first point cloud and store the recorded positions as a first 1-dimensional array, and/or store one or more properties or readings of each point at the respective recorded position as the first 1-dimensional array; recording the position, relative to an arbitrary origin, of a projection onto the plane of each point among the first point cloud, and store the recorded positions as a first 2-dimensional array, and/or store one or more properties or readings of each point at the respective recorded position as the first 2-dimensional array; recording the position, relative to the arbitrary origin, of the projection onto the line of each point among the rotated second point cloud, and store the recorded positions as a second 1-dimensional array, and/or store one or more properties or readings of each point at the respective recorded position as the second 1-dimensional array; recording the position, relative to an arbitrary origin, of a projection onto the plane of each point among the rotated second point cloud, and store the recorded positions as a second 2-dimensional array, and/or store one or more properties or readings of each point at the respective recorded position as the second 2-dimensional array; finding a translation along the line of the second 1-dimensional array relative to the first 1-dimensional array at which a greatest degree of matching between the first 1-dimensional array and the second 1-dimensional array is computed, and record the translation at which the greatest degree of matching is computed; and finding a translation, in the plane, of the second 2-dimensional array relative to the first 2-dimensional array at which a greatest degree of matching between the first 2-dimensional array and the second 2-dimentional array is computed, and record said translation; outputting either: the first point cloud and the rotated second point cloud as a merged point cloud in the common coordinate system, with the rotated second point cloud translated along the line and in the plane in the common coordinates system by the respective recorded translation; and/or a vector representation of the recorded translation along the line and in the plane, and a vector representation of the three axes of rotation and the respective found angles of rotation; and/or the rotated second point cloud translated along the line and in the plane in the common coordinates system by the respective recorded translation.
14. A method comprising: obtaining a plurality of individual 3D datasets representing respective portions of a subject, the translational and rotational relations among the plurality of individual 3D datasets being unknown; performing the method of claim 13, including, for each combination of a pair of individual 3D datasets among the plurality of individual 3D datasets:
performing the storing, transforming, finding, and storing steps of the method of claim 13, and storing, in addition to the angle of rotation about the respective one or more axes of rotation, the magnitude of the corresponding greatest degree of matching between the angular distribution of the first set of vectors and the angular distribution of the second set of vectors; for each individual 3D dataset among the plurality of individual 3D datasets: of all pairs including the individual 3D dataset as a member of the pair, identifying the pair for which the stored greatest degree of matching is the maximum, and optionally any pair for which the stored greatest degree of matching is within a defined range of said maximum, and performing the translational alignment process of claim 13 on the or each identified pair; performing the obtaining steps of claim 13 with the or each identified pair as the first and second 3D datasets, performing the rotating step of claim 13 on the identified pair with the angle of rotation about the respective one or more axes of rotation stored for the pair, with the rotated second point cloud translated along the line and the plane in the common coordinates system by the respective recorded translation from the translational alignment process of claim 13, thereby obtaining a pairwise point cloud representation of the pair of individual 3D datasets with a fixed rotational and translational relation between the pair; merging the pairwise point cloud representations of the pairs into a single point cloud by co-locating instances of the same individual 3D dataset appearing in more than one of the pairwise point cloud representations.
15. An image processing method, comprising
obtaining (S201a, S201b) a point-cloud representation of a subject, the point-cloud representation being composed of a plurality of individual scans, each scan being a point-cloud representation of a portion of the subject, each scan overlapping one or more other scans and having a defined rotational and translational relation to said one or more other scans, wherein at least one of the scans is overlapping more than one other scan, executing a global alignment process comprising: for each of the plurality of individual scans as a subject point cloud, setting the one or more other scans overlapping with the subject scan as a reference point cloud, for each individual scan of the reference point cloud, executing a rotational alignment algorithm and translational alignment algorithm (S203-S208) on the subject point cloud and the individual scan of the reference point cloud with the defined rotational and translation relation between the subject point cloud and the individual scan of the reference point cloud in the point-cloud representation of the subject as an initial estimation, to rotate and translate the subject point cloud relative to the individual scan of the reference point cloud until a process error criterion is met (S209), to obtain a revised defined rotational and translational relation between the subject point cloud and the individual scan of the reference point cloud; combining the revised defined rotational and translational relations between the subject point cloud and each of the individual scans in the reference point cloud; implementing the combined revised defined rotational and translational relation between the subject point cloud and the reference point cloud in the point-cloud representation of the subject; wherein executing the rotational alignment algorithm and executing the translational alignment algorithm comprise rotationally aligning the pair of scans and aligning the pair of scans, respectively, as first and second 3D datasets, using the apparatus according to any of claims 1 to 12, using the method of claim 13, or using the method of claim 14; the method further comprising:
repeating (S210) the global alignment process one or more times, with a progressively stricter process error criterion on each repetition.
</claims>
</document>
