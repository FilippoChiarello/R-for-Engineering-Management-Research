<document>

<filing_date>
2019-06-21
</filing_date>

<publication_date>
2020-12-24
</publication_date>

<priority_date>
2019-06-21
</priority_date>

<ipc_classes>
G06T1/20
</ipc_classes>

<assignee>
INTEL CORPORATION
</assignee>

<inventors>
MAIYURAN, SUBRAMANIAM
NAVALE, ADITYA
APODACA, MICHAEL
SHARMA, SAURABH
VENKATESH, ABHISHEK
CHIVUKULA, VAMSEE VARDHAN
SCHLUESSLER, TRAVIS
</inventors>

<docdb_family_id>
73653473
</docdb_family_id>

<title>
ASYNCHRONOUS EXECUTION MECHANISM
</title>

<abstract>
An apparatus to facilitate asynchronous execution at a processing unit. The apparatus includes one or more processors to detect independent task passes that may be executed out of order in a pipeline of the processing unit, schedule a first set of processing tasks to be executed at a first set of processing elements at the processing unit and schedule a second set of tasks to be executed at a second set of processing elements, wherein execution of the first set of tasks at the first set of processing elements is to be performed simultaneous and in parallel to execution of the second set of tasks at the second set of processing elements.
</abstract>

<claims>
1. An apparatus to facilitate asynchronous execution at a processing unit, comprising: one or more processors to detect independent task passes that may be executed out of order in a pipeline of the processing unit, schedule a first set of processing tasks to be executed at a first set of processing elements at the processing unit and schedule a second set of tasks to be executed at a second set of processing elements, wherein execution of the first set of tasks at the first set of processing elements is to be performed simultaneous and in parallel to execution of the second set of tasks at the second set of processing elements.
2. The apparatus of claim 1, wherein the one or more processors further schedule the first and second set of processing tasks at a third set of processing elements.
3. The apparatus of claim 1, wherein the one or more processors transmit the first set of tasks to a first queue and transmit the second set of tasks to a second queue.
4. The apparatus of claim 3, wherein the one or more processors transmit the first set of tasks to a first command streamer at the processing unit and transmit the second set of tasks to a second command streamer.
5. The apparatus of claim 4, wherein the one or more processors receives information specifying independent tasks.
6. The apparatus of claim 5, wherein the information comprises a directed acyclic graph.
7. The apparatus of claim 6, wherein the directed acyclic graph comprises a plurality of nodes representing units of tasks and a plurality of edges representing resource dependency between operations.
8. The apparatus of claim 4, wherein the one or more processors generate a directed acyclic graph.
9. The apparatus of claim 1, wherein the one or more processors identify whether a first task pass has a first bottleneck at the first set of processing elements and second pass has a second bottleneck at the second set of processing elements.
10. The apparatus of claim 2, wherein the one or more processors schedule the first task pass and the second task pass in parallel based on identifying the first bottleneck and the second bottleneck.
11. A method to facilitate asynchronous execution at a processing unit, comprising: detecting independent task passes that may be executed out of order in a pipeline of the processing unit; scheduling a first set of processing tasks to be executed at a first set of processing elements at the processing unit; and scheduling a second set of tasks to be executed at a second set of processing elements, wherein execution of the first set of tasks at the first set of processing elements is to be performed simultaneous and in parallel to execution of the second set of tasks at the second set of processing elements.
12. The method of claim 11, further comprising scheduling the first and second set of processing tasks at a third set of processing elements.
13. The method of claim 12, further comprising: transmitting the first set of tasks to a first command streamer at the processing unit; and transmitting the second set of tasks to a second command streamer.
14. The method of claim 11, further comprising receiving a directed acyclic graph comprising a plurality of nodes representing units of tasks and a plurality of edges representing resource dependency between operations.
15. The method of claim 11, further comprising generating a directed acyclic graph comprising a plurality of nodes representing units of tasks and a plurality of edges representing resource dependency between operations.
16. At least one computer readable medium having instructions stored thereon, which when executed by one or more processors, cause the processors to: detect independent task passes that may be executed out of order in a pipeline of the processing unit; schedule a first set of processing tasks to be executed at a first set of processing elements at the processing unit; and schedule a second set of tasks to be executed at a second set of processing elements, wherein execution of the first set of tasks at the first set of processing elements is to be performed simultaneous and in parallel to execution of the second set of tasks at the second set of processing elements.
17. The computer readable medium of claim 16, having instructions stored thereon, which when executed by one or more processors, further cause the processors to schedule the first and second set of processing tasks at a third set of processing elements.
18. The computer readable medium of claim 17, having instructions stored thereon, which when executed by one or more processors, further cause the processors to: transmit the first set of tasks to a first command streamer at the processing unit; and transmit the second set of tasks to a second command streamer.
19. The computer readable medium of claim 16, having instructions stored thereon, which when executed by one or more processors, further cause the processors to receive a directed acyclic graph comprising a plurality of nodes representing units of tasks and a plurality of edges representing resource dependency between operations.
20. The computer readable medium of claim 16, having instructions stored thereon, which when executed by one or more processors, further cause the processors to identify whether a first task pass has a first bottleneck at the first set of processing elements and second pass has a second bottleneck at the second set of processing elements.
</claims>
</document>
