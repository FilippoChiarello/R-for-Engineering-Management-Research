<document>

<filing_date>
2019-02-26
</filing_date>

<publication_date>
2020-08-27
</publication_date>

<priority_date>
2019-02-26
</priority_date>

<ipc_classes>
G06K9/62,G06T7/593
</ipc_classes>

<assignee>
BAIDU USA
BAIDU USA
</assignee>

<inventors>
YANG, RUIGANG
WANG, PENG
CHENG, XINJING
</inventors>

<docdb_family_id>
72141741
</docdb_family_id>

<title>
Systems and methods for depth estimation using convolutional spatial propagation networks
</title>

<abstract>
Presented are systems and methods for improving speed and quality of real-time per-pixel depth estimation of scene layouts from a single image by using a 3D end-to-end Convolutional Spatial Propagation Network (CSPN). An efficient linear propagation model performs propagation using a recurrent convolutional operation. The affinity among neighboring pixels may be learned through a deep convolutional neural network (CNN). The CSPN may be applied to two depth estimation tasks, given a single image: (1) to refine the depth output of existing methods, and (2) to convert sparse depth samples to a dense depth map, e.g., by embedding the depth samples within the propagation procedure. For stereo depth estimation, the 3D CPSN is applied to stereo matching by adding a diffusion dimension over discrete disparity space and feature scale space. This aids the recovered stereo depth to generate more details and to avoid error matching from noisy appearance caused by sunlight, shadow, and similar effects.
</abstract>

<claims>
1. A method for end-to-end training a 3D model for stereo depth estimation, the method comprising: receiving, at a 3D model that comprises a plurality of stacks, a 4D cost volume that uses feature maps produced from a stereo image pair; for each of the plurality of stacks: using a 3D convolution to produce a 3D affinity matrix-disparity volume pair; using the pair to perform a 3D convolutional spatial propagation network (3D CSPN) operation that propagates, within N iteration steps, a local area along three dimensions to produce an updated disparity volume; and using the updated disparity volume to: perform a 2D convolution to obtain a 2D affinity matrix; and perform a disparity regression to obtain a corresponding 2D disparity map; concatenating the 2D disparity maps into a multi-scale disparity stack; concatenating the 2D affinity matrixes to obtain a 3D affinity matrix; applying the 3D CSPN to the multi-scale disparity stack and the 3D affinity matrix to combine contexts from neighboring pixels to generate an output disparity map; and using the output disparity map to obtain a stereo depth estimation.
2. The method according to claim 1, further comprising: given a feature map and target size for a pooled feature map, partitioning a weight map into pooling regions; within each region, applying a 3D CSPN to a spatial pyramid pooling (SPP) module to compute a pooling kernel; and outputting a 1-channel weight map.
3. The method according to claim 2, further comprising using the spatial pooling modules to: concatenate features of a spatial pyramid into the 4D volume; and for each layer of the spatial pyramid, learning a transformation kernel to obtain fused feature map.
4. The method according to claim 1, wherein the 4D cost volume has been generated by: applying convolutional neural networks (CNNs) that share a set of weights to the stereo image pair to obtain the feature maps; applying the feature maps to respective spatial pooling modules that generate pooling results; and using the pooling results to form the 4D cost volume.
5. The method according to claim 4, wherein applying the feature maps to the spatial pooling modules comprises concatenating representations from sub-regions that have different sizes.
6. The method according to claim 4, further comprising padding the feature maps to obtain a single regressed disparity map for a final depth estimation.
7. The method according to claim 6, wherein a padding dimension is reduced to 1 within a single iteration.
8. The method according to claim 1, wherein the 3D affinity matrix-disparity volume pair is produced by applying by applying a 3D convolution to a network to capture a global image context.
9. The method according to claim 1, wherein the 3D affinity matrix has been generated by the by at least one CNN.
10. The method according to claim 1, further comprising bilinearly upsampling the updated disparity volume.
11. A method for stereo depth estimation from stereo image pair, the method comprising: receiving a stereo image pair by a 3D model that has been trained end-to-end by using a 3D convolutional spatial propagation network (CSPN) that, in response to receiving a 4D cost volume, updates a disparity volume associated with a 3D affinity matrix-disparity volume pair by propagating a local area along three dimensions; and using the updated disparity volume to estimate a stereo depth from the stereo image pair.
12. The method according to claim 11, further comprising given a feature map and target size for a pooled feature map, partitioning a weight map into pooling regions; within each region, applying a 3D CSPN to a spatial pyramid pooling (SPP) module to compute a pooling kernel; and outputting a 1-channel weight map.
13. The method according to claim 11, further comprising using the spatial pooling modules to: concatenate features of a spatial pyramid into the 4D volume; and for each layer of the spatial pyramid, learning a transformation kernel to obtain fused feature map.
14. The method according to claim 11, wherein the updated disparity volume is used to perform a 2D convolution to obtain a 2D affinity matrix that is concatenated to obtain a 3D affinity matrix, and to perform a disparity regression to obtain a 2D disparity map that is concatenated into a multi-scale disparity stack.
15. The method according to claim 11, wherein the 3D CSPN is applied to the 2D disparity maps and the multi-scale disparity stack to combine contexts from neighboring pixels to generate an output disparity map that is used to obtain a single regressed disparity map for a final depth estimation.
16. A disparity map acquisition system for performing a disparity regression for stereo depth estimation, the system comprising: a processor; and a non-transitory computer-readable medium comprising instructions that, when executed by the processor, cause steps to be performed, the steps comprising: receiving, at a 3D model that comprises a plurality of stacks, a 4D cost volume that uses feature maps produced from a stereo image pair; for each of the plurality of stacks: using a 3D convolution to produce a 3D affinity matrix-disparity volume pair; using the pair to perform a 3D convolutional spatial propagation network (3D CSPN) operation that propagates, within N iteration steps, a local area along three dimensions to produce an updated disparity volume; and using the updated disparity volume to: perform a 2D convolution to obtain a 2D affinity matrix; and perform a disparity regression to obtain a corresponding 2D disparity map; concatenating the 2D disparity maps into a multi-scale disparity stack; concatenating the 2D affinity matrixes to obtain a 3D affinity matrix; applying the 3D CSPN to the multi-scale disparity stack and the 3D affinity matrix to combine contexts from neighboring pixels to generate an output disparity map; and using the output disparity map to obtain a stereo depth estimation.
17. The method according to claim 16, further comprising: given a feature map and target size for a pooled feature map, partitioning a weight map into pooling regions; within each region, applying a 3D CSPN to a spatial pyramid pooling (SPP) module to compute a pooling kernel; and outputting a 1-channel weight map.
18. The method according to claim 17, further comprising using the spatial pooling modules to: concatenate features of a spatial pyramid into the 4D volume; and for each layer of the spatial pyramid, learning a transformation kernel to obtain fused feature map.
19. The method according to claim 16, wherein the 4D cost volume has been generated by: applying convolutional neural networks (CNNs) that share a set of weights to a stereo image pair to obtain feature maps; applying the feature maps to respective spatial pooling modules that generate pooling results, wherein applying the feature maps to the spatial pooling modules comprises concatenating representations from sub-regions that have different sizes; and using the pooling results to form the 4D cost volume.
20. The method according to claim 16, further comprising padding the feature maps to obtain a single regressed disparity map for a final depth estimation, the padding dimension being reduced to 1 within a single iteration.
</claims>
</document>
