<document>

<filing_date>
2020-09-25
</filing_date>

<publication_date>
2021-01-14
</publication_date>

<priority_date>
2018-05-31
</priority_date>

<ipc_classes>
G06K9/62,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
HUAWEI TECHNOLOGIES COMPANY
UNIVERSITY OF SYDNEY
</assignee>

<inventors>
QIAN LI
LIU, JIANZHUANG
WU, XIAOFEI
LI, Wen
OUYANG, Wanli
ZHANG, Weichen
XU, Dong
</inventors>

<docdb_family_id>
66943222
</docdb_family_id>

<title>
METHOD FOR TRAINING DEEP NEURAL NETWORK AND APPARATUS
</title>

<abstract>
The present disclosure relates to artificial intelligence, and proposes a cooperative adversarial network. A loss function is set at a lower layer of the cooperative adversarial network, and is used to learn a domain discriminating feature. In addition, a cooperative adversarial target function includes the loss function and a domain invariant loss function that is set at a last layer (that is, a higher layer) of the cooperative adversarial network, to learn both the domain discriminating feature and a domain-invariant feature. Further, an enhanced collaborative adversarial network is proposed. Based on the collaborative adversarial network, target domain data is added to training of the collaborative adversarial network, an adaptive threshold is set based on precision of a task model, to select a target domain training sample, network confidence is discriminated based on a domain, and a weight of the target domain training sample is set.
</abstract>

<claims>
1. A method for training a deep neural network, comprising: extracting a lower-layer feature and a higher-layer feature of sample data in each of source domain data and target domain data, wherein data distribution of the target domain data is different from data distribution of the source domain data; calculating, by using a first loss function, a first loss corresponding to the sample data based on the higher-layer feature of the sample data in each of the source domain data and the target domain data and a corresponding domain label; calculating, by using a second loss function, a second loss corresponding to the sample data based on the lower-layer feature of the sample data in each of the source domain data and the target domain data and a corresponding domain label; calculating, by using a third loss function, a third loss corresponding to the sample data in the source domain data based on the higher-layer feature of the sample data in the source domain data and a corresponding sample label; and updating a parameter of a target deep neural network based on the first loss, the second loss, and the third loss, wherein gradient reversal is performed on a gradient of the first loss, and the gradient reversal can implement a reverse conduction gradient for loss increasing.
2. The training method according to claim 1, wherein the target deep neural network comprises a feature extraction module, a task module, a domain-invariant feature module, and a domain discriminating feature module, the feature extraction module comprises at least one lower-layer feature network layer and a higher-layer feature network layer, any one of the at least one lower-layer feature network layer can be used for extracting a lower-layer feature, the higher-layer feature network layer is used for extracting a higher-layer feature, the domain-invariant feature module is configured to enhance domain invariance of the higher-layer feature extracted by the feature extraction module, and the domain discriminating feature module is configured to enhance domain discriminating of the lower-layer feature extracted by the feature extraction module; and the updating a parameter of a target deep neural network based on the first loss, the second loss, and the third loss comprises: calculating a total loss based on the first loss, the second loss, and the third loss; and updating parameters of the feature extraction module, the task module, the domain-invariant feature module, and the domain discriminating feature module based on the total loss.
3. The training method according to claim 2, wherein the calculating, by using a first loss function, a first loss corresponding to the sample data based on the higher-layer feature of the sample data in each of the source domain data and the target domain data and a corresponding domain label comprises: inputting the higher-layer feature of the sample data in each of the source domain data and the target domain data into the domain-invariant feature module to obtain a first result corresponding to the sample data; and calculating, by using the first loss function, the first loss corresponding to the sample data based on the first result corresponding to the sample data in each of the source domain data and the target domain data and the corresponding domain label; the calculating, by using a second loss function, a second loss corresponding to the sample data based on the lower-layer feature of the sample data in each of the source domain data and the target domain data and a corresponding domain label comprises: inputting the lower-layer feature of the sample data in each of the source domain data and the target domain data into the domain discriminating feature module to obtain a second result corresponding to the sample data; and calculating, by using the second loss function, the second loss corresponding to the sample data based on the second result corresponding to the sample data in each of the source domain data and the target domain data and the corresponding domain label; and the calculating, by using a third loss function, a third loss corresponding to the sample data in the source domain data based on the higher-layer feature of the sample data in the source domain data and a corresponding sample label comprises: inputting the higher-layer feature of the sample data in the source domain data into the task module to obtain a third result corresponding to the sample data in the source domain data; and calculating, by using the third loss function, the third loss corresponding to the sample data in the source domain data based on the third result corresponding to the sample data in the source domain data and the corresponding sample label.
4. The training method according to claim 2, wherein the domain-invariant feature module further comprises: a gradient reversal module; and the training method further comprises: performing the gradient reversal on the gradient of the first loss by using the gradient reversal module.
5. The training method according to claim 3, further comprising: inputting the higher-layer feature of the sample data in the target domain data into the task module to obtain a corresponding prediction sample label and corresponding confidence; and selecting target domain training sample data from the target domain data based on the confidence corresponding to the sample data in the target domain data, wherein the target domain training sample data is sample data that is in the target domain data and whose corresponding confidence satisfies a preset condition.
6. The training method according to claim 5, further comprising: setting a weight of the target domain training sample data based on a first result corresponding to the target domain training sample data.
7. The training method according to claim 6, wherein the setting a weight of the target domain training sample data based on a first result corresponding to the target domain training sample data comprises: setting the weight of the target domain training sample data based on a similarity between the first result corresponding to the target domain training sample data and a domain label, wherein the similarity indicates a difference between the first result and the domain label.
8. The training method according to claim 7, wherein the setting the weight of the target domain training sample data based on a similarity between the first result corresponding to the target domain training sample data and a domain label comprises: calculating a first difference between the first result corresponding to the target domain training sample data and a domain label of a source domain, and a second difference between the first result corresponding to the target domain training sample data and a domain label of a target domain; and when an absolute value of the first difference is greater than an absolute value of the second difference, setting the weight of the target domain training sample data to a smaller value, or if an absolute value of the first difference is not greater than an absolute value of the second difference, setting the weight of the target domain training sample data to a larger value.
9. The training method according to claim 7, wherein if the first result corresponding to the target domain training sample data is an intermediate value between a first domain label value and a second domain label value, the weight of the target domain training sample data is set to a maximum value, the first domain label value is a value corresponding to a domain label of a source domain, and the second domain label value is a value corresponding to a domain label of a target domain.
10. The training method according to claim 5, before the selecting target domain training sample data from the target domain data based on the confidence corresponding to the sample data in the target domain data, further comprising: setting an adaptive threshold based on precision of a task model, wherein the task model comprises the feature extraction module and the task module, and the adaptive threshold is positively correlated to the precision of the task model, wherein the preset condition is that the confidence is greater than or equal to the adaptive threshold.
11. The training method according to claim 10, wherein the adaptive threshold is calculated by using the following logical function: wherein Tc is the adaptive threshold, A is the precision of the task model, and Î»c is a hyperparameter used to control an inclination degree of the logical function.
12. The training method according to claim 5, wherein the training method further comprises: extracting, by using the feature extraction module, a lower-layer feature and a higher-layer feature of the target domain training sample data; calculating, by using the first loss function, a first loss corresponding to the target domain training sample data based on the higher-layer feature of the target domain training sample data and a corresponding domain label; calculating, by using the second loss function, a second loss corresponding to the target domain training sample data based on the lower-layer feature of the target domain training sample data and a corresponding domain label; calculating, by using the third loss function, a third loss corresponding to the target domain training sample data based on the higher-layer feature of the target domain training sample data and a corresponding prediction sample label; calculating, based on the first loss, the second loss, and the third loss corresponding to the target domain training sample data, a total loss corresponding to the target domain training sample data, wherein gradient reversal is performed on a gradient of the first loss corresponding to the target domain training sample data; and updating the parameters of the feature extraction module, the task module, the domain-invariant feature module, and the domain discriminating feature module based on the total loss corresponding to the target domain training sample data and the weight of the target domain training sample data.
13. The training method according to claim 12, wherein the calculating, by using the first loss function, a first loss corresponding to the target domain training sample data based on the higher-layer feature of the target domain training sample data and a corresponding domain label comprises: inputting the higher-layer feature of the target domain training sample data into the domain-invariant feature module to obtain a first result corresponding to the target domain training sample data; and calculating, by using the first loss function, the first loss corresponding to the target domain training sample data based on a first result corresponding to the target domain training sample data and the corresponding domain label; the calculating, by using the second loss function, a second loss corresponding to the target domain training sample data based on the lower-layer feature of the target domain training sample data and a corresponding domain label comprises: inputting the lower-layer feature of the target domain training sample data into the domain discriminating feature module to obtain a second result corresponding to the target domain training sample data; and calculating, by using the second loss function, the second loss corresponding to the target domain training sample data based on the second result corresponding to the target domain training sample data and the corresponding domain label; and the calculating, by using the third loss function, a third loss corresponding to the target domain training sample data based on the higher-layer feature of the target domain training sample data and a corresponding prediction sample label comprises: inputting the higher-layer feature of the target domain training sample data into the task module to obtain a third result corresponding to the target domain training sample data; and calculating, by using the third loss function, the third loss corresponding to the target domain training sample data based on the third result corresponding to the target domain training sample data and the corresponding prediction sample label.
14. A training device, comprising: a non-transitory memory; and a processor coupled with the memory, wherein the memory is configured to store instructions and the processor is configured to execute the instructions, to perform steps comprising: extracting a lower-layer feature and a higher-layer feature of sample data in each of source domain data and target domain data, wherein data distribution of the target domain data is different from data distribution of the source domain data; calculating, by using a first loss function, a first loss corresponding to the sample data based on the higher-layer feature of the sample data in each of the source domain data and the target domain data and a corresponding domain label; calculating, by using a second loss function, a second loss corresponding to the sample data based on the lower-layer feature of the sample data in each of the source domain data and the target domain data and a corresponding domain label; calculating, by using a third loss function, a third loss corresponding to the sample data in the source domain data based on the higher-layer feature of the sample data in the source domain data and a corresponding sample label; and updating a parameter of a target deep neural network based on the first loss, the second loss, and the third loss, wherein gradient reversal is performed on a gradient of the first loss, and the gradient reversal can implement a reverse conduction gradient for loss increasing.
15. A computer-readable non-transitory storage medium, wherein the computer-readable storage medium stores a computer program, and when the computer program is executed by a processor, the processor performs a method comprising: extracting a lower-layer feature and a higher-layer feature of sample data in each of source domain data and target domain data, wherein data distribution of the target domain data is different from data distribution of the source domain data; calculating, by using a first loss function, a first loss corresponding to the sample data based on the higher-layer feature of the sample data in each of the source domain data and the target domain data and a corresponding domain label; calculating, by using a second loss function, a second loss corresponding to the sample data based on the lower-layer feature of the sample data in each of the source domain data and the target domain data and a corresponding domain label; calculating, by using a third loss function, a third loss corresponding to the sample data in the source domain data based on the higher-layer feature of the sample data in the source domain data and a corresponding sample label; and updating a parameter of a target deep neural network based on the first loss, the second loss, and the third loss, wherein gradient reversal is performed on a gradient of the first loss, and the gradient reversal can implement a reverse conduction gradient for loss increasing.
</claims>
</document>
