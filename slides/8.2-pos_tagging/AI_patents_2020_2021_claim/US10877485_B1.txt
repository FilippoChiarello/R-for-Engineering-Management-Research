<document>

<filing_date>
2020-04-30
</filing_date>

<publication_date>
2020-12-29
</publication_date>

<priority_date>
2018-04-05
</priority_date>

<ipc_classes>
G05B13/02,G05D1/00,G05D1/02
</ipc_classes>

<assignee>
AMBARELLA INTERNATIONAL
</assignee>

<inventors>
MARTIN, PATRICK
</inventors>

<docdb_family_id>
71075075
</docdb_family_id>

<title>
Handling intersection navigation without traffic lights using computer vision
</title>

<abstract>
An apparatus includes a capture device and a processor. The capture device may be configured to generate a plurality of video frames corresponding to an area outside of a vehicle. The processor may be configured to perform operations to detect objects in the video frames, detect an intersection and other vehicles at the intersection based on the objects detected in the video frames, determine a vehicle sequence for traversing the intersection and monitor the other vehicles traversing the intersection using the operations. The vehicle sequence may be determined in response to local rules. The vehicle sequence may be used to determine when the vehicle traverses the intersection.
</abstract>

<claims>
1. An apparatus comprising: a capture device configured to generate pixel data corresponding to an exterior view from a vehicle; and a processor configured to (i) generate video frames from said pixel data, (ii) perform computer vision operations on said video frames to detect objects in said video frames, (iii) detect (a) an intersection and (b) other vehicles at said intersection based on said objects detected in said video frames, (iv) determine a vehicle sequence for traversing said intersection, and (v) monitor said other vehicles traversing said intersection using said computer vision operations to determine when said other vehicles have stopped, an order of arrival of said vehicle and said other vehicles, and a spatial relation between said vehicle and said other vehicles, wherein (a) said vehicle sequence is determined in response to (i) rules for navigating said intersection stored by a driving policy module programmed according to local regulations, (ii) said order of arrival of said vehicle at said intersection, and (iii) said order of arrival of said other vehicles at said intersection, (b) said processor determines when said vehicle stops at said intersection using sensor fusion operations to make inferences based on (i) sensors of said vehicle and (ii) said computer vision operations, and (c) said vehicle sequence is used to determine when said vehicle traverses said intersection.
2. The apparatus according to claim 1, wherein said intersection does not have traffic lights and said apparatus handles navigation of said intersection.
3. The apparatus according to claim 2, wherein said processor is configured to navigate said intersection autonomously.
4. The apparatus according to claim 1, wherein (i) said vehicle sequence comprises a dynamic ordered list of said vehicle and said other vehicles at said intersection and (ii) said dynamic ordered list indicates whether said vehicle or one of said other vehicles has a right of way to traverse said intersection based on said local regulations.
5. The apparatus according to claim 4, wherein said dynamic ordered list is modified as said vehicle and said other vehicles arrive at or leave said intersection.
6. The apparatus according to claim 4, wherein said apparatus is further configured to communicate an updated version of said dynamic ordered list by implementing at least one of (a) vehicle-to-vehicle communication or (b) vehicle-to-infrastructure communication.
7. The apparatus according to claim 1, wherein said vehicle sequence is used to determine a right of way between said vehicle and said other vehicles for navigating said intersection.
8. The apparatus according to claim 1, wherein said processor is further configured to (i) determine characteristics of said objects detected in said video frames and (ii) adjust said vehicle sequence in response to said characteristics.
9. The apparatus according to claim 8, wherein (i) said characteristics comprise gestures performed by a person and (ii) said gestures are detected by analyzing a movement of said person in a sequence of video frames.
10. The apparatus according to claim 8, wherein (i) said characteristics comprise at least one of (a) flashing headlights of one of said other vehicles and (b) a person waving said vehicle through said intersection and (ii) said vehicle sequence is adjusted by moving said vehicle to a beginning of said vehicle sequence.
11. The apparatus according to claim 8, wherein (i) said characteristics comprise at least one of (a) one of said other vehicles not stopping, (b) a pedestrian crossing or (c) detecting an emergency vehicle and (ii) said vehicle sequence is adjusted by moving said vehicle down said vehicle sequence.
12. The apparatus according to claim 1, wherein said computer vision operations are implemented by a convolutional neural network.
13. The apparatus according to claim 12, wherein said convolutional neural network is trained using fleet learning.
14. The apparatus according to claim 13, wherein (i) said fleet learning comprises capturing reference images using capture devices implemented in a plurality of vehicles, (ii) said reference images comprise areas exterior to said plurality of vehicles, (iii) said reference images are used as training data for said convolutional neural network and (iv) said training data comprises said reference images from many different vehicles.
15. The apparatus according to claim 1, wherein said processor has a plurality of co-processors.
16. The apparatus according to claim 1, wherein (i) said apparatus comprises a second capture device configured to implement a stereo camera pair with said capture device and (ii) said computer vision operations comprise performing stereo vision to determine depth information based on said video frames captured by said stereo camera pair.
17. The apparatus according to claim 1, wherein (i) said computer vision operations are configured to determine when said other vehicles stop at said intersection, (ii) said other vehicles that stop at said intersection after said vehicle are added to said vehicle sequence before said vehicle and (iii) said other vehicles that stop at said intersection after said vehicle are added to said vehicle sequence after said vehicle.
18. The apparatus according to claim 1, wherein (i) said driving policy module is further configured to store local customs and (ii) said local customs are used to determine said vehicle sequence.
19. The apparatus according to claim 1, wherein said apparatus is configured to enable said vehicle to drive autonomously and operate with said other vehicles that are not autonomous vehicles.
</claims>
</document>
