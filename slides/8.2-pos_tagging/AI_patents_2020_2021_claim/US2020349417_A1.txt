<document>

<filing_date>
2020-07-17
</filing_date>

<publication_date>
2020-11-05
</publication_date>

<priority_date>
2018-01-17
</priority_date>

<ipc_classes>
G06K9/62,G06N3/04
</ipc_classes>

<assignee>
ACHLER, Tsvi
</assignee>

<inventors>
ACHLER, Tsvi
</inventors>

<docdb_family_id>
67302469
</docdb_family_id>

<title>
SYSTEMS AND METHODS TO DEMONSTRATE CONFIDENCE AND CERTAINTY IN FEEDFORWARD AI METHODS
</title>

<abstract>
A computer-implemented method includes obtaining a first neural network trained to recognize one or more patterns; converting said first neural network to a mathematically equivalent second network; and then using said second network to determine one or more factors that influence pattern recognition by said first neural network.
</abstract>

<claims>
I claim:
1. A computer-implemented method comprising: obtaining a first neural network trained to recognize one or more patterns; converting said first neural network to an equivalent second neural network; and using at least said second neural network to determine one or more factors that influence recognition of a pattern by said first neural network.
2. The method of claim 1, wherein the first neural network is a multilayered network comprising a plurality of layers.
3. The method of claim 2, wherein the second neural network comprises the same number of layers as the first neural network.
4. The method of claim 1, wherein the first neural network comprises a feedforward network.
5. The method of claim 1, wherein the second neural network comprises a feedforward-feedback network.
6. The method of claim 1, wherein the first network includes a first number of input modules, a second number of output modules, and a third number of feed-forward connectors, and wherein the second neural network includes a fourth number of input modules, a fifth number of output modules, and a sixth number of feed-forward/feedback connectors, wherein the first number is equal to the fourth number, and the second number is equal to the fifth number, and the third number is equal to the sixth number.
7. The method of claim 1, wherein the first network includes a seventh number of nonlinearities between layers of the first network, and wherein the second neural network includes an eighth number of nonlinearities between layers of the second neural network, and wherein the seventh number is equal to the eighth number.
8. The method of claim 1, wherein said converting comprises: for each connection of the first network having a feedforward weight, forming, in the second neural network, a corresponding connection having a corresponding feedforward-feedback weight pair.
9. The method of claim 1, wherein said using comprises: using the second neural network's weights to iterate between feedforward and feedback until recognition of said pattern is complete, producing a desired recognition state.
10. The method of claim 9, wherein said using further comprises: using the first network's weights to perform recognition of said pattern.
11. The method of claim 9, further comprising: determining expected input activity using said desired recognition state and one or more weights on said second neural network.
12. The method of claim 9, further comprising: determining an expected pattern for a particular node.
13. The method of claim 1, wherein said using comprises determining one or more of: (i) one or more expected inputs that were not found; and (ii) one or more present inputs that were not expected.
14. The method of claim 1, wherein the second neural network comprises: one or more input modules, one or more output modules, and one or more feed-forward connectors, and one or more feedback connectors, wherein said one or more input modules are each adapted: (a) to receive and store input information received from sensors, (b) to receive back-transmitted output information from one or more feed-back connectors, (c) to modulate the input information using back-transmitted information to form modulated input information, (d) to forward-transmit the modulated input information using said one or more feed-forward connectors to said one or more output modules; wherein said one or more output modules are each adapted: (a) to store output information as stored output information, (b) to receive modulated input information forward-transmitted by one or more feed-forward connectors, (c) to modulate the stored output information using forward-transmitted information received from one or more feed-forward connectors, (d) to store the modulated output information as output information, and (d) to back-transmit the modulated output information using one or more feed-back connectors to said one or more input modules; wherein each of the one or more feed-forward connectors modifies and transmits modulated input information as forward-transmitted information from one of the one or more input modules to one of the one or more output modules, and wherein each feed-forward connector is associated with a feed-forward connector weight used to modify the information transmitted; and wherein each of the one or more feed-back connectors modifies and transmits modulated output information as back-transmitted information from one of the one or more output modules to one of the one or more input modules, wherein each of the feed-back connectors is associated with a feed-back connector weight that is used to modify the information transmitted.
15. The method of claim 1, wherein the second neural network is mathematically equivalent to the first neural network.
16. A recognition device having hardware, including at least one processor and associated memory, the device comprising: a network including one or more input modules, one or more output modules, and one or more feed-forward connectors, wherein said one or more input modules are each adapted: (a) to receive and store input information received from sensors, (b) to receive back-transmitted output information from one or more feed-back connectors, (c) to modulate the input information using back-transmitted information to form modulated input information, (d) to forward-transmit the modulated input information using said one or more feed-forward connectors to said one or more output modules; wherein said one or more output modules are each adapted: (a) to store output information as stored output information, (b) to receive modulated input information forward-transmitted by one or more feed-forward connectors, (c) to modulate the stored output information using forward-transmitted information received from one or more feed-forward connectors, (d) to store the modulated output information as output information, and (d) to back-transmit the modulated output information using one or more feed-back connectors to said one or more input modules; wherein each of the one or more feed-forward connectors modifies and transmits modulated input information as forward-transmitted information from one of the one or more input modules to one of the one or more output modules, and wherein each feed-forward connector is associated with a feed-forward connector weight used to modify the information transmitted; and wherein each of the one or more feed-back connectors modifies and transmits modulated output information as back-transmitted information from one of the one or more output modules to one of the one or more input modules, wherein each of the feed-back connectors is associated with a feed-back connector weight that is used to modify the information transmitted.
17. The recognition device of claim 16, where completion of operations comprises a cycle, and wherein the device repeats these cycles, and wherein the recognition device is further constructed and adapted: to calculate a state of a current cycle with a component to sum one or more of: (i) the input module modified information for all inputs; and/or (ii) the output module modified information for all outputs; and/or (iii) the module modified information; and to store the state of the current cycle; and to compare the state of the current cycle with a stored state of a previous cycle; and to stop the device if a change between the state of the current cycle with a stored state of a previous cycle is less than a threshold.
18. The recognition device of claim 16, further constructed and adapted to store weights of feedback connectors that back-transmit information from an individual output module, wherein said stored weights are is used to indicate sensor input values suited for that individual output module.
19. The recognition device of claim 16, further constructed and adapted: to calculate a sum of the back-transmitted output information received by an individual input module, to compare input information received by sensors of the same input module with the sum of the back-transmitted output information, to determine if a first sum of the back-transmitted output information received is greater than sensor information, and, based on whether the first sum of the back-transmitted output information received is greater than sensor information, to indicate that an input was expected and not adequately found in the sensors, to determine if a second sum of the back-transmitted output information received is less than sensor information, and, based on whether the second sum of the back-transmitted output information received is less than sensor information, to indicate that an input was not expected in the sensors, and to determine if a third sum of the back-transmitted output information received is equivalent to sensor information, and, based on said third sum of the back-transmitted output information received is equivalent to sensor information, to indicate that an input was expected and found in the sensors.
20. The recognition device of claim 16, further constructed and adapted: to learn or modify (a) an existing association, or (b) add a new recognition category with a new output node, or (c) add a new input sensor modularly, without modifying existing weights of the device that do not directly connect to the new input sensor or new output node, and to modify an existing forward-transmitting connector from input and its associated existing back-transmitting connector with an updated association; and/or to add a new non-existing output module with (a) associated new forward-transmitting connector from input (b) associated new back-transmitting connector to same input; and/or to add a new input node with (a) associated new back-transmitting connector from output (b) associated new forward-transmitting connector to same output.
21. The recognition device of claim 16, further comprising: a labeled data set with associated input patterns, wherein: data for each label is averaged to form a calculated average, and an output node is created for each label, and weights of feedback and feedforward mechanisms transmitting between that output node are determined by the calculated average.
22. The recognition device of claim 16, comprising: a first layer which receives inputs from sensors, one or more intermediate layers which receive a output of a previous layer as sensor input for the intermediate layer, and a top layer that serves as outputs of the network.
23. The recognition device of claim 22, wherein: the inputs are arranged in a manner that allows a smaller array of inputs to spatially sample a subspace of a larger input set, and wherein the smaller array is repetitively tiled throughout a larger array of inputs, and wherein the next layer is used to tile spatial inputs.
24. The recognition device of claim 23, comprising: a connection to transmit modulated input information from the layer above to the output module of the layer below using one or more feed-back connectors, wherein output modules of the layer below modulate the output information based on information obtained from one or more feed-back connectors form the input layer above.
25. The recognition device of claim 24, wherein one or more inputs or layers are configured in a manner to allow recognition of movement or sequences in time.
26. The recognition device of claim 25, wherein one or more layers delay processing in time to retain activation of a previous state, and wherein one or more layers with input sensors combine retained activation of one or more layers representing delayed information.
27. An article of manufacture comprising non-transitory computer-readable media having computer-readable instructions stored thereon, the computer-readable instructions including instructions for implementing a computer-implemented method, said method operable on a device comprising hardware including memory and at least one processor and running a service on said hardware, said method comprising the method of claim 1.
28. A system comprising: (a) hardware including memory and at least one processor, and (b) a service running on said hardware, wherein said service is configured to: perform the method of claim 1.
</claims>
</document>
