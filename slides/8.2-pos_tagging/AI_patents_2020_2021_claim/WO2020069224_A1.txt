<document>

<filing_date>
2019-09-26
</filing_date>

<publication_date>
2020-04-02
</publication_date>

<priority_date>
2018-09-28
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
APPLIED MATERIALS
</assignee>

<inventors>
DIDARI, SIMA
LIAO, TIANQING
RAJAGOPAL, HARIKRISHNAN
</inventors>

<docdb_family_id>
69947635
</docdb_family_id>

<title>
LONG SHORT-TERM MEMORY ANOMALY DETECTION FOR MULTISENSOR EQUIPMENT MONITORING
</title>

<abstract>
Methods, systems, and non-transitory computer readable medium are provided for long short-term memory (LSTM) anomaly detection for multi-sensor equipment monitoring. A method includes training a LSTM recurrent neural network (RNN) model for semiconductor processing fault detection. The training includes generating training data for the LSTM RNN model and providing the training data to train the LSTM RNN model on first training input and first target output to generate a trained LSTM RNN model for the semiconductor processing fault detection. The training data includes the first training input and the first target output based on normal runs of manufacturing processes of semiconductor processing equipment. Another method includes providing input based on runs of manufacturing processes of semiconductor processing equipment to a trained LSTM RNN model; obtaining one or more outputs from the trained LSTM RNN model; and using the one or more outputs for semiconductor processing fault detection.
</abstract>

<claims>
In the claims:
1. A method comprising:
training a long short-term memory (LSTM) recurrent neural network (RNN) model for semiconductor processing fault detection, the training of the LSTM RNN model comprising:
generating training data for the LSTM RNN model, wherein the generating of the training data comprises generating first training input and first target output based on normal runs of manufacturing processes of semiconductor processing equipment; and
providing the training data to train the LSTM RNN model on the first training input and the first target output to generate a trained LSTM RNN model for the semiconductor processing fault detection.
2. The method of claim 1 further comprising:
receiving, from a plurality of sensors, trace data corresponding the normal runs of the manufacturing processes of the semiconductor processing equipment; and time windowing the trace data to generate a plurality of sequenced data sets, wherein each of the plurality of sequenced data sets corresponds to a respective time window, wherein the first training input and the first target output are based on at least a subset of the plurality of sequenced data sets, wherein semiconductor processing fault detection is associated with one or more of semiconductor manufacturing for wafers or display manufacturing.
3. The method of claim 2, wherein the first training input comprises a first subset of the plurality of sequenced data sets at a first set of windows of time and a second subset of the plurality of sequenced data sets at a second set of windows of time, wherein each window of time of the second set of windows of time is offset from a corresponding window of time of the first set of windows of time by one or more windows of time.
4. The method of claim 2, wherein the first target output is same as the first training input, wherein the first training input comprises the plurality of sequenced data sets.
5. The method of claim 1, wherein the LSTM RNN model comprises a plurality of layers of LSTM cells, wherein output of a first layer of the plurality of layers is input to a second layer of the plurality of layers.
6. The method of claim 1, wherein the LSTM RNN model comprises an encoder and a decoder, wherein the encoder determines a compressed representation of the first training input, wherein the decoder uses the compressed representation to predict the first target output.
7. A method comprising:
providing input to a trained long short-term memory (LSTM) recurrent neural network (RNN) model, wherein the input is based on runs of manufacturing processes of semiconductor processing equipment;
obtaining one or more outputs from the trained LSTM RNN model, the one or more outputs comprising reconstruction data; and
using the one or more outputs for semiconductor processing fault detection.
8. The method of claim 7 further comprising:
receiving, from a plurality of sensors, trace data corresponding to the manufacturing processes of the semiconductor processing equipment; and
time windowing the trace data to generate a plurality of sequenced data sets, wherein each of the plurality of sequenced data sets corresponds to a respective time window, wherein the input comprises the plurality of sequenced data sets, wherein semiconductor processing fault detection is associated with one or more of semiconductor manufacturing for wafers or display manufacturing.
9. The method of claim 8, wherein the input comprises the plurality of sequenced data sets at a first set of windows of time, wherein the reconstruction data comprises predicted sequenced data sets at a second set of windows of time, wherein each window of time of the second set of windows of time is offset from a corresponding window of time of the first set of windows of time by one or more windows of time.
10. The method of claim 7, wherein the LSTM RNN model comprises a plurality of layers of LSTM cells, wherein output of a first layer of the plurality of layers is input to a second layer of the plurality of layers.
11. The method of claim 7, wherein the LSTM RNN model comprises an encoder and a decoder, wherein the input comprises a current plurality of sequenced data sets, wherein the encoder determines a compressed representation of the input, wherein the decoder uses the compressed representation to predict a future plurality of sequenced data sets.
12. The method of claim 7, wherein using the one or more outputs for
semiconductor processing fault detection comprises:
comparing the input to the reconstruction data to generate model
reconstruction error; and
identifying an anomaly responsive to determining that the model
reconstruction error is greater than a threshold error.
13. The method of claim 12 further comprising:
generating a plurality of anomaly scores from the one or more outputs, wherein each of the plurality of anomaly scores corresponds to a respective sensor of a plurality of sensors; and
ranking contribution to the model reconstruction error by each of the plurality of sensors based on the plurality of anomaly scores.
14. The method of claim 12, further comprising:
causing an anomaly response action to occur in response to detecting the anomaly.
15. A non-transitory computer readable storage medium having instructions stored thereon, which, when executed by a processing device, cause the processing device to perform operations comprising: providing input to a trained long short-term memory (LSTM) recurrent neural network (RNN) model, wherein the input is based on runs of manufacturing processes of semiconductor processing equipment;
obtaining one or more outputs from the trained LSTM RNN model, the one or more outputs comprising reconstruction data; and
using the one or more outputs for semiconductor processing fault detection.
16. The non-transitory computer readable storage medium of claim 15, wherein the operations further comprise:
receiving, from a plurality of sensors, trace data corresponding to the manufacturing processes of the semiconductor processing equipment; and
time windowing the trace data to generate a plurality of sequenced data sets, wherein each of the plurality of sequenced data sets corresponds to a respective time window, wherein the input comprises the plurality of sequenced data sets, wherein semiconductor processing fault detection is associated with one or more of semiconductor manufacturing for wafers or display manufacturing.
17. The non-transitory computer readable storage medium of claim 16, wherein the input comprises the plurality of sequenced data sets at a first set of windows of time, wherein the reconstruction data comprises predicted sequenced data sets at a second set of windows of time, wherein each window of time of the second set of windows of time is offset from a corresponding window of time of the first set of windows of time by one or more windows of time.
18. The non-transitory computer readable storage medium of claim 15, wherein the LSTM RNN model comprises a plurality of layers of LSTM cells, wherein output of a first layer of the plurality of layers is input to a second layer of the plurality of layers.
19. The non-transitory computer readable storage medium of claim 15, wherein the LSTM RNN model comprises an encoder and a decoder, wherein the input comprises a current plurality of sequenced data sets, wherein the encoder determines a compressed representation of the input, wherein the decoder uses the compressed representation to predict a future plurality of sequenced data sets.
20. The non-transitory computer readable storage medium of claim 15, wherein the operations further comprise:
comparing the input to the reconstruction data to generate model reconstruction error;
determining that the model reconstruction error is greater than a threshold error; and
flagging the model reconstruction error.
</claims>
</document>
