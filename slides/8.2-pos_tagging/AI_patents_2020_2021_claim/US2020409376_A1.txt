<document>

<filing_date>
2020-08-17
</filing_date>

<publication_date>
2020-12-31
</publication_date>

<priority_date>
2016-02-29
</priority_date>

<ipc_classes>
B25J9/16,G05D1/02
</ipc_classes>

<assignee>
AI
MEHRNIA, SOROUSH
FATH, LUKAS
EBRAHIMI AFROUZI, ALI
</assignee>

<inventors>
MEHRNIA, SOROUSH
FATH, LUKAS
EBRAHIMI AFROUZI, ALI
</inventors>

<docdb_family_id>
71517577
</docdb_family_id>

<title>
OBSTACLE RECOGNITION METHOD FOR AUTONOMOUS ROBOTS
</title>

<abstract>
Provided is a method for operating a robot, including capturing images of a workspace, comparing at least one object from the captured images to objects in an object dictionary, identifying a class to which the at least one object belongs using an object classification unit, instructing the robot to execute at least one action based on the object class identified, capturing movement data of the robot, and generating a planar representation of the workspace based on the captured images and the movement data, wherein the captured images indicate a position of the robot relative to objects within the workspace and the movement data indicates movement of the robot.
</abstract>

<claims>
1. A method for operating a robot, comprising: capturing, by at least one image sensor disposed on a robot, images of a workspace; obtaining, by a processor of the robot or via the cloud, the captured images; comparing, by the processor of the robot or via the cloud, at least one object from the captured images to objects in an object dictionary; identifying, by the processor of the robot or via the cloud, a class to which the at least one object belongs using an object classification unit; instructing, by the processor of the robot, the robot to execute at least one action based on the object class identified; capturing, by at least one sensor of the robot, movement data of the robot; and generating, by the processor of the robot or via the cloud, a spatial representation of the workspace based on the captured images and the movement data, wherein the captured images indicate a position of the robot relative to objects within the workspace and the movement data indicates movement of the robot.
2. The method of claim 1, wherein a planar representation or a combination of a spatial and planar representation is generated instead of the spatial representation.
3. The method of claim 1, wherein the robot executes the at least one action in at least one of: a current work session and future work sessions.
4. The method of claim 1, wherein comparing the at least one object from the captured images to objects in an object dictionary comprises generating a feature vector and characteristics data of the at least one object from the captured images.
5. The method of claim 4, wherein feature vector and characteristics data comprises any of edge characteristic combinations, basic shape characteristic combinations, size characteristic combinations, and color characteristic combinations.
6. The method of claim 1, wherein comparing the at least one object with objects in the object dictionary is performed using a neural network.
7. The method of claim 1, wherein the at least one action comprises at least one of executing an altered navigation path to avoid driving over the object identified and maneuvering around the object identified and continuing along the planned navigation path.
8. The method of claim 1, the at least one action is based at least on real time observations.
9. The method of claim 1, wherein the object dictionary is based on a training set in which images of a plurality of examples of the objects in the object dictionary are processed by the processor under varied lighting conditions and camera poses to extract and compile feature vector and characteristics data and associate that feature vector and characteristics data with a corresponding object.
10. The method of claim 1, wherein the object dictionary comprises any of: cables, cords, wires, toys, jewelry, garments, socks, shoes, shoelaces, feces, liquids, keys, food items, remote controls, plastic bags, purses, backpacks, earphones, cell phones, tablets, laptops, chargers, animals, fridges, televisions, chairs, tables, light fixtures, lamps, fan fixtures, cutlery, dishware, dishwashers, microwaves, coffee makers, smoke alarms, plants, books, washing machines, dryers, watches, blood pressure monitors, blood glucose monitors, first aid items, power sources, Wi-Fi repeaters, entertainment devices, appliances, and Wi-Fi routers.
11. The method of claim 1, further comprising: determining, by the processor of the robot or via the cloud, distances to objects in the captured images; identifying, by the processor of the robot or via the cloud, an opening in the workspace based on the distances to objects; and segmenting, by the processor of the robot or via the cloud, the workspace into subareas based on at least a position of one opening in the workspace.
12. The method of claim 1, further comprising: determining, by the processor of the robot or via the cloud, distances to objects in the captured images; and instructing, by the processor of the robot, the robot to avoid the objects in at least one of: a current work session and future work sessions.
13. The method of claim 1, wherein the at least one sensor comprises at least one of: an optical tracking sensor, an imaging sensor, an inertial measurement unit, an odometry encoder, a LIDAR sensor, a depth camera, and a gyroscope.
14. The method of claim 1, wherein capturing movement data comprises: capturing, by an optical tracking sensor, a plurality of images of surfaces within a field of view of the optical tracking sensor while the robot moves within the workspace; obtaining, by the processor of the robot or via the cloud, the plurality of images; determining, by the processor of the robot or via the cloud, linear movement of the optical tracking sensor based on the plurality of images captured, wherein linear movement of the optical tracking sensor is equivalent to linear movement of the robot; and determining, with the processor of the robot or via the cloud, rotational movement of the robot based on the linear movement of the optical tracking sensor.
15. The method of claim 1, wherein capturing movement data comprises: capturing, by an odometry encoder, odometry data; determining, with the processor of the robot or via the cloud, a number of wheel rotations based on the odometry data; and determining, with the processor of the robot or via the cloud, movement of the robot based on the number of wheel rotations.
16. The method of claim 1, wherein capturing movement data comprises: capturing, by an inertial measurement unit, inertial measurement unit data; and determining, with the processor of the robot or via the cloud, movement of the robot based on the inertial data.
17. The method of claim 1, wherein capturing movement data comprises: capturing, by a 360 degrees rotating LIDAR, distance data; and determining, with the processor of the robot or via the cloud, movement of the robot based on the changes in the distance data.
18. The method of claim 1, wherein capturing movement data comprises: capturing, by a depth camera, distance data; and determining, with the processor of the robot or via the cloud, movement of the robot based on the changes in the distance data.
19. The method of claim 1, wherein capturing movement data comprises: capturing, by at least one sensor, second movement data of the robot from a previous position to a current position; and correcting, by the processor of the robot or via the cloud, the movement data based on a translation vector of the second movement data describing movement of the robot from the previous position to the current position to account for error in the movement data caused by slippage of the robot.
20. The method of claim 1, wherein generating the spatial representation of the workspace further comprises: determining, by the processor of the robot or via the cloud, an overlapping area of a first image and a second image by comparing sensor readings of the first image to sensor readings of the second image, wherein: the first image and the second image are taken from different positions, and the sensor readings of the first image and the sensor readings of the second image comprise raw pixel intensity values; spatially aligning, by the processor of the robot or via the cloud, sensor readings of the first image and sensor readings of the second image based on the overlapping area; and inferring, by the processor of the robot or via the cloud, features of the workspace based on the spatially aligned sensor readings of the first image and the second image.
21. The method of claim 1, wherein generating the spatial representation of the workspace further comprises: determining, by the processor of the robot or via the cloud, an overlapping area of a first depth reading and a second depth reading by comparing sensor readings of the first depth reading to sensor readings of the second reading, wherein the first depth reading and the second depth readings are taken from different positions; and spatially aligning and integrating, by the processor of the robot or via the cloud, sensor readings of the first depth readings with sensor readings of the second depth reading based on the overlapping area to create a spatial representation of a larger portion of the workspace.
22. The method of claim 1, wherein generating the spatial representation of the workspace further comprises: collecting, by a LIDAR sensor, depth readings from the LIDAR sensor to objects in the workspace at a first position of the robot and a second position of the robot; and aligning, by the processor of the robot or via the cloud, the depth readings collected from the second position of the robot with the depth readings collected from the first position of the robot after executing a displacement towards undiscovered areas, wherein new depth readings collected from the undiscovered areas are integrated with the previously collected depth readings until all areas of the workspace are discovered.
23. The method of claim 1, wherein the robot performs work in the entirety of the workspace.
24. The method of claim 1, wherein the robot performs work in the workspace by driving along segments having a linear motion trajectory, the segments forming a boustrophedon pattern that covers at least part of the workspace.
25. The method of claim 24, wherein the boustrophedon pattern comprises at least four segments with motion trajectories in alternating directions.
26. The method of claim 25, wherein the distance between the segments is determined based on a length of a brush of the robot.
27. The method of claim 1, further comprising: creating, by the processor of the robot or via the cloud, a first iteration of spatial representation of the workspace, wherein: the first iteration of the spatial representation is based at least on sensor data sensed by at least one sensor in a first position and orientation, and the robot is configured to move in the workspace to change a location of the sensed area as the robot moves; selecting, by the processor of the robot or via the cloud, a first undiscovered area of the workspace; in response to selecting the first undiscovered area, causing, by the processor of the robot, the robot to move to a second closer position and orientation relative to the first undiscovered area to sense data in at least part of the first undiscovered area; and determining, by the processor of the robot or via the cloud, that the sensed area overlaps with at least part of the workspace previously discovered. recognizing, by the processor of the robot or via the cloud, an undiscovered area of the workspace based on newly observed sensor data sensed by the at least one sensor and distinguishing a previously visited area from a non-visited area.
28. The method of claim 1, further comprising: determining, by the processor of the robot or via the cloud, a navigation path of the robot based on the spatial representation of the workspace, wherein the navigation path is based on a set of the most desired trajectories to navigate the robot from a first location to a second location; and controlling, by the processor of the robot, an actuator of the robot to cause the robot to move along the determined navigation path.
29. The method of claim 28, further comprising: comparing, by the processor of the robot or via the cloud, the movement of the robot with an intended trajectory of the robot along the determined navigation path; and correcting, by the processor of the robot or via the cloud, the position of the robot within the spatial representation of the workspace based on newly observed sensor data, comprising: generating, with the processor of the robot or via the cloud, virtually simulated robots located at different possible locations within the workspace; comparing, with the processor of the robot or via the cloud, at least part of the newly observed sensor data with spatial representations of the workspace, each spatial representation corresponding with a perspective of a virtually simulated robot; identifying, with the processor of the robot or via the cloud, the current location of the robot as a location of a virtually simulated robot with which the at least part of the newly observed sensor data best fits the corresponding spatial representation of the workspace; inferring, with the processor of the robot or via the cloud, a most likely current location of the robot; and correcting, with the processor of the robot or via the cloud, the position of the robot within the spatial representation of the workspace to the most likely current location of the robot inferred.
30. The method of claim 1, further comprising: receiving, by an application of a communication device paired with the robot, at least one input designating at least one of: an operation of the robot; a movement of the robot; a deletion, addition, or modification of a schedule of the robot; a deletion, addition, or modification to the spatial representation of the workspace; a deletion, addition, or modification of a subarea; a deletion, addition, or modification of a keep-out zone; a deletion, addition, or modification of a navigation path of the robot; information or instruction required in pairing the robot with a Wi-Fi router; and information for programming the robot; and displaying, by the application of the communication device paired with the robot, at least one of: the spatial representation of the workspace; a navigation path of the robot; and a camera view of the robot.
31. The method of claim 1, further comprising: observing, by the processor of the robot, at least one of: a gesture, a voice command, and a movement of a person or pet; and instructing, by the processor of the robot, the robot to execute at least one action in response to the observation.
32. The method of claim 1, wherein the robot comprises at least one of: a speaker for playing music, a Wi-Fi repeater, a screen for telepresence, a charging socket, an over-the-air inductive charging mechanism, a charging port for a mobile device, at least one sensor for measuring distances to objects, and at least one sensor for perceiving obstacles.
33. The method of claim 1, wherein at least some processing is offloaded to the cloud.
34. The method of claim 1, further comprising: emitting, by a light source disposed on the robot, a structured light on surfaces of the workspace, wherein the light source is any of a laser, a light emitting diode, and an infrared light and wherein the light source is in the form of a line or at least one point; capturing, by an image sensor, images of the projected structured light; and determining, by the processor of the robot or via the cloud, depth to the surfaces on which the structured light is emitted based on the images and geometry of the structured light in the images.
35. The method of claim 1, further comprising: establishing a connection between the robot and the cloud; and registering the robot with a backend database maintained by a manufacturer of the robot, wherein the manufacturer monitors the robot.
36. The method of claim 1, wherein the robot performs a task of cleaning with at least one of: a main brush, a side brush, a dry mop, a wet mop, and a steam mechanism.
37. The method of claim 36, wherein the wet mop comprises a fluid reservoir that dispenses fluid passively through apertures or using a motorized mechanism.
38. The method of claim 1, wherein the robot navigates to a docking station to empty a bin of the robot after a predetermined amount of area covered by the robot.
39. The method of claim 1, wherein the robot navigates to a docking station to fill up a fluid reservoir of the robot.
40. The method of claim 1, wherein the processor uses complementary data from an image sensor and a structured light sensor to generate the spatial representation, wherein data from the structured light sensor is used to generate a floor plan and data from the image sensor captures objects or features in the workspace.
41. The method of claim 1, wherein the processor uses complementary data from an image sensor and a LIDAR sensor to generate the spatial representation, wherein data from the LIDAR sensor is used to generate a floor plan and data from the image sensor captures objects or features in the workspace.
42. The method of claim 1, further comprising: detecting, by the processor of the robot or via the cloud, a room of the workspace in real time as the robot traverses the room.
43. A robot, comprising: a chassis; a set of wheels coupled to the chassis; a processor; a tangible, non-transitory, machine-readable medium storing instructions that when executed by the processor effectuate operations comprising: capturing, by at least one image sensor disposed on a robot, images of a workspace; obtaining, by a processor of the robot or via the cloud, the captured images; comparing, by the processor of the robot or via the cloud, at least one object from the captured images to objects in an object dictionary; identifying, by the processor of the robot or via the cloud, a class to which the at least one object belongs using an object classification unit; instructing, by the processor of the robot, the robot to execute at least one action based on the object class identified; determining, by the processor of the robot or via the cloud, a navigation path of the robot based on a spatial representation of the workspace, wherein the navigation path is based on a set of the most desired trajectories to navigate the robot from a first location to a second location; and controlling, by the processor of the robot, an actuator of the robot to cause the robot to move along the determined navigation path.
</claims>
</document>
