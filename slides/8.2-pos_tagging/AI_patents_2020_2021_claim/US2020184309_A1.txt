<document>

<filing_date>
2019-12-05
</filing_date>

<publication_date>
2020-06-11
</publication_date>

<priority_date>
2018-12-06
</priority_date>

<ipc_classes>
G06F17/16,G06F7/483,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
MIPS TECH
</assignee>

<inventors>
PATEL, SANJAY
</inventors>

<docdb_family_id>
70971389
</docdb_family_id>

<title>
NEURAL NETWORK PROCESSING USING SPECIALIZED DATA REPRESENTATION
</title>

<abstract>
Techniques for neural network processing using specialized data representation are disclosed. Input data for manipulation in a layer of a neural network is obtained. The input data includes image data, where the image data is represented in bfloat16 format without loss of precision. The manipulation of the input data is performed on a processor that supports single-precision operations. The input data is converted to a 16-bit reduced floating-point representation, where the reduced floating-point representation comprises an alternative single-precision data representation mode. The input data is manipulated with one or more 16-bit reduced floating-point data elements. The manipulation includes a multiply and add-accumulate operation. The manipulation further includes a unary operation, a binary operation, or a conversion operation. A result of the manipulating is forwarded to a next layer of the neural network.
</abstract>

<claims>
1. A processor-implemented method for data manipulation comprising: obtaining input data for manipulation in a layer of a neural network, wherein the manipulation is performed on a processor that supports single-precision operations; converting the input data to a 16-bit reduced floating-point representation, wherein the reduced floating-point representation comprises an alternative single-precision data representation mode; manipulating the input data with one or more 16-bit reduced floating-point data elements; and forwarding a result of the manipulating to a next layer of the neural network.
2. The method of claim 1 wherein the manipulating includes a multiply and add-accumulate operation.
3. The method of claim 1 wherein the manipulating includes a unary operation, a binary operation, or a conversion operation.
4. The method of claim 1 wherein the input data comprises image data.
5. The method of claim 4 wherein the image data is represented in bfloat 16 format without loss of precision.
6. The method of claim 4 wherein the image data uses an 8-bit unsigned integer RGB representation.
7. The method of claim 1 wherein the one or more 16-bit reduced floating-point data elements comprise one or more neural network weights.
8. The method of claim 7 wherein the weights are trained using single-precision data representations.
9. The method of claim 8 further comprising converting the single-precision data representations into 16-bit reduced floating-point data representations for use in neural network processing.
10. 10-15. (canceled)
16. The method of claim 1 wherein the manipulating comprises a dot-product operation.
17. The method of claim 1 wherein an operation employs a table lookup function to accomplish a division, a square root, a reciprocal, or a reciprocal square root calculation.
18. The method of claim 17 wherein the table lookup function uses 7+1 input bits to yield a 19-bit output.
19. 19-22. (canceled)
23. The method of claim 1 further comprising performing left or right versions of commands for the processor that support single-precision operations to handle 8-bit unsigned integer input.
24. The method of claim 23 wherein the performing enables hybrid data type support.
25. The method of claim 24 wherein the hybrid data type support includes support for an 8-bit unsigned integer representation and a 16-bit reduced floating-point representation.
26. The method of claim 1 further comprising including commands for the processor that support single-precision operands and 16-bit reduced floating-point representation operands in a same operation.
27. 27-31. (canceled)
32. The method of claim 1 wherein the 16-bit reduced floating-point representation comprises a bfloat 16 data representation.
33. 33-34. (canceled)
35. The method of claim 1 wherein the 16-bit reduced floating-point representation comprises a reduced mantissa floating-point representation.
36. The method of claim 1 wherein the manipulating includes mixed floating-point data elements.
37. The method of claim 36 wherein the mixed floating-point data elements include single-precision data elements.
38. A computer program product embodied in a non-transitory computer readable medium for data manipulation, the computer program product comprising code which causes one or more processors to perform operations of: obtaining input data for manipulation in a layer of a neural network, wherein the manipulation is performed on a processor that supports single-precision operations; converting the input data to a 16-bit reduced floating-point representation, wherein the reduced floating-point representation comprises an alternative single-precision data representation mode; manipulating the input data with one or more 16-bit reduced floating-point data elements; and forwarding a result of the manipulating to a next layer of the neural network.
39. A computer system for data manipulation comprising: a memory which stores instructions; one or more processors coupled to the memory wherein the one or more processors, when executing the instructions which are stored, are configured to: obtain input data for manipulation in a layer of a neural network, wherein the manipulation is performed on a processor that supports single-precision operations; convert the input data to a 16-bit reduced floating-point representation, wherein the reduced floating-point representation comprises an alternative single-precision data representation mode; manipulate the input data with one or more 16-bit reduced floating-point data elements; and forward a result of the manipulating to a next layer of the neural network.
</claims>
</document>
