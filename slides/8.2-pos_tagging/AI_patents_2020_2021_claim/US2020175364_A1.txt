<document>

<filing_date>
2018-05-22
</filing_date>

<publication_date>
2020-06-04
</publication_date>

<priority_date>
2017-05-19
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
DEEPMIND TECHNOLOGIES
</assignee>

<inventors>
DA MOTTA SALLES BARRETO, ANDRE
MODAYIL, JOSEPH VARUGHESE
SILVER, DAVID
VAN HASSELT, HADO PHILLIP
XU, ZHONGWEN
</inventors>

<docdb_family_id>
62217992
</docdb_family_id>

<title>
TRAINING ACTION SELECTION NEURAL NETWORKS USING A DIFFERENTIABLE CREDIT FUNCTION
</title>

<abstract>
Methods, systems, and apparatus, including computer programs encoded on computer storage media, for reinforcement learning. A reinforcement learning neural network selects actions to be performed by an agent interacting with an environment to perform a task in an attempt to achieve a specified result. The reinforcement learning neural network has at least one input to receive an input observation characterizing a state of the environment and at least one output for determining an action to be performed by the agent in response to the input observation. The system includes a reward function network coupled to the reinforcement learning neural network. The reward function network has an input to receive reward data characterizing a reward provided by one or more states of the environment and is configured to determine a reward function to provide one or more target values for training the reinforcement learning neural network.
</abstract>

<claims>
1. A neural network system for reinforcement learning, the neural network system comprising: a reinforcement learning neural network to select actions to be performed by an agent interacting with an environment to perform a task in an attempt to achieve a specified result, the reinforcement learning neural network having at least one input to receive an input observation characterizing a state of the environment and at least one output for determining an action to be performed by the agent in response to the input observation; and a reward function network, coupled to the reinforcement learning neural network, having an input to receive reward data characterizing a reward provided by one or more states of the environment, and configured to determine a reward function to provide one or more target values for training the reinforcement learning neural network.
2. A neural network system as claimed in claim 1 wherein the reward function comprises a weighted sum of reward predictions for n future time steps, where n is an integer equal to or greater than 1.
3. A neural network system as claimed in claim 2 wherein the weighted sum comprises an exponentially weighted sum with decay parameter λ defining a decay of the weighted sum away from a current time step.
4. A neural network system as claimed in claim 3 wherein the reward function network is configured to perform computations weighted by λ, wherein the reward function network is configured to learn a value for λ.
5. A neural network system as claimed in claim 4 wherein λ is a function of the time step.
6. A neural network system as claimed in claim 1 wherein the reward function network includes or more learnable parameters to determine a λ-value; and wherein the one or more target values are dependent upon the λ-value.
7. A neural network system as claimed in claim 6 wherein the reward function network includes a λ-network coupled to the reinforcement learning neural network to determine the λ-value from a state of the reinforcement learning neural network.
8. A neural network system as claimed in claim 6 wherein the one or more target values comprise λ-return values.
9. A neural network system as claimed in claim 1 further comprising a reward function target generator to generate reward function targets for training one or more learnable parameters of the reward function network.
10. A neural network system as claimed in claim 9 wherein the reward function targets comprise alternate λ-return values generated independently of the one or more target values from the reward function network.
11. A neural network system as claimed in claim 10 wherein the reward function target generator is configured to perform an alternate rollout from a current state of the environment to determine the reward function targets.
12. A neural network system as claimed in claim 10 further comprising memory to store the target values for training the reinforcement learning neural network, and wherein the reward function target generator is configured to retrieve the stored target values to provide the reward function targets.
13. A neural network system as claimed in claim 1 wherein the reinforcement learning neural network comprises a recurrent neural network to provide a representation of a sequence of states of the environment comprising a sequence of state-dependent values, and wherein the reward function network is configured to generate the one or more target values from the sequence of state-dependent values.
14. A neural network system as claimed in claim 13 wherein the reward function network has an input to receive state-dependent reward value data for the sequence of states of the environment.
15. A neural network system as claimed in claim 13 including episodic memory to store state and reward data from previous states of the system, and wherein the reward function network is configured to receive reward data from the episodic memory.
16. A neural network system as claimed in claim 1 wherein the reward function comprises a weighted sum of reward predictions for n future action steps, where n is an integer equal to or greater than 1, and wherein the reward function network is configured to determine a parameter defining a rate of exponential decay of the weighted sum n away from a current time step.
17. A method of training a reinforcement learning neural network, the method comprising: training a reinforcement learning neural network by performing a plurality of reinforcement learning steps on input observation data characterizing a state of an environment to learn an action policy for the reinforcement learning neural network, wherein the reinforcement learning neural network is trained using a weighted set of n step returns where n is an integer equal to or greater than one and an n step return defines an estimated reward return for a plurality of n action steps; and jointly training a reward function network with the reinforcement learning neural network to learn a parameter defining a rate of exponential decay of the weighted set of n step returns with a number of steps over which the return is calculated.
18. A method as claimed in claim 17 further comprising independently sampling states of the environment to determine a target for training the reward function network.
19. (canceled)
20. One or more non-transitory computer-readable storage media storing instructions that when executed by one or more computers cause the one or more computers to implement a neural network system for reinforcement learning, the neural network system comprising: a reinforcement learning neural network to select actions to be performed by an agent interacting with an environment to perform a task in an attempt to achieve a specified result, the reinforcement learning neural network having at least one input to receive an input observation characterizing a state of the environment and at least one output for determining an action to be performed by the agent in response to the input observation; and a reward function network, coupled to the reinforcement learning neural network, having an input to receive reward data characterizing a reward provided by one or more states of the environment, and configured to determine a reward function to provide one or more target values for training the reinforcement learning neural network.
21. The non-transitory computer-readable storage media of claim 20, wherein the reward function comprises a weighted sum of reward predictions for n future time steps, where n is an integer equal to or greater than 1.
</claims>
</document>
