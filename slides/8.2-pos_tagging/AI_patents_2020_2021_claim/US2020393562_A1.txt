<document>

<filing_date>
2019-06-13
</filing_date>

<publication_date>
2020-12-17
</publication_date>

<priority_date>
2019-06-13
</priority_date>

<ipc_classes>
G01M15/14,G01S13/86,G01S13/934,G01S17/10,G01S17/894,G01S17/933,G01S19/14,G01W1/00
</ipc_classes>

<assignee>
BOEING COMPANY
</assignee>

<inventors>
EVANS, NICK S.
STAUDINGER, TYLER C.
</inventors>

<docdb_family_id>
71092437
</docdb_family_id>

<title>
Methods And Systems For Acoustic Machine Perception For An Aircraft
</title>

<abstract>
In an example, a method is described. The method includes causing one or more sensors arranged on an aircraft to acquire, over a window of time, first data associated with a first object that is within an environment of the aircraft, where the one or more sensors include one or more of a light detection and ranging (LIDAR) sensor, a radar sensor, or a camera, causing an array of microphones arranged on the aircraft to acquire, over approximately the same window of time as the first data is acquired, first acoustic data associated with the first object, and training a machine learning model by using the first acoustic data as an input value to the machine learning model and by using an azimuth, a range, an elevation, and a type of the first object identified from the first data as ground truth output labels for the machine learning model.
</abstract>

<claims>
1. A method comprising: causing one or more sensors arranged on an aircraft to acquire, over a window of time, first data associated with a first object that is within an environment of the aircraft, wherein the one or more sensors include one or more of a light detection and ranging sensor, a radar sensor, or a camera; causing an array of microphones arranged on the aircraft to acquire, over approximately the same window of time as the first data is acquired, first acoustic data associated with the first object; and training, by a processor, a machine learning model by using the first acoustic data as an input value to the machine learning model and by using an azimuth of the first object, a range of the first object, an elevation of the first object, and a type of the first object identified from the first data as ground truth output labels for the machine learning model, wherein the machine learning model is configured to predict, based on target acoustic data subsequently acquired by the array of microphones and associated with a target object within the environment of the aircraft, an azimuth of the target object, a range of the target object, an elevation of the target object, and a type of the target object.
2. The method of claim 1, wherein causing the array of microphones arranged on the aircraft to acquire the first acoustic data associated with the first object comprises causing one or more microphones arranged on each wing of the aircraft to acquire the first acoustic data associated with the first object.
3. The method of claim 1, wherein the azimuth of the first object, the range of the first object, and the elevation of the first object identified from the first data are relative to the aircraft, the method further comprising: detecting, by the processor, over approximately the same window of time as the first data and the first acoustic data are acquired, automatic dependent surveillance-broadcast information broadcast by the first object, wherein the automatic dependent surveillance-broadcast information indicates an azimuth of the first object relative to the Earth, a range of the first object relative to the Earth, an elevation of the first object relative to the Earth, the type of the first object, a heading of the first object, and a speed of the first object, and wherein training the machine learning model comprises training the machine learning model by using the azimuth of the first object relative to the Earth, the range of the first object relative to the Earth, the elevation of the first object relative to the Earth, the type of the first object, the heading of the first object, and the speed of the first object indicated by the automatic dependent surveillance-broadcast information broadcast by the first object as at least a portion of the ground truth output labels for the machine learning model.
4. The method of claim 1, further comprising: receiving, by the processor, over approximately the same window of time as the first data and the first acoustic data are acquired, information indicating a state of an engine of the aircraft, wherein training the machine learning model comprises training the machine learning model by using the state of the engine as one of the ground truth output labels for the machine learning model, wherein the machine learning model is configured to predict, based on the target acoustic data, that the engine is generating an anomalous acoustic signature.
5. The method of claim 1, further comprising: determining, over approximately the same window of time as the first data and the first acoustic data are acquired, a global positioning system location of the aircraft; receiving a weather report; and determining one or more weather conditions of the environment indicated by the weather report as being present at the global positioning system location over approximately the same window of time as the first data and the first acoustic data are acquired, wherein training the machine learning model comprises training the machine learning model by using the one or more weather conditions of the environment as one or more of the ground truth output labels for the machine learning model, wherein the machine learning model is configured to predict, based on the target acoustic data, that the one or more weather conditions are present in the environment.
6. The method of claim 1, wherein training the machine learning model comprises training the machine learning model by using one or more weather conditions of the environment identified from the first data as one or more of the ground truth output labels for the machine learning model, wherein the machine learning model is configured to predict, based on the target acoustic data, that the one or more weather conditions are present in the environment.
7. The method of claim 1, wherein the method is performed while the aircraft is in motion.
8. A method comprising: causing an array of microphones arranged on an aircraft to acquire target acoustic data associated with a target object within an environment of the aircraft; and executing, by a processor, a machine learning model to predict, based on the target acoustic data, an azimuth of the target object, a range of the target object, an elevation of the target object, and a type of the target object, wherein the machine learning model was trained by (i) causing one or more sensors arranged on the aircraft to acquire first data associated with a first object that was within the environment of the aircraft, wherein the one or more sensors include one or more of a light detection and ranging sensor, a radar sensor, or a camera, (ii) causing the array of microphones arranged on the aircraft to acquire, over approximately the same window of time as the first data was acquired, first acoustic data associated with the first object, (iii) the processor using the first acoustic data as an input value to the machine learning model, and (iv) the processor using an azimuth of the first object, a range of the first object, an elevation of the first object, and a type of the first object identified from the first data as ground truth output labels for the machine learning model.
9. The method of claim 8, further comprising: based on one or more of the azimuth of the target object, the range of the target object, the elevation of the target object, or the type of the target object, causing the aircraft to move and avoid the target object.
10. The method of claim 8, wherein causing the array of microphones arranged on the aircraft to acquire the target acoustic data associated with the target object comprises causing one or more microphones arranged on each wing of the aircraft to acquire the target acoustic data associated with the target object.
11. The method of claim 8, wherein the azimuth of the first object, the range of the first object, and the elevation of the first object identified from the first data are relative to the aircraft, and wherein the machine learning model was further trained by: detecting, by the processor, over approximately the same window of time as the first data and the first acoustic data were acquired, automatic dependent surveillance-broadcast information broadcast by the first object, wherein the automatic dependent surveillance-broadcast information indicates an azimuth of the first object relative to the Earth, a range of the first object relative to the Earth, an elevation of the first object relative to the Earth, the type of the first object, a heading of the first object, and a speed of the first object, and using the azimuth of the first object relative to the Earth, the range of the first object relative to the Earth, the elevation of the first object relative to the Earth, the type of the first object, the heading of the first object, and the speed of the first object indicated by the automatic dependent surveillance-broadcast information broadcast by the first object as at least a portion of the ground truth output labels for the machine learning model.
12. The method of claim 8, wherein the machine learning model was further trained by: receiving, by the processor, over approximately the same window of time as the first data and the first acoustic data were acquired, information indicating a state of an engine of the aircraft, and using the state of the engine as one of the ground truth output labels for the machine learning model, wherein the machine learning model is configured to predict, based on the target acoustic data, that the engine is generating an anomalous acoustic signature.
13. The method of claim 8, wherein the machine learning model was further trained by: determining, over approximately the same window of time as the first data and the first acoustic data were acquired, a global positioning system location of the aircraft, receiving a weather report, determining one or more weather conditions of the environment indicated by the weather report as being present at the global positioning system location over approximately the same window of time as the first data and the first acoustic data were acquired, and using the one or more weather conditions of the environment as one or more of the ground truth output labels for the machine learning model, wherein the machine learning model is configured to predict, based on the target acoustic data, that the one or more weather conditions are present in the environment.
14. The method of claim 8, wherein the machine learning model was further trained by using one or more weather conditions of the environment identified from the first data as one or more of the ground truth output labels for the machine learning model, wherein the machine learning model is configured to predict, based on the target acoustic data, that the one or more weather conditions are present in the environment.
15. A system comprising: an aircraft; one or more sensors arranged on the aircraft, wherein the one or more sensors include one or more of a light detection and ranging sensor, a radar sensor, or a camera; an array of microphones arranged on the aircraft; and a computing device having a processor and memory storing instructions executable by the processor to perform a set of operations comprising: causing the one or more sensors to acquire, over a window of time, first data associated with a first object that is within an environment of the aircraft; causing the array of microphones to acquire, over approximately the same window of time as the first data is acquired, first acoustic data associated with the first object; and training a machine learning model by using the first acoustic data as an input value to the machine learning model and by using an azimuth of the first object, a range of the first object, an elevation of the first object, and a type of the first object identified from the first data as ground truth output labels for the machine learning model, wherein the machine learning model is configured to predict, based on target acoustic data subsequently acquired by the array of microphones and associated with a target object within the environment of the aircraft, an azimuth of the target object, a range of the target object, an elevation of the target object, and a type of the target object.
16. The system of claim 15, wherein the computing device is onboard the aircraft.
17. The system of claim 15, wherein the set of operations are performed while the aircraft is in motion.
18. The system of claim 15, wherein the array of microphones comprises one or more microphones arranged on each wing of the aircraft.
19. The system of claim 15, wherein the azimuth of the first object, the range of the first object, and the elevation of the first object identified from the first data are relative to the aircraft, the set of operations further comprising: detecting, over approximately the same window of time as the first data and the first acoustic data are acquired, automatic dependent surveillance-broadcast information broadcast by the first object, wherein the automatic dependent surveillance-broadcast information indicates an azimuth of the first object relative to the Earth, a range of the first object relative to the Earth, an elevation of the first object relative to the Earth, the type of the first object, a heading of the first object, and a speed of the first object, and wherein training the machine learning model comprises training the machine learning model by using the azimuth of the first object relative to the Earth, the range of the first object relative to the Earth, the elevation of the first object relative to the Earth, the type of the first object, the heading of the first object, and the speed of the first object indicated by the automatic dependent surveillance-broadcast information broadcast by the first object as at least a portion of the ground truth output labels for the machine learning model.
20. The system of claim 15, the set of operations further comprising: receiving, over approximately the same window of time as the first data and the first acoustic data are acquired, information indicating a state of an engine of the aircraft, wherein training the machine learning model comprises training the machine learning model by using the state of the engine as one of the ground truth output labels for the machine learning model, wherein the machine learning model is configured to predict, based on the target acoustic data, that the engine is generating an anomalous acoustic signature.
</claims>
</document>
