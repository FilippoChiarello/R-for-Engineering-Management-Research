<document>

<filing_date>
2019-06-03
</filing_date>

<publication_date>
2020-12-03
</publication_date>

<priority_date>
2019-06-03
</priority_date>

<ipc_classes>
G06N20/00,G06T7/168,G06T7/174,G06T7/194,G06T7/50,G06T7/73
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
HUA, GANG
LIU BO
DIXIT, Mandar Dilip
</inventors>

<docdb_family_id>
70554253
</docdb_family_id>

<title>
NOVEL POSE SYNTHESIS
</title>

<abstract>
Examples are disclosed that relate to computing devices and methods for synthesizing a novel pose of an object. One example provides a method comprising receiving a reference image of an object corresponding to an original viewpoint. The reference image of the object is translated into a depth map of the object, and a new depth map of the object is synthesized to correspond to a new viewpoint. A new image of the object is generated from the new viewpoint based on the new depth map of the object and the reference image of the object.
</abstract>

<claims>
1. Enacted on a computing system, a method for synthesizing a novel pose of an object, the method comprising: receiving a reference image of an object corresponding to an original viewpoint; translating the reference image of the object into a reference depth map of the object; synthesizing a new depth map of the object corresponding to a new viewpoint; and based on the new depth map of the object and the reference image of the object, generating a new image of the object from the new viewpoint.
2. The method of claim 1, wherein translating the reference image of the object into the reference depth map of the object comprises: inputting the reference image of the object into a domain transfer module; and receiving the reference depth map of the object from the domain transfer module.
3. The method of claim 2, further comprising receiving a foreground mask from the domain transfer module, the foreground mask identifying pixels associated with the object.
4. The method of claim 2, wherein the domain transfer module comprises a domain transfer model, the method further comprising training the domain transfer model on a dataset of paired images and depth maps.
5. The method of claim 1, wherein synthesizing the new depth map of the object corresponding to the new viewpoint comprises: inputting the reference depth map of the object into a depth map generator; receiving the new depth map of the object from the depth map generator; and refining the new depth map of the object using a 3D depth refinement module.
6. The method of claim 5, further comprising receiving a sequence of new depth maps from the depth map generator, and wherein refining the new depth map comprises using a 3D convolutional neural network to enforce consistency among the sequence of new depth maps.
7. The method of claim 1, wherein generating the new image of the object comprises: mapping the reference image of the object to an appearance parameter; mapping the new depth map of the object to a shape parameter; and combining the shape parameter and the appearance parameter to generate the new image of the object from the new viewpoint.
8. The method of claim 1, wherein generating the new image of the object comprises: inputting the reference image of the object and the new depth map of the object into an identity recovery model; and receiving the new image of the object from the identity recovery model.
9. The method of claim 8, further comprising training the identity recovery model on unpaired depth and image data.
10. The method of claim 8, further comprising training the identity recovery model by: using a first structure encoder to map the reference image to a reference shape parameter; using a second structure encoder to map the new depth map to a new shape parameter; using a first appearance encoder to map the reference image to a reference appearance parameter; using a second appearance encoder to map the new depth map to a new appearance parameter; and combining each of the reference shape parameter and the new shape parameter with one of the reference appearance parameter and the new appearance parameter to generate an image.
11. The method of claim 8, further comprising training the identity recovery model using supervised learning and unsupervised learning.
12. The method of claim 11, further comprising directly supervising the training using the reference image of the object, the reference depth map, and the new depth map.
13. A computing device comprising: a processor; and a storage device storing instructions executable by the processor to receive a reference image of an object corresponding to an original viewpoint; translate the reference image of the object into a reference depth map of the object; synthesize a new depth map of the object corresponding to a new viewpoint; and based on the new depth map of the object and the reference image of the object, generate a new image of the object from the new viewpoint.
14. The computing device of claim 13, wherein the instructions are further executable to translate the reference image of the object into the reference depth map of the object by: inputting the reference image of the object into a domain transfer module; and receiving the reference depth map of the object from the domain transfer module.
15. The computing device of claim 14, wherein the instructions are further executable to receive a foreground mask from the domain transfer module, the foreground mask identifying pixels associated with the object.
16. The computing device of claim 13, wherein the instructions are further executable to generate the new image of the object by: mapping the reference image of the object to an appearance parameter; mapping the new depth map of the object to a shape parameter; and combining the shape parameter and the appearance parameter to generate the new image of the object from the new viewpoint.
17. The computing device of claim 13, wherein the instructions are further executable to generate the new image of the object by: inputting the reference image of the object and the new depth map of the object into an identity recovery model; and receiving the new image of the object from the identity recovery model.
18. The computing device of claim 17, wherein the instructions are further executable to train the identity recovery model by: using a first structure encoder to map the reference image to a reference shape parameter; using a second structure encoder to map the new depth map to a new shape parameter; using a first appearance encoder to map the reference image to a reference appearance parameter; using a second appearance encoder to map the new depth map to a new appearance parameter; and combining each of the reference shape parameter and the new shape parameter with one of the reference appearance parameter and the new appearance parameter to generate an image.
19. The computing device of claim 18, wherein the instructions are further executable to directly supervise the training using the reference image of the object, the reference depth map, and the new depth map.
20. A computing device comprising: a processor; and a storage device storing instructions executable by the processor to receive a reference image of an object corresponding to an original viewpoint; translate the reference image of the object into a reference depth map of the object; synthesize a new depth map of the object corresponding to a new viewpoint; map the reference image of the object to an appearance parameter; map the new depth map of the object to a shape parameter; and combine the shape parameter and the appearance parameter to generate a new image of the object from the new viewpoint.
</claims>
</document>
