<document>

<filing_date>
2019-10-14
</filing_date>

<publication_date>
2020-02-06
</publication_date>

<priority_date>
2015-09-25
</priority_date>

<ipc_classes>
G06F3/01,G06F3/03,G06F3/16,G06K9/00
</ipc_classes>

<assignee>
APPLE
</assignee>

<inventors>
CHEN, CHONG
GERNOTH, THORSTEN
GUO, HAITAO
SHI, XIAOJIN
TANG, FENG
</inventors>

<docdb_family_id>
58409154
</docdb_family_id>

<title>
Multi Media Computing Or Entertainment System For Responding To User Presence And Activity
</title>

<abstract>
Intelligent systems are disclosed that respond to user intent and desires based upon activity that may or may not be expressly directed at the intelligent system. In some embodiments, the intelligent system acquires a depth image of a scene surrounding the system. A scene geometry may be extracted from the depth image and elements of the scene may be monitored. In certain embodiments, user activity in the scene is monitored and analyzed to infer user desires or intent with respect to the system. The interpretation of the user's intent as well as the system's response may be affected by the scene geometry surrounding the user and/or the system. In some embodiments, techniques and systems are disclosed for interpreting express user communication, e.g., expressed through hand gesture movements. In some embodiments, such gesture movements may be interpreted based on real-time depth information obtained from, e.g., optical or non-optical type depth sensors.
</abstract>

<claims>
1. A non-transitory program storage device, readable by a processor and comprising instructions stored thereon to cause one or more processors to: acquire a depth image of a scene in a vicinity of a first device; store the depth image in a memory; develop a scene geometry based upon the depth image; monitor the activity of one or more humans present in the scene geometry, wherein one of the one or more humans comprises a user of the first device; determine whether the user is engaged in conversation with at least one of the one or more humans; and in response to a determination that the user is engaged in conversation with the at least one of the one or more humans, adjust an output of the first device based, at least in part, upon a characteristic of the determined conversation.
2. The non-transitory program storage device of claim 1, wherein the output of the first device comprises an audio output.
3. The non-transitory program storage device of claim 1, wherein the characteristic of the determined conversation comprises an indication that the determined conversation is at least one of: a phone conversation, a conversation wherein the user is not facing the first device, a conversation wherein the user is facing the first device, or a conversation wherein the user is whispering.
4. The non-transitory program storage device of claim 1, wherein the instructions stored thereon to adjust an output of the first device further cause the one or more processors to perform at least one of the following actions: turn down the volume of the first device, pause media being rendered by the first device, or cause the first device to enter a power-saving mode.
5. The non-transitory program storage device of claim 1, wherein the instructions stored thereon further cause the one or more processors to: acquire an image of the scene; detect a human face in the acquired image corresponding to the user; employ the depth image to detect a head orientation associated with the detected human face; and correlate the detected human face with the head orientation.
6. The non-transitory program storage device of claim 5, wherein the instructions stored thereon further cause the one or more processors to: determine whether the user is engaging in conversation with at least one of the one or more humans based, at least in part, upon the correlated detected human face and the head orientation.
7. The non-transitory program storage device of claim 1, wherein the instructions stored thereon further cause the one or more processors to: identify the user of the first device; and determine preferences associated with the user, wherein the determination of whether the user is engaged in conversation with at least one of the one or more humans is further based, at least in part, upon the determined preferences.
8. A method, comprising: acquiring a depth image of a scene in a vicinity of a first device; storing the depth image in a memory; developing a scene geometry based upon the depth image; monitoring the activity of one or more humans present in the scene geometry, wherein one of the one or more humans comprises a user of the first device; determining whether the user is engaged in conversation with at least one of the one or more humans; and in response to a determination that the user is engaged in conversation with the at least one of the one or more humans, adjusting an output of the first device based, at least in part, upon a characteristic of the determined conversation.
9. The method of claim 8, wherein the output of the first device comprises an audio output.
10. The method of claim 8, wherein the characteristic of the determined conversation comprises an indication that the determined conversation is at least one of: a phone conversation, a conversation wherein the user is not facing the first device, a conversation wherein the user is facing the first device, or a conversation wherein the user is whispering.
11. The method of claim 8, wherein adjusting an output of the first device further comprises performing at least one of the following actions: turning down the volume of the first device, pausing media being rendered by the first device, or causing the first device to enter a power-saving mode.
12. The method of claim 8, further comprising: acquiring an image of the scene; detecting a human face in the acquired image corresponding to the user; employing the depth image to detect a head orientation associated with the detected human face; correlating the detected human face with the head orientation; and determining whether the user is engaging in conversation with at least one of the one or more humans based, at least in part, upon the correlated detected human face and the head orientation.
13. The method of claim 8, further comprising: identifying the user of the first device; and determining preferences associated with the user, wherein the determination of whether the user is engaged in conversation with at least one of the one or more humans is further based, at least in part, upon the determined preferences.
14. An electronic device comprising: a memory; a depth sensor; one or more processors, communicatively coupled to the memory, wherein the memory stores instructions to cause the one or more processors to: acquire, from the depth sensor, a depth image of a scene in a vicinity of the electronic device; store the depth image in the memory; develop a scene geometry based upon the depth image; monitor the activity of one or more humans present in the scene geometry, wherein one of the one or more humans comprises a user of the electronic device; determine whether the user is engaged in conversation with at least one of the one or more humans; and; in response to a determination that the user is engaged in conversation with the at least one of the one or more humans, adjust an output of the electronic device based, at least in part, upon a characteristic of the determined conversation.
15. The electronic device of claim 14, wherein the output of the electronic device comprises an audio output.
16. The electronic device of claim 14, wherein the characteristic of the determined conversation comprises an indication that the determined conversation is at least one of: a phone conversation, a conversation wherein the user is not facing the electronic device, a conversation wherein the user is facing the electronic device, or a conversation wherein the user is whispering.
17. The electronic device of claim 14, wherein the instructions stored in the memory to adjust an output of the electronic device further cause the one or more processors to perform at least one of the following actions: turn down the volume of the electronic device, pause media being rendered by the electronic device, or cause the electronic device to enter a power-saving mode.
18. The electronic device of claim 14, wherein the instructions stored in the memory further cause the one or more processors to: acquire an image of the scene; detect a human face in the acquired image corresponding to the user; employ the depth image to detect a head orientation associated with the detected human face; and correlate the detected human face with the head orientation.
19. The electronic device of claim 18, wherein the instructions stored in the memory further cause the one or more processors to: determine whether the user is engaging in conversation with at least one of the one or more humans based, at least in part, upon the correlated detected human face and the head orientation.
20. The electronic device of claim 14, wherein the instructions stored in the memory further cause the one or more processors to: identify the user of the first device; and; determine preferences associated with the user, wherein the determination of whether the user is engaged in conversation with at least one of the one or more humans is further based, at least in part, upon the determined preferences.
</claims>
</document>
