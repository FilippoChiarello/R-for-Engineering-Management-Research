<document>

<filing_date>
2019-11-12
</filing_date>

<publication_date>
2020-06-18
</publication_date>

<priority_date>
2018-12-14
</priority_date>

<ipc_classes>
G06F16/28,G06K9/00
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
WANG QIANG
ZHANG CHAO
LEE, HYONG EUK
</inventors>

<docdb_family_id>
71072671
</docdb_family_id>

<title>
METHOD AND APPARATUS FOR DETERMINING TARGET OBJECT IN IMAGE BASED ON INTERACTIVE INPUT
</title>

<abstract>
Provided are methods and apparatuses for determining a target object in an image based on an interactive input. A target object determining method acquires first feature information corresponding to an image and second feature information corresponding to an interactive input; and determines a target object corresponding to the interactive input from among objects in the image based on the first feature information and the second feature information.
</abstract>

<claims>
1. A method of determining a target object in an image based on an interactive input, the method comprising: acquiring first feature information corresponding to an image and second feature information corresponding to an interactive input; and determining a target object corresponding to the interactive input from among objects in the image based on the first feature information and the second feature information.
2. The method of claim 1, wherein the acquiring of the first feature information and the second feature information comprises: acquiring semantic feature information between each of the objects included in the image and at least one another object included in the image.
3. The method of claim 2, wherein the acquiring of the semantic feature information comprises: acquiring the semantic feature information between each of the objects included in the image and the at least one another object included in the image based on position information of each of the objects.
4. The method of claim 2, wherein the acquiring of the semantic feature information comprises: determining at least one region proposal based on each of the objects included in the image and the at least one another object included in the image; acquiring classification feature information of an object in the region proposal; acquiring region semantic feature information between objects in the region proposal; and generating the semantic feature information between each of the objects included in the image and the at least one another object included in the image based on the classification feature information and the region semantic feature information.
5. The method of claim 4, prior to the generating of the semantic feature information, further comprising: performing a joint correction with respect to the classification feature information and the region semantic feature information based on the classification feature information and the region semantic feature information.
6. The method of claim 4, prior to the generating of the semantic feature information, further comprising: determining a reference region based on the region proposal; acquiring region feature information of the reference region; and performing a joint correction with respect to the classification feature information, the region semantic feature information, and the region feature information based on the classification feature information, the region semantic feature information, and the region feature information.
7. The method of claim 4, wherein the region proposal comprises one of the objects included in the image and one of the at least one another object included in the image.
8. The method of claim 1, wherein the first feature information comprises at least one of global visual feature information corresponding to the image, visual feature information corresponding to each of the objects included in the image, relative position information between the objects included in the image, relative size feature information between the objects included in the image, and semantic feature information between the objects included in the image.
9. The method of claim 8, wherein the determining of the target object comprises: performing a fusion processing on the first feature information prior to determining the target object.
10. The method of claim 1, further comprising: acquiring training data comprising a sample image; determining at least one region proposal based on each of objects included in the sample image and at least one another object included in the sample image; determining a reference region based on the at least one region proposal and acquiring region feature information of the reference region; generating a region title based on the region feature information; and training a neural network model for acquiring semantic feature information between the objects included in the image based on training data supervised with the region title.
11. The method of claim 1, wherein the acquiring of the first feature information and the second feature information comprises: performing a word vector conversion with respect to the interactive input; and acquiring the second feature information corresponding to the interactive input based on a word vector.
12. The method of claim 11, wherein the acquiring of the first feature information and the second feature information further comprises determining whether a word of the interactive input belongs to a first word prior to performing the word vector conversion with respect to the interactive input, and the performing of the word vector conversion comprises using a word vector of a second word having a relatively high similarity with a word vector of the first word as a word vector corresponding to the first word, in response to the word of the interactive input being determined to belong to the first word.
13. The method of claim 12, wherein the first word represents a word having a use frequency less than a first setting value, and the second word represents a word having a use frequency greater than a second setting value.
14. The method of claim 1, wherein the interactive input comprises at least one of a voice input or a text input.
15. A non-transitory computer-readable recording medium storing instructions, that when executed by a processor, causes the processor to perform the method of claim 1.
16. An apparatus for determining a target object in an image based on an interactive input, the apparatus comprising: a feature acquirer configured to acquire first feature information corresponding to an image and second feature information corresponding to an interactive input; and a target object determiner configured to determine a target object corresponding to the interactive input from among objects included in the image based on the first feature information and the second feature information.
17. The apparatus of claim 16, wherein the first feature information comprises at least one of global visual feature information corresponding to the image, visual feature information corresponding to each of the objects included in the image, relative position information between the objects included in the image, relative size feature information between the objects included in the image, or semantic feature information between the objects included in the image.
18. The apparatus of claim 16, wherein the feature acquirer is further configured to determine at least one region proposal based on each of the objects included in the image and at least one another object included in the image, acquire classification feature information of an object in the region proposal, acquire region semantic feature information between objects in the region proposal, determine a reference region based on the region proposal, acquire region feature information of the reference region, perform a joint correction with respect to the classification feature information, the region semantic feature information, and the region feature information based on the classification feature information, the region semantic feature information, and the region feature information, and generate the semantic feature information between each of the objects included in the image and the at least one another object included in the image based on the corrected classification feature information, the corrected region semantic feature information, and the corrected region feature information.
19. The apparatus of claim 16, wherein the feature acquirer is further configured to perform a word vector conversion with respect to the interactive input, and acquire the second feature information corresponding to the interactive input based on a word vector.
20. The apparatus of claim 16, wherein the feature acquirer is further configured to determine whether a word of the interactive input belongs to a first word when performing the word vector conversion with respect to the interactive input, and use a word vector of a second word having a relatively high similarity with a word vector of the first word as a word vector corresponding to the first word, in response to the word of the interactive input being determined to belong to the first word, wherein the first word represents a word having a use frequency less than a first setting value, and the second word represents a word having a use frequency greater than a second setting value.
</claims>
</document>
