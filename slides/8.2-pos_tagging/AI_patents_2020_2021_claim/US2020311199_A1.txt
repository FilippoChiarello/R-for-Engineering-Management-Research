<document>

<filing_date>
2019-03-29
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2019-03-29
</priority_date>

<ipc_classes>
G06F40/247,G06F40/284,G06F40/30,G10L15/19
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
CHAI, Junyi
Li, Bing
YAN, Rui
DENG, Yonggang
GUAN, Maochen
HE, Yujie
</inventors>

<docdb_family_id>
69726840
</docdb_family_id>

<title>
Ontology entity type detection from tokenized utterance
</title>

<abstract>
A server computing device, including memory storing a knowledge graph including a plurality of ontology entities. The server computing device may further include a processor configured to receive a tokenized utterance including a plurality of words and one or more metadata tokens. The processor may extract a respective word embedding vector from each word included in the tokenized utterance. Based on a glossary file, the processor may determine a respective ontology entity type of each word included in the tokenized utterance. The processor may extract a character embedding vector from each character included in the tokenized utterance. Based on the plurality of word embedding vectors, the plurality of respective ontology entity types of the words, and the plurality of character embedding vectors, the processor may determine a predefined intention of the tokenized utterance using at least one recurrent neural network. The predefined intention may indicate a target ontology entity type.
</abstract>

<claims>
1. A server computing device comprising: memory storing a knowledge graph including a plurality of ontology entities connected by a plurality of edges and having a respective plurality of ontology entity types; a processor configured to: receive a tokenized utterance including a plurality of words and one or more metadata tokens, wherein each word includes one or more characters; based on the tokenized utterance, at an intention detector: extract a respective word embedding vector from each word included in the tokenized utterance; based on a glossary file, determine a respective ontology entity type of each word included in the tokenized utterance; extract a character embedding vector from each character included in the tokenized utterance; based on the plurality of word embedding vectors, the plurality of respective ontology entity types of the words, and the plurality of character embedding vectors, determine a predefined intention of the tokenized utterance using at least one recurrent neural network, wherein the predefined intention indicates a target ontology entity type selected from the plurality of ontology entity types of the plurality of ontology entities included in the knowledge graph.
2. The server computing device of claim 1, wherein the processor is further configured to apply a 1-D convolution to each character embedding vector.
3. The server computing device of claim 2, wherein: the processor is further configured to apply max pooling to an output of the 1-D convolution; and the at least one recurrent neural network is configured to receive an output of the max pooling.
4. The server computing device of claim 1, wherein the at least one recurrent neural network includes a first bidirectional long short-term memory (LSTM) network and a second bidirectional LSTM network.
5. The server computing device of claim 4, wherein the processor is further configured to perform batch normalization on an output of the second bidirectional LSTM network.
6. The server computing device of claim 5, wherein the processor is further configured to apply a softmax function to an output of the batch normalization.
7. The server computing device of claim 1, wherein the glossary file is a summary of the knowledge graph.
8. A server computing device comprising: a processor configured to: receive a tokenized utterance including a plurality of words and one or more metadata tokens, wherein each word includes one or more characters; based on the tokenized utterance, at an entity mention detector: extract a respective word embedding vector from each word included in the tokenized utterance; based on a glossary file, determine an ontology entity type of each word included in the tokenized utterance; extract a character embedding vector from each character included in the tokenized utterance; based on the plurality of word embedding vectors, the plurality of ontology entity types, and the plurality of character embedding vectors, determine at least one input ontology entity token of the tokenized utterance using at least one recurrent neural network, wherein the at least one input ontology entity token includes a vector indicating one or more input ontology entity types of the tokenized utterance.
9. The server computing device of claim 8, wherein the processor is further configured to apply a 1-D convolution to each character embedding vector.
10. The server computing device of claim 9, wherein: the processor is further configured to apply maximum pooling to an output of the 1-D convolution; and the at least one recurrent neural network is configured to receive an output of the maximum pooling.
11. The server computing device of claim 8, wherein the at least one recurrent neural network includes a first bidirectional long short-term memory (LSTM) network and a second bidirectional LSTM network.
12. The server computing device of claim 11, wherein the processor is further configured to perform batch normalization on an output of the second bidirectional LSTM network.
13. The server computing device of claim 12, wherein the processor is further configured to apply a conditional random field algorithm to an output of the batch normalization.
14. The server computing device of claim 8, wherein the glossary file is a summary of a knowledge graph that includes a plurality of ontology entities connected by a plurality of edges.
15. A server computing device comprising: a processor configured to: at a relation mention detector: receive, from an entity mention detector, an input ontology entity token of a tokenized utterance; receive, from an intention detector, a predefined intention of the tokenized utterance; receive, from a preprocessor, a parse tree that indicates a word dependency structure of the tokenized utterance; identify at least one semantic relation and/or literal relation between the predefined intention and the input ontology entity token based at least in part on the parse tree; and generate a query backbone from the input ontology entity token, the predefined intention, and the semantic relation and/or literal relation; and generate a structured query based on the query backbone.
16. The server computing device of claim 15, further comprising memory storing a knowledge graph including a plurality of ontology entities connected by a plurality of edges and having a respective plurality of ontology entity types.
17. The server computing device of claim 16, wherein the processor is further configured to query the knowledge graph for an output ontology entity using the structured query.
18. The server computing device of claim 15, wherein: at least one of the input ontology entity token and the predefined intention includes a respective literal; and the processor is further configured to detect a literal relation between the input ontology entity token and the predefined intention.
19. The server computing device of claim 18, wherein each literal is selected from the group consisting of a date, a time, a string, and a numerical value.
20. The server computing device of claim 15, wherein the query backbone includes a subject, a predicate, and a direct object.
</claims>
</document>
