<document>

<filing_date>
2020-04-22
</filing_date>

<publication_date>
2020-10-29
</publication_date>

<priority_date>
2019-04-23
</priority_date>

<ipc_classes>
G06N3/08,G06Q30/06,G06T11/00,G06T7/90
</ipc_classes>

<assignee>
L'OREAL
</assignee>

<inventors>
AARABI, PARHAM
ELMOZNINO, ERIC
ZHANG, Yuze
</inventors>

<docdb_family_id>
70476188
</docdb_family_id>

<title>
MACHINE IMAGE COLOUR EXTRACTION AND MACHINE IMAGE CONSTRUCTION USING AN EXTRACTED COLOUR
</title>

<abstract>
Provided are systems and methods to perform colour extraction from swatch images and to define new images using extracted colours. Source images may be classified using a deep learning net (e.g. a CNN) to indicate colour representation strength and drive colour extraction. A clustering classifier is trained to use feature vectors extracted by the net. Separately, pixel clustering is useful when extracting the colour. Cluster count can vary according to classification. In another manner, heuristics (with or without classification) are useful when extracting. Resultant clusters are evaluated against a set of (ordered) expected colours to determine a match. Instances of standardized swatch images may be defined from a template swatch image and respective extracted colours using image processing. The extracted colour may be presented in an augmented reality GUI such as a virtual try-on application and applied to a user image such as a selfie using image processing.
</abstract>

<claims>
What we claim is:
1. A method comprising: classifying a source colour image showing a colour of an item of interest to indicate a strength of a representation of the colour in the source colour image, the classifying using a deep learning network model to extract image features with which to determine the strength; extracting colour information from the source colour image, responsive to the classifying, by clustering pixels of the source colour image in accordance with respective colours of the pixels and identifying at least one candidate colour as the extracted colour in response to the clustering of the pixels; and providing the extracted colour to define a new image using image processing.
2. The method of claim 1, wherein classifying identifies the source colour image as strongly representing the colour and wherein extracting colour information responsively produces one candidate colour from a dominant colour identified by the clustering of pixels.
3. The method of claim 1, wherein classifying identifies the source colour image as weakly representing the colour and wherein extracting colour information responsively produces a plurality of candidate colours identified by the clustering of pixels.
4. The method of claim 1 wherein classifying uses a cluster-based classification responsive to image feature vectors extracted by the deep learning network model.
5. The method of claim 1, wherein clustering pixels defines a number of clusters k from which to determine the at least one candidate colour, which number k varies in response to the classifying.
6. The method of claim 1, wherein item of interest is a product and the source colour image comprises a product swatch image advertising the product.
7. The method of claim 1, wherein classifying performs a cluster-based classification that: measures an image feature vector for the source colour image produced by the deep learning network model against a plurality of image feature clusters; and provides, as a classification of the source colour image, a respective class associated with one of the plurality of image feature clusters that is nearest to the image feature vector for the source colour image, the respective class indicating the strength of representation of colour.
8. The method of claim 7, wherein the plurality of image feature clusters are generated from training data comprising training images showing colour processed by the deep neural network model to produce image features vectors with which to define the plurality of image feature clusters and wherein each of the training images is respectively assigned to one of the respective classes indicating the strength of representation of colour.
9. The method of claim 1, wherein the network model comprises a convolutional neural network (CNN) pretrained to process images of objects to define feature vectors for the images of objects.
10. The method of claim 1 comprising acquiring a starting image with which to define the new image and defining the new image from the starting image using the extracted colour to apply to an object for display within the new image.
11. The method of claim 10, wherein the starting image comprises a first selfie image, the new image comprises a new selfie image including the object and the object comprises a product for simulation on the first selfie image.
12. The method of claim 11, wherein the product comprises a makeup product to virtually try on using augmented reality and the source colour image comprises a swatch image of a makeup product.
13. The method of claim 12 comprising presenting the new image in a graphical user interface (GUI) providing a makeup product to virtually try on using an augmented reality experience, the GUI further configured to: receive a selfie image of a user and a selection of the new image and apply the extracted colour to a portion of the selfie image to virtually try on the makeup product as represented by the new image.
14. A computing machine comprising a processor coupled to a storage device, the storage device storing instructions, which when executed by the processor, configure the computing machine to: classify a source colour image showing a colour of an item of interest to indicate a strength of a representation of the colour in the source colour image, wherein to classify uses a deep learning network model to extract image features with which to determine the strength and provide a classification; extract colour information from the source colour image, responsive to the classification, by clustering pixels of the source colour image in accordance with respective colours of the pixels and identify at least one candidate colour as the extracted colour in response to the clustering of the pixels; and provide the extracted colour to define a new image using image processing.
15. A computing machine comprising: a processor coupled to a storage device, the storage device storing instructions, which when executed by the processor, configure the computing machine to: obtain a source colour image comprising pixels, the source colour image having a region with a colour to be extracted; cluster the pixels without regard to intensity into a plurality of clusters each cluster of the plurality characterized by a respective characterizing colour; evaluate the plurality of clusters against a set of colour expectations comprising respective expected colours to be present in the image, using the expected colours to search the plurality of clusters for a colour match to define the extracted colour; and provide the extracted colour to define a new image using image processing.
16. The computing machine of claim 15 wherein to cluster the pixels comprises applying k-means clustering to the pixels without regard to intensity.
17. The computing machine of claim 15 wherein to search for the colour match comprises determining a similarity of the respective characterizing colour of one of the plurality of clusters with a respective expected colour of one of the set of colour expectations.
18. The computing machine of claim 17 wherein the search locates the colour match when the respective characterizing colour is similar to the respective expected colour in both a brightness and a direction in accordance with respective threshold requirements for the respective expected colour.
19. The computing machine of claim 15 wherein the instructions configure the computing machine to define the set of colour expectations as a ordered set in accordance with a priority order and wherein to evaluate searches the set of colour expectations using the plurality of clusters in accordance with the priority order, stopping when the colour match is located.
20. The computing machine of claim 15 wherein the instructions configure the computing machine to acquire a starting image with which to define the new image and defining the new image from the starting image using the extracted colour to apply to an object for display within the new image; wherein the starting image comprises a first selfie image and the new image comprises a new selfie image including the object and wherein the object comprises a product for simulation on the first selfie image.
21. The computing machine of claim 20 wherein the computing machine comprises a mobile device and wherein the product comprises a makeup product to virtually try on using augmented reality.
22. A method comprising: by a processor of a computing device: obtaining a source colour image comprising pixels, the source colour image having a region with a colour to be extracted; clustering the pixels without regard to intensity into a plurality of clusters each cluster of the plurality characterized by a respective characterizing colour; evaluating the plurality of clusters against a set of colour expectations comprising respective expected colours to be present in the image, using the expected colours to search the plurality of clusters for a colour match to define the extracted colour; and providing the extracted colour to define a new image using image processing.
23. The method of claim 22 wherein the pixels are defined in a first colour space and wherein the method comprises converting the pixels to a second colour space that is intensity independent.
24. The method of claim 22 comprising defining the set of colour expectations as a ordered set in accordance with a priority order and wherein to evaluate comprises searching the set of colour expectations using the plurality of clusters in accordance with the priority order, stopping when the colour match is located.
25. The method of claim 22 comprising combining two or more clusters of the k clusters into a combined cluster if each of the two or more provides a colour match in response to a search; and using the combined cluster to define the extracted colour.
26. The method of claim 22 comprising: acquiring a starting image with which to define the new image and defining the new image from the starting image using the extracted colour to apply to an object for display within the new image; and wherein: the starting image comprises a first selfie image and the new image comprises a new selfie image including the object; the object comprises a product for simulation on the first selfie image; and the product comprises a makeup product to virtually try on using augmented reality.
27. The method of claim 22 wherein the source colour image comprises a swatch image of a makeup product.
28. The method of claim 22 comprising presenting the new image in a graphical user interface (GUI) providing a makeup product to virtually try on using an augmented reality experience, the GUI further configured to: receive a selfie image of a user and a selection of the new image and apply the extracted colour of the new image to a portion of the selfie image to virtually try on the makeup product as represented by the new image.
</claims>
</document>
