<document>

<filing_date>
2020-03-09
</filing_date>

<publication_date>
2020-09-24
</publication_date>

<priority_date>
2019-03-15
</priority_date>

<ipc_classes>
G06K9/00,G06N3/08,G06T7/11
</ipc_classes>

<assignee>
NVIDIA CORPORATION
</assignee>

<inventors>
MYRONENKO, ANDRIY
ROTH, HOLGER REINHARD
WANG XIAOSONG
XU, DAGUANG
XU, ZIYUE
YANG DONG
ZHANG LING
</inventors>

<docdb_family_id>
70190122
</docdb_family_id>

<title>
TECHNIQUES TO TRAIN A NEURAL NETWORK USING TRANSFORMATIONS
</title>

<abstract>
Apparatuses, systems, and techniques to perform training of neural networks using stacked transformed images. In at least one embodiment, a neural network is trained on stacked transformed images and trained neural network is provided to be used for processing images from an unseen domain distinct from a source domain, wherein stacked transformed images are transformed according to transformation aspects related to domain variations.
</abstract>

<claims>
WHAT IS CLAIMED IS:
1. A processor comprising:
one or more circuits to help train a first one or more neural networks on a first set of images using one or more graphics processing units to identify one or more objects within one or more images of a second set of images, wherein the first set of images are images from a first domain, wherein the second set of images are images from a second domain, and wherein the first set of images are transformed prior to training based on expected differences between the first domain and the second domain.
2. The processor of claim 1, further comprising:
first storage to store the first set of images;
a transformer to transform a first image of the first set of images according to an image aspect, to form a first transformed image; and
second storage to the first transformed image, for use in training the first one or more neural networks.
3. The processor of claim 2, wherein the image aspect comprises one or more of a quality aspect, an appearance aspect, or a spatial configuration aspect.
4. The processor of claim 3, wherein the transformer includes logic for selecting an image aspect value for the image aspect among a range of aspect values, to be used for transforming the first image according to the image aspect and the image aspect value.
5. The processor of claim 2, further comprising segmentation storage for storing segmentation data of the first image.
6. The processor of claim 5, wherein the image aspect comprises a spatial configuration aspect and wherein the transformer modifies the first image according to spatial configuration aspect parameters and modifies the segmentation data of the first image according to the spatial configuration aspect parameters.
7. The processor of claim 2, wherein the image aspect comprises a spatial configuration aspect and wherein the first image is a volume image, the processor further comprising:
an image cropper, to crop the first image into sub-volume images, wherein sub-volume images are processed separately.
8. The processor of claim 7, wherein the image cropper is a cropper that interpolates within a minimal cuboid containing a 3D coordinate grid.
9. A processor comprising:
a trained neural network using one or more graphics processing units to identify one or more objects within one or more images of a second set of images, wherein the trained neural network is a neural network trained on a first set of images, wherein the first set of images are images from a first domain, wherein the second set of images are images from a second domain, and wherein the first set of images are transformed prior to training the trained neural network based on expected differences between the first domain and the second domain.
10. The processor of claim 9, further comprising:
storage for domain difference data representing the expected differences between the first domain and the second domain; and
an input of the trained neural network for receiving the domain difference data to use in an image processing process.
11. The processor of claim 10, wherein the expected differences between the first domain and the second domain correspond to one or more of a quality aspect, an appearance aspect, or a spatial configuration aspect.
12. The processor of claim 9, wherein the second set of images comprises medical images.
13. The processor of claim 12, wherein the first set of images are images obtained using a first medical device and the second set of images are images obtained using a second medical device different from the first medical device.
14. The processor of claim 9, wherein the first set of images comprises volumetric images.
15. A method, using one or more graphics processing units, of processing images, comprising:
training a first neural network with a first set of images and outputs to help a trained neural network infer outputs from an input image of a second set of images, wherein the first set of images are images from a first domain, wherein the second set of images are images from a second domain, and wherein the first set of images are transformed prior to training based on expected differences between the first domain and the second domain.
16. The method of claim 15, wherein training the first neural network comprises:
obtaining the first set of images, comprising at least a first image;
obtaining a segmentation of the first image, wherein the segmentation represents boundaries of objects depicted in the first image;
determining a transform aspect parameter, wherein the transform aspect parameter corresponds to at least one of the expected differences between the first domain and the second domain;
determining a transform aspect parameter value;
transforming the first image based on the transform aspect parameter value to form a transformed first image;
training the first neural network with the transformed first image.
17. The method of claim 16, further comprising:
determining whether the first image can be transformed as a whole using a memory; and
cropping the first image into a plurality of sub-volumes for loading into the memory separately.
18. The method of claim 16, further comprising generating a plurality of transformed images from the first image, using a plurality of transform aspect parameters.
19. The method of claim 18, wherein the plurality of transform aspect parameters comprise a quality aspect, an appearance aspect, and/or a spatial configuration aspect.
20. The method of claim 16, wherein the transform aspect parameter comprises a spatial configuration aspect parameter, the method further comprising:
modifying the first image according to the spatial configuration aspect parameter; and
modifying the segmentation of the first image according to the spatial configuration aspect parameter.
21. The method of claim 20, wherein modifying the first image according to the spatial configuration aspect parameter comprises cropping sub-volumes of the first image randomly for loading into a memory to apply the transform aspect parameter value to the first image.
22. The method of claim 16, further comprising training the first neural network over a plurality of training epochs, using a distinct transform aspect parameter for each of the plurality of training epochs.
23. A method, using one or more graphics processing units, of processing images, comprising:
identifying one or more objects within one or more images of a second set of images using a trained neural network, wherein the trained neural network is a neural network trained on a first set of images, wherein the first set of images are images from a first domain, wherein the second set of images are images from a second domain, and wherein the first set of images are transformed prior to training the trained neural network based on expected differences between the first domain and the second domain.
24. The method of claim 23, further comprising:
determining a domain difference representing the expected differences between the first domain and the second domain;
providing the domain difference as an input to the trained neural network; and using the domain difference in an image processing process.
25. The method of claim 23, wherein the second set of images comprises medical images.
26. The method of claim 25, wherein the first set of images are images obtained using a first medical device and the second set of images are images obtained using a second medical device different from the first medical device.
27. The method of claim 23, wherein the first set of images comprises volumetric images.
28. The method of claim 23, wherein the expected differences between the first domain and the second domain correspond to one or more of a quality aspect, an appearance aspect, or a spatial configuration aspect.
</claims>
</document>
