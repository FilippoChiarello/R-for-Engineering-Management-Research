<document>

<filing_date>
2016-12-20
</filing_date>

<publication_date>
2020-02-18
</publication_date>

<priority_date>
2016-02-04
</priority_date>

<ipc_classes>
G06F17/11,G06N3/04,G06N3/08,G06N7/00
</ipc_classes>

<assignee>
NEC CORPORATION
NEC LABORATORIES AMERICA
</assignee>

<inventors>
SOHN, KIHYUK
</inventors>

<docdb_family_id>
59497846
</docdb_family_id>

<title>
Distance metric learning with N-pair loss
</title>

<abstract>
A method includes receiving N pairs of training examples and class labels therefor. Each pair includes a respective anchor example, and a respective non-anchor example capable of being a positive or a negative training example. The method further includes extracting features of the pairs by applying a DHCNN, and calculating, for each pair based on the features, a respective similarly measure between the respective anchor and no example. The method additionally includes calculating a similarity score based on the respective similarity measure for each pair. The score represents similarities between all anchor points and positive training examples in the pairs relative to similarities between all anchor points and negative training examples in the pairs. The method further includes maximizing the similarity score for the anchor example for each pair to pull together the training examples from a same class while pushing apart the training examples from different classes.
</abstract>

<claims>
1. A computer-implemented method, comprising: receiving, by a processor, N pairs of training examples and class labels for the training examples that correspond to a plurality of classes, wherein each of the N pairs includes a respective anchor example and further includes a respective non-anchor example capable of being a positive training example or a negative training example; extracting, by the processor, features of the N pairs by applying a deep convolutional neural network to the N pairs and to the class labels; calculating, by the processor for each of the N pairs based on the features, a respective similarly measure between the respective anchor example and the respective non-anchor example; calculating, by the processor, a similarity score based on the respective similarity measure for each of the N pairs, the similarity score representing one or more similarities between all anchor points and all positive training examples in the N pairs relative to one or more similarities between all of the anchor points and all negative training examples in the N pairs; and maximizing, by the processor, the similarity score for the respective anchor example for each of the N pairs to pull together in a distribution space the training examples from a same one of the plurality of classes while pushing apart in the distribution space the training examples from different ones of the plurality of classes; and verifying a user and providing the user access to an entity, based on a prediction generated using the deep convolutional neural network.
2. The computer-implemented method of claim 1, wherein each of the N pairs of training examples corresponds to a different one of the plurality of classes.
3. The computer-implemented method of claim 2, wherein the plurality of classes are randomly selected as a subset from a set of classes, and wherein the set of classes includes the plurality of classes and one or more other classes.
4. The computer-implemented method of claim 1, wherein said maximizing step is capable of simultaneously pushing N-1 examples away from a single reference sample from among the N pairs of training examples, in the distribution space.
5. The computer-implemented method of claim 1, wherein said maximizing step is capable of simultaneously pushing N-1 examples towards a single reference sample from among the N pairs of training examples, in the distribution space.
6. The computer-implemented method of claim 1, wherein the deep convolutional neural network is configured to include embedding vectors that are trained to satisfy a set of constraints on each loss function in a set of loss functions, wherein the deep convolutional neural network is trained using the set of loss functions.
7. The computer-implemented method of claim 1, wherein said maximizing step comprising computing a gradient of a logarithm of the similarity score.
8. The computer-implemented method of claim 1, wherein said maximizing step maximizes an objective function for deep metric learning.
9. The computer-implemented method of claim 1, wherein a total number of the plurality of classes at least one of (i) changes over time, (ii) is larger than a threshold amount, and (iii) is unknown.
</claims>
</document>
