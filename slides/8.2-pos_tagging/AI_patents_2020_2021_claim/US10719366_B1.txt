<document>

<filing_date>
2017-07-24
</filing_date>

<publication_date>
2020-07-21
</publication_date>

<priority_date>
2017-07-24
</priority_date>

<ipc_classes>
G06F9/445,G06F9/50
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
</assignee>

<inventors>
GASSER, TIMOTHY DAVID
</inventors>

<docdb_family_id>
71611814
</docdb_family_id>

<title>
Dynamic and selective hardware acceleration
</title>

<abstract>
Methods, systems, and computer-readable media for dynamic and selective hardware acceleration are disclosed. An indirection layer receives a plurality of computation calls generated by an application. The application and the indirection layer are executed by at least one processor of a computing device, and the application links to the indirection layer. The indirection layer determines whether to execute individual computation calls using the at least one processor or using the hardware accelerator. The indirection layer sends one or more of the computation calls and associated data to the hardware accelerator. The one or more of the computation calls are executed using the hardware accelerator, and one or more results are returned from the hardware accelerator to the indirection layer.
</abstract>

<claims>
1. A system, comprising: at least one processor and a memory storing program instructions executable by the at least one processor, wherein the at least one processor and the memory are hosted in a computing device in a multi-tenant provider network, wherein the program instructions comprise an application and an indirection layer, wherein the application is executable to generate computation calls, and wherein the application links to the indirection layer; and a hardware accelerator in the multi-tenant provider network; and wherein the indirection layer is executable by the at least one processor to: determine whether to execute individual ones of the computation calls using the at least one processor or using the hardware accelerator; wherein for a particular computation call of the computation calls, the indirection layer is configured to: send the particular computation call to be separately executed by both the at least one processor and the hardware accelerator; and compare a first result of execution of the particular computation call by the hardware accelerator to a second result of execution of the particular computation call by the at least one processor; and send one or more other ones of the computation calls and associated data to the hardware accelerator, wherein the one or more other ones of the computation calls are executed by the hardware accelerator using the associated data, and wherein one or more results of the one or more other ones of the computation calls are returned from the hardware accelerator to the indirection layer.
2. The system as recited in claim 1, wherein the hardware accelerator comprises a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC).
3. The system as recited in claim 1, wherein, in determining whether to execute the individual ones of the computation calls using the at least one processor or using the hardware accelerator, the indirection layer is executable by the at least one processor to perform the determining based at least in part on a size of the associated data compared to a threshold size for the selective utilization of the hardware accelerator.
4. The system as recited in claim 1, wherein the indirection layer is executable by the at least one processor to: initiate execution of the one or more of the computation calls using the at least one processor; and compare the one or more results of the one or more of the computation calls from the hardware accelerator to one or more results of the one or more of the computation calls from the at least one processor.
5. A computer-implemented method, comprising: receiving, at a library, a plurality of calls generated by an application, wherein the application and the library are executed by at least one processor of a computing device, and wherein the application links to the library; using the library, determining whether to execute individual ones of the calls using the at least one processor or using one or more hardware accelerators; executing a particular call of the calls on both the at least one processor and on the one or more hardware accelerators; comparing a first result of executing the particular call on the one or more hardware accelerators to a second result of executing the particular call on the at least one processor; and sending one or more other ones of the calls and associated data to the one or more hardware accelerators, wherein the one or more other ones of the calls are executed by the one or more hardware accelerators using the associated data, and wherein one or more results of the one or more other ones of the calls are returned from the one or more hardware accelerators to the library.
6. The method as recited in claim 5, wherein the one or more hardware accelerators comprise a field-programmable gate array (FPGA).
7. The method as recited in claim 5, wherein the one or more hardware accelerators comprise an application-specific integrated circuit (ASIC).
8. The method as recited in claim 5, wherein determining whether to execute the individual ones of the calls using the at least one processor or using the one or more hardware accelerators comprises estimating a time of completion using the at least one processor and a time of completion using the one or more hardware accelerators based at least in part on monitoring of a plurality of prior calls.
9. The method as recited in claim 5, wherein determining whether to execute the individual ones of the calls using the at least one processor or using the one or more hardware accelerators comprises determining a current availability of the one or more hardware accelerators based at least in part on a history of calls sent to the one or more hardware accelerators and not completed.
10. The method as recited in claim 5, wherein the computing device represents a virtual compute instance offered by a multi-tenant provider network.
11. The method as recited in claim 5, wherein determining whether to execute the individual ones of the calls using the at least one processor or using the one or more hardware accelerators is performed based at least in part on a size of the associated data compared to a threshold size for the selective utilization of the one or more hardware accelerators.
12. The method as recited in claim 5, further comprising: using the at least one processor to execute the one or more of the calls; and comparing the one or more results of the one or more of the calls from the one or more hardware accelerators to one or more results of the one or more of the computation calls from the at least one processor.
13. A non-transitory computer-readable storage medium storing program instructions computer-executable to perform: receiving, at a library, a plurality of computation calls generated by an application, wherein the application and the library are executed by at least one processor of a computing device in a multi-tenant provider network, and wherein the application links to the library; using the library, determining whether to execute individual ones of the computation calls using the at least one processor or using a hardware accelerator; executing a particular computation call of the computation calls by both the at least one processor and the hardware accelerator; comparing a first result of executing the particular computation call by the hardware accelerator to a second result of executing the particular computation call by the at least one processor; and sending one or more other ones of the computation calls and associated data to the hardware accelerator, wherein the one or more other ones of the computation calls are executed for the associated data using the hardware accelerator, and wherein one or more results of the one or more of the computation calls are returned from the hardware accelerator to the library.
14. The non-transitory computer-readable storage medium as recited in claim 13, wherein the hardware accelerator comprises a field-programmable gate array (FPGA).
15. The non-transitory computer-readable storage medium as recited in claim 14, wherein an FPGA image including programming for the FPGA is selected from a marketplace associated with the multi-tenant provider network, the marketplace providing access to a plurality of different FPGA images.
16. The non-transitory computer-readable storage medium as recited in claim 15, wherein a different FPGA image including additional programming for the FPGA is selected from the marketplace, and wherein the FPGA is reprogrammed from the programming to the additional programming.
17. The non-transitory computer-readable storage medium as recited in claim 13, wherein the hardware accelerator comprises an application-specific integrated circuit (ASIC).
18. The non-transitory computer-readable storage medium as recited in claim 13, wherein the hardware accelerator is hosted in the computing device and accessed by the at least one processor over a local bus.
19. The non-transitory computer-readable storage medium as recited in claim 13, wherein the hardware accelerator is hosted in an additional computing device in the multi-tenant provider network and accessed by the computing device over a network connection.
20. The non-transitory computer-readable storage medium as recited in claim 13, wherein the hardware accelerator is integrated with the at least one processor in the computing device.
</claims>
</document>
