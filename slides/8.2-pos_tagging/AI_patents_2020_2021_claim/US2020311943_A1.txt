<document>

<filing_date>
2020-06-16
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2018-03-01
</priority_date>

<ipc_classes>
G06K9/03,G06K9/62,G06N3/04,G06N3/08,G06T7/11
</ipc_classes>

<assignee>
TENCENT TECHNOLOGY (SHENZHEN) COMPANY
</assignee>

<inventors>
FAN, QI
DAI, Yurong
</inventors>

<docdb_family_id>
62959655
</docdb_family_id>

<title>
IMAGE PROCESSING METHOD AND APPARATUS, AND STORAGE MEDIUM
</title>

<abstract>
An image processing method performed by a computing device deployed with a deep-learning neural network is provided. An image, including an object to be segmented from the image, is acquired. The object is segmented from the image by using the deep-learning neural network to acquire a first segmentation result. Correction information input by a user with respect to the first segmentation result is acquired. Based on correction information, the first segmentation result is modified by using the deep-learning neural network, to acquire a second segmentation result.
</abstract>

<claims>
1. An image processing method performed by a computing device deployed with a deep-learning neural network, the image processing method comprising: acquiring an image, the image comprising an object to be segmented from the image; segmenting the object from the image by using the deep-learning neural network to acquire a first segmentation result; acquiring correction information input by a user with respect to the first segmentation result; and modifying, based on correction information, the first segmentation result by using the deep-learning neural network, to acquire a second segmentation result.
2. The image processing method according to claim 1, wherein the deep-learning neural network comprises a first part and a second part connected to each other, the second part being closer to an output side of the deep-learning neural network than the first part, the segmenting the object comprises segmenting the object from the image by using the first part and the second part of the deep-learning neural network to acquire the first segmentation result, and the modifying comprises modifying, based on the correction information, the first segmentation result by using the second part of the deep-learning neural network, to acquire the second segmentation result.
3. The image processing method according to claim 1, further comprising, prior to the segmenting the object: acquiring a training data set, and converting all object types in the training data set into a foreground type to generate a new training data set; and training the deep-learning neural network by using the new training data set.
4. The image processing method according to claim 1, wherein a basic network of the deep-learning neural network is a fully convolutional network, a branch of a region proposal network existing on a first convolutional layer of the fully convolutional network, the first convolutional layer being lower than a highest convolutional layer of the fully convolutional network and higher than a lowest convolutional layer of the fully convolutional network.
5. The image processing method according to claim 4, wherein the segmenting the object comprises: outputting, according to the image, a position-sensitive feature map by using all convolutional layers of the fully convolutional network; generating a rectangular frame by using the region proposal network and projecting the rectangular frame onto the position-sensitive feature map; and segmenting the object in the rectangular frame through assembling training to acquire the first segmentation result.
6. The image processing method according to claim 5, wherein the modifying the first segmentation result comprises: generating a correction distance map based on the correction information; inputting, to a third convolutional layer in parallel, the correction distance map and a feature map outputted from a second convolutional layer of the fully convolutional network, the third convolutional layer being higher than the second convolutional layer, and the second convolutional layer and the third convolutional layer being lower than the highest convolutional layer of the fully convolutional network and higher than the lowest convolutional layer of the fully convolutional network; outputting a new position-sensitive feature map by using the third convolutional layer and a convolutional layer higher than the third convolutional layer; and segmenting the object in the rectangular frame again through assembling training to acquire the second segmentation result.
7. The image processing method according to claim 1, wherein the acquiring the correction information comprises: acquiring tapping by the user on a region in the first segmentation result.
8. The image processing method according to claim 1, wherein the acquiring the image comprises: acquiring the image and a selection region of the image, the selection region comprising the object to be segmented from the image.
9. An image processing apparatus, comprising: at least one memory configured to store program code; and at least one processor configured to read the program code and operate as instructed by the program code, the program code comprising: acquiring code configured to cause at least one of the at least one processor to acquire an image, the image comprising an object to be segmented from the image; interaction code configured to cause at least one of the at least one processor to acquire correction information input by a user with respect to a first segmentation result; and processing code configured to cause at least one of the at least one processor to segment the object from the image by using a deep-learning neural network to acquire the first segmentation result, and modify, based on the correction information, the first segmentation result by using the deep-learning neural network to acquire a second segmentation result.
10. The image processing apparatus according to claim 9, wherein a basic network of the deep-learning neural network is a fully convolutional network, a branch of a region proposal network existing on a first convolutional layer of the fully convolutional network, the first convolutional layer being lower than a highest convolutional layer of the fully convolutional network and higher than a lowest convolutional layer of the fully convolutional network.
11. The image processing apparatus according to claim 10, wherein the processing code comprises: code configured to cause at least one of the at least one processor to output, according to the image, a position-sensitive feature map by using all convolutional layers of the fully convolutional network; code configured to cause at least one of the at least one processor to generate a rectangular frame by using the region proposal network and projecting the rectangular frame onto the position-sensitive feature map; and code configured to cause at least one of the at least one processor to segment the object in the rectangular frame through assembling training to acquire the first segmentation result.
12. The image processing apparatus according to claim 11, wherein the processing code further comprises: code configured to cause at least one of the at least one processor to generate a correction distance map based on the correction information; code configured to cause at least one of the at least one processor to input, to a third convolutional layer in parallel, the correction distance map and a feature map outputted from a second convolutional layer of the fully convolutional network, the third convolutional layer being higher than the second convolutional layer, and the second convolutional layer and the third convolutional layer being lower than the highest convolutional layer of the fully convolutional network and higher than the lowest convolutional layer of the fully convolutional network; code configured to cause at least one of the at least one processor to output a new position-sensitive feature map by using the third convolutional layer and a convolutional layer higher than the third convolutional layer; and code configured to cause at least one of the at least one processor to segment the object in the rectangular frame again through assembling training to acquire the second segmentation result.
13. The image processing apparatus according to claim 9, wherein the interaction code further causes at least one of the at least one processor to acquire tapping by the user on a region in the first segmentation result.
14. The image processing apparatus according to claim 9, wherein the deep-learning neural network comprises a first part and a second part connected to each other, the second part being closer to an output side of the deep-learning neural network than the first part, the processing code further causes at least one of the at least one processor to segment the object from the image by using the first part and the second part of the deep-learning neural network to acquire the first segmentation result, and modify, based on the correction information, the first segmentation result by using the second part of the deep-learning neural network, to acquire the second segmentation result.
15. The image processing apparatus according to claim 9, wherein the program code further comprises: code configured to cause at least one of the at least one processor to acquire a training data set, and convert all object types in the training data set into a foreground type to generate a new training data set; and code configured to cause at least one of the at least one processor to train the deep-learning neural network by using the new training data set.
16. The image processing apparatus according to claim 9, wherein the acquiring code further causes at least one of the at least one processor to acquire the image and a selection region of the image, the selection region comprising the object to be segmented from the image.
17. A non-transitory computer-readable storage medium storing a computer program, which, when executed by at least one processor, causes the at least one processor to perform: acquiring an image and a cropping region of the image, the cropping region comprising an object to be cropped from the image; cropping the object from the image by using a deep-learning neural network to acquire a first cropping result; acquiring correction information input by a user with respect to the first cropping result; modifying, based on the correction information, the first cropping result by using the deep-learning neural network to acquire a second cropping result; and acquiring the object in the second cropping result.
</claims>
</document>
