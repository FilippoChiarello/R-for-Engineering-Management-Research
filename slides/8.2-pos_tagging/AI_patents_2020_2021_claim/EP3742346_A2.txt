<document>

<filing_date>
2020-05-20
</filing_date>

<publication_date>
2020-11-25
</publication_date>

<priority_date>
2019-05-23
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
HTC CORPORATION
</assignee>

<inventors>
Chen, Szu-Ying
Yu, Chun-Hsien
Chang, Che-Han
Chang, Edward
</inventors>

<docdb_family_id>
70802709
</docdb_family_id>

<title>
METHOD FOR TRAINING GENERATIVE ADVERSARIAL NETWORK (GAN), METHOD FOR GENERATING IMAGES BY USING GAN, AND COMPUTER READABLE STORAGE MEDIUM
</title>

<abstract>
The disclosure provides a method for training generative adversarial network (GAN), a method for generating images by using GAN, and a computer readable storage medium. The method may train the first generator of the GAN with available training samples belonging to the first type category and share the knowledge learnt by the first generator to the second generator. Accordingly, the second generator may learn to generate (fake) images belonging to the second type category even if there are no available training data during training the second generator.
</abstract>

<claims>
1. A method for training a generative adversarial network (GAN) (100), wherein the GAN (100) comprises a first generator (G1), a second generator (G2), a discriminator (D), and a prediction network (E), comprising: receiving, by the first generator (G1), a first random input (z) and a first category indication (y1) and accordingly generating a first output image (x'1), wherein the first generator (G1) and the second generator (G2) are both characterized by a plurality of first neural network weightings, the first category indication (y1) indicates that the first output image (x'1) corresponds to a first type category, and the first type category has available training samples; predicting, by the prediction network (E), a first semantic embedding vector (v'1) corresponding to the first output image (x'1); generating a first comparing result by comparing the first semantic embedding vector (v'1) with a second semantic embedding vector (v1) corresponding to the first type category; receiving, by the second generator (G2), a second random input (z) and a second category indication (y2) and accordingly generating a second output image (x'2), wherein the second category indication (y2) indicates that the second output image (x'2) corresponds to a second type category; predicting, by the prediction network (E), a third semantic embedding vector (v'2) corresponding to the second output image (x'2); generating a second comparing result by comparing the third semantic embedding vector (v'2) with a fourth semantic embedding vector (v2) corresponding to the second type category; generating, by the discriminator (D), a discriminating result (DR) via discriminating between the first output image (x'1) and at least one reference image (RI) belonging to the first type category, wherein the discriminator (D) is characterized by a plurality of second neural network weightings; updating the second neural network weightings based on the discriminating result (DR); updating the first neural network weightings based on the discriminating result (DR), the first comparing result and the second comparing result.
2. The method according to claim 1, wherein the discriminating result (DR) is used to formulate a first loss function for training the discriminator (D) and a second loss function for training the first generator (G1) and the second generator (G2), and the second neural network weightings are updated subject to minimizing the first loss function.
3. The method according to claim 2, wherein the first comparing result is used to formulate a first semantic loss function (Lse(G1)), the second comparing result is used to formulate a second semantic loss function (Lse(G2)), and the first neural network weightings are updated subject to minimizing a total loss function, wherein the total loss function is characterized by the second loss function, the first semantic loss function (Lse(G1)), and the second semantic loss function (Lse(G2)).
4. The method according to claim 1, wherein the first category indication (y1) is defined as a first one-hot vector indicating the first type category or a first specific semantic embedding vector indicating the first type category.
5. The method according to claim 1, wherein the second category indication (y2) is defined as a second one-hot vector indicating the second type category or a second specific semantic embedding vector indicating the second type category.
6. The method according to claim 1, wherein the prediction network (E) is an embedding regression network pre-trained with the available training samples belonging to the first type category.
7. The method according to claim 1, wherein the second type category has no training samples.
8. A method for training a generative adversarial network (GAN) (400), wherein the GAN (400) comprises a first generator (G1), a second generator (G2), a discriminator (D) and a color estimator (H), comprising: receiving, by the first generator (G1), a first input image (x) and a category indication (y) and accordingly generating a first output image (x'1) via replacing a first color of a first specific region in the first input image (x) with a first target color, wherein the first target color belongs to a first type category having a plurality of training color samples, and the first generator (G1) and the second generator (G2) are partially characterized by a plurality of first neural network weightings; generating, by the discriminator (D), a discriminating result (DR) and a classification result (CR) based on the first output image (x'1); receiving, by the second generator (G2), a second input image (x) and a target color indication (c) and accordingly generating a second output image (x'2) via replacing a second color of a second specific region in the second input image (x) with a second target color, wherein the second target color corresponds to the target color indication (c), and the second target color does not belonging to the first type category; estimating, by the color estimator (H), a region color corresponding to the second specific region in the second output image (x'2) and generating a color comparing result (CC) by comparing the region color with the target color; generating, by the first generator (G1), a cycle image (CI2) according to the second output image (x'2) and an original category indication (y') and generating a cycle-consistency result by comparing the cycle image (CI2) with the second input image (x); updating the discriminator (D) based on the discriminating result (DR) and the classification result (CR); updating the first generator (G1) and the second generator (G2) based on the discriminating result (DR), the color comparing result (CC), and the cycle-consistency result.
9. The method according to claim 8, wherein the first generator (G1) comprises a first convolutional neural network (CNN) (F1), a mask network (M), and a first combiner (C1), and the method comprises: using the first CNN (F1) to generate a first foreground image (FI1) based on the first input image (x) and the category indication (y); using the mask network (M) to generate a first probability map (PM1) corresponding to the first input image (x), wherein each pixel in the first probability map (PM1) is labelled with a probability of corresponding to the first specific region; using the first combiner (C1) to retrieve a first partial image in the first foreground image (FI1) based on the first region in the first probability map (PM1), retrieve a second partial image in the first input image (x) based on the second region in the first probability map (PM1), and combining the first partial image and the second partial image as the first output image (x'1).
10. The method according to claim 9, wherein the mask network (M) is characterized by a plurality of third neural network weightings, and the method further comprising:
updating the mask network (M) via updating the third neural network weightings based on the discriminating result (DR), the color comparing result (CC), and the cycle-consistency result.
11. The method according to claim 9, wherein the target color indication (c) is a 3D RGB color vector, the second generator (G2) comprises a second CNN (F2), the mask network (M), and a second combiner (C1), and the method comprises: using the second CNN (F2) to generate a second foreground image (FI2) based on the second input image (x) and the target color indication (c); using the mask network (M) to generate a second probability map (PM2) corresponding to the second input image (x), wherein each pixel in the second probability map (PM2) is labelled with a probability of corresponding to the second specific region; using the second combiner (C2) to retrieve a third partial image in the second foreground image (FI2) based on the third region in the second probability map (FI2), retrieve a fourth partial image in the second input image (x) based on the fourth region in the second probability map (FI2), and combining the third partial image and the fourth partial image as the second output image (x'2).
12. The method according to claim 11, wherein the first foreground image (FI1) is represented as conv1×1(x;T1(x,y)), the second foreground image (FI2) is represented as conv1×1(x;T2(x,c)), wherein x represents the first input image (x), y represents the category indication (y), c represents the target color indication (c), T1(x,y) and T2(x,c) are convolutional neural networks that generate 1x1 convolutional filters.
13. The method according to claim 11, comprising: retrieving, by the color estimator (H), the second probability map (PM2) and the second foreground image (FI2); estimating the region color via calculating a weighted average of the second foreground image (FI2) weighted by the second probability map (PM2).
14. The method according to claim 8, comprising: generating, by the discriminator (D), the discriminating result (DR) via discriminating the first output image (x'1) with a real image; predicting, by the discriminator (D), a predicted category of the first output image (x'1); generating, by the discriminator (D), the classification result (CR) via comparing the predicted category with the first type category.
15. The method according to claim 8, wherein the discriminator (D) is characterized by a plurality of second neural network weightings, the discriminating result (DR) and the classification result (CR) are used to formulate a first loss function for training the discriminator (D), and the method comprises:
updating the discriminator (D) via updating the second neural network weightings subject to minimizing the first loss function.
16. The method according to claim 8, wherein the discriminating result (DR), the color comparing result (CC), and the cycle-consistency result are used to formulate a second loss function for training the first generator (G1) and the second generator (G2), and the method comprises:
updating the first generator (G1) and the second generator (G2) via updating the first neural network weightings subject to minimizing the second loss function.
</claims>
</document>
