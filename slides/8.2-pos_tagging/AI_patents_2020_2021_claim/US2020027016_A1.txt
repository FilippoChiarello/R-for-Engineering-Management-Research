<document>

<filing_date>
2018-01-31
</filing_date>

<publication_date>
2020-01-23
</publication_date>

<priority_date>
2017-01-31
</priority_date>

<ipc_classes>
G06F17/16,G06N20/00,G06N7/00
</ipc_classes>

<assignee>
UNIVERSITY OF CALIFORNIA
</assignee>

<inventors>
GHASEMZADEH, MOHAMMAD
ROUHANI, BITA DARVISH
KOUSHANFAR, FARINAZ
</inventors>

<docdb_family_id>
63041050
</docdb_family_id>

<title>
HARDWARE-BASED MACHINE LEARNING ACCELERATION
</title>

<abstract>
A method for hardware-based machine learning acceleration is provided. The method may include partitioning, into a first batch of data and a second batch of data, an input data received at a hardware accelerator implementing a machine learning model. The input data may be a continuous stream of data samples. The input data may be partitioned based at least on a resource constraint of the hardware accelerator. An update of a probability density function associated with the machine learning model may be performed in real time. The probability density function may be updated by at least processing, by the hardware accelerator, the first batch of data before the second batch of data. An output may be generated based at least on the updated probability density function. The output may include a probability of encountering a data value. Related systems and articles of manufacture, including computer program products, are also provided.
</abstract>

<claims>
1. A system, comprising: at least one processor; and at least one memory including program code which when executed by the at least one processor provides operations comprising: partitioning, into a first batch of data and a second batch of data, an input data received at a hardware accelerator implementing a machine learning model, the input data comprising a continuous stream of data samples, and the input data being partitioned based at least on a resource constraint of the hardware accelerator; updating a probability density function associated with the machine learning model, the probability density function being updated by at least processing, by the hardware accelerator, the first batch of data before the second batch of data; and generating, based at least on the updated probability density function, an output comprising a probability of encountering a data value.
2. The system of claim 1, wherein the probability density function is updated in real time such that the updating of the probability density function is performed at a same time and/or substantially at the same time as the generation of the output comprising the probability of encountering the data value.
3. The system of claim 1, wherein each data sample comprises a plurality of data values corresponding to a plurality of features, and wherein the first batch of data and the second batch of data each comprise some but not all of the plurality of features.
4. The system of claim 1, wherein the first batch of data and the second batch of data each comprise some but not all of the data samples included in the input data.
5. The system of claim 1, wherein the machine learning model comprises a probabilistic machine learning model configured to perform an inference task.
6. The system of claim 5, wherein the probabilistic machine learning model comprises a Bayesian network and/or a belief network.
7. The system of claim 1, wherein the hardware accelerator processes the first batch of data and/or the second batch of data by at least applying, to the first batch of data and/or the second batch of data, one or more Markov Chain Monte Carlo techniques.
8. The system of claim 7, wherein the first batch of data and/or the second batch of data each comprise a matrix, and wherein the application of the one or more Markov Chain Monte Carlo techniques includes performing a sequence of dot product operations between two or more matrices comprising the first batch of data and/or the second batch of data.
9. The system of claim 8, wherein the hardware accelerator includes a tree adder configured to perform the sequence of dot product operations by at least performing, in parallel, at least a portion of a plurality of addition operations and/or multiplication operations comprising the sequence of dot product operations.
10. The system of claim 1, wherein the probability of encountering the data value changes upon processing the second batch of data, and wherein the output includes a first probability of encountering the data value given the first batch of data and a second probability of encountering the data value given the second batch of data.
11. The system of any of claims 1-10, wherein the hardware accelerator comprises one or more application specific integrated circuits (ASICs) and/or field programmable gate arrays (FPGAs).
12. The system of claim 1, wherein the probability density function includes a predictive function, wherein the predictive function is associated with a mean and a covariance of a prior distribution of the input data, and wherein the prior distribution of the input data indicates the probability of encountering the data value without taking into account the first batch of data and/or the second batch of data.
13. The system of claim 12, wherein the update to the probability density function comprises updating, based at least on the first batch of data and/or the second batch of data, the mean and/or the covariance of the prior distribution.
14. The system of claim 12, wherein the update to the probability density function further comprises determining, based at least on the prior distribution, a gradient of a posterior distribution of the input data, and wherein the posterior distribution of the input data indicates the probability of encountering the data value given the first batch of data and/or the second batch of data.
15. The system of claim 14, wherein the determination of the gradient includes computing an inverse of a covariance matrix corresponding to the covariance of the prior distribution, wherein the inverse of the covariance matrix is computed by at least performing a plurality of QR decompositions, and wherein the plurality of QR decompositions are performed to compute an inverse of an upper triangular matrix R.
16. The system of claim 15, wherein the hardware accelerator is configured to compute the inverse of the upper triangular matrix R by at least performing back-substitution.
17. The system of claim 1, wherein the partitioning of the input data is further based at least on a dimensionality of the input data and/or a rate at which the input data is received at the hardware accelerator.
18. The system of claim 1, further comprising: dividing, into a first portion of data and a second portion of data, the first batch of data; and storing the first portion of data and the second portion of data in different memory blocks to at least enable the first portion of data and the second portion of data to be accessed simultaneously for processing by the hardware accelerator during the update of the probability density function.
19. 19-20. (canceled)
21. A computer-implemented method, comprising: partitioning, into a first batch of data and a second batch of data, an input data received at a hardware accelerator implementing a machine learning model, the input data comprising a continuous stream of data samples, and the input data being partitioned based at least on a resource constraint of the hardware accelerator; performing a real time update of a probability density function associated with the machine learning model, the probability density function being updated by at least processing, by the hardware accelerator, the first batch of data before the second batch of data; and generating, based at least on the updated probability density function, an output comprising a probability of encountering a data value.
22. 22-39. (canceled)
40. A non-transitory computer readable medium storing instructions, which when executed by at least one data processor, result in operations comprising: partitioning, into a first batch of data and a second batch of data, an input data received at a hardware accelerator implementing a machine learning model, the input data comprising a continuous stream of data samples, and the input data being partitioned based at least on a resource constraint of the hardware accelerator; performing a real time update of a probability density function associated with the machine learning model, the probability density function being updated by at least processing, by the hardware accelerator, the first batch of data before the second batch of data; and generating, based at least on the updated probability density function, an output comprising a probability of encountering a data value.
41. (canceled)
</claims>
</document>
