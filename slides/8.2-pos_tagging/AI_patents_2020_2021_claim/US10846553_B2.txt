<document>

<filing_date>
2019-03-20
</filing_date>

<publication_date>
2020-11-24
</publication_date>

<priority_date>
2019-03-20
</priority_date>

<ipc_classes>
G06K9/00,G06K9/34,G06N20/00,G06N3/08
</ipc_classes>

<assignee>
SAP
</assignee>

<inventors>
REISSWIG, CHRISTIAN
HOEHNE, JOHANNES
KATTI, ANOOP RAVEENDRA
SPINACI, MARCO
</inventors>

<docdb_family_id>
68342598
</docdb_family_id>

<title>
Recognizing typewritten and handwritten characters using end-to-end deep learning
</title>

<abstract>
Disclosed herein are system, method, and computer program product embodiments for optical character recognition using end-to-end deep learning. In an embodiment, an optical character recognition system may train a neural network to identify characters of pixel images, assign index values to the characters, and recognize different formatting of the characters, such as distinguishing between handwritten and typewritten characters. The neural network may also be trained to identify, groups of characters and to generate bounding boxes to group these characters. The optical character recognition system may then analyze documents to identify character information based on the pixel data and produce segmentation masks, such as a type grid segmentation mask, and one or more bounding box masks. The optical character recognition system may supply these masks as an output or may combine the masks to generate a version of the received document having optically recognized characters.
</abstract>

<claims>
1. A computer implemented method, comprising: receiving a document image; analyzing pixels of the document image using a neural network to identify characters of the document image and formatting of the characters; generating a first segmentation mask using the neural network, wherein index values replace the characters; and generating a second segmentation mask using the neural network to distinguish a first subset of characters of the document image from a second subset of characters of the document image, wherein the formatting of the first subset of characters differs from the formatting of the second subset of characters.
2. The computer implemented method of claim 1, wherein the formatting of the first subset of characters includes handwritten text and the formatting of the second subset of characters includes typewritten text.
3. The computer implemented method of claim 1, further comprising: generating one or more bounding boxes indicating groups of characters in the document image; and combining the one or more bounding boxes into a bounding box mask, wherein the one or more bounding boxes are located in positions corresponding to the groups of characters in the document image.
4. The computer implemented method of claim 3, further comprising: overlaying the first segmentation mask; the second segmentation mask, and the bounding box mask on the document image.
5. The computer implemented method of claim 1, wherein the formatting of the first subset of characters includes a first language and the formatting of the second subset of characters includes a second language.
6. The computer implemented method of claim 1, wherein the formatting of the first subset of characters includes handwriting corresponding to a first individual and the formatting of the second subset of characters includes handwriting corresponding to a second individual.
7. The computer implemented method of claim 1, wherein the second segmentation mask includes a first color for the first subset of characters and a second color that differs from the first color for the second subset of characters.
8. A system, comprising: a memory; and at least one processor coupled to the memory and configured to: receive a document image; analyze pixels of the document mage using a neural network to identify characters of the document image and formatting of the characters; generate a first segmentation mask using the neural network, wherein index values replace the characters; and generate a second segmentation mask using the neural network to distinguish a first subset of characters of the document image from a second subset of characters of the document image, wherein the formatting of the first subset of characters differs from the formatting of the second subset of characters.
9. The system of claim 8, wherein the formatting of the first subset of characters includes handwritten text and the formatting of the second subset of characters includes typewritten text.
10. The system of claim 8, wherein the at least one processor is further configured to: generate one or more bounding boxes indicating groups of characters in the document image; and combine the one or more bounding boxes into a bounding box mask, wherein the one or more bounding boxes are located in positions corresponding to the groups of characters in the document image.
11. The system of claim 10, wherein the at least one processor is further configured to: overlay the first segmentation mask, the second segmentation mask, and the bounding box mask on the document image.
12. The system of claim 8, wherein the formatting of the first subset of characters includes a first language and the formatting of the second subset of characters includes a second language.
13. The system of claim 8, wherein the formatting of the first subset of characters includes handwriting corresponding to a first individual and the formatting of the second subset of characters includes handwriting corresponding to a second individual.
14. The system of claim 8, wherein the second segmentation mask includes a first color for the first subset of characters and a second color that differs from the first color for the second subset of characters.
15. A non-transitory computer-readable device having instructions stored thereon that, when executed by at least one computing device, cause the at least one computing device to perform operations comprising: receiving a document image; analyzing pixels of the document image using a neural network to identify characters of the document image and formatting of the characters; generating a first segmentation mask using the neural network, wherein index values replace the characters; and generating a second segmentation mask using the neural network to distinguish a first subset of characters of the document image from a second subset of characters of the document image, wherein the formatting of the first subset of characters differs from the formatting of the second subset of characters.
16. The non-transitory computer-readable device of claim 15, the operations further comprising: generating one or more bounding boxes indicating groups of characters in the document image; combining the one or more bounding boxes into a bounding box mask, wherein the one or more bounding boxes are located in positions corresponding to the groups of characters in the document image; and overlaying the first segmentation mask, the second segmentation mask, and the bounding box mask on the document image.
17. The non-transitory computer-readable device of claim 15, wherein the formatting of the first subset of characters includes handwritten text and the formatting of the second subset of characters includes typewritten text.
18. The non-transitory computer-readable device of claim 15, wherein the formatting of the first subset of characters includes a first language and the formatting of the second subset of characters includes a second language.
19. The non-transitory computer-readable device of claim 15, wherein the formatting of the first subset of characters includes handwriting corresponding to a first individual and the formatting of the second subset of characters includes handwriting corresponding to a second individual.
20. The non-transitory computer-readable device of claim 15, wherein the second segmentation mask includes a first color for the first subset of characters and a second color that differs from the first color for the second subset of characters.
</claims>
</document>
