<document>

<filing_date>
2016-09-23
</filing_date>

<publication_date>
2020-09-01
</publication_date>

<priority_date>
2015-09-24
</priority_date>

<ipc_classes>
G06F11/30,G06F11/34,G06N20/00
</ipc_classes>

<assignee>
APPLE
</assignee>

<inventors>
AL-DAHLE, AHMAD
GARG, KSHITIZ
</inventors>

<docdb_family_id>
72241661
</docdb_family_id>

<title>
Sensor fusion and deep learning
</title>

<abstract>
Some embodiments provide a sensor data-processing system which detects and classifies objects detected in an environment via fusion of sensor data representations generated by multiple separate sensors. The sensor data-processing system can fuse sensor data representations generated by multiple sensor devices into a fused sensor data representation and can further detect and classify features in the fused sensor data representation. Feature detection can be implemented based at least in part upon utilizing a feature-detection model generated via one or more of deep learning and traditional machine learning. The sensor data-processing system can adjust sensor data processing of representations generated by sensor devices based on external factors including indications of sensor health and environmental conditions. The sensor data-processing system can be implemented in a vehicle and provide output data associated with the detected objects to a navigation system which navigates the vehicle according to the output data.
</abstract>

<claims>
1. An apparatus, comprising: a sensor data-processing system comprising one or more processors and memory configured to: preprocess a plurality of sensor data representations of an environment, generated by a plurality of separate sensor devices, to combine the plurality of sensor data representations into a fused sensor data representation of the environment; after combining the sensor data representations, detect a feature associated with at least one object in the fused sensor data representation of the environment; identify one or more object classifications for the feature as detected in the fused sensor data representation; detect the feature associated with the at least one object in respective ones of the plurality of sensor data representations of the environment; determine corresponding one or more classification confidence values for the one or more object classifications based on the feature as detected in the respective ones of the plurality of sensor data representations; and classify the at least one object according to the one or more object classifications and the one or more classification confidence values.
2. The apparatus of claim 1, wherein the sensor data-processing system is configured to: based on detecting the feature associated with the object, identify a particular region of a sensor data representation of the environment in which the detected feature is located.
3. The apparatus of claim 1, wherein the plurality of separate sensor devices comprise two or more of a visible light sensor, a near infrared sensor, a LIDAR sensor, a stereo sensor, a radar device, or an IMU sensor.
4. The apparatus of claim 1, wherein the sensor data-processing system is configured to: detect the feature associated with the object based on processing the fused sensor data representation via at least one of: at least one traditional machine learning algorithm; or at least one deep learning algorithm.
5. The apparatus of claim 1, wherein: fusing the plurality of sensor data representations comprises weighting a contribution of separate sensor data representations generated by separate sensor devices to the fused sensor data representation based on separate confidence values associated with the separate sensor data representations; and the sensor data-processing system is configured to adjust at least one confidence level associated with sensor data representations generated by at least one sensor device, based on a determination that sensor data representations generated by the at least one sensor device are at least partially degraded.
6. The apparatus of claim 5, wherein the sensor data-processing system is configured to: determine that sensor data representations generated by the at least one sensor device are at least partially degraded based on external condition data which indicates an external environment condition of the environment.
7. The apparatus of claim 5, wherein the sensor data-processing system is configured to: determine that sensor data representations generated by the at least one sensor device are at least partially degraded based on performance state information, indicating a health state of the at least one sensor device.
8. A method, comprising: performing, by one or more computer systems: preprocessing a plurality of sensor data representations of an environment, generated by a plurality of separate sensor devices, to combine the plurality of sensor data representations into a fused sensor data representation of the environment; after combining the sensor data representations, detecting a feature, associated with at least one object, in the fused sensor data representation of the environment; identifying one or more object classifications for the feature as detected in the fused sensor data representation; detect the feature associated with the at least one object in respective ones of the plurality of sensor data representations of the environment; determine corresponding one or more classification confidence values for the one or more object classifications based on the feature as detected in the respective ones of the plurality of sensor data representations; and classifying the at least one object according to the one or more object classifications and the one or more confidence values.
9. The method of claim 8, comprising: based on detecting the feature associated with the object, identifying a particular region of a sensor data representation of the environment in which the detected feature is located.
10. The method of claim 8, comprising: discarding the detected feature based at least in part upon a determination that the one or more confidence values do not satisfy a threshold level.
11. The method of claim 8, comprising: detecting the feature associated with the object based on processing the fused sensor data representation via at least one of: at least one traditional machine learning algorithm; or at least one deep learning algorithm.
12. The method of claim 8, wherein: fusing the plurality of sensor data representations comprises weighting a contribution of separate sensor data representations generated by separate sensor devices to the fused sensor data representation based on separate confidence values associated with the separate sensor data representations; and the method comprises adjusting at least one confidence level associated with sensor data representations generated by at least one sensor device, based on a determination that sensor data representations generated by the at least one sensor device are at least partially degraded.
13. The method of claim 12, comprising: determining that sensor data representations generated by the at least one sensor device are at least partially degraded based on external condition data which indicates an external environment condition of the environment.
14. The method of claim 12, comprising: determining that sensor data representations generated by the at least one sensor device are at least partially degraded based on performance state information, indicating a health state of the at least one sensor device.
15. A non-transitory, computer-readable medium storing a program of instructions which, when executed by at least one computer system, causes the at least one computer system to: preprocess a plurality of sensor data representations of an environment, generated by a plurality of separate sensor devices, to combine the plurality of sensor data representations into a fused sensor data representation of the environment; and after combining the sensor data representations, detect a feature, associated with at least one object, in the fused sensor data representation of the environment; identify one or more object classifications for the feature as detected in the fused sensor data representation; detect the feature associated with the at least one object in respective ones of the plurality of sensor data representations of the environment; determine corresponding one or more classification confidence values for the one or more object classifications based on the feature as detected in the respective ones of the plurality of sensor data representations; and classify the at least one object according to the one or more object classifications and the one or more classification confidence values.
16. The non-transitory, computer-readable medium of claim 15, wherein the program of instructions, when executed by the at least one computer system, cause the at least one computer system to: based on detecting the feature associated with the object, identify a particular region of a sensor data representation of the environment in which the detected feature is located.
17. The non-transitory, computer-readable medium of claim 16, wherein the program of instructions, when executed by the at least one computer system, cause the at least one computer system to: generate a list of bounding boxes indicating regions of the sensor data representation of the environment having detected features and corresponding object classifications for the detected features, wherein the list of bounding boxes includes a bounding box for the particular region and the at least one object.
18. The non-transitory, computer-readable medium of claim 15, wherein: fusing the plurality of sensor data representations comprises weighting a contribution of separate sensor data representations generated by separate sensor devices to the fused sensor data representation based on separate confidence values associated with the separate sensor data representations; and the program of instructions, when executed by the at least one computer system, cause the at least one computer system to: adjust at least one confidence level associated with sensor data representations generated by at least one sensor device, based on a determination that sensor data representations generated by the at least one sensor device are at least partially degraded.
19. The non-transitory, computer-readable medium of claim 18, wherein the program of instructions, when executed by the at least one computer system, cause the at least one computer system to: determine that sensor data representations generated by at least one sensor device are at least partially degraded based on external condition data which indicates an external environment condition of the environment.
20. The non-transitory, computer-readable medium of claim 18, wherein the program of instructions, when executed by the at least one computer system, cause the at least one computer system to: determine that sensor data representations generated by the at least one sensor device are at least partially degraded based on performance state information, indicating a health state of the at least one sensor device.
</claims>
</document>
