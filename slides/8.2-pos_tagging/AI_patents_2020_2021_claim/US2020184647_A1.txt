<document>

<filing_date>
2018-06-08
</filing_date>

<publication_date>
2020-06-11
</publication_date>

<priority_date>
2017-06-08
</priority_date>

<ipc_classes>
A61B5/00,A61B6/00,A61B6/03,G06N3/04,G06N3/08,G06T7/00,G06T7/174,G16H30/40,G16H50/50
</ipc_classes>

<assignee>
GOVERNMENT OF THE UNITED STATES OF AMERICA, AS REPRESENTED BY THE SECRETARY, DEPARTMENT OF HEALTH AND HUMAN SERVICES
</assignee>

<inventors>
SUMMERS, RONALD M.
MOLLURA, DANIEL JOSEPH
HARRISON, ADAM PATRICK
XU, ZIYUE
LU, LE
</inventors>

<docdb_family_id>
64566695
</docdb_family_id>

<title>
PROGRESSIVE AND MULTI-PATH HOLISTICALLY NESTED NETWORKS FOR SEGMENTATION
</title>

<abstract>
Methods include processing image data through a plurality of network stages of a progressively holistically nested convolutional neural network, wherein the processing the image data includes producing a side output from a network stage m, of the network stages, where m>1, based on a progressive combination of an activation output from the network stage m and an activation output from a preceding stage m−1. Image segmentations are produced. Systems include a 3D imaging system operable to obtain 3D imaging data for a patient including a target anatomical body, and a computing system comprising a processor, memory, and software, the computing system operable to process the 3D imaging data through a plurality of progressively holistically nested convolutional neural network stages of a convolutional neural network.
</abstract>

<claims>
1. A method, comprising: processing image data through a plurality of network stages of a progressively holistically nested convolutional neural network; wherein the processing the image data includes producing a side output from a network stage m, of the network stages, where m>1, based on a progressive combination of an activation output from the network stage m and an activation output from a preceding stage m−1.
2. The method of claim 1, wherein the progressive combination is an addition of the activation output of the network stage m and the activation output of the network stage m−1.
3. The method of claim 1, wherein the convolutional neural network includes at least three sequential network stages.
4. The method of claim 1, wherein each network stage includes a plurality of layers including at least a convolutional layer, a nonlinear activation layer, batch normalization layer, and a pooling layer, except a last network stage that does not include a pooling layer.
5. The method of claim 1, wherein the image data comprises a 2D section image of 3D medical imaging data, the method further comprising: generating an image segmentation of an image feature in the 2D section image, the image segmentation corresponding to a mask output of a last stage of the multi-stage convolutional neural network that is based on a progressively created side output of the last stage.
6. The method of claim 1, further comprising training the convolutional neural network through deep supervision at each network stage by processing a set of a plurality of input training images, having respective ground truth image segmentations, through the network stages.
7. The method of claim 6, wherein the training comprises for each input training image: determining a cross-entropy loss at each network stage associated with a side output of the network stage using a class-balancing weight corresponding to an average of image segmentation edge ground truths over the set of input training images; producing an image segmentation estimate at each network stage corresponding to a mask output of the side output of the network stage; and back-propagating through the network stages with gradient descent to update network stage parameters of each network stage based on differences between the image segmentation estimates and the ground truth image segmentation of the input training image.
8. The method of claim 1, wherein the method comprises image segmentation of organs, tumors, or other anatomical bodies of a patient based on the image data, and the image data includes a target body.
9. The method of claim 8, wherein the target body is a pathological lung.
10. The method of claim 9, wherein the pathological lung has an infection, interstitial lung disease, or chronic obstructive pulmonary disease.
11. The method of claim 1, wherein the image data is derived from one or more computerized tomography (CT) scans.
12. The method of claim 1, wherein processing the image data comprises classifying individual pixels or patches of the image data.
13. The method of claim 1, wherein producing the side output comprises applying a progressive constraint on multi-scale pathways.
14. The method of claim 13, wherein the progressive constraint on multi-scale pathways requires no additional convolutional layers or network parameters.
15. The method of claim 1, wherein the method comprises image segmentation of a non-anatomical object that is not an organ, tumor, or anatomical body, and the segmentation is based on natural images and not 3D medical imaging.
16. A computing system comprising a processor and memory, the system operable to implement the method of claim 1.
17. One or more non-transitory computer readable media storing computer-executable instructions, which when executed by a computer cause the computer to perform the method of claim 1.
18. A system comprising: a 3D imaging system operable to obtain 3D imaging data for a patient including a target anatomical body; and a computing system comprising a processor, memory, and software, the computing system operable to: process the 3D imaging data through a plurality of progressively holistically nested convolutional neural network stages of a convolutional neural network, including to produce a side output from a network stage m, of the network stages, where m>1, based on a progressive combination of an activation output from the network stage m and an activation output from a preceding stage m−1; and generate an image segmentation of the target anatomical body based on a progressively created mask output of a last network stage of the network stages.
19. The system of claim 18, wherein the target anatomical body is a pathological lung.
20. The system of claim 19, wherein the pathological lung has an infection, interstitial lung disease, or chronic obstructive pulmonary disease.
21. The system of claim 18, wherein the 3D imaging system comprises a computerized tomography system and the 3D imaging data is derived from one or more computerized tomography scans.
22. The system of claim 18, wherein the computing system is operable to classify individual pixels or patches of the 3D imaging data in a bottom-up approach.
23. The system of claim 18, wherein the computing system is operable to apply a progressive constraint on multi-scale pathways.
24. The system of claim 23, wherein the progressive constraint on multi-scale pathways requires no additional convolutional layers or network parameters.
25. The system of claim 18, wherein the progressive combination is an addition of the activation output of the network stage m and the activation output of the network stage m−1.
26. The system of claim 18, wherein the computing system is operable to train the convolutional neural network through deep supervision at each network stage by processing a set of a plurality of input training images, having respective ground truth image segmentations, through the network stages.
27. The system of claim 26, wherein the computing system is operable to provide the training by, for each input training image: determining a cross-entropy loss at each network stage associated with a side output of the network stage using a class-balancing weight corresponding to an average of image segmentation edge ground truths over the set of input training images; producing an image segmentation estimate at each network stage corresponding to a mask output of the side output of the network stage; and backpropagating through the network stages with gradient descent to update network stage parameters of each network stage based on differences between the image segmentation estimates and the ground truth image segmentation of the input training image.
</claims>
</document>
