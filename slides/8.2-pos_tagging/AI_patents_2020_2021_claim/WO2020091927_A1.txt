<document>

<filing_date>
2019-09-30
</filing_date>

<publication_date>
2020-05-07
</publication_date>

<priority_date>
2018-10-31
</priority_date>

<ipc_classes>
G06F3/16,G06K9/46,G06T11/60
</ipc_classes>

<assignee>
SONY INTERACTIVE ENTERTAINMENT
KRISHNAMURTHY, SUDHA
</assignee>

<inventors>
ZHENG JIAN
OMOTE MASANORI
SINGH, ASHISH
KUMAR, NAVEEN
KRISHNAMURTHY, SUDHA
CHEN, MIN-HENG
JATI, ARINDAM
ADAMS, JUSTICE
</inventors>

<docdb_family_id>
70327979
</docdb_family_id>

<title>
ACTION DESCRIPTION FOR ON-DEMAND ACCESSIBILITY
</title>

<abstract>
A system enhances existing audio visual content with audio describing the action and setting of the visual content. The system may also provide subtitle content describing the important sound or sounds occurring within audio. Accommodation for color or visual impairments may be implemented by selective color substitution. A Graphical Style Modification module may apply a style from one image to another to adapt the style of a video per a gamer's preference.
</abstract>

<claims>
1. A system for enhancing the accessibility of Audio Visual content, the system, comprising: an action description module configured to recognize action happening within a sequence of image frames received from a host system and generate a tag describing the action happening within the sequence of frames.
2. The system of claim 1 wherein the action description module includes a first neural
network configured to generate frame level features from image frame data in the sequence of frames, a second neural network configured to convert image frame level features to sequence window level features, and a third neural network configured to generate a classification of the action happening within the sequence window level features.
3. The system of claim 2 wherein the length of the video sequence window is less than or equal to a time for the action description module to generate the classification.
4. The system of claim 2, wherein the action description module is configured to generate the tag describing the action happening within the sequence of frames from the classification of the action happening within the sequence window level features.
5. The system of claim 2 wherein the image frame data is video game frame data.
6. The system of claim 1 , further comprising a controller coupled to the host system and the action description module, wherein the controller is configured to activate the action description module in response to an input from a user and synchronize the output of the action description module with one or more other neural network modules.
7. The system of claim 6 wherein the one or more other neural network modules includes an Acoustic Effect Annotation module configured classify primary acoustic effects occurring within an audio segment wherein the audio segment is synchronized to occur during presentation of the sequence of image frames.
8. The system of claim 1 further comprising a text to speech synthesis module coupled to the action description module, wherein the text to speech synthesis module is configured to convert the tag to synthesized speech data describing the action taking place within the audio visual content.
9. The system of claim 1 , further comprising a controller coupled to the host system and the action description module, wherein the controller is configured to synchronize presentation of speech corresponding to the synthesized speech with display of the sequence of image frames received from the host system.
10. A method for enhancing the accessibility of Audio Visual content, the system,
comprising:
recognizing action happening within a sequence of image frames received from a host system and generating a tag describing the action happening within the sequence of frames with an action description module.
11. The method of claim 10 wherein recognizing action happening within the sequence of image frames includes using a first to generate frame level features from image frame data in the sequence of frames, using a second neural network to convert image frame level features to sequence window level features, and using a third neural network to generate a classification of the action happening within the sequence window level features.
12. The method of claim 11 wherein the length of the video sequence window is less than or equal to a time for the action description module to generate the classification.
13. The method of claim 11, the generating tag describing the action happening within the sequence of frames includes generating the tag from the classification of the action happening within the sequence window level features.
14. The method of claim 11 wherein the image frame data is video game frame data.
15. The method of claim 10, further using a controller coupled to the host system and the action description module to activate the action description module in response to an input from a user and synchronize the output of the action description module with one or more other neural network modules.
16. The method of claim 15 wherein the one or more other neural network modules includes an Acoustic Effect Annotation module configured classify primary acoustic effects occurring within an audio segment wherein the audio segment is synchronized to occur during presentation of the sequence of image frames.
17. The method of claim 10 further comprising converting the tag to synthesized speech data describing the action taking place within the audio visual content with a text to speech synthesis module.
18. The method of claim 10, further comprising, synchronizing presentation of speech
corresponding to the synthesized speech with display of the sequence of image frames received from the host system with a controller coupled to the host system and the action description module.
19. A non-transitory computer-readable medium having computer readable instructions embodied therein, the instructions being configured upon execution to implement a method for enhancing the accessibility of Audio Visual content, the method, comprising recognizing action happening within a sequence of image frames received from a host system and generating a tag describing the action happening within the sequence of frames with an action description module.
</claims>
</document>
