<document>

<filing_date>
2017-03-21
</filing_date>

<publication_date>
2020-02-04
</publication_date>

<priority_date>
2016-03-21
</priority_date>

<ipc_classes>
A61B5/00,A61B5/16,G10L15/02,G10L15/04,G10L17/26,G10L25/21,G10L25/66
</ipc_classes>

<assignee>
SONDE HEALTH
SONDE HEALTH
</assignee>

<inventors>
JORDAN, RICHARD K.
CHEN, MICHAEL C.
</inventors>

<docdb_family_id>
63583537
</docdb_family_id>

<title>
System and method for anhedonia measurement using acoustic and contextual cues
</title>

<abstract>
This application provides a system for classifying a status of anhedonia, the system including an audio data collector adapted to collect a sample of speech, and a processing module including an audio feature extractor and a classification unit, wherein the audio feature extractor extracts a plurality of acoustic features from the sample of speech, and the classification unit classifies a status of anhedonia from the plurality of acoustic features.
</abstract>

<claims>
1. A system for classifying a status of anhedonia, the system comprising: an audio data collector adapted to collect a sample of speech; a processing module comprising an audio feature extractor and a classification unit; said audio feature extractor adapted to extract a plurality of acoustic features from the sample of speech, wherein said plurality of acoustic features extracted comprise: an auto-correlated feature; a cross-correlated feature; and a coding coefficient of a feature; a contextual data collector adapted to receive contextual data from one or more contextual sensors; said classification unit adapted to classify said status of anhedonia by combining the extracted plurality of acoustic features and said contextual data received from said contextual sensors, wherein a combination of Random Forest and Gini Impurity is used for selecting said extracted acoustic features, wherein said selected acoustic features are weighted and entered into predictive models, wherein said predictive models determine an overall score of anhedonia, and wherein said contextual data determines component scores of anhedonia; and a display unit adapted to display said classified status.
2. The system of claim 1, wherein the audio data collector comprises: a microphone adapted to convert an acoustic energy into a voltage signal; an operational amplifier coupled to the microphone adapted to amplify the voltage signal; and an analog-to-digital converter adapted to convert the voltage signal into digital data.
3. The system of claim 1, wherein the audio data collector comprises: a plurality of audio signal capture units, wherein each audio signal capture unit comprises: a microphone adapted to convert an acoustic energy into a voltage signal; an operational amplifier coupled to the microphone adapted to amplify the voltage signal; and an analog-to-digital converter adapted to convert the voltage signal into digital data; a microphone array controller adapted to select the digital data captured from one or more of said audio signal capture units, and provide the selected digital data to an audio data transmitter; and said audio data transmitter adapted to transmit the digital data across a communication link to the processing module.
4. The system of claim 1, wherein the audio feature extractor comprises a digital signal processor, wherein the digital signal processor is coupled to the audio data collector for said extraction of the plurality of acoustic features from the sample of speech, wherein the classification unit comprises a general-purpose processor, and wherein the general-purpose processor is coupled to an output of the audio feature extractor to receive the plurality of acoustic features extracted by the digital signal processor of the audio feature extractor.
5. The system of claim 1, wherein the plurality of acoustic features extracted by the audio feature extractor further comprises two or more feature types, wherein said feature types comprise: a time domain feature; a spectral domain feature; and a perceptual model feature.
6. The system of claim 1, further comprising: said classification unit determining a sequence of traversal through an acyclic graph of the acoustic features, in accordance with a statistical distribution of the acoustic features; said classification unit traversing the acyclic graph by detecting a threshold crossing for each node of the acyclic graph; and said classification unit providing an output, wherein the output is predictive of said status of anhedonia.
7. A method of classifying a status of anhedonia in a human subject, the method comprising: collecting a sample of speech from a microphone, by an audio collector; extracting a plurality of acoustic features from the collected sample of speech, by an audio feature extractor of a processor module, wherein said plurality of acoustic features extracted comprise: an auto-correlated feature; a cross-correlated feature; and a coding coefficient of a feature; receiving contextual data from one or more contextual sensors, by a contextual data collector; combining the plurality of acoustic features and the contextual data to classify said status of anhedonia in the human subject, by a classification unit of the processor module, wherein a combination of Random Forest and Gini Impurity is used for selecting said extracted acoustic features, wherein said selected acoustic features are weighted and entered into predictive models, wherein said predictive models determine an overall score of anhedonia, and wherein said contextual data determines component scores of anhedonia; and displaying the classified status, by a display unit.
8. The method of claim 7, further comprising: converting an acoustic energy into a voltage signal, by a microphone of the audio data collector; amplifying the voltage signal, by an operational amplifier of the audio data collector, wherein the operational amplifier is coupled to the microphone; and converting the voltage signal into digital data, by an analog-to-digital converter of the audio data collector, wherein the analog-to-digital converter is coupled to the operational amplifier.
9. The method of claim 7, further comprising: providing a plurality of audio signal capture units, wherein each audio signal capture unit performs the steps comprising: converting an acoustic energy into a voltage signal, by a microphone of the audio data collector; amplifying the voltage signal, by an operational amplifier coupled to the microphone; and converting the voltage signal into digital data, by an analog-to-digital converter coupled to the operational amplifier.
10. The method of claim 7, further comprising: providing a digital signal processor in said processor module for said extraction of said plurality of acoustic features from the sample of speech; and receiving the plurality of acoustic features extracted by the digital signal processor and classifying the sample of speech to a predetermined status of anhedonia, by the classification unit of the processor module.
11. The method of claim 7, wherein the plurality of acoustic features further comprises two or more feature types, wherein said feature types comprise: a time domain feature; a spectral domain feature; and a perceptual domain feature.
12. The method of claim 7, further comprising: determining, in accordance with a statistical distribution of the acoustic features, a sequence of traversal through an acyclic graph of the acoustic features, by the classification unit; traversing the acyclic graph by detecting a threshold crossing for each node of the acyclic graph, by the classification unit; providing an output, by the classification unit, wherein the output is the status of anhedonia.
13. The system of claim 3, wherein the processor module further comprises an audio data receiver adapted to receive the digital data from the audio data transmitter, wherein the audio feature extractor further comprises a digital signal processor coupled to an output of the audio data receiver, wherein the digital signal processor is configured to extract the plurality of acoustic features from the sample of speech, wherein the classification unit comprises a general-purpose processor coupled to an output of the digital signal processor, and wherein the general-purpose processor is configured to receive the plurality of acoustic features from the digital signal processor.
14. The method of claim 9, wherein an audio data receiver in the processor module receives the digital data from the audio data transmitter, wherein a digital signal processor in the processor module coupled to an output of the audio data receiver extracts the plurality of acoustic features from the sample of speech, and wherein a general-purpose processor in the processor module coupled to an output of the digital signal processor receives the plurality of acoustic features from the digital signal processor.
15. The method of claim 7, wherein said contextual sensors comprise one or more of an accelerometer and a light sensor.
16. The system of claim 1, wherein said contextual sensors comprise one or more of an accelerometer and a light sensor.
</claims>
</document>
