<document>

<filing_date>
2019-09-05
</filing_date>

<publication_date>
2020-04-30
</publication_date>

<priority_date>
2018-10-23
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
HRL LABORATORIES
</assignee>

<inventors>
HOWARD, MICHAEL
MARTIN CHARLES
PILLY, PRAVEEN
KOLOURI, SOHEIL
KETZ, NICHOLAS
STEPP, NIGEL
</inventors>

<docdb_family_id>
70279600
</docdb_family_id>

<title>
ARTIFICIAL NEURAL NETWORK AND METHOD OF TRAINING AN ARTIFICIAL NEURAL NETWORK WITH EPIGENETIC NEUROGENESIS
</title>

<abstract>
A method for retraining an artificial neural network trained on data from an old task includes training the artificial neural network on data from a new task different than the old task, calculating a drift, utilizing Sliced Wasserstein Distance, in activation distributions of a series of hidden layer nodes during the training of the artificial neural network with the new task, calculating a number of additional nodes to add to at least one hidden layer based on the drift in the activation distributions, resetting connection weights between input layer nodes, hidden layer nodes, and output layer nodes to values before the training of the artificial neural network on the data from the new task, adding the additional nodes to the at least one hidden layer, and training the artificial neural network on data from the new task.
</abstract>

<claims>
1. A method for retraining an artificial neural network trained on data from an old task, the artificial neural network comprising an input layer having a plurality of input layer nodes, at least one hidden layer having a plurality of hidden layer nodes, and an output layer having a plurality of output layer nodes, the method comprising: training the artificial neural network on data from the old task and data from a new task different than the old task;
calculating a drift, utilizing Sliced Wasserstein Distance, in activation distributions of the plurality of hidden layer nodes during training of the artificial neural network with the data from the old task and data from the new task;
calculating a number of additional nodes to add to the at least one hidden layer based on the drift in the activation distributions;
resetting connection weights between the plurality of input layer nodes, the plurality of hidden layer nodes, and the plurality of output layer nodes to values before the training of the artificial neural network on the data from the new task;
adding the additional nodes to the at least one hidden layer; and
training the artificial neural network on data from the new task.
2. The method of claim 1 , wherein data from the old task comprises training data retained from input and target output distributions of the old task.
3. The method of claim 1 , wherein the data from the old task comprises synthetic data generated from a model of input and target output distributions of the old task.
4. The method of claim 1 , wherein the calculating the number of additional nodes is calculated according to Equation 1 :
nodes = c * log ) + b (Equation 1 )
wherein Nn0des is the number of additional nodes, c and b are user-specified constants and D is the drift in the activation distributions.
5. The method of claim 1 , wherein:
the additional nodes comprise a first set of additional nodes and a second set of additional nodes, and
the adding the additional nodes comprises adding the first set of additional nodes to a first hidden layer of the at least one hidden layer and adding the second set of additional nodes to a second hidden layer of the at least one hidden layer.
6. The method of claim 5, wherein the adding the additional nodes further comprises connecting each additional node of the first set of additional nodes to each additional node of the second set of additional nodes.
7. The method of claim 6, wherein the adding the additional nodes further comprises connecting the plurality of hidden layer nodes in the first hidden layer to the second set of additional nodes added to the second hidden layer.
8. The method of claim 7, wherein the adding the additional nodes comprises not connecting the first set of additional nodes added to the first hidden layer to the plurality of hidden layer nodes in the second hidden layer.
9. The method of claim 1 , further comprising adding a plurality of new output layer nodes to the output layer.
10. The method of claim 9, wherein the adding the additional nodes comprises adding the additional nodes to a last hidden layer of the at least one hidden layer adjacent to the output layer, and connecting the additional nodes only to the plurality of new output layer nodes.
11. The method of claim 10, further comprising connecting each of the plurality of nodes of the at least one hidden layer adjacent to the output layer to each of the plurality of new output layer nodes.
12. The method of claim 1 , wherein the adding the additional nodes to the at least one hidden layer comprises adding the additional nodes to a first hidden layer of the at least one hidden layer, and wherein the method further comprises connecting each of the plurality of input layer nodes to each of the additional nodes in the first hidden layer.
13. The method of claim 1 , wherein the training the artificial neural network on the data from the new task includes minimizing a loss function with stochastic gradient descent.
14. An artificial neural network trained by epigenetic neurogenesis to perform an old task and a new task, the artificial neural network comprising:
an input layer comprising a plurality of input layer nodes; a first hidden layer comprising a plurality of first hidden layer nodes and at least one new first hidden layer node;
a second hidden layer comprising a plurality of second hidden layer nodes and at least one new second hidden layer node; and
an output layer comprising a plurality of output layer nodes,
wherein the at least one new first hidden layer node is connected to the at least one new second hidden layer node,
wherein the at least one new first hidden layer node is not connected to the plurality of second hidden layer nodes,
wherein each of the plurality of first hidden layer nodes is connected to the at least one new second hidden layer node, and
wherein each of the plurality of input layer nodes is connected to the at least one new first hidden layer node.
15. The artificial neural network of claim 14, wherein the output layer further comprises a plurality of new output layer nodes.
16. The artificial neural network of claim 15, wherein the second hidden layer is a last hidden layer adjacent to the output layer, and wherein the at least one new second layer hidden node is connected only to the plurality of new output layer nodes.
17. The artificial neural network of claim 16, wherein each of the plurality of second hidden layer nodes is connected to each of the plurality of output layer nodes and each of the plurality of new output layer nodes.
18. The artificial neural network of claim 14, wherein the second hidden layer is a last hidden layer adjacent to the output layer, and wherein the at least one new second layer hidden node is connected to each of the plurality of the output layer nodes.
19. The artificial neural network of claim 14, wherein each of the plurality of input layer nodes is connected to each of the plurality of first hidden layer nodes.
20. The artificial neural network of claim 14, wherein each of the plurality of first hidden layer nodes is connected to each of the plurality of second hidden layer nodes.
</claims>
</document>
