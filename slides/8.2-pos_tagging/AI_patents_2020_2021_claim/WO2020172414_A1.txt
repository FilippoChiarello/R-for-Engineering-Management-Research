<document>

<filing_date>
2020-02-20
</filing_date>

<publication_date>
2020-08-27
</publication_date>

<priority_date>
2019-02-21
</priority_date>

<ipc_classes>
G16H20/40,G16H30/40,G16H50/20
</ipc_classes>

<assignee>
THEATOR
</assignee>

<inventors>
ASSELMANN, DOTAN
WOLF, TAMIR
</inventors>

<docdb_family_id>
69811980
</docdb_family_id>

<title>
SYSTEMS AND METHODS FOR ANALYSIS OF SURGICAL VIDEOS
</title>

<abstract>
Systems and methods for analysis and review of surgical videos are disclosed. The systems and methods may include indexing characterized surgical intraoperative events, analyzing and cataloging surgical footage based on complexity, generating an intraoperative surgical event summary, overlaying a timeline on a surgical video, and/or generating a compilation of surgical event footage. The systems and methods may further include analysis of surgical videos for estimating surgical pressure, estimating a source and extent of fluid leakage, detecting an omitted surgical event, predicting a post discharge risk for a patient, updating a predicted outcome, providing real time recommendations to a surgeon, determining insurance reimbursement, adjusting an operating room schedule, and/or populating a postoperative report.
</abstract>

<claims>
1. A computer-implemented method for reviewing surgical video, the method comprising:
accessing at least one video of a surgical procedure;
causing the at least one video to be output for display;
overlaying on the at least one video outputted for display a surgical timeline, wherein the surgical timeline includes markers identifying at least one of a surgical phase, an intraoperative surgical event, and a decision making junction; and
enabling a surgeon, while viewing playback of the at least one video to select one or more
markers on the surgical timeline, and thereby cause a display of the video to skip to a location associated with the selected marker.
2. The method of claim 1, wherein the markers are coded by at least one of a color or a criticality level.
3. The method of claim 1, wherein the surgical timeline includes textual information identifying portions of the surgical procedure.
4. The method of claim 1, wherein the at least one video includes a compilation of footage from a
plurality of surgical procedures, arranged in procedural chronological order, wherein the compilation of footage depicts complications from the plurality of surgical procedures; and wherein the one or more markers are associated with the plurality of surgical procedures and are displayed on a common timeline.
5. The method of claim 1 , wherein the one or more markers include a decision making junction marker corresponding to a decision making junction of the surgical procedure and the selection of the decision making junction marker enables the surgeon to view two or more alternative video clips from two or more corresponding other surgical procedures; and wherein the two or more video clips present differing conduct.
6. The method of claim 1, wherein the one or more markers include a decision making junction marker corresponding to a decision making junction of the surgical procedure; and wherein the selection of the decision making junction marker causes a display of one or more alternative possible decisions related to the selected decision making junction marker.
7. The method of claim 6, wherein one or more estimated outcomes associated with the one or more alternative possible decisions are displayed in conjunction with the display of the one or more alternative possible decisions.
8. The method of claim 7, wherein the one or more estimated outcomes are a result of an analysis of a plurality of videos of past surgical procedures including respective similar decision making junctions.
9. The method of claim 6, wherein information related to a distribution of past decisions made in
respective similar past decision making junctions are displayed in conjunction with the display of the alternative possible decisions.
10. The method of claim 9, wherein the decision making junction of the surgical procedure is associated with a first patient, and the respective similar past decision making junctions are selected from past surgical procedures associated with patients with similar characteristics to the first patient.
11. The method of claim 9, wherein the decision making junction of the surgical procedure is associated with a first medical professional, and the respective similar past decision making junctions are selected from past surgical procedures associated with medical professionals with similar characteristics to the first medical professional.
12. The method of claim 9, wherein the decision making junction of the surgical procedure is associated with a first prior event in the surgical procedure, and the similar past decision making junctions are selected from past surgical procedures including prior events similar to the first prior event.
13. The method of claim 1, wherein the markers include intraoperative surgical event markers, selection of an intraoperative surgical event marker enables the surgeon to view alternative video clips from differing surgical procedures, and wherein the alternative video clips present differing ways in which a selected intraoperative surgical event was handled.
14. The method of claim 1, wherein the overlay on the video output is displayed before the end of the surgical procedure depicted in the displayed video.
15. The method of claim 8, wherein the analysis is based on one or more electronic medical records associated with the plurality of videos of past surgical procedures.
16. The method of claim 8, wherein the respective similar decision making junctions are similar to the decision making junction of the surgical procedure according to a similarity metric.
17. The method of claim 8, wherein the analysis includes usage of an implementation of a computer vision algorithm.
18. The method of claim 1, wherein the markers relate to intraoperative surgical events and the selection of an intraoperative surgical event marker enables the surgeon to view alternative video clips from differing surgical procedures.
19. A system for reviewing surgical videos, the system comprising:
at least one processor configured to:
access at least one video of a surgical procedure;
cause the at least one video to be output for display;
overlay on the at least on video outputted for display a surgical timeline, wherein the surgical timeline includes markers identifying at least one of a surgical phase, an intraoperative surgical event, and a decision making junction; and
enable a surgeon, while viewing playback of the at least one video to select one or more markers on the surgical timeline, and thereby cause a display of the video to skip to a location associated with the selected marker.
20. A non-transitory computer readable medium comprising instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling surgical video review, the operations comprising: accessing at least one video of a surgical procedure;
causing the at least one video to be output for display;
overlaying on the at least on video outputted for display a surgical timeline, wherein the surgical timeline includes markers identifying at least one of a surgical phase, an intraoperative surgical event, and a decision making junction; and
enabling a surgeon, while viewing playback of the at least one video to select one or more markers on the surgical timeline, and thereby cause a display of the video to skip to a location associated with the selected marker.
21. A computer-implemented method for video indexing, the method comprising:
accessing video footage to be indexed, the video footage to be indexed including footage of a particular surgical procedure;
analyzing the video footage to identify a video footage location associated with a surgical phase of the particular surgical procedure;
generating a phase tag associated with the surgical phase;
associating the phase tag with the video footage location;
analyzing the video footage to identify an event location of a particular intraoperative surgical event within the surgical phase;
associating an event tag with the event location of the particular intraoperative surgical event; storing an event characteristic associated with the particular intraoperative surgical event;
associating at least a portion of the video footage of the particular surgical procedure with the phase tag, the event tag, and the event characteristic in a data structure that contains additional video footage of other surgical procedures, wherein the data structure also includes respective phase tags, respective event tags, and respective event characteristics associated with one or more of the other surgical procedures;
enabling a user to access the data structure through selection of a selected phase tag, a selected event tag, and a selected event characteristic of video footage for display; performing a lookup in the data structure of surgical video footage matching the at least one selected phase tag, selected event tag, and selected event characteristic to identify a matching subset of stored video footage; and
causing the matching subset of stored video footage to be displayed to the user, to thereby enable the user to view surgical footage of at least one intraoperative surgical event sharing the selected event characteristic, while omitting playback of video footage lacking the selected event characteristic.
22. The method of claim 21, wherein enabling the user to view surgical footage of at least one intraoperative surgical event that has the selected event characteristic, while omitting playback of portions of selected surgical events lacking the selected event characteristic, includes sequentially presenting to the user portions of surgical footage of a plurality of intraoperative surgical events sharing the selected event characteristic, while omitting playback of portions of selected surgical events lacking the selected event characteristic.
23. The method of claim 21, wherein the stored event characteristic includes an adverse outcome of the surgical event and wherein causing the matching subset to be displayed includes enabling the user to view surgical footage of a selected adverse outcome while omitting playback of surgical events lacking the selected adverse outcome.
24. The method of claim 21, wherein the stored event characteristic includes a surgical technique and wherein causing the matching subset to be displayed includes enabling the user to view surgical footage of a selected surgical technique while omitting playback of surgical footage not associated with the selected surgical technique.
25. The method of claim 21, wherein the stored event characteristic includes a surgeon skill level, and wherein causing the matching subset to be displayed includes enabling the user to view footage exhibiting a selected surgeon skill level while omitting playback of footage lacking the selected surgeon skill level.
26. The method of claim 21, wherein the stored event characteristic includes a physical patient
characteristic, and wherein causing the matching subset to be displayed includes enabling the user to view footage exhibiting a selected physical patient characteristic while omitting playback of footage lacking the selected physical patient characteristic.
27. The method of claim 21, wherein the stored event characteristic includes an identity of a specific surgeon, and wherein causing the matching subset to be displayed includes enabling the user to view footage exhibiting an activity by a selected surgeon while omitting playback of footage lacking activity by the selected surgeon.
28. The method of claim 21, wherein the stored event characteristic includes physiological response, and wherein causing the matching subset to be displayed includes enabling the user to view footage exhibiting a selected physiological response while omitting playback of footage lacking the selected physiological response.
29. The method of claim 21, wherein analyzing the video footage to identify the video footage location associated with at least one of the surgical event or the surgical phase includes performing computer image analysis on the video footage to identify at least one of a beginning location of the surgical phase for playback or a beginning of a surgical event for playback.
30. The method of claim 21, further comprising accessing aggregate data related to a plurality of surgical procedures similar to the particular surgical procedure, and presenting to the user statistical information associated with the selected event characteristic.
31. The method of claim 21, wherein the accessed video footage includes video footage captured via at least one image sensor located in at least one of a position above an operating table, in a surgical cavity of a patient, within an organ of a patient or within vasculature of a patient.
32. The method of claim 21, wherein identifying the video footage location is based on user input.
33. The method of claim 21, wherein identifying the video footage location includes using computer analysis to analyze frames of the video footage.
34. The method of claim 29, wherein the computer image analysis includes using a neural network model trained using example video frames including previously-identified surgical phases to thereby identify at least one of a video footage location or a phase tag.
35. The method of claim 21, further comprising determining the stored event characteristic based on user input.
36. The method of claim 21, further comprising determining the stored event characteristic based on a computer analysis of video footage depicting the particular intraoperative surgical event.
37. The method of claim 21, wherein generating the phase tag is based on a computer analysis of video footage depicting the surgical phase.
38. The method of claim 21, wherein identifying a matching subset of stored video footage includes using computer analysis to determine a degree of similarity between the matching subset of stored video and the selected event characteristic.
39. A surgical video indexing system, including:
at least one processor configured to:
access video footage to be indexed, the video footage to be indexed including footage of a
particular surgical procedure;
analyze the video footage to generate a phase tag;
identify a video footage location associated with a surgical phase of the surgical procedure; associate the phase tag with the video footage location;
analyze the video footage to identify an event location of a particular intraoperative surgical event;
associate an event tag with the event location of the particular intraoperative surgical event; store an event characteristic of the particular intraoperative surgical event;
associate at least a portion of the video footage of the particular surgical procedure with the phase tag, the event tag, and the event characteristic in a data structure that contains additional video footage of other surgical procedures, wherein the data structure also includes a respective phase tag, respective event tag, and respective event characteristics associated with one or more other surgical procedures;
enable a user to access the data structure through selection of a selected phase tag, a selected event tag, and a selected event characteristic of video for display; perform a lookup in the data structure of surgical video footage matching the at least one selected phase tag, selected event tag, or selected event characteristic to identify a matching subset of stored video footage; and
cause the matching subset of stored video footage to be displayed to the user, to thereby enable the user to view surgical footage of at least one intraoperative surgical event sharing the selected event characteristic, while omitting playback of video footage lacking the selected event characteristic.
40. A non-transitory computer readable medium including instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling video indexing, the operations including:
accessing video footage to be indexed, the video footage to be indexed including footage of a particular surgical procedure;
analyzing the video footage to generate a phase tag;
identifying a video footage location associated with a surgical phase of the surgical procedure; associating the phase tag with the video footage location;
analyzing the video footage to identify an event location of a particular intraoperative surgical event;
associating an event tag with the event location of the particular intraoperative surgical event; storing an event characteristic of the particular intraoperative surgical event;
associating at least a portion of the video footage of the particular surgical procedure with the phase tag, the event tag, and the event characteristic in a data structure that contains additional video footage of other surgical procedures, wherein the data structure also includes a respective phase tag, respective event tag, and respective event characteristic associated with at least one other surgical procedure;
enabling a user to access the data structure through selection of a selected phase tag, a selected event tag, and a selected event characteristic of video for display;
performing a lookup in the data structure of surgical video footage matching the at least one selected phase tag, selected event tag, and selected event characteristic to identify a matching subset of stored video footage; and
causing the matching subset of stored video footage to be displayed to the user, to thereby enable the user to view surgical footage of the at least one other intraoperative surgical event sharing the selected event characteristic, while omitting playback of video footage lacking the selected event characteristic.
41. A computer-implemented method for generating surgical summary footage, the method comprising:
accessing particular surgical footage containing a first group of frames associated with at least one intraoperative surgical event and a second group of frames not associated with surgical activity; accessing historical data based on historical surgical footage of prior surgical procedures, wherein the historical data includes information that distinguishes portions of surgical footage into frames associated with intraoperative surgical events and frames not associated with surgical activity;
distinguishing in the particular surgical footage the first group of frames from the second group of frames based on the information of the historical data; and
upon request of a user, presenting to the user an aggregate of the first group of frames of the particular surgical footage, while omitting presentation to the user of the second group of frames.
42. The method of claim 41, wherein the information that distinguishes portions of the historical surgical footage into frames associated with an intraoperative surgical event includes an indicator of at least one of a presence or a movement of a surgical tool.
43. The method of claim 41, wherein the information that distinguishes portions of the historical surgical footage into frames associated with an intraoperative surgical event includes detected tools and anatomical features in associated frames.
44. The method of claim 41, wherein the request of the user includes an indication of at least one type of intraoperative surgical event of interest, and wherein the first group of frames depicts at least one intraoperative surgical event of the at least one type of intraoperative surgical event of interest.
45. The method of claim 41 , wherein the request of the user includes a request to view a plurality of intraoperative surgical events in the particular surgical footage, and wherein presenting to the user an aggregate of the first group of frames includes displaying the first group frames in chronological order with chronological frames of the second group omitted.
46. The method of claim 41, wherein:
the historical data further includes historical surgical outcome data and respective
historical cause data;
the first group of frames includes a cause set of frames and an outcome set of frames; the second group of frames includes an intermediate set of frames; and wherein the method further comprises:
analyzing the particular surgical footage to identify a surgical outcome and a respective cause of the surgical outcome, the identifying being based on the historical outcome data and respective historical cause data;
detecting, based on the analyzing, the outcome set of frames in the particular surgical footage, the outcome set of frames being within an outcome phase of the surgical procedure;
detecting, based on the analyzing, a cause set of frames in the particular surgical footage, the cause set of frames being within a cause phase of the surgical procedure remote in time from the outcome phase, and wherein the intermediate set of frames is within an intermediate phase interposed between the cause set of frames and the outcome set of frames;
generating a cause-effect summary of the surgical footage, wherein the cause-effect summary includes the cause set of frames and the outcome set of frames and omits the intermediate set of frames; and
wherein the aggregate of the first group of frames presented to the user includes the cause-effect summary.
47. The method of claim 46, wherein the cause phase includes a surgical phase in which the cause
occurred, and wherein the cause set of frames is a subset of the frames in the cause phase.
48. The method of claim 46, wherein the outcome phase includes a surgical phase in which the outcome is observable, and wherein the outcome set of frames is a subset of frames in the outcome phase.
49. The method of claim 46, wherein the method further comprises using a machine learning model trained to identify surgical outcomes and respective causes of the surgical outcomes using the historical data to analyze the particular surgical footage.
50. The method of claim 51, wherein the particular surgical footage depicts a surgical procedure
performed on a patient and captured by at least one image sensor in an operating room, and wherein the method further comprises exporting the first group of frames for storage in a medical record of the patient.
51. The method of claim 50, wherein the method further comprises generating an index of the at least one intraoperative surgical event, and exporting the first group of frames includes generating a compilation of the first group of frames, the compilation including the index and being configured to enable viewing of the at least one intraoperative surgical event based on a selection of one or more index items.
52. The method of claim 51, wherein the compilation contains a series of frames of differing
intraoperative events stored as a continuous video.
53. The method of claim 50, the method further including associating the first group of frames with a unique patient identifier and updating a medical record including the unique patient identifier.
54. The method of claim 51, wherein a location of the at least one image sensor is at least one of above an operating table in the operating room or within the patient.
55. The method of claim 51, wherein distinguishing in the particular surgical footage the first group of frames from the second group of frames includes:
analyzing the particular surgical footage to detect a medical instrument; analyzing the particular surgical footage to detect an anatomical structure; analyzing the video to detect a relative movement between the detected medical
instrument and the detected anatomical structure; and
distinguishing the first group of frames from the second group of frames based on the relative movement, wherein the first group of frames includes surgical activity frames and the second group of frames includes non-surgical activity frames, and wherein presenting the aggregate thereby enables a surgeon preparing for surgery to omit the non-surgical activity frames during a video review of the abridged presentation.
56. The method of claim 55, wherein distinguishing the first group of frames from the second group of frames is further based on a detected relative position between the medical instrument and the anatomical structure.
57. The method of claim 55, wherein distinguishing the first group of frames from the second group of frames is further based on a detected interaction between the medical instrument and the anatomical structure.
58. The method of claim 55, wherein omitting the non-surgical activity frames includes omitting a
majority of frames that capture non-surgical activity.
59. A system for generating surgical summary footage, the system including:
at least one processor configured to:
access particular surgical footage containing a first group of frames associated with at least one intraoperative surgical event and a second group of frames not associated with surgical activity;
access historical data associated with historical surgical footage of prior surgical
procedures, wherein the historical data includes information that distinguishes portions of the historical surgical footage into frames associated with intraoperative surgical events and frames not associated with surgical activity; distinguish in the particular surgical footage the first group of frames from the second group of frames based on the information of the historical data; and
upon request of a user, present to the user an aggregate of the first group of frames of the particular surgical footage, while omitting presentation to the user of the second group of frames.
60. A non-transitory computer readable medium comprising instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling generating surgical summary footage, the operations comprising:
accessing particular surgical footage containing a first group of frames associated with at least one intraoperative surgical event and a second group of frames not associated with surgical activity;
accessing historical data associated with historical surgical footage of prior surgical procedures, wherein the historical data includes information that distinguishes portions of the historical surgical footage into frames associated with intraoperative surgical events and frames not associated with surgical activity; distinguishing in the particular surgical footage the first group of frames from the second group of frames based on the information of the historical data; and upon request of a user, presenting to the user an aggregate of the first group of frames of the particular surgical footage, while omitting presentation to the user of the second group of frames.
61. A computer-implemented method for surgical preparation, the method comprising:
accessing a repository of a plurality of sets of surgical video footage reflecting a plurality of surgical procedures performed on differing patients and including intraoperative surgical events, surgical outcomes, patient characteristics, surgeon characteristics, and intraoperative surgical event characteristics;
enabling a surgeon preparing for a contemplated surgical procedure to input case-specific information corresponding to the contemplated surgical procedure;
comparing the case-specific information with data associated with the plurality of sets of surgical video footage to identify a group of intraoperative events likely to be encountered during the contemplated surgical procedure;
using the case-specific information and the identified group of intraoperative events likely to be encountered to identify specific frames in specific sets of the plurality of sets of surgical video footage corresponding to the identified group of intraoperative events, wherein the identified specific frames include frames from the plurality of surgical procedures performed on differing patients;
determining that a first set and a second set of video footage from differing patients contain frames associated with intraoperative events sharing a common characteristic;
omitting an inclusion of the second set from a compilation to be presented to the surgeon and including the first set in the compilation to be presented to the surgeon; and enabling the surgeon to view a presentation including the compilation containing frames from the differing surgical procedures performed on differing patients.
62. The method of claim 61, further comprising enabling a display of a common surgical timeline
including one or more chronological markers corresponding to one or more of the identified specific frames along the presentation.
63. The method of claim 61, wherein enabling the surgeon to view the presentation includes sequentially displaying discrete sets of video footage of the differing surgical procedures performed on differing patients.
64. The method of claim 63, wherein sequentially displaying discrete sets of video footage includes displaying an index of the discrete sets of video footage enabling the surgeon to select one or more of the discrete sets of video footage.
65. The method of claim 64, wherein the index includes a timeline parsing the discrete sets into
corresponding surgical phases and textual phase indicators.
66. The method of claim 65, wherein the timeline includes an intraoperative surgical event marker corresponding to an intraoperative surgical event, and wherein the surgeon is enabled to click on the intraoperative surgical event marker to display at least one frame depicting the corresponding intraoperative surgical event.
67. The method of claim 61, wherein the case-specific information corresponding to the contemplated surgical procedure is received from an external device.
68. The method of claim 61, wherein comparing the case-specific information with data associated with the plurality of sets of surgical video footage includes using an artificial neural network to identify the group of intraoperative events likely to be encountered during the contemplated surgical procedure.
69. The method of claim 68, wherein using the artificial neural network includes providing the
case-specific information to the artificial neural network as an input.
70. The method of claim 61, wherein the case-specific information includes a characteristic of a patient associated with the contemplated procedure.
71. The method of claim 70, wherein the characteristic of the patient is received from a medical record of the patient.
72. The method of claim 71, wherein the case-specific information includes information relating to a surgical tool.
73. The method of claim 72, where the information relating to the surgical tool includes at least one of a tool type or a tool model.
74. The method of claim 71, wherein the common characteristic includes a characteristic of the differing patients.
75. The method of claim 61, wherein the common characteristic includes an intraoperative surgical event characteristic of the contemplated surgical procedure.
76. The method of claim 61, wherein determining that a first set and a second set of video footage from differing patients contain frames associated with intraoperative events sharing a common characteristic includes using an implementation of a machine learning model to identify the common characteristic.
77. The method of claim 76, wherein the method further comprises using example video footage to train the machine learning model to determine whether two sets of video footage share the common characteristic, and wherein implementing the machine learning model includes implementing the trained machine learning model.
78. The method of claim 61, wherein the method further comprises training a machine learning model to generate an index of the repository based on the intraoperative surgical events, the surgical outcomes, the patient characteristics, the surgeon characteristics, and the intraoperative surgical event characteristics; and generating the index of the repository, and wherein comparing the casespecific information with data associated with the plurality of sets includes searching the index.
79. A surgical preparation system, comprising: at least one processor configured to:
access a repository of a plurality of sets of surgical video footage reflecting a plurality of surgical procedures performed on differing patients and including intraoperative surgical events, surgical outcomes, patient characteristics, surgeon
characteristics, and intraoperative surgical event characteristics;
enable a surgeon preparing for a contemplated surgical procedure to input case-specific information corresponding to the contemplated surgical procedure;
compare the case-specific information with data associated with the plurality of sets of surgical video footage to identify a group of intraoperative events likely to be encountered during the contemplated surgical procedure;
use the case-specific information and the identified group of intraoperative events likely to be encountered to identify specific frames in specific sets of the plurality of sets of surgical video footage corresponding to the identified group of intraoperative events, wherein the identified specific frames include frames from the plurality of surgical procedures performed on differing patients;
determine that a first set and a second set of video footage from differing patients contain frames associated with intraoperative events sharing a common characteristic; omit an inclusion of the second set from a compilation to be presented to the surgeon and including the first set in the compilation to be presented to the surgeon; and enable the surgeon to view a presentation including the compilation and including frames from the differing surgical procedures performed on differing patients..
80. A non-transitory computer readable medium comprising instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling surgical preparation, the operations comprising:
accessing a repository of a plurality of sets of surgical video footage reflecting a plurality of surgical procedures performed on differing patients and including intraoperative surgical events, surgical outcomes, patient characteristics, surgeon characteristics, and intraoperative surgical event characteristics;
enabling a surgeon preparing for a contemplated surgical procedure to input case-specific information corresponding to the contemplated surgical procedure;
comparing the case-specific information with data associated with the plurality of sets of surgical video footage to identify a group of intraoperative events likely to be encountered during the contemplated surgical procedure;
using the case-specific information and the identified group of intraoperative events likely to be encountered to identify specific frames in specific sets of the plurality of sets of surgical video footage corresponding to the identified group of intraoperative events, wherein the identified specific frames include frames from the plurality of surgical procedures performed on differing patients; determining that a first set and a second set of video footage from differing patients contain frames associated with intraoperative events sharing a common characteristic;
omitting an inclusion of the second set from a compilation to be presented to the surgeon and including the first set in the compilation to be presented to the surgeon; and enabling the surgeon to view a presentation including the compilation and including frames from the differing surgical procedures performed on differing patients.
81. A computer-implemented method for analyzing complexity of surgical footage, the method
comprising:
analyzing frames of the surgical footage to identify in a first set of frames an anatomical structure;
accessing first historical data, the first historical data being based on an analysis of first frame data captured from a first group of prior surgical procedures;
analyzing the first set of frames using the first historical data and using the identified anatomical structure to determine a first surgical complexity level associated with the first set of frames;
analyzing frames of the surgical footage to identify in a second set of frames a medical tool, the anatomical structure, and an interaction between the medical tool and the anatomical structure;
accessing second historical data, the second historical data being based on an analysis of a second frame data captured from a second group of prior surgical procedures; and
analyzing the second set of frames using the second historical data and using the
identified interaction to determine a second surgical complexity level associated with the second set of frames.
82. The method of claim 81, wherein determining the first surgical complexity level further includes identifying in the first set of frames a medical tool.
83. The method of claim 81 , wherein determining the second surgical complexity level is based on time elapsed from the first set of frames to the second set of frames.
84. The method of claim 81, wherein at least one of determining the first complexity level or second complexity level is based on a physiological response.
85. The method of claim 81, wherein the method further comprises determining a level of skill
demonstrated by a healthcare provider in the surgical footage, and wherein at least one of determining the first complexity level or second complexity level is based on the determined level of skill demonstrated by the healthcare provider.
86. The method of claim 81, further comprising determining that the first surgical complexity level is less than a selected threshold, determining that the second surgical complexity level exceeds the selected threshold, and in response to the determination that the first surgical complexity level is less than the selected threshold and the determination that the second surgical complexity level exceeds the selected threshold, storing the second set of frames in a data structure while omitting the first set of frames from the data structure.
87. The method of claim 81, wherein identifying the anatomical structure in the first set of frames is based on an identification of a medical tool and a first interaction between the medical tool and the anatomical structure.
88. The method of claim 81, further comprising:
tagging the first set of frames with the first surgical complexity level;
tagging the second set of frames with the second surgical complexity level; and generating a data structure including the first set of frames with the first tag and the second set of frames with the second tag to enable a surgeon to select the second surgical complexity level, and thereby cause the second set of frames to be displayed, while omitting a display of the first set of frames.
89. The method of claim 81, further comprising using a machine learning model trained to identify
surgical complexity levels using frame data captured from prior surgical procedures to determine at least one of the first surgical complexity level or the second surgical complexity level.
90. The method of claim 81, wherein determining the second surgical complexity level is based on an event that occurred between the first set of frames and the second set of frames.
91. The method of claim 81 , wherein determining at least one of the first surgical complexity level or the second surgical complexity level is based on a condition of the anatomical structure.
92. The method of claim 81, wherein determining at least one of the first surgical complexity level or the second surgical complexity level is based on an analysis of an electronic medical record.
93. The method of claim 81, wherein determining the first surgical complexity level is based on an event that occurred after the first set of frames.
94. The method of claim 81, wherein determining at least one of the first surgical complexity level or the second surgical complexity level is based on a skill level of a surgeon associated with the surgical footage.
95. The method of claim 81, wherein determining the second surgical complexity level is based on an indication that an additional surgeon was called after the first set of frames.
96. The method of claim 81, wherein determining the second surgical complexity level is based on an indication that a particular medicine was administered after the first set of frames.
97. The method of claim 81 , wherein the first historical data includes a machine learning model trained using the first frame data captured from the first group of prior surgical procedures.
98. The method of claim 81, wherein the first historical data includes an indication of a statistical relation between a particular anatomical structure and a particular surgical complexity level.
99. A system for analyzing complexity of surgical footage, the system comprising:
at least one processor configured to:
analyze frames of the surgical footage to identify in a first set of frames an anatomical structure;
access first historical data, the first historical data being based on an analysis of first frame data captured from a first group of prior surgical procedures;
analyze the first set of frames using the first historical data and using the identified
anatomical structure to determine a first surgical complexity level associated with the first set of frames;
analyze frames of the surgical footage to identify in a second set of frames a medical tool, an anatomical structure, and an interaction between the medical tool and the anatomical structure;
access second historical data, the second historical data being based on an analysis of a second frame data captured from a second group of prior surgical procedures; and
analyze the second set of frames using the second historical data and using the identified interaction to determine a second surgical complexity level associated with the second set of frames.
100. A non-transitory computer readable medium comprising instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling surgical video review, the operations comprising:
analyzing frames of the surgical footage to identify in a first set of frames an anatomical structure;
accessing first historical data, the first historical data being based on an analysis of a first frame data captured from a first group of prior surgical procedures;
analyzing the first set of frames using the first historical data and using the identified anatomical structure to determine a first surgical complexity level associated with the first set of frames;
analyzing frames of the surgical footage to identify in a second set of frames a medical tool, an anatomical structure, and an interaction between the medical tool and the anatomical structure;
accessing second historical data, the second historical data being based on an analysis of a second frame data captured from a second group of prior surgical procedures; and
analyzing the second set of frames using the second historical data based on an analysis of frame data captured from prior surgical procedures and using the identified interaction to determine a second surgical complexity level associated with the second set of frames.
101. A non-transitory computer readable medium containing instructions that, when executed by at least one processor, cause the at least one processor to execute a method for enabling adjustments of an operating room schedule, the method comprising:
receiving, from an image sensor positioned in a surgical operating room, visual data tracking an ongoing surgical procedure;
accessing a data structure containing information based on historical surgical data;
analyzing the visual data of the ongoing surgical procedure using the data structure to determine an estimated completion time of the ongoing surgical procedure; accessing a schedule for the surgical operating room including a scheduled time
associated with completion of the ongoing surgical procedure;
calculating, based on the estimated completion time of the ongoing surgical procedure, whether an expected time of completion is likely to result in variance from the scheduled time associated with the completion; and
outputting a notification upon calculation of the variance, to thereby enable subsequent users of the surgical operating room to adjust their schedules accordingly.
102. The non-transitory computer readable medium of claim 101, wherein the notification includes an updated operating room schedule.
103. The non-transitory computer readable medium of claim 101 , wherein the updated operating room schedule enables a queued healthcare professional to prepare for a subsequent surgical procedure.
104. The non-transitory computer readable medium of claim 101, wherein the method further comprises electronically transmitting the notification to a device associated with a subsequent scheduled user of the surgical operating room.
105. The non-transitory computer readable medium of claim 101, wherein the method further comprises:
determining an extent of the variance from the scheduled time associated with the
completion;
in response to a first determined extent, outputting the notification; and in response to a second determined extent, forgoing outputting the notification.
106. The non-transitory computer readable medium of claim 101, wherein the method further comprises:
determining whether the expected time of completion is likely to result in a delay of at least a selected threshold amount of time from the scheduled time associated with the completion;
in response to a determination that the expected time of completion is likely to result in a delay of at least the selected threshold amount of time, outputting the notification; and
in response to a determination that the expected time of completion is not likely to result in a delay of at least the selected threshold amount of time, forgoing outputting the notification.
107. The non-transitory computer readable medium of claim 101, wherein the determining the estimated completion time is based on one or more stored characteristics associated with a healthcare professional conducting the ongoing surgical procedure.
108. The non-transitory computer readable medium of claim 101 , further comprising updating a historical average time to completion based on determined actual time to complete the ongoing surgical procedure.
109. The non-transitory computer readable medium of claim 101, wherein the image sensor is positioned above a patient.
110. The non-transitory computer readable medium of claim 101, wherein the image sensor is positioned on a surgical tool.
111. The non-transitory computer readable medium of claim 101 , wherein analyzing further includes detecting a characteristic event in the received visual data, assessing the information based on historical surgical data to determine an expected time to complete the surgical procedure following an occurrence of the characteristic event in the historical surgical data, and determining the estimated completion time based on the determined expected time to complete.
1 12. The non-transitory computer readable medium of claim 1 11, wherein the method further comprises using historical visual data to train a machine learning model to detect the characteristic event.
113. The non-transitory computer readable medium of claim 101, wherein the method further comprises using historical visual data to train a machine learning model to estimate completion times, and wherein calculating the estimated completion time includes implementing the trained machine learning model trained.
114. The non-transitory computer readable medium of claim 101, wherein the method further comprises using average historical completion times to determine the estimated completion time.
115. The non-transitory computer readable medium of claim 101 , wherein the method further comprises detecting a medical tool in the visual data, and wherein calculating the estimated completion time is based on the detected medical tool.
116. The non-transitory computer readable medium of claim 101, wherein analyzing further includes detecting an anatomical structure in the visual data, and wherein calculating the estimated completion time is based on the detected anatomical structure.
117. The non-transitory computer readable medium of claim 101, wherein analyzing further includes detecting an interaction between an anatomical structure and a medical tool in the visual data, and wherein calculating the estimated completion time is based on the detected interaction.
118. The non-transitory computer readable medium of claim 101 , wherein analyzing further includes determining a skill level of a surgeon in the visual data, and wherein calculating the estimated completion time is based on the determined skill level.
119. A system for enabling adjustments to an operating room schedule, the system comprising:
at least one processor configured to: receive from an image sensor positioned in a surgical operating room, visual data tracking an ongoing surgical procedure;
access a data structure containing information based on historical surgical data;
analyze the visual data of the ongoing surgical procedure using the data structure to determine an estimated completion time of the ongoing surgical procedure; access a schedule for the surgical operating room, including a scheduled time associated with completion of the ongoing surgical procedure;
calculate, based on the estimated completion time of the ongoing surgical procedure, whether an expected time of completion is likely to result in variance from the scheduled time associated with the completion; and
output a notification upon calculation of the variance, to thereby enable subsequent users of the surgical operating room to adjust their schedules accordingly.
120. A computerimplemented method for enabling adjustments to an operating room schedule, the method comprising:
receiving from an image sensor positioned in a surgical operating room, visual data tracking an ongoing surgical procedure;
accessing a data structure containing information based on historical surgical data; analyzing the visual data of the ongoing surgical procedure using the data structure to determine an estimated completion time of the ongoing surgical procedure; accessing a schedule for the surgical operating room including a scheduled time
associated with completion of the ongoing surgical procedure;
calculating, based on the estimated completion time of the ongoing surgical procedure, whether an expected time of completion is likely to result in variance from the scheduled time associated with the completion; and
outputting a notification upon calculation of the variance, to thereby enable subsequent users of the surgical operating room to adjust their schedules accordingly.
121. A computerimplemented method for analyzing surgical images to determine insurance
reimbursement, the method comprising:
accessing video frames captured during a surgical procedure on a patient; analyzing the video frames captured during the surgical procedure to identify in the video frames at least one medical instrument, at least one anatomical structure, and at least one interaction between the at least one medical instrument and the at least one anatomical structure;
accessing a database of reimbursement codes correlated to medical instruments,
anatomical structures, and interactions between medical instruments and anatomical structures;
comparing the identified at least one interaction between the at least one medical
instrument and the at least one anatomical structure with information in the database of reimbursement codes to determine at least one reimbursement code associated with the surgical procedure; and
outputting the at least one reimbursement code for use in obtaining an insurance
reimbursement for the surgical procedure.
122. The method of claim 121, wherein the at least one reimbursement code outputted includes a plurality of outputted reimbursement codes.
123. The method of claim 122, wherein at least two of the plurality of outputted reimbursement codes are based on differing interactions with a common anatomical structure.
124. The method of claim 123, wherein the at least two outputted reimbursement codes are determined based in part on detection of two differing medical instruments.
125. The method of claim 121, wherein determining the at least one reimbursement code is also based on an analysis of a postoperative surgical report.
126. The method of claim 121, wherein the video frames are captured from an image sensor positioned above the patient.
127. The method of claim 121, wherein the video frames are captured from an image sensor associated with a medical device.
128. The method of claim 121, further comprising updating the database by associating the at least one reimbursement code with the surgical procedure.
129. The method of claim 121, further comprising generating correlations between processed
reimbursement codes and at least one of a plurality of medical instruments in historical video footage, a plurality of anatomical structures in the historical video footage, or a plurality of interactions between medical instruments and anatomical structures in the historical video footage; and updating the database based on the generated correlations.
130. The method of claim 129, wherein generating correlations includes implementing a statistical model.
131. The method of claim 129, further comprising using a machine learning model to detect, in the historical video footage, the at least one plurality of medical instruments, plurality of anatomical structures, or plurality of interactions between medical instruments and anatomical structures.
132. The method of claim 121, further comprising analyzing the video frames captured during the surgical procedure to determine a condition of an anatomical structure of the patient; and determining the at least one reimbursement code associated with the surgical procedure based on the determined condition of the anatomical structure.
133. The method of claim 121, further comprising analyzing the video frames captured during the surgical procedure to determine a change in a condition of an anatomical structure of the patient during the surgical procedure; and determining the at least one reimbursement code associated with the surgical procedure based on the determined change in the condition of the anatomical structure.
134. The method of claim 121, further comprising analyzing the video frames captured during the surgical procedure to determine a usage of a particular medical device; and determining the at least one reimbursement code associated with the surgical procedure based on the determined usage of the particular medical device.
135. The method of claim 134, further comprising analyzing the video frames captured during the surgical procedure to determine a type of usage of the particular medical device; in response to a first determined type of usage, determining at least a first reimbursement code associated with the surgical procedure; and in response to a second determined type of usage, determining at least a second reimbursement code associated with the surgical procedure, the at least a first reimbursement code differing from the at least a second reimbursement code.
136. The method of claim 121, further comprising receiving a processed reimbursement code
associated with the surgical procedure, and updating the database based on the processed reimbursement code.
137. The method of claim 136, wherein the processed reimbursement code differs from a
corresponding reimbursement code of the at least one reimbursement codes.
138. The method of claim 121, further comprising analyzing the video frames captured during the surgical procedure to determine an amount of a medical supply of a particular type used in the surgical procedure; and determining the at least one reimbursement code associated with the surgical procedure based on the determined amount.
139. A surgical image analysis system for determining insurance reimbursement, the system
comprising:
at least one processor configured to:
access video frames captured during a surgical procedure on a patient; analyze the video frames captured during the surgical procedure to identify in the video frames at least one medical instrument, at least one anatomical structure, and at least one interaction between the at least one medical instrument and the at least one anatomical structure;
access a database of reimbursement codes correlated to medical instruments, anatomical structures, and interactions between medical instruments and anatomical structures;
compare the identified at least one interaction between the at least one medical
instrument and the at least one anatomical structure with information in the database of reimbursement codes to determine at least one reimbursement code associated with the surgical procedure; and
output the at least one reimbursement code for use in obtaining an insurance reimbursement for the surgical procedure.
140. A non-transitory computer readable medium containing instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling determination of insurance reimbursement, the operations comprising:
accessing video frames captured during a surgical procedure on a patient; analyzing the video frames captured during the surgical procedure to identify in the video frames at least one medical instrument, at least one anatomical structure, and at least one interaction between the at least one medical instrument and the at least one anatomical structure;
accessing a database of reimbursement codes correlated to medical instruments,
anatomical structures, and interactions between medical instruments and anatomical structures;
comparing the identified at least one interaction between the at least one medical
instrument and the at least one anatomical structure with information in the database of reimbursement codes to determine at least one reimbursement code associated with the surgical procedure; and
outputting the at least one reimbursement code for use in obtaining an insurance reimbursement for the surgical procedure.
141. A non-transitory computer readable medium containing instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling automatically populating a post-operative report of a surgical procedure, the operations comprising:
receiving an input of an identifier of a patient;
receiving an input of an identifier of a health care provider;
receiving an input of surgical footage of a surgical procedure performed on the patient by the health care provider;
analyzing a plurality of frames of the surgical footage to derive image-based information for populating a post-operative report of the surgical procedure; and causing the derived image-based information to populate the post-operative report of the surgical procedure.
142. The non-transitory computer readable medium of claim 141, wherein the operations further comprise analyzing the surgical footage to identify one or more phases of the surgical procedure and to identify a property of at least one phase of the identified phases; and wherein the derived image-based information is based on the identified at least one phase and the identified property of the at least one phase.
143. The non-transitory computer readable medium of claim 142, wherein the operations further comprise analyzing the surgical footage to associate a name with the at least one phase; and wherein the derived image-based information includes the name associated with the at least one phase.
144. The non-transitory computer readable medium of claim 142, wherein the operations further comprise determining at least a beginning of the at least one phase; and wherein the derived image-based information is based on the determined beginning.
145. The non-transitory computer readable medium of claim 142, wherein the operations further comprise associating a time marker with the at least one phase; and wherein the derived imagebased information includes the time marker associated with the at least one phase.
146. The non-transitory computer readable medium of claim 141, wherein the operations further comprise transmitting data to the health care provider, the transmitted data including the patient identifier and the derived image-based information.
147. The non-transitory computer readable medium of claim 141, wherein the operations further comprise analyzing the surgical footage to identify at least one recommendation for post operative treatment; and providing the identified at least one recommendation.
148. The non-transitory computer readable medium of claim 141, wherein the caused populating of the post-operative report of the surgical procedure is configured to enable the health care provider to alter at least part of the derived image-based information in the post-operative report.
149. The non-transitory computer readable medium of claim 141, wherein the caused populating of the post-operative report of the surgical procedure is configured to cause at least part of the derived image-based information to be identified in the post-operative report as automatically generated data
150. The non-transitory computer readable medium of claim 141, wherein the operations further comprise: analyzing the surgical footage to identify a surgical event within the surgical footage and to identify a property of the identified surgical event; and wherein the derived image-based information is based on the at identified surgical event and the identified property.
151. The non-transitory computer readable medium of claim 150, wherein the operations further comprise analyzing the surgical footage to determine an event name of the identified surgical event; and wherein the derived image-based information includes the determined event name.
152. The non-transitory computer readable medium of claim 150, wherein the operations further comprise associating a time marker with the identified surgical event; and wherein the derived image-based information includes the time marker.
153. The non-transitory computer readable medium of claim 141, wherein the operations further comprise providing the derived image-based information in a form enabling updating of an electronic medical record.
154. The non-transitory computer readable medium of claim 141, wherein the derived image-based information is based in part on user input.
155. The non-transitory computer readable medium of claim 141, wherein the derived image-based information comprises a first part associated with a first portion of the surgical procedure and a second part associated with a second portion of the surgical procedure, and wherein the operations further comprise: receiving a preliminary post-operative report;
analyzing the preliminary post-operative report to select a first position and a second position within the preliminary post-operative report, the first position is associated with the first portion of the surgical procedure and the second position is associated with the second portion of the surgical procedure; and
causing the first part of the derived image-based information to be inserted at the selected first position and the second part of the derived image-based information to be inserted at the selected second position.
156. The non-transitory computer readable medium of claim 141, wherein the operations further comprise analyzing the surgical footage to select at least part of at least one frame of the surgical footage; and causing the selected at least part of at least one frame of the surgical footage to be included in the post-operative report of the surgical procedure.
157. The non-transitory computer readable medium of claim 156, wherein the operations further comprise:
receiving a preliminary post-operative report; and
analyzing the preliminary post-operative report and the surgical footage to select the at least part of at least one frame of the surgical footage.
158. The non-transitory computer readable medium of claim 141, wherein the operations further comprise:
receiving a preliminary post-operative report;
analyzing the preliminary post-operative report and the surgical footage to identify at least one inconsistency between the preliminary post-operative report and the surgical footage; and
providing an indication of the identified at least one inconsistency.
159. A computer-implemented method of populating a post-operative report of a surgical procedure, the method comprising:
receiving an input of an identifier of a patient;
receiving an input of an identifier of a health care provider;
receiving an input of surgical footage of a surgical procedure performed on the patient by the health care provider;
analyzing a plurality of frames of the surgical footage to identify phases of the surgical procedure based on detected interactions between medical instruments and biological structures and, based on the interactions, associate a name with each identified phase;
determining at least a beginning of each identified phase;
associating a time marker with the beginning of each identified phase; transmitting data to the health care provider, the transmitted data including the patient identifier, the names of the identified phases of the surgical procedure, and time markers associated with the identified phases; and
populating a post-operative report with the transmitted data in a manner that enables the health care provider to alter the phase names in the post-operative report.
160. A system for automatically populating a post-operative report of a surgical procedure, the system comprising:
receive an input of an identifier of a patient;
receive an input of an identifier of a health care provider;
receive an input of surgical footage of a surgical procedure performed on the patient by the health care provider;
analyze a plurality of frames of the surgical footage to identify phases of the surgical procedure based on detected interactions between medical instruments and biological structures and, based on the interactions, associate a name with each identified phase;
determine at least a beginning of each identified phase;
associate a time marker with the beginning of each identified phase;
transmit data to the health care provider, the transmitted data including the patient
identifier, the names of the identified phases of the surgical procedure, and the time marker of the identified phase; and
populate a post-operative report with the transmitted data in a manner that enables the health care provider to alter the phase names in the post-operative report.
161. A non-transitory computer readable medium containing instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling determination and notification of an omitted event in a surgical procedure, the operations comprising:
accessing frames of video captured during a specific surgical procedure; accessing stored data identifying a recommended sequence of events for the surgical procedure;
comparing the accessed frames with the recommended sequence of events to identify an indication of a deviation between the specific surgical procedure and the recommended sequence of events for the surgical procedure;
determining a name of an intraoperative surgical event associated with the deviation; and providing a notification of the deviation including the name of the intraoperative surgical event associated with the deviation.
162. The non-transitory computer readable medium of claim 161, wherein identifying the indication of the deviation and providing the notification occurs in real time during the surgical procedure.
163. The non-transitory computer readable medium of claim 161, wherein the operations further
comprise: receiving an indication that a particular action is about to occur in the specific surgical procedure;
identifying, using the recommended sequence of events, a preliminary action to the particular action;
determining, based on an analysis of the accessed frames, that the identified preliminary action did not yet occurred;
in response to the determination that the identified preliminary action did not yet
occurred, identifying the indication of the deviation.
164. The non-transitory computer readable medium of claim 161, wherein the specific surgical procedure is a cholecystectomy.
165. The non-transitory computer readable medium of claim 161, wherein the recommended sequence of events is based on a critical view of safety.
166. The non-transitory computer readable medium of claim 161, wherein the specific surgical procedure is an appendectomy.
167. The non-transitory computer readable medium of claim 161, wherein the specific surgical procedure is a hernia repair.
168. The non-transitory computer readable medium of claim 161 , wherein the specific surgical procedure is a hysterectomy.
169. The non-transitory computer readable medium of claim 161, wherein the specific surgical procedure is a radical prostatectomy.
170. The non-transitory computer readable medium of claim 161 , wherein the specific surgical procedure is a partial nephrectomy, and the deviation includes neglecting to identify a renal hilum.
171. The non-transitory computer readable medium of claim 161, wherein the specific surgical procedure is a thyroidectomy, and the deviation includes neglecting to identify a recurrent laryngeal nerve
172. The non-transitory computer readable medium of claim 161, wherein the operations further comprise identifying a set of frames associated with the deviation, and wherein providing the notification includes displaying the identified set of frames associated with the deviation.
173. The non-transitory computer readable medium of claim 161, wherein the indication that the
particular action is about to occur is based on an input from a surgeon performing the specific surgical procedure.
174. The non-transitory computer readable medium of claim 163, wherein the indication that the
particular action is about to occur is an entrance of a particular medical instrument to a selected region of interest.
175. The non-transitory computer readable medium of claim 161 , wherein identifying the deviation comprises determining that a surgical tool is in a particular anatomical region.
176. The non-transitory computer readable medium of claim 161, wherein the specific surgical procedure is a hemicolectomy.
177. The non-transitory computer readable medium of claim 176, wherein the deviation includes neglecting to perform an anastomosis.
178. The non-transitory computer readable medium of claim 161 , where identifying the indication of the deviation is based on an elapsed time associated with an intraoperative surgical procedure.
179. A computer-implemented method for enabling determination and notification of an omitted event in a surgical procedure, the method comprising:
accessing frames of video captured during a specific surgical procedure; accessing stored data identifying a recommended sequence of events for the surgical procedure;
comparing the accessed frames with the recommended sequence of events to identify a deviation between the specific surgical procedure and the recommended sequence of events for the surgical procedure;
determining a name of an intraoperative surgical event associated with the deviation; and providing a notification of the deviation including the name of the intraoperative surgical event associated with the deviation.
180. A system for enabling determination and notification of an omitted event in a surgical procedure, the system comprising:
at least one processor configured to:
access frames of video captured during a specific surgical procedure;
access stored data identifying a recommended sequence of events for the surgical
procedure;
compare the accessed frames with the recommended sequence of events to identify a deviation between the specific surgical procedure and the recommended sequence of events for the surgical procedure;
determine a name of an intraoperative surgical event associated with the deviation; and provide a notification of the deviation including the name of the intraoperative surgical event associated with the deviation.
181. A non-transitory computer readable medium including instructions that, when executed by at least one processor, cause the at least one processor to execute operations that provide decision support for surgical procedures, the operations comprising:
receiving video footage of a surgical procedure performed by a surgeon on a patient in an operating room;
accessing at least one data structure including image-related data characterizing surgical procedures;
analyzing the received video footage using the image-related data to determine an
existence of a surgical decision making junction;
accessing, in the at least one data structure, a correlation between an outcome and a specific action taken at the decision making junction; and based on the determined existence of the decision making junction and the accessed correlation, outputting a recommendation to a user to undertake the specific action.
182. The non-transitory computer readable medium of claim 181, wherein the instructions are configured to cause the at least one processor to execute the operations in real time during the surgical procedure, and wherein the user is the surgeon.
183. The non-transitory computer readable medium of claim 181, wherein the decision making junction is determined by an analysis of a plurality of differing historical procedures where differing courses of action occurred following a common surgical situation.
184. The non-transitory computer readable medium of claim 181 , wherein the video footage includes images from at least one of an endoscope and an intracorporeal camera.
185. The non-transitory computer readable medium of claim 181, wherein the recommendation includes a recommendation to conduct a medical test.
186. The non-transitory computer readable medium of claim 185, wherein the operations further
comprise:
receiving a result of the medical test; and
based on the determined existence of the decision making junction, the accessed
correlation and the received result of the medical test, outputting a second recommendation to the user to undertake a particular action.
187. The non-transitory computer readable medium of claim 181, wherein the specific action includes brining an additional surgeon to the operating room.
188. The non-transitory computer readable medium of claim 181 , wherein the decision making junction includes at least one of inappropriate access or exposure, retraction of an anatomical structure, misinterpretation of an anatomical structure or a fluid leak.
189. The non-transitory computer readable medium of claim 181, wherein the recommendation includes a confidence level that a desired surgical outcome will occur if the specific action is taken.
190. The non-transitory computer readable medium of claim 181, wherein the recommendation includes a confidence level that a desired outcome will not occur if the specific action is not taken.
191. The non-transitory computer readable medium of claim 181, wherein the recommendation is based on time elapsed since a particular point in the surgical procedure.
192. The non-transitory computer readable medium of claim 181 , wherein the recommendation includes an indication of an undesired surgical outcome likely to occur if the specific action is not undertaken.
193. The non-transitory computer readable medium of claim 181 , wherein the recommendation is based on a skill level of the surgeon.
194. The non-transitory computer readable medium of claim 181, wherein the recommendation is based on a surgical event that occurred in the surgical procedure prior to the decision making junction.
195. The nontransitory computer readable medium of claim 181 , wherein the specific action includes a plurality of steps.
196. The non-transitory computer readable medium of claim 181, wherein the determination of the
existence of the surgical decision making junction is based on at least one of a detected physiological response of an anatomical structure and a motion associated with a surgical tool.
197. The non-transitory computer readable medium of claim 181, wherein the operations further comprise receiving a vital sign of the patient and wherein the recommendation is based on the accessed correlation and the vital sign.
198. The non-transitory computer readable medium of claim 181 , wherein the surgeon is a surgical robot and the recommendation is provided in the form of an instruction to the surgical robot.
199. The non-transitory computer readable medium of claim 181 , wherein the recommendation is based on a condition of a tissue of the patient.
200. The non-transitory computer readable medium of claim 181, wherein the recommendation of the specific action includes a creation of a stoma.
201. A computer-implemented method for providing decision support for surgical procedures, the method including:
receiving video footage of a surgical procedure performed by a surgeon on a patient in an operating room;
accessing at least one data structure including image-related data characterizing surgical procedures;
analyzing the received video footage using the image-related data to determine an
existence of a surgical decision making junction;
accessing, in the at least one data structure, a correlation between an outcome and a specific action taken at the decision making junction; and based on the determined existence of the decision making junction and the accessed correlation, outputting a recommendation to the surgeon to undertake the specific action or to avoid the specific action.
202. A system for providing decision support for surgical procedures, the system including:
at least on processor configured to:
receive video footage of a surgical procedure performed by a surgeon on a patient in an operating room;
access at least one data structure including image-related data characterizing surgical procedures;
analyze the received video footage using the image-related data to determine an
existence of a surgical decision making junction;
access, in the at least one data structure, a correlation between an outcome and a specific action taken at the decision making junction; and based on the determined existence of the decision making junction and the accessed correlation, output a recommendation to the surgeon to undertake the specific action or to avoid the specific action.
203. A non-transitory computer readable medium including instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling estimating contact force on an anatomical structure during a surgical procedure, the operations comprising:
receiving, from at least one image sensor in an operating room, image data of a surgical procedure;
analyzing the received image data to determine an identity of an anatomical structure and to determine a condition of the anatomical structure as reflected in the image data;
selecting a contact force threshold associated with the anatomical structure, the selected contact force threshold being based on the determined condition of the anatomical structure;
receiving an indication of actual contact force on the anatomical structure; comparing the indication of actual contact force with the selected contact force threshold; and
outputting a notification based on a determination that the indication of actual contact force exceeds the selected contact force threshold.
204. The non-transitory computer readable medium of claim 203, wherein the contact force threshold is associated with a tension level.
205. The non-transitory computer readable medium of claim 203, wherein the contact force threshold is associated with a compression level.
206. The non-transitory computer readable medium of claim 203, wherein the actual contact force is associated with a contact between a medical instrument and the anatomical structure.
207. The non-transitory computer readable medium of claim 203, wherein the indication of actual contact force is estimated based on an image analysis of the image data.
208. The non-transitory computer readable medium of claim 203, wherein outputting the notification includes providing a real time warning to a surgeon conducting the surgical procedure.
209. The non-transitory computer readable medium of claim 203, wherein the notification is an
instruction to a surgical robot.
210. The non-transitory computer readable medium of claim 203, wherein the operations further comprise determining from the image data that the surgical procedure is in a fight mode, and wherein the notification is suspended during the fight mode.
211. The non-transitory computer readable medium of claim 203, wherein the operations further comprise determining from the image data that the surgeon is operating in a mode ignoring contact force notifications, and suspending at least temporarily, further contact force notifications based on the determination that the surgeon is operating in the mode ignoring contact force notifications.
212. The non-transitory computer readable medium of claim 203, wherein selecting the contact force threshold is based on a location of contact between the anatomical structure and a medical instrument.
213. The non-transitory computer readable medium of claim 203, wherein selecting the contact force threshold is based on an angle of contact between the anatomical structure and a medical instrument.
214. The non-transitory computer readable medium of claim 203, wherein selecting the contact force threshold includes providing the condition of the anatomical structure to a regression model as an input, and selecting the contact force threshold based on an output of the regression model.
215. The non-transitory computer readable medium of claim 203, wherein selecting the contact force threshold is based on a table of anatomical structures including corresponding contact force thresholds.
216. The non-transitory computer readable medium of claim 203, wherein selecting the contact force threshold is based on actions performed by a surgeon.
217. The non-transitory computer readable medium of claim 203, wherein the indication of actual contact force is received from a surgical tool.
218. The non-transitory computer readable medium of claim 203, wherein the indication of actual contact force is received from a surgical robot.
219. The non-transitory computer readable medium of claim 203, wherein the operations further comprise using a machine learning model trained using training examples to determine the condition of the anatomical structure in the image data.
220. The non-transitory computer readable medium of claim 203, wherein the operations further comprise using a machine learning model trained using training examples to select the contact force threshold.
221. A computerimplemented method for estimating contact force on an anatomical structure during a surgical procedure, the method including:
receiving, from at least one image sensor in an operating room, image data of a surgical procedure;
analyzing the received image data to determine an identity of an anatomical structure and to determine a condition of the anatomical structure as reflected in the image data;
selecting a contact force threshold associated with the anatomical structure, the selected contact force threshold being based on the determined condition of the anatomical structure;
receiving an indication of actual contact force on the anatomical structure; comparing the indication of actual contact force with the selected contact force threshold; and outputting a notification based on a determination that the indication of actual contact force exceeds the selected contact force threshold.
222. A system for estimating contact force on an anatomical structure during a surgical procedure, the system including:
at least one processor configured to:
receive, from at least one image sensor in an operating room, image data of a surgical procedure;
analyze the received image data to determine an identity of an anatomical structure and to determine a condition of the anatomical structure as reflected in the image data; select a contact force threshold associated with the anatomical structure, the selected contact force threshold being based on the determined condition of the anatomical structure;
receive an indication of actual contact force on the anatomical structure; compare the indication of actual contact force with the selected contact force threshold; and
output a notification based on a determination that the indication of actual contact force exceeds the selected contact force threshold.
223. A non-transitory computer readable medium including instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling updating a predicted outcome during a surgical procedure, the operations comprising:
receiving, from at least one image sensor arranged to capture images of a surgical
procedure, image data associated with a first event during the surgical procedure; determining, based on the received image data associated with the first event, a predicted outcome associated with the surgical procedure;
receiving, from at least one image sensor arranged to capture images of a surgical
procedure, image data associated with a second event during the surgical procedure;
determining, based on the received image data associated with the second event, a change in the predicted outcome, causing the predicted outcome to drop below a threshold;
accessing a data structure of image-related data based on prior surgical procedures; identifying, based on the accessed image-related data, a recommended remedial action; and
outputting the recommended remedial action.
224. The non-transitory computer readable medium of claim 223, wherein the recommended remedial action includes a recommendation for a surgeon to take a break from the surgical procedure.
225. The non-transitory computer readable medium of claim 223, wherein the recommended remedial action includes a recommendation to request assistance from another surgeon.
226. The non-transitory computer readable medium of claim 223, wherein the recommended remedial action includes a revision to the surgical procedure.
227. The non-transitory computer readable medium of claim 223, wherein the predicted outcome includes a likelihood of hospital readmission.
228. The non-transitory computer readable medium of claim 223, wherein determining the change n the predicted outcome is based on a magnitude of bleeding.
229. The non-transitory computer readable medium of claim 223, wherein identifying the remedial action is based on an indication that the remedial action is likely to raise the predicted outcome above the threshold.
230. The non-transitory computer readable medium of claim 223, wherein identifying the remedial action includes using a machine learning model trained to identify remedial actions using historical examples of remedial actions and surgical outcomes.
231. The non-transitory computer readable medium of claim 223, wherein determining the predicted outcome includes using a machine learning model trained to determine predicted outcomes based on historical surgical videos and information indicating surgical outcome corresponding to the historical surgical videos.
232. The non-transitory computer readable medium of claim 223, wherein determining the predicted outcome includes identifying an interaction between a surgical tool and an anatomical structure, and determining the predicted outcome based on the identified interaction.
233. The non-transitory computer readable medium of claim 223, wherein determining the predicted outcome is based on a skill level of a surgeon depicted in the image data.
234. The non-transitory computer readable medium of claim 223, wherein the operations further comprise determining a skill level of a surgeon depicted in the image data; and wherein determining the change in the predicted outcome is based on the skill level.
235. The non-transitory computer readable medium of claim 223, wherein the operations further
comprise, in response to the predicted outcome dropping below a threshold, updating a scheduling record associated with a surgical room related to the surgical procedure.
236. The non-transitory computer readable medium of claim 223, wherein determining the change in the predicted outcome is based on a time elapsed between a particular point in the surgical procedure and the second event.
237. The non-transitory computer readable medium of claim 223, wherein determining the predicted outcome is based on a condition of an anatomical structure depicted in the image data.
238. The non-transitory computer readable medium of claim 237, wherein the operations further
comprising determining the condition of the anatomical structure.
239. The non-transitory computer readable medium of claim 223, wherein determining the change in the predicted outcome is based on a change of a color of at least part of the anatomical structure.
240. The non-transitory computer readable medium of claim 223, wherein determining the change in the predicted outcome is based on a change of appearance of at least part of the anatomical structure.
241. A computer-implemented method for updating a predicted outcome during a surgical procedure, the method including:
receiving, from at least one image sensor arranged to capture images of a surgical
procedure, image data associated with a first event during the surgical procedure; determining, based on the received image data associated with the first event, a predicted outcome associated with the surgical procedure;
receiving, from at least one image sensor arranged to capture images of a surgical
procedure, image data associated with a second event during the surgical procedure;
determining, based on the received image data associated with the second event, a change in the predicted outcome, causing the predicted outcome to drop below a threshold;
accessing a data structure of image-related data based on prior surgical procedures; identifying, based on the data structure, a recommended remedial action; and outputting the recommended remedial action.
242. A system for updating a predicted outcome during a surgical procedure, the system including:
at least one processor configured to:
receive, from at least one image sensor arranged to capture images of a surgical
procedure, image data associated with a first event during the surgical procedure; determine, based on the received image data associated with the first event, a predicted outcome associated with the surgical procedure;
receive, from at least one image sensor arranged to capture images of a surgical
procedure, image data associated with a second event during the surgical procedure;
determine, based on the received image data associated with the second event, a change in the predicted outcome, causing the predicted outcome to drop below a threshold;
access a data structure of image-related data based on prior surgical procedures;
identify, based on the data structure, a recommended remedial action; and output the recommended remedial action.
243. A computer-implemented method for analysis of fluid leakage during surgery, the method including:
receiving in real time, intracavitary video of a surgical procedure;
analyzing frames of the intracavitary video to determine an abnormal fluid leakage
situation in the intracavitary video; and
instituting a remedial action when the abnormal fluid leakage situation is determined.
244. The method of claim 243, wherein the fluid includes at least one of blood, bile or urine.
245. The method of claim 243, wherein analyzing includes analyzing the frames of the intracavitary video to identify a blood splash and at least one property of the blood splash, and wherein a selection of the remedial action depends on the at least one property of the identified blood splash.
246. The method of claim 245, wherein the at least one property is associated with a source of the blood splash.
247. The method of claim 245, wherein the at least one property is associated with an intensity of the blood splash.
248. The method of claim 245, wherein the at least one property is associated with a volume of the blood splash.
249. The method of claim 243, wherein analyzing the frames of the intracavitary video includes
determining a property of the abnormal fluid leakage situation, and wherein a selection of the remedial action depends on the determined property.
250. The method of claim 249, wherein the property is associated with a volume of the fluid leakage.
251. The method of claim 249, wherein the property is associated with a color of the fluid leakage.
252. The method of claim 249, wherein the property is associated with a type of fluid associated with the fluid leakage.
253. The method of claim 249, wherein the property is associated with a fluid leakage rate.
254. The method of claim 243, wherein the method further comprises storing the intracavitary video, and, upon determining the abnormal leakage situation, analyzing prior frames of the stored intracavitary video to determine a leakage source.
255. The method of claim 243, wherein instituting the remedial action includes providing a notification of a leakage source.
256. The method of claim 255, wherein determining the leakage source includes identifying a ruptured anatomical organ.
257. The method of claim 243, wherein the method further comprises determining a flow rate associated with the fluid leakage situation, and wherein instituting the remedial action is based on the flow rate.
258. The method of claim 243, wherein the method further comprises determining a volume of fluid loss associated with the fluid leakage situation, and wherein instituting the remedial action is based on the volume of fluid loss.
259. The method of claim 243, wherein analyzing frames of intracavitary video to determine an abnormal fluid leakage situation in intracavitary video comprises determining whether the determined fluid leakage situation is an abnormal fluid leakage situation, and wherein the method further comprises:
in response to a determination that the determined fluid leakage situation is an abnormal fluid leakage situation, instituting the remedial action; and in response to a determination that the determined fluid leakage situation is normal fluid leakage situation, forgoing institution of the remedial action.
260. The method of claim 243, wherein the intracavitary video depicts a surgical robot performing the surgical procedure, and the remedial action includes sending instructions to the robot.
261. A surgical system for analysis of fluid leakage, the system including:
at least one processor configured to:
receive in real time, intracavitary video of a surgical procedure;
analyze frames of the intracavitary video to determine an abnormal fluid leakage
situation in the intracavitary video; and
institute a remedial action when the abnormal fluid leakage situation is determined.
262. A non-transitory computer readable medium including instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling fluid leak detection, the operations comprising:
receiving in real time, intracavitary video of a surgical procedure;
analyzing frames of the intracavitary video to determine an abnormal fluid leakage
situation in the intracavitary video; and
instituting a remedial action when the abnormal fluid leakage situation is determined.
263. A computer-implemented method for predicting post-discharge risk, the method comprising:
accessing frames of video captured during a specific surgical procedure on a patient; accessing stored historical data identifying intraoperative events and associated
outcomes;
analyzing the accessed frames, and based on information obtained from the historical data, identifying in the accessed frames at least one specific intraoperative event; determining, based on information obtained from the historical data and the identified at least one intraoperative event, a predicted outcome associated with the specific surgical procedure; and
outputting the predicted outcome in a manner associating the predicted outcome with the patient.
264. The method of claim 263, wherein identifying the at least one specific intraoperative event is based on at least one of a detected surgical tool in the accessed frames, a detected anatomical structure in the accessed frames, an interaction in the accessed frames between a surgical tool and an anatomical structure, or a detected abnormal fluid leakage situation in the accessed frames.
265. The method of claim 263, wherein a machine learning model is used to identify in the accessed frames the at least one specific intraoperative event, the machine learning model trained using example training data.
266. The method of claim 263, wherein determining the predicted outcome is based on at least one of a characteristic of the patient, an electronic medical record, or a postoperative surgical report.
267. The method of claim 263, wherein a machine learning model is used to determine the predicted outcome associated with the specific surgical procedure based on intraoperative events, the machine learning model trained using training examples.
268. The method of claim 267, wherein determining a predicted outcome includes using the trained machine learning model to predict surgical outcomes based on the identified intraoperative event and an identified characteristic of the patient.
269. The method of claim 267, wherein the method further comprises receiving information identifying a realized surgical outcome following the surgical procedure and updating the machine learning model by training the machine learning model using the received information.
270. The method of claim 263, wherein the method further comprises identifying a characteristic of the patient, and wherein the predicted outcome is also determined based on the identified patient characteristic.
271. The method of claim 270, wherein the patient characteristic is derived from an electronic medical record.
272. The method of claim 270, wherein identifying the patient characteristic includes using a machine learning model to analyze the accessed frames, the machine learning model being trained to identify patient characteristics using training examples of historical surgical procedures and corresponding historical patient characteristics.
273. The method of claim 263, wherein the predicted outcome includes at least one of a post-discharge mishap, a post-discharge adverse event, a post-discharge complication, or an estimate of a risk of readmission.
274. The method of claim 263, further comprising accessing a data structure containing recommended sequences of surgical events, and wherein identifying the at least one specific intraoperative event is based on an identification of a deviation between a recommended sequence of events for the surgical procedure identified in the data structure, and an actual sequence of events detected in the accessed frames.
275. The method of claim 274, wherein the identification of the deviation is based on at least one of a detected surgical tool in the accessed frames, a detected anatomical structure in the accessed frames, or an interaction in the accessed frames between a surgical tool and an anatomical structure.
276. The method of claim 274, wherein the identification of the deviation includes using a machine
learning model trained to identify deviations from recommended sequences of events based on historical surgical video footage, historical recommended sequences of events, and information identifying deviations from the historical recommended sequences of events in the historical video footage.
277. The method of claim 274, wherein identifying the deviation includes comparing the accessed frames to reference frames depicting the recommended sequence of events.
278. The method of claim 263, wherein outputting the predicted outcome includes updating an electronic medical record associated with the patient.
279. The method of claim 263, wherein outputting the predicted outcome includes transmitting the
predicted outcome to a data-receiving device associated with a health care provider.
280. The method of claim 263, wherein the method further comprises determining at least one action likely to improve the predicted outcome based on the accessed frames, and providing a recommendation based on the determined at least one action.
281. A system for predicting post-discharge risk, the system comprising:
at least one processor configured to:
access frames of video captured during a specific surgical procedure on a patient;
access stored historical data identifying intraoperative events and associated outcomes; analyze the accessed frames, and based on information obtained from the historical data, identify in the accessed frames at least one specific intraoperative event;
determine, based on information obtained from the historical data and the identified at least one intraoperative event, a predicted outcome associated with the specific surgical procedure; and
output the predicted outcome in a manner associating the predicted outcome with the patient.
282. A non-transitory computer readable medium containing instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling prediction of post-discharge risk, the operations comprising:
accessing frames of video captured during a specific surgical procedure on a patient; accessing stored historical data identifying intraoperative events and associated
outcomes;
analyzing the accessed frames, and based on information obtained from the historical data, identifying in the accessed frames at least one specific intraoperative event; determining, based on information obtained from the historical data and the identified at least one intraoperative event, a predicted outcome associated with the specific surgical procedure; and
outputting the predicted outcome in a manner associating the predicted outcome with the patient.
</claims>
</document>
