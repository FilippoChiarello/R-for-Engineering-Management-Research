<document>

<filing_date>
2019-01-29
</filing_date>

<publication_date>
2020-07-30
</publication_date>

<priority_date>
2019-01-29
</priority_date>

<ipc_classes>
G06F9/38,G06F9/48,G06N20/00,G06N3/08
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
THEOCHAROUS, GEORGIOS
CHANDAK, YASH
</inventors>

<docdb_family_id>
71731227
</docdb_family_id>

<title>
GENERATING AND PROVIDING PROPOSED DIGITAL ACTIONS IN HIGH-DIMENSIONAL ACTION SPACES USING REINFORCEMENT LEARNING MODELS
</title>

<abstract>
The present disclosure relates to generating proposed digital actions in high-dimensional action spaces for client devices utilizing reinforcement learning models. For example, the disclosed systems can utilize a supervised machine learning model to train a latent representation decoder to determine proposed digital actions based on latent representations. Additionally, the disclosed systems can utilize a latent representation policy gradient model to train a state-based latent representation generation policy to generate latent representations based on the current state of client devices. Subsequently, the disclosed systems can identify the current state of a client device and a plurality of available actions, utilize the state-based latent representation generation policy to generate a latent representation based on the current state, and utilize the latent representation decoder to determine a proposed digital action from the plurality of available actions by analyzing the latent representation.
</abstract>

<claims>
1. A computer-implemented method for generating and providing digital action recommendations based on digital embeddings of action spaces, comprising: performing a step for training a supervised machine learning model comprising a latent representation decoder to generate proposed digital actions; performing a step for training a state-based latent representation generation policy, utilizing a latent representation policy gradient model, to generate latent representations from input states; identifying a current state of a client device and a plurality of available actions corresponding to the client device; and performing a step for generating a proposed digital action from the plurality of available actions and the current state utilizing the state-based latent representation generation policy and the latent representation decoder.
2. The method of claim 1, wherein the current state of the client device comprises at least one of digital content previously distributed to the client device or steps previously taken with respect to a process.
3. The method of claim 1, wherein the proposed digital action comprises at least one of digital content to distribute or a step to take with respect to a process.
4. The method of claim 1, wherein identifying the current state of the client device comprises determining an initial state of the client device from an initial distribution of states.
5. A non-transitory computer readable storage medium comprising instructions that, when executed by at least one processor, cause a computing device to: identify a current state of a client device and a plurality of available actions corresponding to the client device; and generate a proposed digital action for the client device from the plurality of available actions by: generating a latent representation based on the current state of the client device utilizing a state-based latent representation generation policy, wherein: the state-based latent representation generation policy is trained using a latent representation policy gradient model; the latent representation policy gradient model comprises a latent representation decoder; and the latent representation decoder is trained using a supervised machine learning model; and determining the proposed digital action from the plurality of available actions by analyzing the latent representation utilizing the latent representation decoder.
6. The non-transitory computer readable storage medium of claim 5, wherein: the plurality of available actions comprises a plurality of discrete actions, and the latent representation comprises a value from a set of continuous values corresponding to the plurality of discrete actions.
7. The non-transitory computer readable storage medium of claim 5, further comprising instructions that, when executed by the at least one processor, cause the client device to: identify a next state of the client device and a second plurality of available actions corresponding to the client device upon execution of the proposed digital action; and generate a second latent representation based on the next state of the client device utilizing the state-based latent representation generation policy.
8. The non-transitory computer readable storage medium of claim 7, further comprising instructions that, when executed by the at least one processor, cause the client device to determine a second proposed digital action from the second plurality of available actions by analyzing the second latent representation utilizing the latent representation decoder.
9. The non-transitory computer readable storage medium of claim 5, wherein the proposed digital action comprises at least one of digital content to distribute or a step to take with respect to a process.
10. The non-transitory computer readable storage medium of claim 5, wherein the instructions, when executed by the at least one processor, cause the computing device to identify the current state of the client device by determining an initial state of the client device from an initial distribution of states.
11. The non-transitory computer readable storage medium of claim 5, further comprising instructions that, when executed by the at least one processor, cause the computing device to: determine a reward resulting from executing the proposed digital action; and modify the state-based latent representation generation policy based on the reward.
12. A system comprising: at least one processor; and at least one non-transitory computer readable storage medium storing instructions that, when executed by the at least one processor, cause the system to: train a supervised machine learning model comprising a latent representation decoder to generate actions by: utilizing the supervised machine learning model to generate a predicted action from a first training current state and a first training next state; comparing the predicted action to a ground truth action corresponding to the first training current state and the first training next state; utilize a latent representation policy gradient model to train a state-based latent representation generation policy to generate latent representations for use in generating proposed digital actions by: generating a latent representation from a second training current state utilizing the state-based latent representation generation policy; utilizing the latent representation decoder of the supervised machine learning model to generate a proposed action from the latent representation; and modifying the state-based latent representation generation policy using a policy gradient, based on a second training next state and a training reward resulting from the proposed action.
13. The system of claim 12, wherein: the first training current state and the second training current state are the same; the first training next state and the second training next state are the same; and the ground truth action and the proposed action are the same.
14. The system of claim 12, wherein: the supervised machine learning model comprises a latent representation generator and the latent representation decoder, and utilizing the supervised machine learning model to generate the predicted action from the first training current state and the first training next state comprises: utilizing the latent representation generator to generate a second latent representation based on the first training current state and the first training next state; and determining the predicted action by analyzing the second latent representation utilizing the latent representation decoder.
15. The system of claim 14, further comprising instructions that, when executed by the at least one processor, cause the system to train the supervised machine learning model by modifying internal parameters of the latent representation generator using a loss function, based on comparing the predicted action to the ground truth action corresponding to the first training current state and the first training next state.
16. The system of claim 14, wherein the latent representation generator comprises a neural network.
17. The system of claim 12, further comprising instructions that, when executed by the at least one processor, cause the system to utilize the latent representation policy gradient model to train the state-based latent representation generation policy by determining the second training next state and the training reward based on execution of the proposed action.
18. The system of claim 17, further comprising instructions that, when executed by the at least one processor, cause the system to provide the proposed action to a client device, wherein determining the second training next state and the training reward based on execution of the proposed action comprises determining the second training next state and the training reward based on execution of the proposed action by the client device.
19. The system of claim 12, further comprising instructions that, when executed by the at least one processor, cause the system to train the supervised machine learning model by modifying internal parameters of the latent representation decoder using a loss function, based on comparing the predicted action to the ground truth action corresponding to the first training current state and the first training next state.
20. The system of claim 12, wherein: the latent representation comprises a value from a set of continuous values corresponding to a plurality of discrete actions associated with the second training current state, and the plurality of discrete actions associated with the second training current state comprises the proposed action.
</claims>
</document>
