<document>

<filing_date>
2020-02-10
</filing_date>

<publication_date>
2020-10-01
</publication_date>

<priority_date>
2019-03-22
</priority_date>

<ipc_classes>
G06T7/70
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
BOGO, FEDERICA
POLLEFEYS, MARC ANDRE LEON
TEKIN, Bugra
</inventors>

<docdb_family_id>
69743978
</docdb_family_id>

<title>
PREDICTING THREE-DIMENSIONAL ARTICULATED AND TARGET OBJECT POSE
</title>

<abstract>
A data processing system is provided that includes a processor having associated memory, the processor being configured to execute instructions using portions of the memory to cause the processor to, at classification time, receive an input image frame from an image source. The input image frame includes an articulated object (42) and a target object (44). The processor is further caused to process the input image frame using a trained neural network configured to, for each input cell of a plurality of input cells in the input image frame predict a three-dimensional articulated object pose of the articulated object (42) and a three-dimensional target object pose of the target object (44) relative to the input cell. The processor is further caused to output the three-dimensional articulated object pose and the three- dimensional target object pose from the neural network.
</abstract>

<claims>
1. A data processing system, comprising:
a processor having associated memory, the processor being configured to execute instructions using portions of the memory to cause the processor to:
at classification time:
receive an input image frame from an image source, the input image frame including an articulated object and a target object;
process the input image frame using a trained neural network configured to, for each input cell of a plurality of input cells in the input image frame predict a three-dimensional articulated object pose of the articulated object and a three-dimensional target object pose of the target object; and
output the three-dimensional articulated object pose and the three-dimensional target object pose from the neural network.
2. The data processing system of claim 1,
wherein the processor is configured to further process the input image frame by, for each input cell of the plurality of input cells in the input image frame, computing an articulated object presence confidence value and a target object presence confidence value for the input cell; and
output the articulated object presence confidence value and target object presence confidence value with the three-dimensional articulated object pose and the threedimensional target object pose.
3. The data processing system of claim 2, wherein the processor is configured to compute the articulated object presence confidence value and the target object presence confidence value based upon respective Euclidean distances of the predicted threedimensional articulated object pose and the predicted three-dimensional target object pose to corresponding ground truths of the articulated object pose and target object pose of the trained neural network.
4. The data processing system of claim 1, wherein the image source is selected from the group consisting of an image data store and an image sensor.
5. The data processing system of claim 4, wherein the image source is the image sensor, and the image sensor is configured as a visual light camera being configured to capture visual light and/or a depth camera configured to capture infrared light, and wherein the image sensor is configured to capture images of a type selected from the group consisting of a grayscale image, a red-green-blue depth (RGBD) image, an infrared image, and an infrared-depth image.
6. The data processing system of claim 1, wherein the trained neural network is a single shot feedforward fully convolutional neural network.
7. The data processing system of claim 1, wherein each input cell is a pixel or region of pixels.
8. The data processing system of claim 7, wherein to predict each of the threedimensional articulated object pose and three-dimensional target object pose the processor is configured to estimate offsets from a predetermined location in the input cell to each of a plurality of a control points for the three-dimensional articulated object pose and threedimensional target object pose.
9. The data processing system of claim 7, wherein the processor is further configured to, based on estimated three-dimensional locations of the plurality of control points, estimate a six degrees of freedom (6DoF) vector from the estimated offsets by applying a transformation to the estimated offsets.
10. The data processing system of claim 1, wherein the processor is further configured to, at classification time:
compute one or more candidate target object classes for the target object and a respective target object class probability for each of the one or more candidate target object classes; and
compute one or more candidate action classes for the articulated object and a respective action class probability for each of the one or more action classes; and
output the one or more candidate target object classes and candidate target object class probabilities and the one or more candidate action classes and candidate action class probabilities with the three-dimensional articulated object pose and the three-dimensional target object pose, from the neural network.
11. The data processing system of claim 10,
wherein an interaction recurrent neural network is utilized to compute one or more candidate interaction classes and interaction class probabilities.
12. The data processing system of claim 11, wherein, to train the interaction RNN, a composite input of three-dimensional articulated object pose and three-dimensional target object pose data is fed to the interaction RNN to enable the interaction RNN to learn dependencies between the three-dimensional articulated object pose and three-dimensional target object pose.
13. The data processing system of claim 12, wherein the processor is further configured to output the three-dimensional articulated object pose, the three-dimensional target object pose, the one or more candidate target object classes and the target object class probabilities, and the one or more candidate action classes and action class probabilities in a composite data structure to a program for downstream processing.
14. A method comprising:
at a processor during a classification time:
receiving an input image frame from an image source, the input image frame including an articulated object and a target object;
processing the input image frame using a trained neural network configured to, for each input cell of a plurality of input cells in the input image frame predicting a threedimensional articulated object pose of the articulated object and a three-dimensional target object pose of the target object relative to the input cell; and
outputting the three-dimensional articulated object pose and the threedimensional target object pose from the neural network.
15. The method of claim 14, further comprising:
computing one or more candidate target object classes for the target object and a respective target object class probability for each of the one or more candidate target object classes;
computing one or more candidate action classes for the articulated object and a respective action class probability for each of the one or more action classes; and
outputting the one or more candidate target object classes and candidate target object class probabilities and the one or more candidate action classes and candidate action class probabilities with the three-dimensional articulated object pose and the three-dimensional target object pose, from the neural network.
</claims>
</document>
