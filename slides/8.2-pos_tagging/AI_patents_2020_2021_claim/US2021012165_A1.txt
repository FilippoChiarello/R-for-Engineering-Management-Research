<document>

<filing_date>
2019-03-13
</filing_date>

<publication_date>
2021-01-14
</publication_date>

<priority_date>
2018-03-29
</priority_date>

<ipc_classes>
G06F16/2458,G06F16/28,G06K9/62
</ipc_classes>

<assignee>
SHANGHAI ZTTVISION TECHNOLOGIES CO.LTD
</assignee>

<inventors>
JIANG HONG
</inventors>

<docdb_family_id>
63782936
</docdb_family_id>

<title>
DATA PROCESSING METHOD AND DEVICE BASED ON MULTI-SENSOR FUSION, AND MULTI-SENSOR FUSION METHOD
</title>

<abstract>
A data processing method, device and multi-sensor fusion method for multi-sensor fusion, which can group data captured by different sensors in different probe dimensions to simultaneous interpreting deep learning data based on pixel elements in the multi-dimensional matrix structure, thereby realize the more effective data mining and feature extraction to support more effective ability of environment perception and target detection.
</abstract>

<claims>
1. A data processing method of multi-sensor fusion, which is characterized in that, the method comprises: The image data of the target object and at least one set of probe data groups are obtained; the image data is detected by the image acquisition sensor, and the probe data group is detected by other sensors; the image data is used to represent the target image collected by the image acquisition sensor by using at least one pixel data matrix; different probe data groups are the probe data in different information captured dimensions from other sensors; Form a multi-dimensional matrix structure: The multi-dimensional matrix structure includes a plurality of vertically distributed matrix layers, the multiple matrix layers include at least one pixel matrix layer and at least one probe matrix layer, each pixel matrix layer corresponds to a pixel data matrix, each probe matrix layer is used to represent a set of probe data groups, and the probe data elements in the probe data group correspond to the image pixel elements in the image matrix layer; the values of the probe data elements are determined according to the value assignment of the probe data.
2. The method according to claim 1, which is characterized in that the multi-dimensional matrix structure is formed, including: According to the established mapping relationship, the pixel elements of the target object corresponding to each probe data of the target object are determined, the mapping relationship is used to represent the mapping relationship between the probe data at different positions of different probe dimensions in the detection domain of the other sensors and the pixel elements; Assigning the probe data to the probe data element corresponding to the corresponding pixel element; the process of establishing the mapping relationship comprises: The detection coordinate system of each sensor is changed through data processing, and the central axis of the detection domain of each sensor is consistent with the optical axis of the image acquisition sensor, and the detection vertex of the detection domain of each sensor coincides with the entrance pupil center of the image acquisition sensor; According to the optical axis and the pupil center of the image acquisition sensor location, after the change of the detection coordinate system the detection region of each sensor is projected to the two-dimensional plane where the imaging surface of the image acquisition sensor is located, and the projection area corresponding to each sensor is obtained; According to the position relationship between the projection area in the two-dimensional plane and the imaging area of the image acquisition sensor, the mapping relationship is determined.
3. The method according to claim 1, which is characterized in that if the resolution of the other sensors does not match the resolution of the image acquisition sensor, then: The data elements in the probe matrix layer also include a first interpolation data element, and/or: the data elements in the pixel matrix layer also include a second interpolation data element; The value of the probe data element is the corresponding probe data itself, or the value of the probe data element is determined according to the corresponding probe data conversion; The matrix layers in the multi-dimensional matrix structure can be selectively activated.
4. The method according to claim 1, which is characterized in that after forming the multi-dimensional matrix structure, the method also comprises: Any one or more reference target objects detected in the detection domain of the other sensors are positioned, and the target positioning result is obtained; the target positioning result is used to represent the position of the reference target object in the detection space; Mapping the position represented by the target positioning result to the pixel matrix layer to obtain the verification positioning information; The positioning error information is obtained by comparing the calibration positioning information with the original positioning information; the original positioning information is used to represent the position of the pixel elements determined in the pixel matrix layer when the multi-dimensional matrix structure is formed for the same reference target object; According to the positioning error information, adjust the corresponding probe matrix layer to change the correspondence between the sensor matrix layer and the pixel matrix layer; The assignment value of the probe data element in the probe matrix layer is adjusted according to the changed correspondence relationship.
5. The method according to claim 1, which is characterized in that the other sensors are at least one of the following: microwave radar, ultrasonic radar, Lidar, infrared sensor, and terahertz imaging sensor; the probe data of the other sensors includes at least one of the following: range data or distance data, velocity data, acceleration data, orientation data, radar RCS data, and thermal radiation temperature data; The pixel data matrix includes at least one of the following: luminance data matrix, grayscale data matrix, RGB data matrix, R data matrix, G data matrix, B data matrix, YUV data matrix, Y data matrix, U data matrix, V data matrix and optical flow data matrix.
6. A data processing device for multi-sensor fusion, which is characterized in that it comprises: The acquisition module is used to obtain the image data of the target object and at least one set of probe data group; the image data is detected by the image acquisition sensor, and the probe data group is detected by other sensors; the image data is used to characterize the target image collected by the image acquisition sensor by using at least one pixel data matrix; different probe data groups are the probe data in different information captured dimensions from other sensors; A forming module is used to form a multi-dimensional matrix structure; The multi-dimensional matrix structure includes a plurality of vertically distributed matrix layers, the multiple matrix layers include at least one pixel matrix layer and at least one probe matrix layer, each pixel matrix layer corresponds to a pixel data matrix, each probe matrix layer is used to represent a set of probe data groups, and the probe data elements in the probe data group correspond to the image longitudinally; The values of the probe data elements are determined according to the value assignment of the probe data.
7. A multi-sensor fusion method, which is characterized in that: The multi-dimensional probe data from multi-sensor are combined in the way of multi-dimensional pixel matrix, and a multi-dimensional depth perception matrix array with camera pixels as granularity (base unit) is established; In the multi-dimensional pixel matrix, the information contained in each pixel is vertically expanded, a plurality of vertical dimensions are added for each pixel in addition to the originally contained brightness and color information, and the detection information of multiple corresponding dimensions detected by other sensors on the target object mapped by the pixel in the camera detection space can be input into the added vertical dimensionâ€”The information includes at least one of the following: distance, velocity, radar target RCS data, and target thermal radiation temperature distribution data; among them, the multi-dimensional detection information is assembled in a layered way on the object description which originally takes the image pixel as the unit to generate multi-dimensional pixels represented as a matrix array with a unified structure mathematically, so that each original pixel becomes a multi-dimensional pixel therefore the multi-dimensional pixel matrix is obtained.
8. The multi-sensor fusion method according to claim 1, which is characterized in that the camera is a color camera or a monochromatic camera; If the camera is a color camera, the output data matrix of the color camera is RGB or YUV, and the multi-dimensional pixel matrix is obtained by encapsulating the probe information of other sensors on the three-layer data matrix; If the camera is a monochromatic camera, the camera only outputs monochromatic images, and the multi-dimensional pixel matrix is obtained by combining the probe information of other sensors on the basis of single-layer pixel brightness matrix;
9. The multi-sensor fusion method according to claim 1, which is characterized in that the multi-dimensional probe data from the multi-sensor are combined in the form of multi-dimensional pixel matrix, before establishing a multi-dimensional depth perception matrix array with camera pixels as the granularity (base unit), the method also includes: A unified detection domain of the system is established, and the mapping relationship of the detection space of the multi-sensor combination is transformed into the three-dimensional detection space of the camera through the spatial coordinate transformation as the follows: Through the translation, rotation and zooming of the respective coordinate systems, they are unified into the camera's 3D detection space coordinates, so that they are aligned with the camera's optical axis to establish the system's unified detection space and common detection perspective; Then, according to the mapping relationship of the detection field of other sensors (except the camera) on the 2-dimensional object surface corresponding to the imaging surface of the camera, the detection data from multiple dimensions of the multi-sensor is combined in the form of a multi-dimensional pixel matrix, and a multi-dimensional depth perception matrix array with camera pixels as granularity is established, such as: According to the mapping relationship, the target objects detected by other sensors are corresponding to each pixel imaged by the camera, and the detection information of various corresponding dimensions detected by other sensors of the target object mapped by the pixel in the detection space of the camera are assigned to the corresponding positions in the multi-dimensional pixel matrix one by one.
10. The multi-sensor fusion method according to claim 1, which is characterized in that after the target object is detected, the multiple detected targets in the detection domain are independently positioned in the respective sensor detection space area, and the final result is mapped to the corresponding matrix array of multi-dimensional pixels to compare the positioning position error of the same target with the previous geometric space conversion method, then zoom and translate the relative position of the corresponding matrix array and the pixel layer of the camera to reduce the error, at the same time, the values in the corresponding data matrix array (layer) of the multi-dimensional pixels are adjusted according to the new vertical correspondence relationship.
11. The method according to claim 2, which is characterized in that the other sensors are at least one of the following: microwave radar, ultrasonic radar, Lidar, infrared sensor, and terahertz imaging sensor; the probe data of the other sensors includes at least one of the following: range data or distance data, velocity data, acceleration data, orientation data, radar RCS data, and thermal radiation temperature data; The pixel data matrix includes at least one of the following: luminance data matrix, grayscale data matrix, RGB data matrix, R data matrix, G data matrix, B data matrix, YUV data matrix, Y data matrix, U data matrix, V data matrix and optical flow data matrix
12. The method according to claim 3, which is characterized in that the other sensors are at least one of the following: microwave radar, ultrasonic radar, Lidar, infrared sensor, and terahertz imaging sensor; the probe data of the other sensors includes at least one of the following: range data or distance data, velocity data, acceleration data, orientation data, radar RCS data, and thermal radiation temperature data; The pixel data matrix includes at least one of the following: luminance data matrix, grayscale data matrix, RGB data matrix, R data matrix, G data matrix, B data matrix, YUV data matrix, Y data matrix, U data matrix, V data matrix and optical flow data matrix.
13. The method according to claim 4, which is characterized in that the other sensors are at least one of the following: microwave radar, ultrasonic radar, Lidar, infrared sensor, and terahertz imaging sensor; the probe data of the other sensors includes at least one of the following: range data or distance data, velocity data, acceleration data, orientation data, radar RCS data, and thermal radiation temperature data; The pixel data matrix includes at least one of the following: luminance data matrix, grayscale data matrix, RGB data matrix, R data matrix, G data matrix, B data matrix, YUV data matrix, Y data matrix, U data matrix, V data matrix and optical flow data matrix.
</claims>
</document>
