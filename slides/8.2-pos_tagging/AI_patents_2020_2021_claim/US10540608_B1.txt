<document>

<filing_date>
2015-05-22
</filing_date>

<publication_date>
2020-01-21
</publication_date>

<priority_date>
2015-05-22
</priority_date>

<ipc_classes>
G06N20/00,G06N5/04
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
</assignee>

<inventors>
DIRAC, LEO PARKER
NAMBIAR, RAKESH MADHAVAN
RAVICHANDRAN, AVINASH AGHORAM
</inventors>

<docdb_family_id>
69167191
</docdb_family_id>

<title>
Dynamically scaled training fleets for machine learning
</title>

<abstract>
A first set of execution platforms is deployed for a set of operations of a training phase of a machine learning model. Prior to the completion of the training phase, a triggering condition for deployment of a different set of execution platforms is detected. The different set of execution platforms is deployed for a subsequent set of training phase operations.
</abstract>

<claims>
1. A system, comprising: one or more computing devices of a machine learning service implemented at a provider network; wherein the one or more computing devices include one or more respective hardware processors and associated memory storing program instructions that are executable on the one or more hardware processors to cause the one or more computing devices to: receive an indication of a requirement to train a machine learning model; identify a parallel technique to be employed during a training phase of the machine learning model, wherein the parallel technique is configured to be performed in parallel on a set of execution platforms and includes transfer of model parameter synchronization data among the set of execution platforms, and wherein convergence towards an optimization goal of the parallel technique is expected to be correlated with a reduction in the amount of the model parameter synchronization data transferred among the set of execution platforms; determine (a) a first subset of a plurality of execution platforms of the machine learning service to be deployed to perform at least a first set of operations of the training phase using the parallel technique; and (b) one or more conditions during the training phase which are to trigger a deployment of a second subset of the plurality of execution platforms to perform at least a second set of operations of the training phase using the parallel technique subsequent to the first set of operations, wherein the number of execution platforms included in the second subset differs from the number of execution platforms included in the first subset; detect, during the training phrase and using one or more metrics collected during performance of the first set of operations using the parallel technique, that a particular condition of the one or more conditions has been met; deploy, during the training phrase and responsive to the detection, the second subset of execution platforms to perform the second set of operations using the parallel technique; and utilize, after the training phase is terminated in response to a detection that a training goal of the machine learning model has been achieved, the machine learning model to generate one or more predictions.
2. The system as recited in claim 1, wherein to detect that the particular condition has been met, the one or more computing devices determine that an amount of synchronization data transferred among execution platforms implementing the training phase has reached a threshold.
3. The system as recited in claim 1, wherein the training phase includes one or more traversals of a particular training data set, wherein to detect that the particular condition has been met, the one or more computing devices determine that at least a particular fraction of a particular traversal of the one or more traversals has been completed.
4. The system as recited in claim 1, wherein the machine learning model comprises one or more of: (a) a neural network model, (b) a natural language processing model, (c) a logistic regression model, (d) a decision tree model, (e) an ensemble model, (f) a Gaussian process model, (g) a time series model, (h) a regression model other than a logistic regression model or (i) a classification model.
5. The system as recited in claim 1, wherein the parallel technique includes one or more of: (a) a stochastic gradient descent technique, (b) a Broyden-Fletcher-Goldfarb-Shanno (BFGS) technique, or (c) a limited memory BFGS (LMBFGS) technique.
6. A method, comprising: performing, at one or more computing devices: determining (a) a first subset of a plurality of execution platforms to be deployed to perform at least a first set of operations of a training phase of a machine learning model in parallel using a parallel technique; and (b) one or more conditions which are to trigger, prior to a completion of the training phase, a deployment of a second subset of the plurality of execution platforms to perform at least a different set of operations of the training phase using the parallel technique; detecting, during the training phrase, that a particular condition of the one more conditions has been met during performance of the first set of operations using the parallel technique; deploying, during the training phrase and responsive to the detection, the second subset of execution platforms to perform the different set of operations of the training phase using the parallel technique; and terminating the training phase in response to a detection that a training goal of the machine learning model has been attained.
7. The method as recited in claim 6, wherein said detecting that the particular condition has been met includes determining that network bandwidth usage associated with the training phase has reached a threshold.
8. The method as recited in claim 6, wherein the training phase includes an analysis of a particular training data set, wherein said detecting that the particular condition has been met includes determining that analysis of at least a particular subset of the particular training data set has been completed.
9. The method as recited in claim 6, wherein said detecting that the particular condition has been met includes determining that a resource capacity utilization metric associated with one or more execution platforms of the plurality of execution platforms meets a threshold criterion.
10. The method as recited in claim 6, wherein the first set of operations includes an implementation of a first optimization technique, and wherein said detecting that the particular condition has been met includes determining that a different optimization technique is to be employed for the different set of operations.
11. The method as recited in claim 6, wherein said detecting that the particular condition has been met includes determining that a number of unassigned execution platforms of the plurality of execution platforms meets a threshold criterion.
12. The method as recited in claim 6, wherein the number of execution platforms in the second subset differs from the number of execution platforms in the first subset.
13. The method as recited in claim 6, wherein the first subset of execution platforms includes a first execution platform with a particular performance capacity, and wherein the second subset of execution platforms includes a second execution platform with a different performance capacity.
14. The method as recited in claim 6, further comprising: receiving, from a client via a programmatic interface prior to said detecting that the particular condition has been met, an indication of one or more of: (a) an approval of execution platform deployment changes during the training phase, (b) the particular condition, (c) one or more properties of the first subset of the plurality of execution platforms, or (d) one or more properties of the second subset of the plurality of execution platforms.
15. The method as recited in claim 6, wherein at least the first set of operations of the training phase includes determining one or more gradients of a stochastic gradient descent technique.
16. A non-transitory computer-accessible storage medium storing program instructions that when executed on one or more processors cause the one or more processors to: determine a first subset of a plurality of execution platforms to be deployed to perform at least a first set of operations of a training phase of a machine learning model using a parallel technique; detect, prior to completion of the training phase, that a triggering condition for a deployment of a second subset of the plurality of execution platforms to perform at least a different set of operations of the training phase has been met, wherein the second subset of execution platforms are configured to perform the different set of operations using the parallel technique; and initiate, during the training phase and responsive to the detection, a deployment of the second subset of execution platforms to perform the different set of operations of the training phase.
17. The non-transitory computer-accessible storage medium as recited in claim 16, wherein to detect that the triggering condition has been met, the program instructions when executed on the one or more processors determine that an amount of synchronization data transferred among execution platforms implementing the training phase meets a threshold.
18. The non-transitory computer-accessible storage medium as recited in claim 16, wherein the training phase includes a plurality of traversals of a particular training data set, wherein to detect that the triggering condition has been met, the program instructions when executed on the one or more processors determine that at least one traversal of the plurality of traversals has been completed.
19. The non-transitory computer-accessible storage medium as recited in claim 16, wherein to detect the triggering condition, the program instructions when executed on the one or more processors determine that a resource utilization level associated with the plurality of execution platforms exceeds a threshold, and wherein the number of execution platforms in the second subset is smaller than the number of execution platforms in the first subset.
20. The non-transitory computer-accessible storage medium as recited in claim 16, wherein the first subset of execution platforms includes a first execution platform with a particular processor architecture, and wherein the second subset of execution platforms includes a second execution platform with a different processor architecture.
</claims>
</document>
