<document>

<filing_date>
2017-11-03
</filing_date>

<publication_date>
2020-03-03
</publication_date>

<priority_date>
2017-11-03
</priority_date>

<ipc_classes>
G05D1/10,G06K9/66,G06N20/00,G06T7/593,G06T7/70
</ipc_classes>

<assignee>
ZOOX
</assignee>

<inventors>
MOUSAVIAN, ARSALAN
PHILBIN, JAMES WILLIAM VAISEY
</inventors>

<docdb_family_id>
69645495
</docdb_family_id>

<title>
Dense depth estimation of image data
</title>

<abstract>
Systems, devices, and methods are described for generating dense depth estimates, and confidence values associated with such depth estimates, from image data. A machine learning algorithm can be trained using image data and associated depth values captured by one or more LIDAR sensors providing a ground truth. When the algorithm is deployed in a machine vision system, image data and/or depth data can be used to determine dense depth estimates for all pixels of the image data, as well as confidence values for each depth estimate. Such confidence values may be indicative of how confident the machine learned algorithm is of the associated depth estimate.
</abstract>

<claims>
1. A system comprising: one or more processors; and one or more computer-readable media storing instructions executable by the one or more processors, wherein the instructions program the one or more processors to: receive image data captured by at least one image sensor of an autonomous vehicle, the image data including a plurality of pixels; determine a depth estimate associated with a pixel of the plurality of pixels using a machine learned algorithm, the depth estimate providing an indication of a distance between the at least one image sensor and an object represented by the pixel; determine a confidence value associated with the depth estimate using the machine learned algorithm; and generate a trajectory for the autonomous vehicle based at least in part on the image data, the depth estimate, and the confidence value, wherein the machine learned algorithm is trained based at least in part by: receiving a first plurality of data points; receiving ground truth data captured by at least one depth sensor of the autonomous vehicle, the ground truth data including a second plurality of data points, wherein the second plurality of data points comprises fewer data points than the first plurality of data points; clustering the ground truth data into a plurality of clusters; and determining a first portion of clusters as input depth data for training, wherein the first portion of clusters represents a subset of the ground truth data.
2. The system of claim 1, wherein the instructions, when executed by the one or more processors, further program the one or more processors to: determine depth estimates associated with each pixel of the plurality of pixels; and determine individual confidence values associated with each depth estimate.
3. The system of claim 1, wherein the instructions, when executed by the one or more processors, further program the one or more processors to: receive depth data from at least one LIDAR sensor of the autonomous vehicle; and determine the depth estimate associated with the pixel based at least in part on the depth data.
4. The system of claim 3, wherein the instructions, when executed by the one or more processors, further program the one or more processors to: receive, for each pixel of the plurality of pixels, a depth value indicator indicating whether the depth data exists for each pixel, wherein the depth value indicator is based at least in part on a transformation of the depth data into a reference frame of the image data.
5. The system of claim 1, wherein the machine learned algorithm comprises a convolutional neural network, and wherein the first plurality of data points is determined based at least in part on training image data received from the at least one image sensor.
6. The system of claim 5, wherein at least one of the image data, the training image data, or the ground truth data includes simulated data.
7. The system of claim 1, wherein the instructions, when executed by the one or more processors, further program the one or more processors to: determine the depth estimate based at least in part on a rectified linear unit (ReLU) activation function; and determine the confidence value based at least in part on a sigmoid activation function.
8. A method comprising: receiving image data captured by at least one image sensor, the image data including a plurality of pixels; determining a depth estimate associated with a pixel of the plurality of pixels using an algorithm, the depth estimate providing an indication of a distance between a first point associated with the at least one image sensor and an object represented by the pixel; determining a confidence value associated with the depth estimate using the algorithm; and providing the image data, the depth estimate, and the confidence value to at least one of a perception system or a planning system of an autonomous vehicle, wherein the algorithm is a machine learned algorithm trained based at least in part by: receiving a first plurality of data points; receiving ground truth data from at least one depth sensor, the ground truth data including a second plurality of data points; clustering the ground truth data into a plurality of clusters; and determining a first portion of clusters as input depth data for training, wherein the first portion of clusters represents a subset of the ground truth data.
9. The method of claim 8, further comprising: receiving depth data from at least one LIDAR sensor; and determine the depth estimate associated with the pixel based at least in part on the depth data.
10. The method of claim 9, further comprising: receiving, for each pixel of the plurality of pixels, a depth value indicator indicating whether the depth data exists for each pixel, wherein the depth value indicator is based at least in part on a transformation of the depth data into a reference frame of the image data.
11. The method of claim 10, wherein the depth data is a sparse data set relative to a number of the plurality of pixels associated with the image data.
12. The method of claim 8, further comprising: determining depth estimates associated with each of the plurality of pixels; and determining individual confidence values associated with each depth estimate.
13. The method of claim 8, wherein the algorithm comprises a convolutional neural network, and wherein the first plurality of data points is determined based at least in part on training image data received from the at least one image sensor.
14. The method of claim 8, further comprising: determining the depth estimate based at least in part on a rectified linear unit (ReLU) activation function; and determining the confidence value based at least in part on a sigmoid activation function.
15. A non-transitory computer-readable medium having a set of instructions that, when executed, cause one or more processors to perform operations comprising: receiving image data captured by at least one image sensor, the image data including a plurality of image elements; determining a depth estimate associated with an image element of the plurality of image elements using a machine learned algorithm, the depth estimate providing an indication of a distance between a first point associated with the at least one image sensor and an object represented by the image element; determining a confidence value associated with the depth estimate; and providing the image data, the depth estimate, and the confidence value to at least one of a perception system or a planning system of an autonomous vehicle, wherein the machine learned algorithm is trained based at least in part by: receiving a first plurality of data points; receiving ground truth data from at least one depth sensor, the ground truth data including a second plurality of data points; clustering the ground truth data into a plurality of clusters; and determining a first portion of clusters as input depth data for training, wherein the first portion of clusters represents a subset of the ground truth data.
16. The non-transitory computer-readable medium of claim 15, the operations further comprising: receiving depth data from at least one LIDAR sensor; and determine the depth estimate associated with the image element based at least in part on the depth data.
17. The non-transitory computer-readable medium of claim 16, the operations further comprising: determining, for each image element of the plurality of image elements, a depth value indicator indicating whether the data exists for each pixel, wherein the depth value indicator is based at least in part on a transformation of the depth data into a reference frame of the image data.
18. The non-transitory computer-readable medium of claim 15, wherein the machine learned algorithm comprises a convolutional neural network, and wherein the first plurality of data points is determined based at least in part on training image data received from the at least one image sensor.
19. The system of claim 3, wherein the depth data is a sparse data set relative to a number of the plurality of pixels associated with the image data.
20. The non-transitory computer-readable medium of claim 16, wherein the depth data is a sparse data set relative to a number of the plurality of image elements associated with the image data.
</claims>
</document>
