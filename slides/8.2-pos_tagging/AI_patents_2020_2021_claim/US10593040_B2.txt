<document>

<filing_date>
2019-02-19
</filing_date>

<publication_date>
2020-03-17
</publication_date>

<priority_date>
2012-03-28
</priority_date>

<ipc_classes>
A61B5/00,G06T7/00,H04N7/18,H04N9/47
</ipc_classes>

<assignee>
UNIVERSITY OF HOUSTON
</assignee>

<inventors>
ZOURIDAKIS, GEORGE
</inventors>

<docdb_family_id>
49261255
</docdb_family_id>

<title>
Methods for screening and diagnosing a skin condition
</title>

<abstract>
Provided herein are digital-implemented methods for performing simultaneous analyses on an object on the skin of an animal body, for example, a human, to classify the object as a skin cancer, an ulcer or neither. The analyses are performed simultaneously on a hand-held imaging device.
</abstract>

<claims>
1. A digital processor-implemented method for performing simultaneous analyses of an object on the skin of an animal body, comprising the processor executable steps of: imaging the object with a hand-held imaging device; performing simultaneously on the image of the object an analysis of an object of interest as a skin cancer and an analysis of the object as an ulcer; and classifying the object as a skin cancer or as an ulcer or as neither from results obtained from the skin cancer analysis and the ulcer analysis, wherein the step of performing simultaneously the analysis of the object of interest comprises the processor executable steps of: segmenting the imaged object to detect a border of the object; and extracting features from the segmented object image, wherein the processor executable step of segmenting comprises: determining an initial contour of the imaged object; classifying pixels as contained within the initial contour as foreground, as contained without the initial contour as background or as remaining pixels if they are not classified as foreground pixels or background pixels; and applying a supervised classifier to the remaining pixels for classification as foreground or background, wherein the processor executable step of extracting comprises: dividing the segmented object image into regions based on saliency values calculated for at least one patch within the segmented object; dividing the regions into two regions of higher or lower intensity based on average intensity values thereof; and extracting feature representations from a sampling of patches within the intensity regions based on sampling percentages determined for the regions, and wherein the analysis is for an ulcer and is performed simultaneously with steps for classifying the skin cancer, said processor executable step of classifying comprising: inputting the extracted feature representations from the object imaged on the skin into a support vector machine trained with manually segmented objects from an ulcer; and classifying the object as an ulcer or as not an ulcer based on a comparison of the inputted extracted features with those in the support vector machine trained on a known ulcer.
2. The method of claim 1, further comprising: displaying the results as each result occurs.
3. The method of claim 1, wherein the analysis is for a skin cancer, said processor executable step of classifying comprising: inputting the extracted feature representations from the object imaged on the skin into a support vector machine trained with manually segmented objects from a skin cancer; and classifying the object as a skin cancer or as not a skin cancer based on a comparison of the inputted extracted features with those in the support vector machine trained on a known skin cancer.
4. The method of claim 3, wherein the skin cancer is a melanoma.
5. The method of claim 1, wherein the ulcer is a Buruli ulcer.
6. The method of claim 1, wherein the hand-held imaging device is a smart device.
7. The method of claim 1, wherein the animal body is a human body.
8. The method of claim 1, wherein performing the simultaneous analyses occurs in real time.
9. A digital processor-implemented method for performing in real time simultaneous analyses of an object on human skin, comprising the processor executable steps of: imaging the object with a portable smart device; performing simultaneously on the image of the object an analysis of an object of interest as a skin cancer and an analysis of the object as an ulcer; classifying the object as a skin cancer or as an ulcer or as neither from results obtained from the skin cancer analysis and the ulcer analysis; and displaying the results as each result occurs, wherein the analysis is for a skin cancer, said processor executable steps comprising: a) obtain luminance and color components of the imaged object; b) classify pixels comprising the image as object pixels, if they belong to a common luminance and color foreground, as background pixels if they belong to a common luminance and color background or as remaining pixels if they are not classified as foreground pixels or background pixels; and c) apply a supervised classifier to the remaining pixels to classify them as object or background; d) calculate a saliency value for a plurality of patches within the segmented object and separate the patches into regions based on the saliency values; e) calculate an average intensity for the regions to identify them as a higher or as a lower intensity region; f) determine a sampling percentage for the intensity regions; g) sample patches within the intensity regions by corresponding sampling percentages; h) extract one or more feature representations for the object; i) training a support vector machine (SVM) with known manually segmented objects from a skin cancer; and j)classifying the object as a skin cancer or not a skin cancer based on the extracted feature representations inputted into the SVM trained with the manually segmented objects from a known skin cancer, and wherein the analysis is for an ulcer, said processor executable steps comprising: performing simultaneously steps b) to h) for classifying the skin cancer, on the image obtained in step a), and wherein the analysis is for an ulcer, said processor executable steps comprising: performing simultaneously steps b) to h) for classifying the skin cancer, on the image obtained in step a); i) training a support vector machine (SVM) with known manually segmented objects from an ulcer; and j) classifying the object as an ulcer or not an ulcer based on the extracted feature representations inputted into the SVM trained with the manually segmented objects from a known ulcer.
10. The method of claim 9, wherein the skin cancer is a melanoma.
11. The method of claim 9, wherein the ulcer is a Buruli ulcer.
12. The method of claim 9, said method further comprising processor executable steps of: reading input white light image as RGB and the segmentation result of the region; reading input multispectral images in color channels and transform to gray scale; registering multispectral images via maximization of mutual information with white light image as reference; extracting feature representations within the region of interest of multispectral images and within white light images; and selecting one or more relevant features from a pool of the extracted features.
13. The method of claim 9, said method further comprising processor executable steps of: reading input white light image as RGB and the segmentation result of the region; reading input multispectral images in color channels and transform to gray scale; registering multispectral images via maximization of mutual information with white light image as reference; determining a volume fraction of melanin (Vmel), a volume fraction of blood (Vblood), and a fraction of blood that is oxygenated (Voxy) for each region of interest ROI pixel to reconstruct maps of melanin, blood and oxygenating percentage; extracting feature representations within the region of interest from the reconstructed maps; and selecting one or more relevant features from a pool of the extracted features.
</claims>
</document>
