<document>

<filing_date>
2019-08-12
</filing_date>

<publication_date>
2020-02-06
</publication_date>

<priority_date>
2014-11-14
</priority_date>

<ipc_classes>
G06F40/40,G06N3/04
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
BENGIO, SAMY
TOSHEV, ALEXANDER TOSHKOV
ERHAN, DUMITRU
VINYALS, ORIOL
</inventors>

<docdb_family_id>
54834905
</docdb_family_id>

<title>
Generating natural language descriptions of images
</title>

<abstract>
Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating descriptions of input images. One of the methods includes obtaining an input image; processing the input image using a first neural network to generate an alternative representation for the input image; and processing the alternative representation for the input image using a second neural network to generate a sequence of a plurality of words in a target natural language that describes the input image.
</abstract>

<claims>
1. A method performed by one or more computers, the method comprising: obtaining an input image; processing the input image using a first neural network to generate an alternative representation for the input image; and processing the alternative representation for the input image using a second neural network to generate a sequence of a plurality of words in a target natural language that describes the input image.
2. The method of claim 1, wherein the first neural network is a deep convolutional neural network.
3. The method of claim 2, wherein the deep convolutional neural network comprises a plurality of core neural network layers each having a respective set of parameters, wherein processing the input image using the first neural network comprises processing the input through each of the core neural network layers in the sequence, and wherein the alternative representation for the input image is the output generated by a last core neural network layer in the plurality of core neural network layers.
4. The method of claim 3, wherein current values of the respective sets of parameters are determined by training a third neural network on a plurality of training images, and wherein the third neural network includes the plurality of core layers and an output layer configured to, for each training image, receive the output generated by the last core layer for the training image and generate a respective score for each of a plurality of object categories, the respective score for each of the plurality of object categories representing a predicted likelihood that the training image contains an image of an object from the object category.
5. The method of claim 1, wherein the second neural network is a long-short term memory (LSTM) neural network.
6. The method of claim 5, wherein the LSTM neural network is configured to: receive as input a current word in the sequence, map the current word to a numeric representation of the current word, and process the numeric representation to generate, in accordance with a current hidden state of the LSTM neural network and current values of a set of parameters of the LSTM neural network, a respective word score for each word in a set of words that represents a respective likelihood that the word is a next word in the sequence.
7. The method of claim 6, wherein the alternative representation of the input image has the same dimensionality as the numeric representation.
8. The method of claim 6, wherein the set of words includes a vocabulary of words in the target natural language and a special stop word.
9. The method of claim 6, wherein processing the alternative representation for the input image using the LSTM neural network comprises initializing the hidden state of the LSTM neural network using the alternative representation for the input image.
10. The method of claim 9, wherein the words in the sequence are arranged according to an output order, and wherein processing the alternative representation for the input image further comprises, for an initial position in the output order: processing a special start word using the LSTM neural network to generate a respective initial word score for each word in the set of words; and selecting a word from the set of words as a word at the initial position in the output order using the initial word scores.
11. The method of claim 10, wherein processing the alternative representation for the input image further comprises, for each position after the initial position in the output order: processing the word at a preceding position in the output order using the LSTM neural network to generate a respective next word score for each word in the set of words; and selecting a word from the set of words as a word at the position in the output order using the next word scores.
12. The method of claim 11, wherein processing the alternative representation for the input image comprises: determining that, at a particular position in the output order, the selected word for the particular position is the special stop word; and in response, outputting as the sequence of words that describe the input image, the words selected at the positions before the particular position in the output order.
13. The method of claim 5, wherein processing the alternative representation for the input image using the second neural network comprises: processing the alternative representation using the LSTM neural network using a left to right beam search decoding to generate a plurality of possible sequences and a respective sequence score for each of the possible sequences; and selecting one or more highest-scoring possible sequences as descriptions of the input image.
14. A system comprising one or more computers and one or more storage devices storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising: obtaining an input image; processing the input image using a first neural network to generate an alternative representation for the input image; and processing the alternative representation for the input image using a second neural network to generate a sequence of a plurality of words in a target natural language that describes the input image.
15. The system of claim 14, wherein the first neural network is a deep convolutional neural network.
16. The system of claim 15, wherein the deep convolutional neural network comprises a plurality of core neural network layers each having a respective set of parameters, wherein processing the input image using the first neural network comprises processing the input through each of the core neural network layers in the sequence, and wherein the alternative representation for the input image is the output generated by a last core neural network layer in the plurality of core neural network layers.
17. The system of claim 16, wherein current values of the respective sets of parameters are determined by training a third neural network on a plurality of training images, and wherein the third neural network includes the plurality of core layers and an output layer configured to, for each training image, receive the output generated by the last core layer for the training image and generate a respective score for each of a plurality of object categories, the respective score for each of the plurality of object categories representing a predicted likelihood that the training image contains an image of an object from the object category.
18. The system of claim 14, wherein the second neural network is a long-short term memory (LSTM) neural network.
19. The system of claim 18, wherein the LSTM neural network is configured to: receive as input a current word in the sequence, map the current word to a numeric representation of the current word, and process the numeric representation to generate, in accordance with a current hidden state of the LSTM neural network and current values of a set of parameters of the LSTM neural network, a respective word score for each word in a set of words that represents a respective likelihood that the word is a next word in the sequence.
20. A computer program product encoded on one or more non-transitory computer storage media, the computer program product comprising instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising: obtaining an input image; processing the input image using a first neural network to generate an alternative representation for the input image; and processing the alternative representation for the input image using a second neural network to generate a sequence of a plurality of words in a target natural language that describes the input image.
</claims>
</document>
