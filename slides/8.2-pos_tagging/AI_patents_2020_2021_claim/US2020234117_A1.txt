<document>

<filing_date>
2018-08-24
</filing_date>

<publication_date>
2020-07-23
</publication_date>

<priority_date>
2017-08-25
</priority_date>

<ipc_classes>
G06F16/901,G06N3/08,G06N5/04
</ipc_classes>

<assignee>
GOOGLE
</assignee>

<inventors>
HAFNER, DANIJAR
</inventors>

<docdb_family_id>
63490786
</docdb_family_id>

<title>
BATCHED REINFORCEMENT LEARNING
</title>

<abstract>
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for batched reinforcement learning. For example, the batched reinforcement learning techniques can be used to determine a control policy for a robot in simulation and the control policy can then be used to control the robot in the real world. In one aspect, a method includes obtaining a plurality of current observations, each current observation characterizing a current state of a respective environment replica; processing the current observations in parallel using the action selection neural network in accordance with current values of the network parameters to generate an action batch; obtaining a transition tuple batch comprising a respective transition tuple for each of the environment replicas, the respective transition tuple for each environment replica comprising: (i) a subsequent observation and (ii) a reward; and training the action selection neural network on the batch of transition tuples.
</abstract>

<claims>
1. A method of training an action selection neural network to select actions to be performed by an agent interacting with an environment, wherein the action selection neural network has a plurality of network parameters and is configured to receive an input observation and to process the input observation in accordance with the network parameters to generate a network output that defines an action to be performed by the agent in response to the input observation, and wherein the method comprises: obtaining an observation batch comprising a plurality of current observations, each current observation characterizing a current state of a respective one of a plurality of environment replicas; processing the current observations in the observation batch in parallel using the action selection neural network in accordance with current values of the network parameters to generate an action batch that includes, for each environment replica, a respective action to be performed by the agent in response to the current observation characterizing the current state of the environment replica; obtaining a transition tuple batch comprising a respective transition tuple for each of the environment replicas, the respective transition tuple for each environment replica comprising: (i) a subsequent observation characterizing a subsequent state that the environment replica transitioned into as a result of the agent performing the action for the environment replica in the action batch, and (ii) a reward generated as a result of the environment replica transitioning into the subsequent state; and training the action selection neural network on the batch of transition tuples to update the current values of the network parameters using a reinforcement learning technique.
2. The method of claim 1, wherein each environment replica is maintained inside of a separate process.
3. The method of claim 2, wherein obtaining a transition tuple batch comprising a respective transition tuple for each of the environment replicas comprises: providing each of the respective actions in the action batch to the process that maintains the environment replica corresponding to the action to cause the environment replicas to transition into the respective subsequent states in parallel; and obtaining, from each of the processes, the subsequent observation and the reward for the environment replica maintained inside of the process.
4. The method of claim 3, wherein obtaining the transition tuple batch further comprises: after the subsequent observation and the reward have been obtained from all of the processes, generating the transition tuple batch from the data obtained from the processes.
5. The method of claim 2, wherein processing the current observations in the observation batch in parallel using the action selection neural network in accordance with current values of the network parameters to generate an action batch comprises: executing an inference subgraph of a computation graph, wherein the inference subgraph performs batched inference for the action selection neural network on the current observations in the observation batch to generate a respective network output for each current observation and selects a respective action from each network output.
6. The method of claim 5, wherein training the action selection neural network on the batch of transition tuples to update the current values of the network parameters using a reinforcement learning technique comprises: executing a training subgraph of the computation graph that takes as input the training tuple batch and the current values of the network parameters and applies the reinforcement learning technique to the training tuples in the batch to generate update values of the network parameters.
7. The method of claim 2, wherein obtaining a transition tuple batch comprising a respective transition tuple for each of the environment replicas comprises: issuing respective calls in parallel to each of the external processes with the actions for the environment replicas; waiting until a subsequent observation and a reward are obtained from each of the processes in response to the respective calls; and after determining that a subsequent observation and a reward have been obtained from each of the processes, generating the transition tuple batch using the obtained subsequent observations and rewards.
8. The method of claim 1, wherein the reinforcement learning technique is a proximal policy optimization (PPO) algorithm.
9. A system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to perform operations for training an action selection neural network to select actions to be performed by an agent interacting with an environment, wherein the action selection neural network has a plurality of network parameters and is configured to receive an input observation and to process the input observation in accordance with the network parameters to generate a network output that defines an action to be performed by the agent in response to the input observation, and wherein the method comprises: obtaining an observation batch comprising a plurality of current observations, each current observation characterizing a current state of a respective one of a plurality of environment replicas; processing the current observations in the observation batch in parallel using the action selection neural network in accordance with current values of the network parameters to generate an action batch that includes, for each environment replica, a respective action to be performed by the agent in response to the current observation characterizing the current state of the environment replica; obtaining a transition tuple batch comprising a respective transition tuple for each of the environment replicas, the respective transition tuple for each environment replica comprising: (i) a subsequent observation characterizing a subsequent state that the environment replica transitioned into as a result of the agent performing the action for the environment replica in the action batch, and (ii) a reward generated as a result of the environment replica transitioning into the subsequent state; and training the action selection neural network on the batch of transition tuples to update the current values of the network parameters using a reinforcement learning technique.
10. One or more non-transitory computer-readable storage media storing instructions that when executed by one or more computers cause the one or more computers to perform operations for training an action selection neural network to select actions to be performed by an agent interacting with an environment, wherein the action selection neural network has a plurality of network parameters and is configured to receive an input observation and to process the input observation in accordance with the network parameters to generate a network output that defines an action to be performed by the agent in response to the input observation, and wherein the method comprises: obtaining an observation batch comprising a plurality of current observations, each current observation characterizing a current state of a respective one of a plurality of environment replicas; processing the current observations in the observation batch in parallel using the action selection neural network in accordance with current values of the network parameters to generate an action batch that includes, for each environment replica, a respective action to be performed by the agent in response to the current observation characterizing the current state of the environment replica; obtaining a transition tuple batch comprising a respective transition tuple for each of the environment replicas, the respective transition tuple for each environment replica comprising: (i) a subsequent observation characterizing a subsequent state that the environment replica transitioned into as a result of the agent performing the action for the environment replica in the action batch, and (ii) a reward generated as a result of the environment replica transitioning into the subsequent state; and training the action selection neural network on the batch of transition tuples to update the current values of the network parameters using a reinforcement learning technique.
11. The system of claim 9, wherein each environment replica is maintained inside of a separate process.
12. The system of claim 11, wherein obtaining a transition tuple batch comprising a respective transition tuple for each of the environment replicas comprises: providing each of the respective actions in the action batch to the process that maintains the environment replica corresponding to the action to cause the environment replicas to transition into the respective subsequent states in parallel; and obtaining, from each of the processes, the subsequent observation and the reward for the environment replica maintained inside of the process.
13. The system of claim 12, wherein obtaining the transition tuple batch further comprises: after the subsequent observation and the reward have been obtained from all of the processes, generating the transition tuple batch from the data obtained from the processes.
14. The system of claim 11, wherein processing the current observations in the observation batch in parallel using the action selection neural network in accordance with current values of the network parameters to generate an action batch comprises: executing an inference subgraph of a computation graph, wherein the inference subgraph performs batched inference for the action selection neural network on the current observations in the observation batch to generate a respective network output for each current observation and selects a respective action from each network output.
15. The system of claim 14, wherein training the action selection neural network on the batch of transition tuples to update the current values of the network parameters using a reinforcement learning technique comprises: executing a training subgraph of the computation graph that takes as input the training tuple batch and the current values of the network parameters and applies the reinforcement learning technique to the training tuples in the batch to generate update values of the network parameters.
16. The system of claim 11, wherein obtaining a transition tuple batch comprising a respective transition tuple for each of the environment replicas comprises: issuing respective calls in parallel to each of the external processes with the actions for the environment replicas; waiting until a subsequent observation and a reward are obtained from each of the processes in response to the respective calls; and after determining that a subsequent observation and a reward have been obtained from each of the processes, generating the transition tuple batch using the obtained subsequent observations and rewards.
17. The system of claim 9, wherein the reinforcement learning technique is a proximal policy optimization (PPO) algorithm.
18. The non-transitory computer-readable storage media of claim 10, wherein each environment replica is maintained inside of a separate process.
19. The non-transitory computer-readable storage media of claim 18, wherein obtaining a transition tuple batch comprising a respective transition tuple for each of the environment replicas comprises: providing each of the respective actions in the action batch to the process that maintains the environment replica corresponding to the action to cause the environment replicas to transition into the respective subsequent states in parallel; and obtaining, from each of the processes, the subsequent observation and the reward for the environment replica maintained inside of the process.
20. The non-transitory computer-readable storage media of claim 19, wherein obtaining the transition tuple batch further comprises: after the subsequent observation and the reward have been obtained from all of the processes, generating the transition tuple batch from the data obtained from the processes.
</claims>
</document>
