<document>

<filing_date>
2018-05-03
</filing_date>

<publication_date>
2020-10-20
</publication_date>

<priority_date>
2018-05-03
</priority_date>

<ipc_classes>
G01C21/30,G01C21/34,G06K9/00,G06T19/00,G08G1/005,G08G1/017,H04W4/02
</ipc_classes>

<assignee>
ZOOX
</assignee>

<inventors>
KENTLEY-KLAY, TIMOTHY DAVID
KOHEN, MICHAEL MOSHE
AUSTRIA, AUVER CEDRIC
Bass, Donovan Anton
Curtis, Duncan John
</inventors>

<docdb_family_id>
72838791
</docdb_family_id>

<title>
User interface and augmented reality for identifying vehicles and persons
</title>

<abstract>
Techniques for assisting a passenger to identify a vehicle and for assisting a vehicle to identify a passenger are discussed herein. Also discussed herein are techniques for capturing data via sensors on a vehicle or user device and for presenting such data in various formats. For example, in the context of a ride hailing service using autonomous vehicles, the techniques discussed herein can be used to identify a passenger of the autonomous vehicle at the start of a trip, and can be used to assist a passenger to identify an autonomous vehicle that has been dispatched for that particular passenger. Additionally, data captured by sensors of the vehicle and/or by sensors of a user device can be used to initiate a ride, determine a pickup location, orient a user within an environment, and/or provide visualizations or augmented reality elements to provide information and/or enrich a user experience.
</abstract>

<claims>
1. A system comprising: one or more processors; and one or more computer-readable media storing instructions executable by the one or more processors, wherein the instructions, when executed, cause the system to perform operations comprising: sending, from a user device, a request for data; capturing, using one or more sensors of the user device, first image data representing an environment; receiving, at the user device, mission data comprising an indication of a pickup location, wherein the pickup location is determined based in part on a perception system associated with an autonomous vehicle and one or more of sensor data or route data associated with the autonomous vehicle; generating, based at least in part on the first image data and the indication of the pickup location, second image data, the second image data comprising a portion of the first image data and a computer-generated element; and causing the user device to display the second image data.
2. The system of claim 1, wherein the sensor data or the route data comprises a location of the autonomous vehicle, and wherein the computer-generated element comprises an indication of an identity of the autonomous vehicle in the second image data.
3. The system of claim 2, wherein the indication of the identity comprises an outlining of the autonomous vehicle in the second image data or a color overlay of the autonomous vehicle in the second image data.
4. The system of claim 1, the operations further comprising: determining a distance between the autonomous vehicle and the pickup location; determining that the distance is at or below a threshold distance; and generating the computer-generated element based at least in part on the distance being at or below the threshold distance.
5. The system of claim 1, wherein the request further comprises a request to control the autonomous vehicle to navigate to the pickup location.
6. A method comprising: sending, by a user device, a request for data associated with a vehicle; receiving, at the user device, mission data comprising an indication of a pickup location, wherein the pickup location is determined based in part on a perception system associated with the vehicle and one or more of sensor data or route data associated with the vehicle; capturing, by the user device, first image data representing an environment; generating, based at least in part on the first image data and the indication of the pickup location, second image data comprising at least a portion of the first image data and a computer-generated element; and displaying, by the user device, the second image data.
7. The method of claim 6, wherein the computer-generated element indicates a path to the pickup location.
8. The method of claim 7, further comprising: determining a distance between the user device and the pickup location; determining, based at least in part on the first image data, that the distance is less than or equal to a threshold distance; and sending, by the user device, an indication that the user device is less than or equal to the threshold distance of the pickup location.
9. The method of claim 6, wherein the mission data comprises an indication of a location of the vehicle, the method further comprising: determining that a portion of the first image data represents the vehicle, wherein the computer-generated element indicates an identity of the vehicle.
10. The method of claim 9, wherein the computer-generated element indicating the identity of the vehicle comprises an overlay of a unique color over the vehicle or a silhouette of the vehicle.
11. The method of claim 6, wherein the mission data comprises an indication of a location of the vehicle, the method further comprising: determining, based at least in part on the location of the vehicle, a position of the user device, and an orientation of the user device, that at least a portion of the vehicle is obscured by an object in the first image data, wherein the computer-generated element comprises a representation of the at least the portion of the vehicle that is obscured by the object.
12. The method of claim 6, wherein the mission data comprises an indication of a location of the vehicle, the method further comprising: determining, based at least in part on the location of the vehicle, the first image data, a position of the user device, and an orientation of the user device, a rotation from the user device to the vehicle, wherein the computer-generated element comprises one or more graphic directives instructing a user to change to the orientation of the user device.
13. The method of claim 6, wherein the mission data comprises the second image data acquired by an image sensor on the vehicle and a location of the vehicle, and wherein generating the computer-generated element comprises generating, based at least in part on the location, a position of a user, and an orientation of the user device, an indication of the position of the user in the second image data.
14. The method of claim 6, further comprising: receiving, from the vehicle, data based at least in part on at least one of LIDAR data, radar data captured by the vehicle, or map data; and generating the computer-generated element based at least in part on the data.
15. The method of claim 6, wherein the mission data comprises one or more images captured by an image sensor on the vehicle during a period of time immediately preceding the request, the method further comprising displaying the one or more images via the user device.
16. A non-transitory computer-readable medium storing instructions that, when executed, cause one or more processors to perform operations comprising: sending, by a user device, a request for data associated with a vehicle; receiving, at the user device, mission data comprising an indication of a pickup location, wherein the pickup location is determined based in part on a perception system associated with the vehicle and one or more of sensor data or route data associated with the vehicle; capturing, by the user device, first image data representing an environment; generating, based at least in part on the first image data and the indication of the pickup location, second image data comprising at least a portion of the first image data and a computer-generated element; and displaying, by the user device, the second image data.
17. The non-transitory computer-readable medium of claim 16, the operations further comprising generating the computer-generated element to indicate a path from a location to the pickup location.
18. The non-transitory computer-readable medium of claim 16, wherein the mission data comprises an indication of a location of the vehicle, the operations further comprising: determining, based at least in part on the location of the vehicle, a portion of the first image data associated with the vehicle; and generating the computer-generated element to indicate an identity of the vehicle.
19. The non-transitory computer-readable medium of claim 18, the operations further comprising: determining, based at least in part on the first image data, the location, a position of the user device, and an orientation of the user device, that at least a portion of the vehicle is obscured by an object in the first image data, wherein the computer-generated element to indicate an identity of the vehicle comprises a representation of the portion of the vehicle that is obscured by the object.
20. The non-transitory computer-readable medium of claim 16, wherein the mission data comprises an indication of a location of the vehicle, the operations further comprising: determining, based at least in part on the first image data, the location, a position of the user device and an orientation of the user device, that the vehicle is not within a field of view associated with the user device, wherein the computer-generated element comprises an indication of how to orient the user device such that the vehicle is within the field of view of the user device.
</claims>
</document>
