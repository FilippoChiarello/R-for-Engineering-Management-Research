<document>

<filing_date>
2019-01-02
</filing_date>

<publication_date>
2020-07-07
</publication_date>

<priority_date>
2019-01-02
</priority_date>

<ipc_classes>
G06F12/0802,G06N20/00
</ipc_classes>

<assignee>
BAKER HUGHES
</assignee>

<inventors>
EDALUR, RAGHU
RAMINI, PURNA
</inventors>

<docdb_family_id>
71122791
</docdb_family_id>

<title>
Just-in-time data provision based on predicted cache policies
</title>

<abstract>
Systems, methods, and computer readable mediums are provided for predicting a cache policy based on usage patterns. Usage pattern data can be received and used with a predictive model to determine a cache policy associated with a datastore. The cache policy can identify the configuration of predicted output data to be provisioned in the datastore and subsequently provided to a client in a just-in-time manner. The predictive model can be trained to output the cache policy based on usage pattern data received from a usage point, a provider point, or a datastore configuration.
</abstract>

<claims>
1. A method comprising: receiving a usage pattern provided to an application configured on a computing device including a data processor and coupled to a datastore, the usage pattern including a plurality of sequential inputs provided to the application in association with an objective to be performed using an oil and gas computing environment; determining, using the usage pattern and a predictive model, a predicted cache policy corresponding to the datastore and identifying a configuration of predicted output data to be provided via the datastore, the predictive model trained to output the predicted cache policy based on a machine learning process; executing the predicted cache policy at the datastore, the execution causing the provision of the predicted output data to the application from the datastore based on the usage pattern; generating an output, by the application, including the predicted output data, based on executing the predicted cache policy; and providing the output, via the application, to cause the application to execute at least a portion of the objective using a reduced memory allocation within the computing device.
2. The method of claim 1, wherein the oil and gas computing environment is configured with a plurality of computing devices, each including a data processor, to receive inputs and generate outputs associated with operational, diagnostic, analytical, and/or search objectives corresponding to a plurality of deployed assets used in oil and gas production and refinement operations.
3. The method of claim 2, wherein the plurality of computing devices includes computing devices configured as a usage point, a provider point, a datastore, and a data source.
4. The method of claim 1, wherein the datastore includes a datastore associated with an application provider.
5. The method of claim 1, wherein the datastore includes a datastore associated with a third-party.
6. The method of claim 1, wherein the predicted cache policy includes an expiration parameter identifying a duration of time for the predicted output data to persist in the datastore prior to removal from the datastore.
7. The method of claim 6, further comprising removing output data from the datastore at an end of the duration of time identified in the expiration parameter or based on receiving a second usage pattern.
8. The method of claim 1, wherein the configuration of predicted output data includes a format associated with the datastore, the application, or a specific named user of the application.
9. The method of claim 1, wherein the machine learning process is configured to generate the predictive model based on usage patterns corresponding to data collected from a usage point within the oil and gas computing environment, a provider point within the oil and gas computing environment, or a data source within the oil and gas computing environment.
10. The method of claim 8, wherein the machine learning process is configured to generate new versions of the predictive model based on a user-configurable usage pattern collection schedule, each new version including one or more new or updated predicted cache policies.
11. The method of claim 10, wherein the user-configurable data collection schedule includes data collection occurring continuously, every hour, every day, every week, every month, or during a user-defined time-period.
12. The method of claim 1, wherein the usage pattern is received in response to monitoring data generated by the oil and gas computing environment.
13. The method of claim 1, wherein the datastore includes a hardware cache or a software cache.
14. A system comprising: a memory storing computer-readable instructions and a plurality of prediction models; and a processor, the processor configured to execute the computer-readable instructions, which when executed, cause the processor to perform operations comprising: receiving a usage pattern provided to an application configured on a computing device including a data processor and coupled to a datastore, the usage pattern including a plurality of sequential inputs provided to the application in association with an objective to be performed using an oil and gas computing environment; determining, using the usage pattern and a predictive model, a predicted cache policy corresponding to the datastore and identifying a configuration of predicted output data to be provided via the datastore, the predictive model trained to output the predicted cache policy based on a machine learning process; executing the predicted cache policy at the datastore, the execution causing the provision of the predicted output data to the application from the datastore based on the usage pattern; generating an output, by the application, including the predicted output data, based on executing the predicted cache policy; and providing, via the application, the output to cause the application to complete at least a portion of the objective using a reduced memory allocation within the computing device.
15. The system of claim 14, wherein the oil and gas computing environment is configured with a plurality of computing devices, each including a data processor, to receive inputs and generate outputs associated with operational, diagnostic, analytical, and/or search objectives corresponding to a plurality of deployed assets used in oil and gas production and refinement operations.
16. The system of claim 15, wherein the plurality of computing devices includes computing devices configured as a usage point, a provider point, a datastore, and a data source.
17. The system of claim 14, wherein the datastore includes a datastore associated with an application provider.
18. The system of claim 14, wherein the datastore includes a datastore associated with a third-party.
19. The system of claim 14, wherein the predicted cache policy includes an expiration parameter identifying a duration of time for the predicted output data to persist in the datastore prior to removal from the datastore.
20. The system of claim 19, further comprising removing output data from the datastore at an end of the duration of time identified in the expiration parameter or based on receiving a second usage pattern.
21. The system of claim 14, wherein the configuration of predicted output data includes a format associated with the datastore, the application, or a specific named user of the application.
22. The system of claim 14, wherein the datastore includes a hardware cache or a software cache.
</claims>
</document>
