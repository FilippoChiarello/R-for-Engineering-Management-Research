<document>

<filing_date>
2019-09-30
</filing_date>

<publication_date>
2020-01-23
</publication_date>

<priority_date>
2019-08-30
</priority_date>

<ipc_classes>
G06F3/01,G06T7/70
</ipc_classes>

<assignee>
LG ELECTRONICS
</assignee>

<inventors>
KIM, SUNG JIN
JEON, YOUNG HYEOG
KIM, BEOMOH
</inventors>

<docdb_family_id>
68070466
</docdb_family_id>

<title>
AUGMENTED REALITY DEVICE AND GESTURE RECOGNITION CALIBRATION METHOD THEREOF
</title>

<abstract>
Disclosed are an augmented reality device and a gesture recognition calibration method thereof. The gesture recognition calibration method of the augmented reality device according to an embodiment of the present disclosure has the effect in that it is possible to perform more effective gesture recognition by determining the main eyesight of a user by calculating scores using coordinate systems. An AR device of the present disclosure may be associated with an artificial intelligence module, a drone ((Unmanned Aerial Vehicle, UAV), a robot, a VR (Virtual Reality) device, a device associated with 5G services, etc.
</abstract>

<claims>
1. A gesture recognition calibration method of an augmented reality device, the method comprising: estimating a position of an object that is indicated by a first indicative gesture of a user; determining whether the estimated position of the object corresponds to a left coordinate system or a right coordinate system; calculating a score for each of the left coordinate system and the right coordinate system, wherein the score for the left coordinate system is increased when it is determined that the estimated position of the object corresponds to the left coordinate system and the score for the right coordinate system is increased when it is determined that the estimated position of the object corresponds to the right coordinate system; and determining a main eyesight of the user based on the calculated scores, wherein the left coordinate system is related to a gaze by the left eye of the user and the right coordinate system is related to gaze by the right eye of the user.
2. The method of claim 1, wherein: the main eyesight of the user is determined as the left eye of the user when the score for the left coordinate system is greater than the score for the right coordinate system; and the main eyesight of the user is determined as the right eye of the user when the score for the right coordinate system is greater than the score for the left coordinate system.
3. The method of claim 1, further comprising: sensing a second object at a position that is indicated by a second indicative gesture of the user based on the main eyesight; and outputting information about the second object through an augmented reality (AR) device, wherein the information about the second object is output using AR.
4. The method of claim 3, wherein the AR device is AR glasses or an AR mobile terminal.
5. The method of claim 1, wherein estimating the position of the object comprises: detecting a fingertip of the user according to the first indicative gesture; and determining 3D coordinates corresponding to the fingertip of the user and estimating the position of the object using the 3D coordinates.
6. The method of claim 5, wherein the first indicative gesture is recognized through a camera, and the 3D coordinates are determined using a coordinate system of the camera, the left coordinate system, and the right coordinate system.
7. The method of claim 6, wherein the 3D coordinates are determined using distances and angles between the camera and each of the left eye of the user and the right eye of the user.
8. An augmented reality device comprising: a transceiver for transmitting and receiving wireless signals; a sensor; and a processor configured to: sense a first indicative gesture of a user via the sensor; estimate a position of an object that is indicated by the first indicative gesture; determine whether the estimated position of the corresponds to a left coordinate system or a right coordinate system; calculate a score for each of the left coordinate system and the right coordinate system, wherein the score for the left coordinate system is increased when it is determined that the estimated position of the object corresponds to the left coordinate system and the score for the right coordinate system is increased when it is determined that the estimated position of the object corresponds to the right coordinate system; and determine a main eyesight of the user based on the calculated scores, wherein the left coordinate system is related to a gaze by the left eye of the user and the right coordinate system is related to gaze by the right eye of the user.
9. The augmented reality device of claim 8, wherein: the main eyesight of the user is determined as the left eye of the user when the score for the left coordinate system is greater than the score for the right coordinate system; and the main eyesight of the user is determined as the right eye of the user when the score for the right coordinate system is greater than the score for the left coordinate system.
10. The augmented reality device of claim 8, wherein the processor is further configured to: sense a second object at a position that is indicated by a second indicative gesture of the user sensed via the sensor based on the main eyesight; and output information about the second object using augmented reality (AR).
11. The augmented reality device of claim 10, wherein the AR device is AR glasses or an AR mobile terminal.
12. The augmented reality device of claim 8, wherein the processor is further configured to estimate the position of the object by: detecting a fingertip of the user according to the first indicative gesture; and determining 3D coordinates corresponding to the fingertip of the user, and estimates the position of the object using the 3D coordinates.
13. The augmented reality device of claim 12, wherein the sensor is a camera and the 3D coordinates are determined using a coordinate system of the camera, the left coordinate system, and the right coordinate system.
14. The augmented reality device of claim 13, wherein the 3D coordinates are determined using distances and angles between the camera and each of the left eye of the user and the right eye of the user.
15. A machine-readable non-transitory medium having stored thereon machine-executable instructions for gesture recognition calibration of an augmented reality device, the instructions comprising: estimating a position of an object that is indicated by a first indicative gesture of a user; determining whether the estimated position of the object corresponds to a left coordinate system or a right coordinate system; calculating a score for each of the left coordinate system and the right coordinate system, wherein the score for the left coordinate system is increased when it is determined that the estimated position of the object corresponds to the left coordinate system and the score for the right coordinate system is increased when it is determined that the estimated position of the object corresponds to the right coordinate system; and determining a main eyesight of the user based on the calculated scores, wherein the left coordinate system is related to a gaze by the left eye of the user and the right coordinate system is related to gaze by the right eye of the user.
</claims>
</document>
