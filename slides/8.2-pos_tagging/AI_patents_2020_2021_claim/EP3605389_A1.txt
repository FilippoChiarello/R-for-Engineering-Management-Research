<document>

<filing_date>
2019-07-25
</filing_date>

<publication_date>
2020-02-05
</publication_date>

<priority_date>
2018-07-30
</priority_date>

<ipc_classes>
G06K9/00
</ipc_classes>

<assignee>
NCSOFT CORPORATION
</assignee>

<inventors>
JANG, HANYOUNG
KWON, BYUNGJUN
YU, MOONWON
</inventors>

<docdb_family_id>
67438993
</docdb_family_id>

<title>
MOTION SYNTHESIS APPARATUS AND MOTION SYNTHESIS METHOD
</title>

<abstract>
A motion synthesis motion synthesis method including: obtaining, by a motion synthesis apparatus, content feature values and style feature valuesaccording to content motion data and style motion data; generating, by the motion synthesis apparatus, target feature values using the obtained content feature values and style feature values; recognizing, by the motion synthesis apparatus, synthesized motion data and obtaining synthesized motion feature values from the recognized synthesized motion data; and obtaining, by the motion synthesis apparatus, loss by using the synthesized motion feature values and the target feature values and updating the synthesized motion data according to the obtained loss.
</abstract>

<claims>
1. A motion synthesis method comprising: obtaining, by a motion synthesis apparatus, content feature values and style feature valuesaccording to content motion data and style motion data; generating, by the motion synthesis apparatus, target feature values using the obtained content feature values and style feature values; recognizing, by the motion synthesis apparatus, synthesized motion data and obtaining synthesized motion feature values from the recognized synthesized motion data; and obtaining, by the motion synthesis apparatus, loss by using the synthesized motion feature values and the target feature values and updating the synthesized motion data according to the obtained loss.
2. The motion synthesis method of claim 1, wherein the updating of the synthesized motion data comprises:
using a back-propagation algorithm until the synthesized motion feature values and the target feature values are matched.
3. The motion synthesis method of claim 1, wherein the motion synthesis apparatus obtains feature values using an untrained convolutional neural network.
4. The motion synthesis method of claim 1, wherein the content motion data and the style motion data are animation data.
5. The motion synthesis method of claim 1, wherein the content motion data and the style motion data comprise information about a bone.
6. The motion synthesis method of claim 1, further comprising:
obtaining style loss using the style feature values, and assigning a weight to the obtained style loss to generate the target feature values.
7. A motion synthesis apparatus, the motion synthesis apparatus comprising a processor configured to: obtain content feature values and style feature values according to content motion data and style motion data; generate target feature values using the obtained content feature values and style feature values; recognize synthesized motion data and obtains synthesized motion feature values from the recognized synthesized motion data; and obtain loss by using the synthesized motion feature values and the target feature values and updates the synthesized motion data according to the obtained loss.
8. The motion synthesis apparatus of claim 7, wherein the updating of the synthesized motion data comprises:
using a back-propagation algorithm until the synthesized motion feature values and the target feature values are matched.
9. The motion synthesis apparatus of claim 7, wherein the processor is configured to obtain feature values using an untrained convolutional neural network.
10. The motion synthesis apparatus of claim 7, wherein the content motion data and the style motion data are animation data.
11. The motion synthesis apparatus of claim 7, wherein the content motion data and the style motion data comprise information about a bone.
12. The motion synthesis apparatus of claim 7, the motion synthesis apparatus configured to:
obtain style loss using the style feature values, and assign a weight to the obtained style loss to generate the target feature values.
</claims>
</document>
