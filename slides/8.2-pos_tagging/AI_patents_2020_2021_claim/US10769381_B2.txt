<document>

<filing_date>
2017-06-21
</filing_date>

<publication_date>
2020-09-08
</publication_date>

<priority_date>
2016-12-15
</priority_date>

<ipc_classes>
G06F16/27,G06F16/30,G06F16/35,G06F16/901,G06F17/27,G06F17/30,G06F40/30
</ipc_classes>

<assignee>
QUID
</assignee>

<inventors>
CIULLA, FABIO
GOODSON, ROBERT
MUSIAL, WOJCIECH
TACCHI, RUGGERO ALTAIR
</inventors>

<docdb_family_id>
59350427
</docdb_family_id>

<title>
Topic-influenced document relationship graphs
</title>

<abstract>
Provided is a process of enhancing or suppressing measures of relationships between documents based on the relationships arising from text pertaining to selected topics, the process including: obtaining a corpus of documents; obtaining a set of topics by self-extracting topics according to the tokens present in text of the documents or manual provision; receiving a selected topic among the set of topics, the selection indicating that relationships between the documents are to be enhanced or suppressed in virtue of the relationships arising from text relating to the selected topic; forming a relationship graph of the documents, wherein: the relationships between pairs of the documents are determined based on co-occurrence of n-grams in the pairs of the documents, and wherein the relationships are enhanced or suppressed in response to co-occurring n-grams being in the respective set of n-grams of the selected topic.
</abstract>

<claims>
1. A tangible, non-transitory, machine-readable medium storing instructions that when executed by one or more processors effectuate operations comprising: obtaining, with one or more processors, a topic vector for a given topic, wherein: the given topic appears in a corpus of natural language text among a plurality of other topics, the topic vector includes a plurality of topic-specific scores for respective n-grams, the topic-specific scores are indicative of respective n-grams being associated with the given topic in natural language text, and at least some of the respective n-grams of the topic vector appear in the corpus; inferring, with one or more processors, a plurality of relationships from the corpus of natural language text at least in part by performing a computational linguistic analysis of the natural language text based on the topic vector, wherein, for a given n-gram having a given topic-specific score in the topic vector, and a given relationship among the plurality of relationships based on the given n-gram, the given relationship is suppressed or enhanced based on the given topic-specific score relative to a computational linguistic analysis of the natural language text that is not based on the topic vector by adjusting a contribution of a co-occurrence of the given n-gram in a pair of documents to a semantic similarity relationship between the pair of documents; and storing, with one or more processors, the plurality of relationships in memory.
2. The medium of claim 1, wherein obtaining a topic vector comprises: obtaining a topic model that includes a mapping of each of a plurality of topics to a set of n-grams, each n-gram in the set having, for each topic, a topic-specific score indicating a conditional probability of the respective topic being exhibited upon observing the respective n-gram in a document.
3. The medium of claim 1, wherein obtaining a topic vector comprises: inferring, with supervised machine learning, the topic vector from a labeled set of documents, the labeled set of documents being the same, overlapping, or otherwise different from the corpus.
4. The medium of claim 3, wherein obtaining a topic vector comprises: inferring, from the labeled set documents, the topic vector with steps for learning topic-specific scores based on a training set.
5. The medium of claim 3, wherein, for at least one document of the labeled set of documents, the at least one document contains multiple topics, and the at least one document is associated with a plurality of labels that each indicate a respective range of text in the document associated with both a respective identifier of a respective topic and a respective score indicating a pertinence of the respective topic to the respective range of text.
6. The medium of claim 1, wherein obtaining a topic vector comprises: obtaining a training set of documents; designating n-grams in the training set of documents as pertaining to topics; iteratively adjusting the designations to make n-grams that, within the training set of documents, tend to co-occur in a document more likely to be designated with the same topic.
7. The medium of claim 1, wherein obtaining a topic vector comprises: obtaining a training set of documents; for each document in the training set, for each n-gram in each respective document of the training set: for each topic of a plurality of topics, determining: an amount of n-grams in the respective document designated as pertaining to the respective topic; and an amount of all instances of n-grams of a specified size in all of the documents of the training set designated as pertaining to the respective topic; for the respective document and n-gram in the training set, designating the respective n-gram as pertaining to a topic selected according to probabilities of the topics based on the determined amounts.
8. The medium of claim 7, wherein: the amount of n-grams in the respective document designated as pertaining to the respective topic is a proportion relative to a total number of n-grams of the same length as the respective n-gram in the respective document; the amount of all instances of n-grams in all of the documents of the training set designated as pertaining to the respective topic is a proportion relative to all documents in the training set; and the specified size is a same size as a length in words of the respective n-gram.
9. The medium of claim 7, wherein probabilities of the topics based on the determined amounts are based on a conditional probability of a respective topic being exhibited given a respective document multiplied by a conditional probability of the respective n-gram occurring given that the respective topic is exhibited.
10. The medium of claim 1, wherein the plurality of relationships are of at least one of the following types of relationships: semantic similarity of documents; similarity of sentiments expressed in documents; similarity of terminology in documents; or relatedness of entities mentioned in documents.
11. The medium of claim 1, comprising: forming a graph in which the inferred relationships serve as edges of the graph.
12. The medium of claim 11, wherein nodes of the graph correspond to: documents of the corpus; paragraphs of the corpus; entities mentioned in the corpus; or terminology in the corpus.
13. The medium of claim 1 wherein the topic vector is obtained, at least in part, by: obtaining an indication of time in which relationships between the given topic and n-grams are to be applied; selecting the given topic vector from among a plurality of topic vectors pertaining to the given topic, the plurality of topic vectors corresponding to different durations of time and characterizing n-gram associations with the given topic during the different durations of time.
14. The medium of claim 1, wherein: the given relationship is enhanced based on the topic-specific score by an amount specified at least in part specified by a user.
15. The medium of claim 14, comprising: obtaining a different topic vector pertaining to a different topic from the given topic; wherein the given relationship is enhanced based on another topic-specific score for the given n-gram from the different topic vector.
16. The medium of claim 14, wherein: the given relationship is enhanced based on the other topic-specific score by another amount specified at least in part by the user, the amount and the other amount being different amounts indicated by user input.
17. The medium of claim 1, wherein performing computational linguistic analysis of the natural language text based on the topic vector comprises: inferring relationships based on distributional semantic analysis in which pertinence of n-grams to the given topic indicated in the topic vector modulate effects of the respective n-grams on a result of the distributional semantic analysis.
18. The medium of claim 1, wherein performing computational linguistic analysis of the natural language text based on the topic vector comprises: steps for forming a semantic similarity graph.
19. The medium of claim 1 comprises sending instructions to a client computing device to display a graphical representation of at least some of the relationships.
20. A method, comprising: obtaining, with one or more processors, a topic vector for a given topic, wherein: the given topic appears in a corpus of natural language text among a plurality of other topics, the topic vector includes a plurality of topic-specific scores for respective n-grams, the topic-specific scores are indicative of respective n-grams being associated with the given topic in natural language text, and at least some of the respective n-grams of the topic vector appear in the corpus; inferring, with one or more processors, a plurality of relationships from the corpus of natural language text at least in part by performing a computational linguistic analysis of the natural language text based on the topic vector, wherein, for a given n-gram having a given topic-specific score in the topic vector, and a given relationship among the plurality of relationships based on the given n-gram, the given relationship is suppressed or enhanced based on the given topic-specific score relative to a computational linguistic analysis of the natural language text that is not based on the topic vector by adjusting a contribution of a co-occurrence of the given n-gram in a pair of documents to a semantic similarity relationship between the pair of documents; and storing, with one or more processors, the plurality of relationships in memory.
21. The method of claim 20, wherein obtaining a topic vector comprises: obtaining a topic model that includes a mapping of each of a plurality of topics to a set of n-grams, each n-gram in the set having, for each topic, a topic-specific score indicating a conditional probability of the respective topic being exhibited upon observing the respective n-gram in a document.
22. The method of claim 20, wherein obtaining a topic vector comprises: inferring, with supervised machine learning, the topic vector from a labeled set of documents, the labeled set of documents being the same, overlapping, or otherwise different from the corpus.
23. The method of claim 22, wherein obtaining a topic vector comprises: inferring, from the labeled set documents, the topic vector with steps for learning topic-specific scores based on a training set.
24. The method of claim 22, wherein, for at least one document of the labeled set of documents, the at least one document contains multiple topics, and the at least one document is associated with a plurality of labels that each indicate a respective range of text in the document associated with both a respective identifier of a respective topic and a respective score indicating a pertinence of the respective topic to the respective range of text.
25. The method of claim 20, wherein obtaining a topic vector comprises: obtaining a training set of documents; designating n-grams in the training set of documents as pertaining to topics; iteratively adjusting the designations to make n-grams that, within the training set of documents, tend to co-occur in a document more likely to be designated with the same topic.
26. The method of claim 20, wherein obtaining a topic vector comprises: obtaining a training set of documents; for each document in the training set, for each n-gram in each respective document of the training set: for each topic of a plurality of topics, determining: an amount of n-grams in the respective document designated as pertaining to the respective topic; and an amount of all instances of n-grams of a specified size in all of the documents of the training set designated as pertaining to the respective topic; for the respective document and n-gram in the training set, designating the respective n-gram as pertaining to a topic selected according to probabilities of the topics based on the determined amounts.
27. The method of claim 26, wherein: the amount of n-grams in the respective document designated as pertaining to the respective topic is a proportion relative to a total number of n-grams of the same length as the respective n-gram in the respective document; the amount of all instances of n-grams in all of the documents of the training set designated as pertaining to the respective topic is a proportion relative to all documents in the training set; and the specified size is a same size as a length in words of the respective n-gram.
28. The method of claim 26, wherein probabilities of the topics based on the determined amounts are based on a conditional probability of a respective topic being exhibited given a respective document multiplied by a conditional probability of the respective n-gram occurring given that the respective topic is exhibited.
29. The method of claim 20, wherein the plurality of relationships are of at least one of the following types of relationships: semantic similarity of documents; similarity of sentiments expressed in documents; similarity of terminology in documents; or relatedness of entities mentioned in documents.
30. The method of claim 20, comprising: forming a graph in which the inferred relationships serve as edges of the graph.
31. The method of claim 30, wherein nodes of the graph correspond to: documents of the corpus; paragraphs of the corpus; entities mentioned in the corpus; or terminology in the corpus.
32. The method of claim 20 wherein the topic vector is obtained, at least in part, by: obtaining an indication of time in which relationships between the given topic and n-grams are to be applied; selecting the given topic vector from among a plurality of topic vectors pertaining to the given topic, the plurality of topic vectors corresponding to different durations of time and characterizing n-gram associations with the given topic during the different durations of time.
33. The method of claim 20, wherein: the given relationship is enhanced based on the topic-specific score by an amount specified at least in part specified by a user.
34. The method of claim 33, comprising: obtaining a different topic vector pertaining to a different topic from the given topic; wherein the given relationship is enhanced based on another topic-specific score for the given n-gram from the different topic vector.
35. The method of claim 33, wherein: the given relationship is enhanced based on the other topic-specific score by another amount specified at least in part by the user, the amount and the other amount being different amounts indicated by user input.
36. The method of claim 20, wherein performing computational linguistic analysis of the natural language text based on the topic vector comprises: inferring relationships based on distributional semantic analysis in which pertinence of n-grams to the given topic indicated in the topic vector modulate effects of the respective n-grams on a result of the distributional semantic analysis.
37. The method of claim 20, wherein performing computational linguistic analysis of the natural language text based on the topic vector comprises: steps for forming a semantic similarity graph.
38. The method of claim 20 comprises sending instructions to a client computing device to display a graphical representation of at least some of the relationships.
</claims>
</document>
