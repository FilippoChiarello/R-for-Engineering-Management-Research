<document>

<filing_date>
2018-09-28
</filing_date>

<publication_date>
2020-04-02
</publication_date>

<priority_date>
2018-09-28
</priority_date>

<ipc_classes>
G06N5/04,G06N99/00
</ipc_classes>

<assignee>
AMAZON TECHNOLOGIES
</assignee>

<inventors>
STEFANI STEFANO
GELLA, GANESH KUMAR
SARTORELLO, ENRICO
SABBINENI, NAVNEET
KANDOI, NIKHIL
POKKUNURI, RAMA KRISHNA SANDEEP
SUTARIA, KALPESH N.
LI, CHENG RAN
</inventors>

<docdb_family_id>
69947742
</docdb_family_id>

<title>
PRE-WARMING SCHEME TO LOAD MACHINE LEARNING MODELS
</title>

<abstract>
Techniques for hosting adding and warming a host are described. In some instances, a method of determining that at least one group of hosts is to be increased by adding an additional host to the group of hosts; sending a request to the group of hosts for a list of machine learning models loaded per host of the group of hosts; receiving, from each host, the list of loaded machine learning models; loading at least a proper subset of list of loaded machine learning models into random access memory of the at least one group; receiving a request to perform an inference; routing the request to the additional host of the group of hosts; performing an inference using the additional host of the group of hosts; and providing a result of the inference to an external entity is described.
</abstract>

<claims>
1. A computer-implemented method comprising: receiving a candidate set of nodes to use in a first group of hosts and a set of all the machine learning models hosted in a second group of hosts; loading at least some of the set of machine learning models hosted in the second group of hosts into random access memory of the first group; receiving a request to perform an inference using one of the loaded machine learning models; routing the request to a host of the first group of hosts; performing an inference using the host of the first group of hosts; and providing a result of the inference to the external entity.
2. The computer-implemented method of claim 1, wherein the first group of hosts is generated as a result of determining the second group of hosts was overloaded.
3. The computer-implemented method of claim 2, wherein determining the second group of hosts was overloaded comprises: maintaining a cache miss rate of hosts of the second group of hosts over a time period, each host the second group of hosts to cache a plurality of machine learning models; tracking a number of unique machine learning models requested in the second group of hosts over the time period; and wherein when either the first or second metric exceeds a threshold, the second group of hosts is overloaded.
4. The computer-implemented method of claim 1, wherein each host maps to at least one virtual node and an identifier of the machine learning model is hashed to at least one of the virtual nodes.
5. The computer-implemented method of claim 1, wherein which host to route to is randomly determined and a location of the randomly determined host dictates the group of hosts to route the request to.
6. The computer-implemented method of claim 1, wherein each host caches a first plurality of machine learning models loaded in random access memory and caches a second, different plurality of machine learning models in a disk according to a least frequently used caching model.
7. The computer-implemented method of claim 6, wherein when a previously unused model is cached into the random access memory, the least frequently used machine learning model in the random access memory is flushed to the disk.
8. A computer-implemented method comprising: determining that at least one group of hosts is to be increased by adding an additional host to the group of hosts; sending a request to the group of hosts for a list of machine learning models loaded per host of the group of hosts; receiving, from each host, the list of loaded machine learning models; loading at least a proper subset of list of loaded machine learning models into random access memory of the at least one group; receiving a request to perform an inference; routing the request to the additional host of the group of hosts; performing an inference using the additional host of the group of hosts; and providing a result of the inference to an external entity.
9. The computer-implemented method of claim 8, wherein each host maps to at least one virtual node and an identifier of the machine learning model is hashed to at least one of the virtual nodes.
10. The computer-implemented method of claim 8, wherein the request is routed to the additional host based on random determination.
11. The computer-implemented method of claim 8, wherein each host caches a first plurality of machine learning models loaded in random access memory and caches a second, different plurality of machine learning models in a disk according to a least frequently used caching model.
12. The computer-implemented method of claim 11, wherein when a previously unused model is cached into the random access memory, the least frequently used machine learning model in the random access memory is flushed to the disk.
13. The computer-implemented method of claim 8, further comprising: storing data including the request and inference result in a data hub accessible to a subscribing entity.
14. The computer-implemented method of claim 8, wherein the request is received from a bot.
15. A system comprising: a plurality of compute nodes having a plurality of machine learning models loaded for use inference operations, the plurality of compute nodes logically divided as plurality of groups of hosts and implemented by a first one or more electronic devices; and an inference service implemented by a second one or more electronic devices, the inference service including instructions that upon execution are to cause: receiving a candidate set of compute nodes to use in a first logical group of hosts and a set of all the machine learning models hosted in a second logical group of hosts; loading at least some of the set of machine learning models hosted in the second group of hosts into random access memory of the first logical group of hosts; receiving a request to perform an inference using one of the loaded machine learning models; routing the request to a host of the first logical group of hosts; performing an inference using the host of the first logical group of hosts; and providing a result of the inference to the external entity.
16. The system of claim 15, wherein the first logical group of hosts is generated as a result of determining the second group of hosts was overloaded.
17. The system of claim 15, wherein determining the second logical group of hosts was overloaded comprises: maintaining a cache miss rate of hosts of the second logical group of hosts over a time period, each host the second logical group of hosts to cache a plurality of machine learning models; tracking a number of unique machine learning models requested in the second logical group of hosts over the time period; and wherein when either the first or second metric exceeds a threshold, the second logical group of hosts is overloaded.
18. The system of claim 15, wherein each host maps to at least one virtual node and an identifier of the machine learning model is hashed to at least one of the virtual nodes.
19. The system of claim 15, wherein which host to route to is randomly determined and a location of the randomly determined host dictates the logical group of hosts to route the utterance to.
20. The system of claim 15, wherein each host caches a first plurality of machine learning models loaded in random access memory and caches a second, different plurality of machine learning models in a disk according to a least frequently used caching model.
</claims>
</document>
