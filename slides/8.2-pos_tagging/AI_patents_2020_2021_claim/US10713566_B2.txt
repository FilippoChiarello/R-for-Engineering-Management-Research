<document>

<filing_date>
2016-10-11
</filing_date>

<publication_date>
2020-07-14
</publication_date>

<priority_date>
2016-10-11
</priority_date>

<ipc_classes>
G06N3/08
</ipc_classes>

<assignee>
SIEMENS
</assignee>

<inventors>
CHAKRABORTY, AMIT
AKROTIRIANAKIS, IOANNIS
HE, XI
</inventors>

<docdb_family_id>
61830055
</docdb_family_id>

<title>
Efficient calculations of negative curvature in a hessian free deep learning framework
</title>

<abstract>
A method for training a deep learning network includes defining a loss function corresponding to the network. Training samples are received and current parameter values are set to initial parameter values. Then, a computing platform is used to perform an optimization method which iteratively minimizes the loss function. Each iteration comprises the following steps. An eigCG solver is applied to determine a descent direction by minimizing a local approximated quadratic model of the loss function with respect to current parameter values and the training dataset. An approximate leftmost eigenvector and eigenvalue is determined while solving the Newton system. The approximate leftmost eigenvector is used as negative curvature direction to prevent the optimization method from converging to saddle points. Curvilinear and adaptive line-searches are used to guide the optimization method to a local minimum. At the end of the iteration, the current parameter values are updated based on the descent direction.
</abstract>

<claims>
1. A computer-implemented method for training a deep learning network, the method comprising: defining a loss function corresponding to the deep learning network; receiving a training dataset comprising a plurality of training samples; setting current parameter values of the deep learning network to initial parameter values; using a computing platform to perform an optimization method which iteratively minimizes the loss function over a plurality of iterations, wherein each iteration comprises: applying an eigCG solver to (a) determine a descent direction by minimizing a local approximated quadratic model of the loss function with respect to current parameter values and the training dataset and (b) approximate a leftmost eigenvector and a responding eigenvalue of a matrix of second-order partial derivatives of the loss function, using the leftmost eigenvector as a negative curvature direction to prevent the optimization method from converging to saddle points, selecting a final descent direction as either the descent direction or the negative curvature direction based on a test on a rate of decrease of the local approximated quadratic model of the loss function in the negative curvature direction, using a curvilinear line-search or adaptive line-search to guide the optimization method to a local minimum, and updating the current parameter values based on the final descent direction.
2. The method of claim 1, wherein the local approximated quadratic model of the loss function comprises the loss function applied to the current parameter values, the gradient of the loss function when applied to the current parameter values, and an approximation of the curvature of the loss function.
3. The method of claim 2, wherein the local approximate quadratic model incorporates stochastic Hessian information corresponding to the training dataset.
4. The method of claim 3, wherein the stochastic Hessian information comprises a random sampling the training dataset.
5. The method of claim 4, wherein the random sampling the training dataset is resampled during each of the plurality of iterations.
6. The method of claim 1, wherein the initial parameter values are selected based on a random sampling of the training dataset.
7. The method of claim 1, wherein the current parameter values are further updated according to a learning rate determined by a line search method.
8. The method of claim 7, wherein the line search method is an Armijo line search method or Goldstein line-search method.
9. A system for training a deep learning network, the system comprising: one or more processors; a non-transitory, computer-readable storage medium in operable communication with the processors, wherein the computer-readable storage medium comprises one or more programming instructions that, when executed, cause the processors to: define a loss function corresponding to the deep learning network; receive a training dataset comprising a plurality of training samples; set current parameter values of the deep learning network to initial parameter values; using a computing platform to iteratively minimizing the loss function over a plurality of iterations, wherein each iteration comprises: applying an eigCG solver to (a) determine a descent direction by minimizing a local approximated quadratic model of the loss function with respect to current parameter values and the training dataset and (b) approximate a leftmost eigenvector and a responding eigenvalue of a matrix of second-order partial derivatives of the loss function, using the leftmost eigenvector as a negative curvature direction to prevent the optimization method from converging to saddle points, selecting a final descent direction as either the descent direction or the negative curvature direction based on a test on a rate of decrease of the local approximated quadratic model of the loss function in the negative curvature direction, using a curvilinear line-search or adaptive line-search to guide the optimization method to a local minimum, and updating the current parameter values based on the final descent direction.
10. The system of claim 9, wherein the local approximated quadratic model of the loss function comprises the loss function applied to the current parameter values, the gradient of the loss function when applied to the current parameter values, and an approximation of the curvature of the loss function.
11. The system of claim 10, wherein the local approximate quadratic model incorporates stochastic Hessian information corresponding to the training dataset.
12. The system of claim 11, wherein the stochastic Hessian information comprises a random sampling the training dataset.
13. The system of claim 12, wherein the random sampling the training dataset is resampled during each of the plurality of iterations.
14. The system of claim 9, wherein the initial parameter values are selected based on a random sampling of the training dataset.
15. The system of claim 9, wherein the one or more processors are part of a parallel computing platform which is used to parallelize one or more processing operations included in iteratively minimizing the loss function.
16. The system of claim 15, wherein the training dataset is divided into a plurality of training subsets and the eigCG solver is applied to each of the training subsets in parallel to determine the descent direction by minimizing the local approximated quadratic model of the loss function with respect to the current parameter values and the training subset.
17. An article of manufacture for training a neural network, the article of manufacture comprising a non-transitory, tangible computer-readable medium holding computer-executable instructions for performing a method comprising: receive a training dataset comprising a plurality of training samples; setting current parameter values of the deep learning network to initial parameter values; iteratively minimizing a loss function corresponding to the neural network over a plurality of iterations, wherein each iteration comprises: applying an eigCG solver to (a) determine a descent direction by minimizing a local approximated quadratic model of the loss function with respect to current parameter values and the training dataset and (b) approximate a leftmost eigenvector and a responding eigenvalue of a matrix of second-order partial derivatives of the loss function, using the leftmost eigenvector as a negative curvature direction to prevent the optimization method from converging to saddle points, selecting a final descent direction as either the descent direction or the negative curvature direction based on a test on a rate of decrease of the local approximated quadratic model of the loss function in the negative curvature direction, using a curvilinear line-search or adaptive line-search to guide the optimization method to a local minimum, and updating the current parameter values based on the final descent direction.
</claims>
</document>
