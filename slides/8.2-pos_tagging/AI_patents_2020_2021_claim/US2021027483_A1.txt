<document>

<filing_date>
2020-10-12
</filing_date>

<publication_date>
2021-01-28
</publication_date>

<priority_date>
2018-03-06
</priority_date>

<ipc_classes>
G06T11/60,G06T7/50,G06T7/70
</ipc_classes>

<assignee>
AT&T INTELLECTUAL PROPERTY I (AMERICAN TELEPHONE AND TELEGRAPH COMPANY INTELLECTUAL PROPERTY I)
</assignee>

<inventors>
ZAVESKY, ERIC
HAN BO
</inventors>

<docdb_family_id>
67841960
</docdb_family_id>

<title>
COLLABORATIVE VISUAL ENHANCEMENT DEVICES
</title>

<abstract>
A method may include a processing system including at least one processor detecting an object in a captured image, capturing features of the object, determining a location of a first device, and determining a bearing from the first device to the object. The processing system may further transmit the features of the object, the location of the first device, and the bearing from the first device to the object to a second device. Another method may include a processing system including at least one processor receiving, from a second device, features of an object, a location of the second device, and a bearing from the second device to the object. The processing system may further provide an assistance to orient the first device to view the object using the features of the object, the location of the second device, and the bearing from the second device to the object.
</abstract>

<claims>
1. A first device comprising: a processing system including at least one processor; and a computer-readable medium storing instructions which, when executed by the processing system, cause the processing system to perform operations, the operations comprising: detecting an object in a captured image; capturing features of the object; determining a location of the first device; determining a bearing from the first device to the object; and transmitting the features of the object, the location of the first device, and the bearing from the first device to the object to a second device.
2. The first device of claim 1, wherein the operations further comprise: determining a range to the object, wherein the transmitting further comprises transmitting the range to the object.
3. The first device of claim 1, wherein the operations further comprise: identifying the object or a type of the object; and receiving information about the object from a network-based data source.
4. The first device of claim 3, the operations further comprising: presenting the information via the first device.
5. The first device of claim 1, wherein the detecting the object comprises: applying an image salience model to the captured image.
6. The first device of claim 1, further comprising: a camera, wherein the captured image is obtained via the camera; a global positioning system unit, wherein the determining the location is via the global positioning system unit.
7. The first device of claim 1, further comprising: a gyroscope; and a compass, wherein the determining the bearing from the first device to the object is based upon measurements from the gyroscope and the compass.
8. A method comprising: detecting, by a processing system including at least one processor, an object in a captured image; capturing, by the processing system, features of the object; determining, by the processing system, a location of a first device; determining, by the processing system, a bearing from the first device to the object; and transmitting, by the processing system, the features of the object, the location of the first device, and the bearing from the first device to the object to a second device.
9. The method of claim 8, further comprising: determining a range to the object, wherein the transmitting further comprises transmitting the range to the object.
10. The method of claim 8, further comprising: identifying the object or a type of the object; and receiving information about the object from a network-based data source.
11. The method of claim 10, further comprising: presenting the information via the first device.
12. The method of claim 8, wherein the detecting the object comprises: applying an image salience model to the captured image.
13. The method of claim 8, wherein the captured image is obtained via a camera, and wherein the determining the location is via a global positioning system unit.
14. The method of claim 8, wherein the determining the bearing from the first device to the object is based upon measurements from a gyroscope and a compass.
15. A non-transitory computer-readable medium storing instructions which, when executed by a processor, cause the processor to perform operations, the operations comprising: detecting an object in a captured image; capturing features of the object; determining a location of a first device; determining a bearing from the first device to the object; and transmitting the features of the object, the location of the first device, and the bearing from the first device to the object to a second device.
16. The non-transitory computer-readable medium of claim 15, wherein the operations further comprise: determining a range to the object, wherein the transmitting further comprises transmitting the range to the object.
17. The non-transitory computer-readable medium of claim 15, wherein the operations further comprise: identifying the object or a type of the object; and receiving information about the object from a network-based data source.
18. The non-transitory computer-readable medium of claim 17, the operations further comprising: presenting the information via the first device.
19. The non-transitory computer-readable medium of claim 15, wherein the detecting the object comprises: applying an image salience model to the captured image.
20. The non-transitory computer-readable medium of claim 15, wherein the captured image is obtained via a camera and wherein the determining the location is via a global positioning system unit.
</claims>
</document>
