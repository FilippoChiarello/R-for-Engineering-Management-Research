<document>

<filing_date>
2019-01-15
</filing_date>

<publication_date>
2020-07-16
</publication_date>

<priority_date>
2019-01-15
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
NVIDIA CORPORATION
</assignee>

<inventors>
HEINRICH, GREG
FROSIO, IURI
</inventors>

<docdb_family_id>
71516722
</docdb_family_id>

<title>
ASYNCHRONOUS EARLY STOPPING IN HYPERPARAMETER METAOPTIMIZATION FOR A NEURAL NETWORK
</title>

<abstract>
One embodiment of a method includes adjusting a plurality of hyperparameters corresponding to a plurality of neural networks trained asynchronously relative to each other using a plurality of computer systems. The method further includes asynchronously measuring one or more performance metrics associated with the plurality of neural networks being trained. The method further includes ceasing the adjusting of the plurality of hyperparameters corresponding to one or more of the plurality of neural networks if the one or more performance metrics associated with the one or more of the plurality of neural networks are below a threshold.
</abstract>

<claims>
1. A method, comprising: adjusting a plurality of hyperparameters corresponding to a plurality of neural networks trained asynchronously relative to each other using a plurality of computer systems; asynchronously measuring one or more performance metrics associated with the plurality of neural networks being trained; and ceasing the adjusting of the plurality of hyperparameters corresponding to one or more of the plurality of neural networks if the one or more performance metrics associated with the one or more of the plurality of neural networks are below a threshold.
2. The method of claim 1, further comprising, upon ceasing the adjusting of the plurality of hyperparameters corresponding to the one or more of the plurality of neural networks, asynchronously initiating training of one or more additional neural networks on a subset of the plurality of computer systems previously used to train the one or more of the plurality of neural networks.
3. The method of claim 1, wherein adjusting the plurality of hyperparameters comprises selecting a set of hyperparameter values for each neural network in the plurality of neural networks.
4. The method of claim 1, wherein asynchronously measuring the one or more performance metrics comprises collecting the one or more performance metrics at an end of a training phase used to train a first neural network after a second neural network has previously completed the training phase.
5. The method of claim 1, wherein ceasing the adjusting of the plurality of hyperparameters comprises increasing a proportion of the plurality of neural networks for which the adjusting of the plurality of hyperparameters is ceased with successive training phases used to train the plurality of neural networks.
6. The method of claim 1, wherein ceasing the adjusting of the plurality of hyperparameters comprises selecting the one or more of the plurality of neural networks based on the one or more performance metrics associated with training the plurality of neural networks.
7. The method of claim 6, wherein selecting the one or more of the plurality of neural networks comprises: selecting a first neural network that completes a training phase at a first time for continued training; and selecting a second neural network with a performance metric that is lower than the threshold and that completes the training phase at a second time that is later than the first time for inclusion in the one or more of the plurality of neural networks.
8. The method of claim 6, wherein selecting the one or more of the plurality of neural networks further comprises adjusting at least one of the first time and the second time based on a number of computational resources used to train at least one of the first neural network and the second neural network.
9. The method of claim 1, wherein the threshold comprises a quantile associated with the one or more performance metrics.
10. The method of claim 1, wherein the plurality of hyperparameters comprise at least one of a learning rate, a regularization parameter, a convergence parameter, a model topology, a number of training samples, a parameter-optimization technique, and a model type.
11. A non-transitory computer readable medium storing instructions that, when executed by a processor, cause the processor to at least: adjust a plurality of hyperparameters corresponding to a plurality of machine learning models trained asynchronously relative to each other using a plurality of computer systems; asynchronously measure one or more performance metrics associated with the plurality of machine learning models being trained; and cease the adjusting of the plurality of hyperparameters corresponding to one or more of the plurality of machine learning models if the one or more performance metrics associated with the one or more of the plurality of machine learning models is below a threshold.
12. The non-transitory computer-readable medium of claim 11, further storing instructions that, when executed by the processor, cause the processor to at least asynchronously initiate training of one or more additional machine learning models on a subset of the plurality of computer systems previously used to train the one or more of the plurality of machine learning models.
13. The non-transitory computer-readable medium of claim 11, wherein ceasing the adjusting of the plurality of hyperparameters comprises selecting the one or more of the plurality of machine learning models based on the one or more performance metrics and an eviction rate associated with training of the plurality of machine learning models.
14. The non-transitory computer-readable medium of claim 11, wherein asynchronously measuring the one or more performance metrics associated with the plurality of machine learning models being trained comprises collecting the one or more performance metrics up to a maximum number of training phases used to asynchronously train the plurality of machine learning models.
15. The non-transitory computer-readable medium of claim 11, wherein adjusting the plurality of hyperparameters comprises selecting a set of hyperparameter values for each machine learning model in the plurality of machine learning models.
16. A system, comprising: a memory storing one or more instructions; and a processor that executes the instructions to at least: adjust a plurality of hyperparameters corresponding to a plurality of machine learning models trained asynchronously relative to each other using a plurality of computer systems; asynchronously measure one or more performance metrics associated with the plurality of machine learning models being trained; and cease the adjusting of the plurality of hyperparameters corresponding to one or more of the plurality of machine learning models if the one or more performance metrics associated with the one or more of the plurality of machine learning models is below a threshold.
17. The system of claim 16, wherein the processor further executes the instructions to at least asynchronously initiate training of one or more additional machine learning models on a subset of the plurality of computer systems previously used to train the one or more of the plurality of machine learning models.
18. The system of claim 16, wherein ceasing the adjusting of the plurality of hyperparameters comprises selecting the one or more of the plurality of machine learning models based on the one or more performance metrics, an eviction rate associated with training the plurality of machine learning models, and training speeds associated with training the plurality of machine learning models.
19. The system of claim 16, wherein the plurality of hyperparameters comprise at least one of a learning rate, a regularization parameter, a convergence parameter, a model topology, a number of training samples, a parameter-optimization technique, and a model type.
20. The system of claim 16, wherein the plurality of machine learning models comprise a neural network.
</claims>
</document>
