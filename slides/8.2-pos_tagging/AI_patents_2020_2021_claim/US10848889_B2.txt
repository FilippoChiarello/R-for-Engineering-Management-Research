<document>

<filing_date>
2019-01-04
</filing_date>

<publication_date>
2020-11-24
</publication_date>

<priority_date>
2017-06-30
</priority_date>

<ipc_classes>
G06T7/20,G10L19/008,G10L21/0208,G10L21/0216,G10L21/0272,H04R5/00,H04S3/00,H04S7/00
</ipc_classes>

<assignee>
APPLE
</assignee>

<inventors>
JOHNSON, MARTIN, E.
ATKINS, JOSHUA D.
SHEAFFER, JONATHAN D.
WOOD, STUART J.
</inventors>

<docdb_family_id>
64735026
</docdb_family_id>

<title>
Intelligent audio rendering for video recording
</title>

<abstract>
Image analysis of a video signal is performed to produce first metadata, and audio analysis of a multi-channel sound track associated with the video signal is performed to produce second metadata. A number of time segments of the sound track are processed, wherein each time segment is processed by either (i) spatial filtering of the audio signals or (ii) spatial rendering of the audio signals, not both, wherein for each time segment a decision was made to select between the spatial filtering or the spatial rendering, in accordance with the first and second metadata. A mix of the processed sound track and the video signal is generated. Other embodiments are also described and claimed.
</abstract>

<claims>
1. A method for intelligently rendering audio for a video recording, the method comprising: accessing a video recording, wherein the video recording is produced by a portable electronic device and captures i) motion in a scene as a video signal produced by a camera in the portable electronic device, and ii) sound in the scene as a sound track that comprises a plurality of audio signals produced by a plurality of microphones, respectively, in the portable electronic device; performing image analysis of the video signal to produce first metadata, and audio analysis of the sound track to produce second metadata; processing a plurality of time segments of the sound track, wherein each time segment is processed by either (i) spatial filtering of the audio signals or (ii) spatial rendering of the audio signals, based on a decision made in accordance with the first and second metadata; and generating a mix of the processed sound track and the video signal.
2. The method of claim 1 wherein performing the image analysis comprises processing the video signal by an image classifier that (i) detects image features within the scene and (ii) generates the first metadata that describes the detected image features, and wherein performing the audio analysis comprises processing the plurality of audio signals by an audio classifier that (i) detects acoustic features in the scene and (ii) generates the second metadata that describes the detected acoustic features.
3. The method of claim 2 wherein the decision as between spatial filtering and spatial rendering is made by comparing (i) a combination of the image features and the acoustic features with (ii) predefined image features and predefined acoustic features, in order to categorize the combination; and selecting either the spatial filtering or the spatial rendering based on a particular category that matches the combination of the image features and acoustic features.
4. The method of claim 1 wherein the spatial filtering of the audio signals comprises focusing on sound from a single direction in space wherein sound from other directions are filtered out, using one of dereverberation, source separation or beamforming.
5. The method of claim 4 wherein the sound from the single direction is rendered monophonically in the processed sound track.
6. The method of claim 4 wherein the decision is made to perform spatial filtering whenever there is a single dominant sound source in the scene that is surrounded by distracters or by ambient noise.
7. The method of claim 1 wherein the spatial rendering of the audio signals comprises retaining sounds from a plurality of directions in space, and rendering them, either using binaural techniques or loudspeaker panning techniques, at their respective directions or positions in space.
8. The method of claim 7 wherein whenever the decision is made to perform spatial rendering, the entirety of a soundscape in the scene is being captured.
9. The method of claim 1 wherein the decision is made to perform spatial filtering in response to the first and second metadata indicating detected speech and a detected face in a same time segment.
10. The method of claim 1 wherein the decision is made in accordance with a machine learning algorithm that has been trained with audio content that has been labeled by expert listeners.
11. A portable electronic device for intelligently rendering audio for a video recording made by the device, the portable electronic device comprising: a plurality of microphones configured to capture sound in a scene as a sound track having a plurality of audio signals produced by the plurality of microphones, respectively; a video camera configured to capture motion in a scene as a video signal; a processor; and memory having stored therein instructions that when executed by the processor perform image analysis of the video signal to produce first metadata and audio analysis of the sound track to produce second metadata, process a plurality of time segments of the sound track, wherein each time segment is processed by (i) spatial filtering of the audio signals or (ii) spatial rendering of the audio signals to produce a processed sound track, based on a decision made in accordance with the first and second metadata, and generate a mix of the processed sound track and the video signal.
12. The device of claim 11 wherein performing the image analysis comprises processing the video signal by an image classifier that (i) detects image features within the scene and (ii) generates the first metadata that describes the detected image features, and wherein performing the audio analysis comprises processing the plurality of audio signals by an audio classifier that (i) detects acoustic features in the scene and (ii) generates the second metadata that describes the detected acoustic features.
13. The device of claim 12 wherein the decision as between spatial filtering and spatial rendering is made by comparing (i) a combination of the image features and the acoustic features with (ii) predefined image features and predefined acoustic features, in order to categorize the combination; and selecting either the spatial filtering or the spatial rendering based on a particular category that matches the combination of the image features and acoustic features.
14. The device of claim 11 wherein the spatial filtering of the audio signals comprises focusing on sound from a single direction in space wherein sound from other directions are filtered out, using one of dereverberation, source separation or beamforming.
15. The device of claim 14 wherein the sound from the single direction is rendered monophonically in the processed sound track.
16. The device of claim 14 wherein the decision is made to perform spatial filtering whenever there is a single dominant sound source in the scene that is surrounded by distracters or by ambient noise.
17. The device of claim 11 wherein the spatial rendering of the audio signals comprises retaining sounds from a plurality of directions in space, and rendering them, either using binaural techniques or loudspeaker panning techniques, at their respective directions or positions in space.
18. The device of claim 17 wherein whenever the decision is made to perform spatial rendering, the entirety of a soundscape in the scene is captured.
19. The device of claim 11 wherein the decision is made to perform spatial filtering in response to the first and second metadata indicating detected speech and a detected face in a same time segment.
20. The device of claim 11 wherein the decision is made in accordance with a machine learning algorithm that has been trained with audio content that has been labeled by expert listeners.
</claims>
</document>
