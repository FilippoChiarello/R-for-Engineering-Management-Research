<document>

<filing_date>
2019-07-26
</filing_date>

<publication_date>
2021-01-28
</publication_date>

<priority_date>
2019-07-26
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,G06T11/60,G06T7/10
</ipc_classes>

<assignee>
ADOBE
</assignee>

<inventors>
LIN ZHE
ZHANG, JIANMING
PERAZZI, FEDERICO
Zhang, He
</inventors>

<docdb_family_id>
71526464
</docdb_family_id>

<title>
UTILIZING A NEURAL NETWORK HAVING A TWO-STREAM ENCODER ARCHITECTURE TO GENERATE COMPOSITE DIGITAL IMAGES
</title>

<abstract>
The present disclosure relates to utilizing a neural network having a two-stream encoder architecture to accurately generate composite digital images that realistically portray a foreground object from one digital image against a scene from another digital image. For example, the disclosed systems can utilize a foreground encoder of the neural network to identify features from a foreground image and further utilize a background encoder to identify features from a background image. The disclosed systems can then utilize a decoder to fuse the features together and generate a composite digital image. The disclosed systems can train the neural network utilizing an easy-to-hard data augmentation scheme implemented via self-teaching. The disclosed systems can further incorporate the neural network within an end-to-end framework for automation of the image composition process.
</abstract>

<claims>
1. A non-transitory computer-readable medium storing instructions thereon that, when executed by at least one processor, cause a computing device to: identify a foreground image, a background image, and a segmentation mask corresponding to the foreground image; and generate a composite digital image based on the foreground image, the background image, and the segmentation mask by: generating a foreground feature map based on the foreground image and the segmentation mask utilizing a foreground encoder of a multi-level fusion neural network; generating a background feature map based on the background image and the segmentation mask utilizing a background encoder of the multi-level fusion neural network; and generating the composite digital image based on the foreground feature map and the background feature map using a decoder of the multi-level fusion neural network.
2. The non-transitory computer-readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computing device to generate an inverted segmentation mask based on the segmentation mask corresponding to the foreground image, wherein generating the background feature map based on the background image and the segmentation mask comprises generating the background feature map based on the background image and the inverted segmentation mask.
3. The non-transitory computer-readable medium of claim 1, wherein the instructions, when executed by the at least one processor, cause the computing device to identify the segmentation mask corresponding to the foreground image by generating the segmentation mask based on the foreground image utilizing a foreground segmentation neural network.
4. The non-transitory computer-readable medium of claim 3, further comprising instructions that, when executed by the at least one processor, cause the computing device to modify the segmentation mask corresponding to the foreground image utilizing a mask refinement neural network.
5. The non-transitory computer-readable medium of claim 1, further comprising instructions that, when executed by the at least one processor, cause the computing device to: identify a first layer-specific feature map generated by the foreground encoder and a second layer-specific feature map generated by the background encoder; and provide the first layer-specific feature map and the second layer-specific feature map to a layer of the decoder of the multi-level fusion neural network via skip links, wherein generating the composite digital image based on the foreground feature map and the background feature map using the decoder of the multi-level fusion neural network comprises generating the composite digital image further based on the first layer-specific feature map and the second layer-specific feature map using the decoder.
6. The non-transitory computer-readable medium of claim 5, wherein a layer of the foreground encoder corresponding to the first layer-specific feature map is at a same encoder level as a layer of the background encoder corresponding to the second layer-specific feature map.
7. The non-transitory computer-readable medium of claim 1, wherein the composite digital image comprises a foreground object from the foreground image portrayed against a scene from the background image.
8. The non-transitory computer-readable medium of claim 1, wherein the foreground image comprises a training foreground image and the background image comprises a training background image; and further comprising instructions that, when executed by the at least one processor, cause the computing device to train the multi-level fusion neural network to generate composite digital images by: comparing the composite digital image to a target composite digital image to determine a loss; and modifying parameters of the multi-level fusion neural network based on the determined loss.
9. The non-transitory computer-readable medium of claim 8, wherein the instructions, when executed by the at least one processor, cause the computing device to identify the foreground image by generating the foreground image utilizing the multi-level fusion neural network.
10. The non-transitory computer-readable medium of claim 8, further comprising instructions that, when executed by the at least one processor, cause the computing device to generate the target composite digital image utilizing the multi-level fusion neural network.
11. A system comprising: at least one memory device comprising a multi-level fusion neural network trained to generate composite digital images, the multi-level fusion neural network comprising a foreground encoder, a background encoder, and a decoder; at least one server device that causes the system to: identify a foreground image and a background image; generate a segmentation mask based on the foreground image utilizing a foreground segmentation neural network; generate a foreground feature map based on the foreground image and the segmentation mask utilizing the foreground encoder of the multi-level fusion neural network; generate a background feature map based on the background image and the segmentation mask utilizing the background encoder of the multi-level fusion neural network; combine the foreground feature map and the background feature map to generate a combined feature map; and generate a composite digital image based on the combined feature map using the decoder of the multi-level fusion neural network.
12. The system of claim 11, wherein the at least one server device causes the system to train the multi-level fusion neural network to generate composite digital images by: generating a target composite digital image based on an easy training foreground image and a first training background image utilizing the multi-level fusion neural network; generating a hard training foreground image based on the easy training foreground image and a second training background image utilizing the multi-level fusion neural network; generating a predicted composite digital image based on the hard training foreground image and the first training background image utilizing the multi-level fusion neural network; and modifying parameters of the multi-level fusion neural network based on a comparison of the predicted composite digital image and the target composite digital image.
13. The system of claim 12, wherein: the easy training foreground image comprises a foreground object portrayed against a pure color background; and the hard training foreground image comprises the foreground object portrayed against a background from the second training background image, wherein the background varies in detail.
14. The system of claim 12, wherein the at least one server device causes the system to: compare the predicted composite digital image and the target composite digital image to determine a L1 loss and a perceptual loss; and determine a combined loss based on the L1 loss and the perceptual loss, wherein modifying the parameters of the multi-level fusion neural network based on the comparison of the predicted composite digital image and the target composite digital image comprises modifying the parameters based on the combined loss.
15. The system of claim 14, wherein the at least one server device causes the system to determine the combined loss by: applying a weight to the perceptual loss to generate a weighted perceptual loss; and combining the L1 loss and the weighted perceptual loss to generate the combined loss.
16. The system of claim 11, wherein the at least one server device causes the system to: generate an inverted segmentation mask based on the segmentation mask corresponding to the foreground image, and generate the background feature map based on the background image and the segmentation mask by generating the background feature map based on the background image and the inverted segmentation mask.
17. The system of claim 11, wherein the at least one server device causes the system to modify a boundary of a foreground object portrayed in the segmentation mask based on the foreground image and the segmentation mask utilizing a mask refinement neural network.
18. In a digital medium environment for editing digital images, a computer-implemented method comprising: identifying a foreground image, a background image, and a segmentation mask corresponding to the foreground image; and performing a step for generating a composite digital image utilizing a multi-level fusion neural network based on the foreground image, the background image, and the segmentation mask.
19. The computer-implemented method of claim 18, wherein identifying the segmentation mask corresponding to the foreground image comprises generating the segmentation mask based on the foreground image utilizing a foreground segmentation neural network.
20. The computer-implemented method of claim 19, further comprising modifying the segmentation mask corresponding to the foreground image utilizing a mask refinement neural network.
</claims>
</document>
