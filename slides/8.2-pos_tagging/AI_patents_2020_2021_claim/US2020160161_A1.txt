<document>

<filing_date>
2019-11-20
</filing_date>

<publication_date>
2020-05-21
</publication_date>

<priority_date>
2018-11-20
</priority_date>

<ipc_classes>
G06F16/22,G06F7/50,G06N3/04,G06N3/063
</ipc_classes>

<assignee>
KAIST (KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY)
KAIST (KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY)
</assignee>

<inventors>
YOO, HOI, JUN
LEE, JIN MOOK
</inventors>

<docdb_family_id>
70727988
</docdb_family_id>

<title>
DEEP NEURAL NETWORK ACCELERATOR INCLUDING LOOKUP TABLE BASED BIT-SERIAL PROCESSING ELEMENTS
</title>

<abstract>
A deep neural network accelerator includes a feature loader that stores input features, a weight memory that stores a weight, and a processing element. The processing element applies 1-bit weight values to the input features to generate results according to the 1-bit weight values, receives a target weight corresponding to the input features from the weight memory, and selects a target result corresponding to the received target weight from among the results to generate output features.
</abstract>

<claims>
1. A deep neural network accelerator comprising: a feature loader configured to store input features; a weight memory configured to store a weight; and a processing element, wherein the processing element is configured to: apply 1-bit weight values to the input features to generate results according to the 1-bit weight values; receive a target weight corresponding to the input features from the weight memory; and select a target result corresponding to the received target weight from among the results to generate output features.
2. The deep neural network accelerator of claim 1, wherein the processing element is configured to: receive first bits of the target weight during a first time; select a first target result corresponding to the first bits from among the results; receive second bits of the target weight during a second time after the first time; select a second target result corresponding to the second bits from among the results; and accumulate the second target result on the first target result.
3. The deep neural network accelerator of claim 1, wherein the processing element is configured to: generate all combinations of the 1-bit weight values; and calculate a partial product of the input features and the 1-bit weight values with respect to each of the combinations to generate the results.
4. The deep neural network accelerator of claim 1, wherein the target weight includes first weight values corresponding to each of the input features and a second weight value deciding whether to invert the target result, wherein the first weight values and the second weight value are a 1-bit weight having a first value or a second value, wherein, when the second weight value is the first value, the processing element selects the target result based on the first weight values and generates the output features based on the target result, and wherein, when the second weight value is the second value, the processing element inverts the first weight values to select the target result and inverts the target result to generate the output features.
5. The deep neural network accelerator of claim 1, wherein the target weight includes weight values respectively corresponding to the input features, the number of bits of each of the weight values is more than one, and wherein the processing element selects a first target result based on least significant bits of the weight values, selects a second target result based on bits of the weight values, which are positioned between the least significant bits and most significant bits of the weight values, and selects a third target result based on the most significant bits of the weight values.
6. The deep neural network accelerator of claim 5, wherein the processing element bit-shifts the second target result, adds the bit-shifted second target result to the first target result to generate an intermediate accumulation result, and bit-shifts the third target result to subtract the bit-shifted third target result from the intermediate accumulation result.
7. The deep neural network accelerator of claim 1, wherein the processing element includes: a first lookup table bundle configured to generate first results corresponding to first features of the input features, to receive a first target weight corresponding to the first features from the weight memory, and to output a first target result corresponding to the first target weight from among the first results; a second lookup table bundle configured to generate second results corresponding to second features of the input features, to receive a second target weight corresponding to the second features from the weight memory, and to output a second target result corresponding to the second target weight from among the second results; and an accelerator configured to accumulate the first target result and the second target result to generate the output features.
8. The deep neural network accelerator of claim 1, wherein the processing element includes: a multiplexer configured to output the input features received from the feature loader or the target weight received from the weight memory, based on an update signal; a first lookup table module configured to store first results corresponding to first features of the input features, to receive a first target weight, which is a first portion of the output target weight, corresponding to the first features from the multiplexer, and to output a first target result corresponding to the first target weight from among the first results; a second lookup table module configured to store second results corresponding to second features of the input features, to receive a second target weight, which is a second portion of the output target weight, corresponding to the second features from the multiplexer, and to output a second target result corresponding to the second target weight from among the second results; and a module adder configured to generate the first results based on the first features, to generate the second results based on the second features, and to accumulate the first and second target results.
9. The deep neural network accelerator of claim 1, wherein the processing element includes: file registers configured to store the results; and multiplexers configured to receive the target weight and to select the target result from the file registers.
10. The deep neural network accelerator of claim 9, wherein the processing element further includes: a weight input circuit configured to receive weight values as much as the number of bits identical to a product of the number of the input features and the number of the multiplexers, at the target weight, and to transfer the received weight values to the multiplexers.
11. The deep neural network accelerator of claim 1, wherein the feature loader aligns a first portion of a first input feature map and a second portion of a second input feature map to generate the input features.
12. A deep neural network accelerator comprising: deep neural network cores each configured to generate an output feature map based on an input feature map and a weight; and an aggregation core configured to receive the output feature map from each of the deep neural network cores and to accumulate the received output feature map to generate a final output feature map, wherein each of the deep neural network cores includes: a weight memory configured to store the weight; feature loaders each configured to store input features being a portion of the input feature map; and processing elements each configured to receive the input features from one of the feature loaders and to generate output features to be included in the output feature map based on a target weight corresponding to the input features, wherein each of the processing elements generates results according to the 1-bit weight values by applying the 1-bit weight values to the input features and generates the output features as a result of selecting a target result corresponding to the received target weight from among the results.
13. The deep neural network accelerator of claim 12, wherein a first feature loader of the feature loaders outputs input features corresponding to a first area of the input feature map as a first processing element of the processing elements, and wherein a second feature loader of the feature loaders outputs input features corresponding to a second area of the input feature map as a second processing element of the processing elements.
14. The deep neural network accelerator of claim 13, wherein a portion of the first area overlaps a portion of the second area.
15. The deep neural network accelerator of claim 12, wherein each of the processing elements includes: a first lookup table bundle configured to generate first results corresponding to first features of the input features, to receive a first target weight corresponding to the first features from the weight memory, and to output a first target result corresponding to the first target weight from among the first results; a second lookup table bundle configured to generate second results corresponding to second features of the input features, to receive a second target weight corresponding to the second features from the weight memory, and to output a second target result corresponding to the second target weight from among the second results; and an accelerator configured to accumulate the first target result and the second target result to generate the output features.
16. The deep neural network accelerator of claim 15, wherein the input feature map includes a first feature map and a second feature map, and wherein each of the feature loaders extracts the first features from the first feature map, extracts the second features from the second feature map, transfers the first features to the first lookup table bundle, and transfers the second features to the second lookup table bundle.
17. The deep neural network accelerator of claim 15, wherein the first lookup table bundle includes: a first lookup table module configured to store a first portion of the first results and to output a first target portion of the first target result based on the first portion; a second lookup table module configured to store a second portion of the first results and to output a second target portion of the first target result based on the second portion; and a first module adder configured to generate the first results based on the first features and to generate the first target result by accumulating the first target portion and the second target portion, and wherein the second lookup table bundle includes: a third lookup table module configured to store a third portion of the second results and to output a third target portion of the second target result based on the third portion; a fourth lookup table module configured to store a fourth portion of the second results and to output a fourth target portion of the second target result based on the fourth portion; and a second module adder configured to generate the second results based on the second features and to generate the second target result by accumulating the third target portion and the fourth target portion.
18. The deep neural network accelerator of claim 12, wherein the target weight includes first weight values corresponding to each of the input features and a second weight value deciding whether to invert the target result, wherein the first weight values and the second weight value are a 1-bit weight having a first value or a second value, wherein, when the second weight value is the first value, the processing element selects the target result based on the first weight values and generates the output features based on the target result, and wherein, when the second weight value is the second value, the processing element inverts the first weight values to select the target result and inverts the target result to generate the output features.
19. The deep neural network accelerator of claim 12, wherein the target weight includes weight values respectively corresponding to the input features, the number of bits of each of the weight values is more than one, and wherein each of the processing elements selects a first target result based on least significant bits of the weight values, selects a second target result based on bits of the weight values, which are positioned between the least significant bits and most significant bits of the weight values, and selects a third target result based on the most significant bits of the weight values.
20. The deep neural network accelerator of claim 19, wherein each of the processing elements bit-shifts the second target result, adds the bit-shifted second target result to the first target result to generate an intermediate accumulation result, and bit-shifts the third target result to subtract the bit-shifted third target result from the intermediate accumulation result.
</claims>
</document>
