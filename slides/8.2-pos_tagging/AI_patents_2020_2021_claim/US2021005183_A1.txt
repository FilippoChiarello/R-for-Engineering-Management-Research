<document>

<filing_date>
2020-07-03
</filing_date>

<publication_date>
2021-01-07
</publication_date>

<priority_date>
2019-07-03
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08,G10L15/16
</ipc_classes>

<assignee>
QUALCOMM
</assignee>

<inventors>
HWANG, KYU WOONG
LEE, Mingu
LEE, Jinkyu
JANG, Hye Jin
</inventors>

<docdb_family_id>
74066087
</docdb_family_id>

<title>
ORTHOGONALLY CONSTRAINED MULTI-HEAD ATTENTION FOR SPEECH TASKS
</title>

<abstract>
A method for operating a neural network includes receiving an input sequence at an encoder. The input sequence is encoded to produce a set of hidden representations. Attention-heads of the neural network calculate attention weights based on the hidden representations. A context vector is calculated for each attention-head based on the attention weights and the hidden representations. Each of the context vectors correspond to a portion of the input sequence. An inference is output based on the context vectors.
</abstract>

<claims>
1. A method for operating a neural network, the method comprising: receiving an input sequence at an encoder; encoding the input sequence to produce hidden representations; calculating attention weights in attention-heads of the neural network based on the hidden representations; calculating a context vector for each attention-head based on the attention weights and the hidden representations, each context vector corresponding to a portion of the input sequence; and outputting an inference based on the context vectors.
2. The method of claim 1, in which the input sequence comprises an acoustic feature and the inference includes an indication of whether a keyword is included in a corresponding portion of the input sequence.
3. The method of claim 1, in which each of the context vectors are orthogonal to other context vectors of other attention-heads.
4. The method of claim 1, in which the attention weight is calculated based on a score function for focus determination.
5. The method of claim 1, further comprising selectively regularizing the neural network based on orthogonality constraints of the attention weights and context vectors.
6. The method of claim 5, in which the selectively regularizing further comprises calculating regularization terms only for positive samples in the input sequence.
7. An apparatus for operating a neural network, the apparatus comprising: a memory; and at least one processor coupled to the memory, the at least one processor being configured: to receive an input sequence at an encoder; to encode the input sequence to produce hidden representations; to calculate attention weights in attention-heads of the neural network based on the hidden representations; to calculate a context vector for each attention-head based on the attention weights and the hidden representations, each context vector corresponding to a portion of the input sequence; and to output an inference based on the context vectors.
8. The apparatus of claim 7, in which the input sequence comprises an acoustic feature and the at least one processor is further configured to output an inference indicating whether a keyword is included in a corresponding portion of the input sequence.
9. The apparatus of claim 7, in which each of the context vectors are orthogonal to other context vectors of other attention-heads.
10. The apparatus of claim 7, in which the at least one processor is further configured to calculate the attention weight based on a score function for focus determination.
11. The apparatus of claim 7, in which the at least one processor is further configured to selectively regularize the neural network based on orthogonality constraints of the attention weights and context vectors.
12. The apparatus of claim 11, in which the at least one processor is further configured to calculate regularization terms only for positive samples in the input sequence.
13. An apparatus for operating a neural network, the apparatus comprising: means for receiving an input sequence at an encoder; means for encoding the input sequence to produce hidden representations; means for calculating attention weights in attention-heads of the neural network based on the hidden representations; means for calculating a context vector for each attention-head based on the attention weights and the hidden representations, each context vector corresponding to a portion of the input sequence; and means for outputting an inference based on the context vectors.
14. The apparatus of claim 13, in which the input sequence comprises an acoustic feature and further comprising means for outputting an inference indicating whether a keyword is included in a corresponding portion of the input sequence.
15. The apparatus of claim 13, in which each of the context vectors are orthogonal to other context vectors of other attention-heads.
16. The apparatus of claim 13, further comprising means for calculating the attention weight based on a score function for focus determination.
17. The apparatus of claim 13, further comprising means for selectively regularizing the neural network based on orthogonality constraints of the attention weights and context vectors.
18. The apparatus of claim 17, further comprising means for calculating regularization terms only for positive samples in the input sequence.
19. A non-transitory computer readable medium having encoded thereon program code for operating a neural network, the program code being executed by a processor and comprising: program code to receive an input sequence at an encoder; program code to separate the input sequence into sequence parts; program code to encode the sequence parts to produce hidden representations; program code to calculate attention weights in attention-heads of the neural network based on the hidden representations; program code to calculate a context vector for each attention-head based on the attention weights and the hidden representations, each context vector corresponding to a portion of the input sequence; and program code to output an inference based on the context vectors.
20. The non-transitory computer readable medium of claim 19, in which the input sequence comprises an acoustic feature and further comprising program code to output an inference indicating whether a keyword is included in a corresponding portion of the input sequence.
21. The non-transitory computer readable medium of claim 19, in which each of the context vectors are orthogonal to other context vectors of other attention-heads.
22. The non-transitory computer readable medium of claim 19, further comprising program code to calculate the attention weight based on a score function for focus determination.
23. The non-transitory computer readable medium of claim 19, further comprising program code to selectively regularize the neural network based on orthogonality constraints of the attention weights and context vectors.
24. The non-transitory computer readable medium of claim 23, further comprising program code to calculate regularization terms only for positive samples in the input sequence.
</claims>
</document>
