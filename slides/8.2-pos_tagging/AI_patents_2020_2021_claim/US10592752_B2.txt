<document>

<filing_date>
2018-03-22
</filing_date>

<publication_date>
2020-03-17
</publication_date>

<priority_date>
2017-03-24
</priority_date>

<ipc_classes>
G06K9/00,G06N5/02,G11B27/02,H04N21/233,H04N21/234,H04N21/25,H04N21/258,H04N21/466,H04N21/81
</ipc_classes>

<assignee>
MIRRIAD ADVERTISING
</assignee>

<inventors>
MCLAUCHLAN, PHILIP
OK, DAVID
HARRIS, TIM
</inventors>

<docdb_family_id>
58464195
</docdb_family_id>

<title>
Predicting future insertion zone metadata
</title>

<abstract>
Aspects of the present disclosure aim to improve upon methods and systems for the incorporation of additional material into source video data. In particular, the method of the present disclosure may use a pre-existing corpus of source video data to produce, test and refine a prediction model for enabling the prediction of the characteristics of placement opportunities. The model may be created using video analysis techniques which obtain metadata regarding placement opportunities and also through the identification of categorical characteristics relating to the source video which may be provided as metadata with the source video, or obtaining through image processing techniques described below. Using the model, the method and system may then be used to create a prediction of insertion zone characteristics for projects for which source video is not yet available, but for which information corresponding to the identified categorical characteristics is known.
</abstract>

<claims>
1. A computer implemented method for determining insertion zone metadata of a new video, the method comprising: obtaining, from data associated with the new video, categorical metadata relating to the new video, the categorical metadata comprising at least one categorical variable; obtaining, for a particular categorical variable of the at least one categorical variable, an associated insertion value, wherein the associated insertion value defines a correlation between the particular categorical variable and one or more insertion zone characteristics, and wherein the associated insertion value is determined using a machine learning module trained by insertion zone metadata determined by digital analysis of one or more instances of source video, from a pre-existing corpus of video, that each have at least one categorical variable in common with the new video; and determining, without performing analysis on image content of the new video, at least one predicted value of at least one insertion zone characteristic of the insertion zone metadata of the new video based at least in part on the associated insertion value.
2. The method according to claim 1, wherein determining the associated insertion value comprises: digitally analysing source video to obtain source video insertion zone metadata, the source video insertion zone metadata comprising at least one value of a corresponding insertion zone characteristic for the source video.
3. The method according to claim 2, wherein digitally analysing the source video comprises analysing a plurality of instances of source video, each instance of source video having at least one categorical variable in common.
4. The method according to claim 3, further comprising creating a prediction function for a categorical variable, the prediction function being dependent on the at least one value of an insertion zone characteristic obtained for the source video, and the prediction function providing one or more insertion values for the categorical variable.
5. The method according to claim 4, wherein the prediction function is time series dependent on the at least one value of the insertion zone characteristic obtained for the source video.
6. The method according to claim 3, wherein the value of an insertion zone characteristic is estimated using a Gaussian process and/or Bayesian mixture models.
7. The method according to claim 1, further comprising determining a Video Impact Score based on the at least one predicted value of the at least one corresponding insertion zone characteristic for the new video.
8. The method according to claim 1, wherein a categorical variable relates to one or more of: video content type; video content length; video content producer; video content personnel; age of video content; intended audience; geographical distribution; intended distribution channels; advertising data; third-party analysis; and video content production time.
9. The method according to claim 1, wherein insertion zone characteristics relate to at least one of: insertion zone position; insertion zone size; insertion zone duration; scene locale; insertion zone character interactivity; insertion zone attentional value; insertion zone proximity; insertion zone orientation; insertion zone blur; insertion zone hero status; and insertion zone repetition.
10. The method according to claim 1, wherein at least one of the following applies: (a) determining insertion zone metadata is further based on client user preference data; and (b) obtaining an insertion value associated with a categorical variable comprises identifying a categorical variable present in source video, utilising one or more of: facial recognition technology, optical character recognition, computer vision techniques, manual annotation, convolutional neural network processing.
11. The method according to claim 1, further comprising determining a correlation between predicted values of insertion zone metadata and one or more client user profiles, the client user profiles comprising preferred values for insertion zone characteristics, and communicating the predicted insertion zone metadata information to a client user having preferred values for insertion zone characteristics which correlate to the predicted values of insertion zone characteristics.
12. A non-transitory computer-readable medium having computer executable instructions stored thereon, which when executed by a computing device cause a system to perform operations, the operations comprising: obtaining, from data associated with the new video, categorical metadata relating to the new video, the categorical metadata comprising at least one categorical variable; obtaining, for a particular categorical variable of the at least one categorical variable, an associated insertion value, wherein the associated insertion value defines a correlation between the particular categorical variable and one or more insertion zone characteristics, and wherein the associated insertion value is determined using a machine learning module trained by insertion zone metadata determined by digital analysis of one or more instances of source video, from a pre-existing corpus of video, that each have at least one categorical variable in common with the new video; and determining, without performing analysis on image content of the new video, at least one predicted value of at least one insertion zone characteristic of the insertion zone metadata of the new video based at least in part on the associated insertion value.
13. The non-transitory computer-readable medium of claim 12, wherein determining the associated insertion value comprises: digitally analysing source video to obtain source video insertion zone metadata, the source video insertion zone metadata comprising at least one value of a corresponding insertion zone characteristic for the source video.
14. The non-transitory computer-readable medium of claim 12, wherein digitally analysing the source video comprises analysing a plurality of instances of source video, each instance of source video having at least one categorical variable in common.
15. The non-transitory computer-readable medium of claim 14, further comprising creating a prediction function for a categorical variable, the prediction function being dependent on the at least one value of an insertion zone characteristic obtained for the source video, and the prediction function providing one or more insertion values for the categorical variable.
16. The non-transitory computer-readable medium of claim 15, wherein the prediction function is time series dependent on the at least one value of the insertion zone characteristic obtained for the source video.
17. A system for determining insertion zone metadata of a new video, the system comprising: one or more processors; and one or more memory modules, wherein the memory modules are configured to store computer executable instructions, which when executed on the one or more processors cause the system to: obtain, from data associated with the new video, categorical metadata relating to the new video, the categorical metadata comprising at least one categorical variable; obtain, for a particular categorical variable of the at least one categorical variable, an associated insertion value, wherein the associated insertion value defines a correlation between the particular categorical variable and one or more insertion zone characteristics, and wherein the associated insertion value is determined using a machine learning module trained by insertion zone metadata determined by digital analysis of one or more instances of source video, from a pre-existing corpus of video, that each have at least one categorical variable in common with the new video; and determine, without performing analysis on image content of the new video, at least one predicted value of at least one insertion zone characteristic of the insertion zone metadata of the new video based at least in part on the associated insertion value.
18. The system of claim 17, wherein a categorical variable relates to one or more of: video content type; video content length; video content producer; video content personnel; age of video content; intended audience; geographical distribution; intended distribution channels; advertising data; third-party analysis; and video content production time.
19. The system of claim 17, wherein insertion zone characteristics relate to at least one of: insertion zone position; insertion zone size; insertion zone duration; scene locale; insertion zone character interactivity; insertion zone attentional value; insertion zone proximity; insertion zone orientation; insertion zone blur; insertion zone hero status; and insertion zone repetition.
20. The system of claim 17, wherein determining a correlation between predicted values of insertion zone metadata and one or more client user profiles, the client user profiles comprising preferred values for insertion zone characteristics, and communicating the predicted insertion zone metadata information to a client user having preferred values for insertion zone characteristics which correlate to the predicted values of insertion zone characteristics.
</claims>
</document>
