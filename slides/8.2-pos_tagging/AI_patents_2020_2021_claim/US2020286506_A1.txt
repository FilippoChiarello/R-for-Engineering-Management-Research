<document>

<filing_date>
2020-03-09
</filing_date>

<publication_date>
2020-09-10
</publication_date>

<priority_date>
2019-03-08
</priority_date>

<ipc_classes>
G10L15/02,G10L15/06,G10L15/22,G10L25/63
</ipc_classes>

<assignee>
TATA CONSULTANCY SERVICES
</assignee>

<inventors>
VIRARAGHAVAN, VENKATA SUBRAMANIAN
PATEL, SACHIN
DESHPANDE, GAURI ASHUTOSH
DUGGIRALA, MAYURI
</inventors>

<docdb_family_id>
69779997
</docdb_family_id>

<title>
METHOD AND SYSTEM USING SUCCESSIVE DIFFERENCES OF SPEECH SIGNALS FOR EMOTION IDENTIFICATION
</title>

<abstract>
This disclosure relates generally to speech signal processing, and more particularly to a method and system for processing speech signal for emotion identification. The system processes a speech signal collected as input, during which a plurality of differential features corresponding to a plurality of frames of the speech signal are extracted. Further, the differential features are compared with an emotion recognition model to identify at least one emotion matching the speech signal, and then the at least one emotion is associated with the speech signal.
</abstract>

<claims>
1. A processor implemented method of speech signal processing, comprising: collecting a speech signal from at least one user, as input, via one or more hardware processors; extracting a plurality of differential features from the speech signal to generate a feature file, via the one or more hardware processors, by: sampling the speech signal at a defined sampling rate to generate a plurality of sampled speech signals; splitting each of the plurality of sampled speech signals to a plurality of overlapping or non-overlapping frames of 20 milliseconds length; iteratively performing for each of the plurality of frames, for a pre-defined number of times: selecting one sample in every M samples of the speech signal in the frame, where M is 2; and calculating differences between adjacent samples of the frame as the differential feature to get an output frame of size (L−(N)), where L is number of samples in a frame divided by M, and N represents number of times the difference between adjacent values is calculated; extracting the plurality of features from the plurality of differential features of the plurality of overlapping or non-overlapping frames, via the one or more hardware processors; comparing each of the differential features with an emotion recognition model to identify at least one emotion corresponding to the speech signal, via the one or more hardware processors; and associating the identified at least one emotion with the speech signal, via the one or more hardware processors.
2. The method as claimed in claim 1, wherein the emotion recognition model comprises of information pertaining to a plurality of speech signal characteristics and corresponding emotions, annotated at an utterance level.
3. The method as claimed in claim 1, wherein identifying the at least one emotion corresponding to the speech signal comprises: identifying matching data in the emotion recognition model, corresponding to each of the plurality of features in the feature file; and identifying at least one emotion tagged against each of the identified matching data in the emotion recognition model.
4. The method as claimed in claim 1, wherein the emotion recognition model is machine learning model generated by using training data comprising a plurality of sentences from at least one speech signal, and a plurality of utterances corresponding each of the plurality of sentences.
5. A system of speech signal processing, comprising: a memory module storing a plurality of instructions; one or more communication interfaces; and one or more hardware processors coupled to the memory module via the one or more communication interfaces, wherein the one or more hardware processors are caused by the plurality of instructions to: collect a speech signal from at least one user, as input; extract a plurality of differential features from the speech signal to generate a feature file, by: sampling the speech signal at a defined sampling rate to generate a plurality of sampled speech signals; splitting each of the plurality of sampled speech signals to a plurality of overlapping or non-overlapping frames of 20 milliseconds length; iteratively performing for each of the plurality of frames, for a pre-defined number of times: selecting one sample in every M samples of the speech signal in the frame, where M is 2; and calculating differences between adjacent samples of the frame as the differential feature to get an output frame of size (L−(N)), where L is number of samples in a frame divided by M, and N represents number of times the difference between adjacent values is calculated; extract the plurality of features from the plurality of differential features of the plurality of overlapping or non-overlapping frames; compare each of the differential features with an emotion recognition model to identify at least one emotion corresponding to the speech signal; and associate the identified at least one emotion with the speech signal.
6. The system as claimed in claim 5, wherein the emotion recognition model comprises of information pertaining to a plurality of speech signal characteristics and corresponding emotions, annotated at an utterance level.
7. The system as claimed in claim 5, wherein the system identifies the at least one emotion corresponding to the speech signal by: identifying matching data in the emotion recognition model, corresponding to each of the plurality of features in the feature file; and identifying at least one emotion tagged against each of the identified matching data in the emotion recognition mode.
8. The system as claimed in claim 5, wherein the system generates the emotion recognition model by using training data comprising a plurality of sentences from at least one speech signal, and a plurality of utterances corresponding each of the plurality of sentences.
9. A method for signal processing, comprising: generating a plurality of differential features corresponding to a signal by iteratively performing for each a plurality of frames of the signal, for a pre-defined number of times: selecting one sample in every M samples of the speech signal in the frame; and calculating differences between adjacent samples of the frame as the differential feature to get an output frame of size (L−(N)), where L is number of samples in a frame divided by M, and N represents number of times the difference between adjacent values is calculated.
10. A non-transitory computer readable medium for speech signal processing, the non-transitory computer readable medium performs the speech signal processing by: collecting a speech signal from at least one user, as input, via one or more hardware processors; extracting a plurality of differential features from the speech signal to generate a feature file, via the one or more hardware processors, by: sampling the speech signal at a defined sampling rate to generate a plurality of sampled speech signals; splitting each of the plurality of sampled speech signals to a plurality of overlapping or non-overlapping frames of 20 milliseconds length; iteratively performing for each of the plurality of frames, for a pre-defined number of times: selecting one sample in every M samples of the speech signal in the frame, where M is 2; and calculating differences between adjacent samples of the frame as the differential feature to get an output frame of size (L−(N)), where L is number of samples in a frame divided by M, and N represents number of times the difference between adjacent values is calculated; extracting the plurality of features from the plurality of differential features of the plurality of overlapping or non-overlapping frames, via the one or more hardware processors; comparing each of the differential features with an emotion recognition model to identify at least one emotion corresponding to the speech signal, via the one or more hardware processors; and associating the identified at least one emotion with the speech signal, via the one or more hardware processors.
11. The non-transitory computer readable medium as claimed in claim 10, wherein the emotion recognition model comprises of information pertaining to a plurality of speech signal characteristics and corresponding emotions, annotated at an utterance level.
12. The non-transitory computer readable medium as claimed in claim 10, wherein identifying the at least one emotion corresponding to the speech signal comprises: identifying matching data in the emotion recognition model, corresponding to each of the plurality of features in the feature file; and identifying at least one emotion tagged against each of the identified matching data in the emotion recognition model.
13. The non-transitory computer readable medium as claimed in claim 10, wherein the emotion recognition model is machine learning model generated by using training data comprising a plurality of sentences from at least one speech signal, and a plurality of utterances corresponding each of the plurality of sentences.
14. A non-transitory computer readable medium for signal processing, the non-transitory computer readable medium performs the speech signal processing by: generating a plurality of differential features corresponding to a signal by iteratively performing for each a plurality of frames of the signal, for a pre-defined number of times: selecting one sample in every M samples of the speech signal in the frame; and calculating differences between adjacent samples of the frame as the differential feature to get an output frame of size (L−(N)), where L is number of samples in a frame divided by M, and N represents number of times the difference between adjacent values is calculated.
</claims>
</document>
