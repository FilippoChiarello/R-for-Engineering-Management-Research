<document>

<filing_date>
2020-02-17
</filing_date>

<publication_date>
2020-09-10
</publication_date>

<priority_date>
2019-03-05
</priority_date>

<ipc_classes>
G06N20/00
</ipc_classes>

<assignee>
HRL LABORATORIES
</assignee>

<inventors>
KHOSLA, DEEPAK
SOLEYMAN, SEAN
</inventors>

<docdb_family_id>
69846558
</docdb_family_id>

<title>
ROBUST, SCALABLE AND GENERALIZABLE MACHINE LEARNING PARADIGM FOR MULTI-AGENT APPLICATIONS
</title>

<abstract>
Described is a learning system for multi-agent applications. In operation, the system initializes a plurality of learning agents. The learning agents include both tactical agents and strategic agents. The strategic agents take an observation from an environment and select one or more of the tactical agents to produce an action that is used to control a platform's actuators or simulated movements in the environment to complete a task. Alternatively, the tactical agents produce the action corresponding to a learned low-level behavior to control the platform's actuators or simulated movements in the environment to complete the task.
</abstract>

<claims>
1. A learning system for multi-agent applications, the system comprising: one or more processors and a memory, the memory being a non-transitory computer-readable medium having executable instructions encoded thereon, such that upon execution of the instructions, the one or more processors perform operations of: initializing a plurality of learning agents, the learning agents including both tactical agents and strategic agents; causing one or more strategic agents to take an observation from an environment and select one or more of the tactical agents to produce an action that is used to control a platform's actuators or simulated movements in the environment to complete a task; and causing one or tactical agents to produce the action corresponding to a learned low-level behavior to control the platform's actuators or simulated movements in the environment to complete the task.
2. The learning system as set forth in claim 1, further comprising operations of: training the learning agents to maximize a reward function returned by the environment; maintaining a fitness level for each learning agent during training, where the fitness level represents an average of a net reward obtained by the learning agent from each episode of training; and selecting one or more learning agents for additional training, based on their fitness with respect to a collective fitness of the learning agents.
3. The learning system as set forth in claim 2, further comprising an operation of adapting one or more of the plurality of learning agents to perform a new task in a new domain by performing one or more operations selected from a group consisting of: re-training a high-level strategy network to produce an optimal behavior, where optimality is based on maximizing reward signals obtained from episodes in the new domain; re-training one or more low-level behavior networks to produce optimal behavior in the new domain; or adding and training new behaviors and re-training the high-level strategy network to select these new behaviors based on maximizing reward signals from the new domain.
4. The learning system as set forth in claim 2, wherein each learning agent is trained in an initial state space, the initial state space being a set of all possible conditions that may exist in a simulated environment at a start of a training episode.
5. The learning system as set forth in claim 4, wherein the initial state space is sequentially expanded after at least two of the learning agents have fitness levels within a predetermined threshold.
6. The learning system as set forth in claim 2, where a difficulty of obtaining positive rewards increases during training.
7. The learning system as set forth in claim 2, where training of learning agents is terminated if no improvement is made for a predetermined number of episodes.
8. The learning system as set forth in claim 1, where different learning agents are initialized and trained with different hyperparameters.
9. The learning system as set forth in claim 1, wherein the low-level behavior includes a behavior selected from a group consisting of pursuit of opponents, evasion of opponents, and evasion of enemy projectiles.
10. The learning system as set forth in claim 1, wherein a function is used for reinforcement learning by the learning agents, the function is based on a Kullback-Leibler divergence between an action probability distribution selected by a strategic agent that is being trained with reinforcement learning, and an average of all probability distributions for all of other strategic agents in the population.
11. A computer program product for multi-agent applications, the computer program product comprising: a non-transitory computer-readable medium having executable instructions encoded thereon, such that upon execution of the instructions by one or more processors, the one or more processors perform operations of: initializing a plurality of learning agents, the learning agents including both tactical agents and strategic agents; causing one or more strategic agents to take an observation from an environment and select one or more of the tactical agents to produce an action that is used to control a platform's actuators or simulated movements in the environment to complete a task; and causing one or tactical agents to produce the action corresponding to a learned low-level behavior to control the platform's actuators or simulated movements in the environment to complete the task.
12. The computer program product as set forth in claim 11, further comprising instructions for causing the one or more processors to perform operations of: training the learning agents to maximize a reward function returned by the environment; maintaining a fitness level for each learning agent during training, where the fitness level represents an average of a net reward obtained by the learning agent from each episode of training; and selecting one or more learning agents for additional training, based on their fitness with respect to a collective fitness of the learning agents.
13. The computer program product as set forth in claim 12, further comprising instructions for causing the one or more processors to perform an operation of adapting one or more of the plurality of learning agents to perform a new task in a new domain by performing one or more operations selected from a group consisting of: re-training a high-level strategy network to produce an optimal behavior, where optimality is based on maximizing reward signals obtained from episodes in the new domain; re-training one or more low-level behavior networks to produce optimal behavior in the new domain; or adding and training new behaviors and re-training the high-level strategy network to select these new behaviors based on maximizing reward signals from the new domain.
14. The computer program product as set forth in claim 12, wherein each learning agent is trained in an initial state space, the initial state space being a set of all possible conditions that may exist in a simulated environment at a start of a training episode.
15. The computer program product as set forth in claim 14, wherein the initial state space is sequentially expanded after at least two of the learning agents have fitness levels within a predetermined threshold.
16. The computer program product as set forth in claim 12, where a difficulty of obtaining positive rewards increases during training.
17. The computer program product as set forth in claim 12, where training of learning agents is terminated if no improvement is made for a predetermined number of episodes.
18. The computer program product as set forth in claim 11, where different learning agents are initialized and trained with different hyperparameters.
19. The computer program product as set forth in claim 11, wherein the low-level behavior includes a behavior selected from a group consisting of pursuit of opponents, evasion of opponents, and evasion of enemy projectiles.
20. The computer program product as set forth in claim 11, wherein a function is used for reinforcement learning by the learning agents, the function is based on a Kullback-Leibler divergence between an action probability distribution selected by a strategic agent that is being trained with reinforcement learning, and an average of all probability distributions for all of other strategic agents in the population.
21. A computer implemented method for multi-agent applications, the method comprising an act of: causing one or more processors to execute instructions encoded on a non-transitory computer-readable medium, such that upon execution, the one or more processors perform operations of: initializing a plurality of learning agents, the learning agents including both tactical agents and strategic agents; causing one or more strategic agents to take an observation from an environment and select one or more of the tactical agents to produce an action that is used to control a platform's actuators or simulated movements in the environment to complete a task; and causing one or tactical agents to produce the action corresponding to a learned low-level behavior to control the platform's actuators or simulated movements in the environment to complete the task.
22. The method as set forth in claim 21, further comprising operations of: training the learning agents to maximize a reward function returned by the environment; maintaining a fitness level for each learning agent during training, where the fitness level represents an average of a net reward obtained by the learning agent from each episode of training; and selecting one or more learning agents for additional training, based on their fitness with respect to a collective fitness of the learning agents.
23. The method as set forth in claim 22, further comprising an operation of adapting one or more of the plurality of learning agents to perform a new task in a new domain by performing one or more operations selected from a group consisting of: re-training a high-level strategy network to produce an optimal behavior, where optimality is based on maximizing reward signals obtained from episodes in the new domain; re-training one or more low-level behavior networks to produce optimal behavior in the new domain; or adding and training new behaviors and re-training the high-level strategy network to select these new behaviors based on maximizing reward signals from the new domain.
24. The method as set forth in claim 22, wherein each learning agent is trained in an initial state space, the initial state space being a set of all possible conditions that may exist in a simulated environment at a start of a training episode.
25. The method as set forth in claim 24, wherein the initial state space is sequentially expanded after at least two of the learning agents have fitness levels within a predetermined threshold.
26. The method as set forth in claim 22, where a difficulty of obtaining positive rewards increases during training.
27. The method as set forth in claim 22, where training of learning agents is terminated if no improvement is made for a predetermined number of episodes.
28. The method as set forth in claim 21, where different learning agents are initialized and trained with different hyperparameters.
29. The method as set forth in claim 21, wherein the low-level behavior includes a behavior selected from a group consisting of pursuit of opponents, evasion of opponents, and evasion of enemy projectiles.
30. The method as set forth in claim 21, wherein a function is used for reinforcement learning by the learning agents, the function is based on a Kullback-Leibler divergence between an action probability distribution selected by a strategic agent that is being trained with reinforcement learning, and an average of all probability distributions for all of other strategic agents in the population.
</claims>
</document>
