<document>

<filing_date>
2020-02-27
</filing_date>

<publication_date>
2020-06-25
</publication_date>

<priority_date>
2018-03-27
</priority_date>

<ipc_classes>
G06K9/62,G06N20/00,G06Q20/40
</ipc_classes>

<assignee>
ALIBABA GROUP
</assignee>

<inventors>
LI, LONGFEI
ZHANG, YALIN
ZHENG, WENHAO
</inventors>

<docdb_family_id>
66166535
</docdb_family_id>

<title>
RISKY TRANSACTION IDENTIFICATION METHOD AND APPARATUS
</title>

<abstract>
A feature extraction is performed on transaction data to obtain a user classification feature and a transaction classification feature. A first dimension feature is constructed based on the user classification feature and the transaction classification feature. A dimension reduction processing is performed on the first dimension feature to obtain a second dimension feature. A probability that the transaction data relates to a risky transaction is determined based on a decision classification of the second dimension feature, where the decision classification is based on a pre-trained deep forest network including a plurality of levels of decision tree forest sets.
</abstract>

<claims>
1. A computer-implemented method, comprising: obtaining feature data describing a transaction initiated by a user of a transaction service, wherein the feature data comprises a set of features belonging to respective feature categories; for each feature category: selecting, from the features belonging to the feature category and in accordance with a respective feature retention rate, a plurality of sampled features; and generating, based on processing the sampled features using a machine learning model, an output specifying a predicted classification of the transaction.
2. The computer-implemented method of claim 1, wherein the machine learning model is a deep forest model.
3. The computer-implemented method of claim 2, wherein the deep forest model includes multiple levels of respective base classifiers.
4. The computer-implemented method of claim 3, further comprising training the deep forest model on training data that specifies a plurality of transaction samples.
5. The computer-implemented method of claim 4, wherein training the deep forest network comprises: collecting a plurality of black samples and white samples, wherein the each black sample relates to a risky transaction, and wherein each white sample relates to a normal transaction; extracting feature data from data associated with the black samples and data associated with the white samples; performing a dimension reduction process on the feature data having a first dimension to obtain sampled feature data; and iteratively performing a training process on the deep forest model.
6. The computer-implemented method of claim 5, wherein iteratively performing a training process comprises, for a current level of respective base classifiers: training each base classifier included in the current level on the sampled feature data; concatenating one or more output features of the current level to features from the sampled feature data; training each base classifier included in a next level by using the concatenated features; and terminating the training process upon determining that a predetermined termination condition is satisfied.
7. The computer-implemented method of claim 6, wherein a number of the black samples is not equal to a number of the white samples, and the method further comprises: prior to training each base classifier: dividing data associated with the black samples and data with the white samples through a k-fold cross validation into one or more training datasets and one or more corresponding validation datasets; training a base classifier on the training datasets; and testing the base classifier on the validation datasets to obtain an indicator that evaluates a performance of the base classifier.
8. The computer-implemented method of claim 5, further comprising: determining a maximum decision tree depth threshold based on a black-to-white sample ratio; and setting a maximum value of the decision tree depth to the maximum depth threshold.
9. A non-transitory, computer-readable medium storing one or more instructions executable by a computer system to perform operations comprising: obtaining feature data describing a transaction initiated by a user of a transaction service, wherein the feature data comprises a set of features belonging to respective feature categories; for each feature category: selecting, from the features belonging to the feature category and in accordance with a respective feature retention rate, a plurality of sampled features; and generating, based on processing the sampled features using a machine learning model, an output specifying a predicted classification of the transaction.
10. The non-transitory, computer-readable medium of claim 9, wherein the machine learning model is a deep forest model.
11. The non-transitory, computer-readable medium of claim 10, wherein the deep forest model includes multiple levels of respective base classifiers.
12. The non-transitory, computer-readable medium of claim 11, further comprising training the deep forest model on training data that specifies a plurality of transaction samples.
13. The non-transitory, computer-readable medium of claim 12, wherein training the deep forest network comprises: collecting a plurality of black samples and white samples, wherein the each black sample relates to a risky transaction, and wherein each white sample relates to a normal transaction; extracting feature data from data associated with the black samples and data associated with the white samples; performing a dimension reduction process on the feature data having a first dimension to obtain sampled feature data; and iteratively performing a training process on the deep forest model.
14. The non-transitory, computer-readable medium of claim 13, wherein iteratively performing a training process comprises, for a current level of respective base classifiers: training each base classifier included in the current level on the sampled feature data; concatenating one or more output features of the current level to features from the sampled feature data; training each base classifier included in a next level by using the concatenated features; and terminating the training process upon determining that a predetermined termination condition is satisfied.
15. A computer-implemented system, comprising: one or more computers; and one or more computer memory devices interoperably coupled with the one or more computers and having tangible, non-transitory, machine-readable media storing one or more instructions that, when executed by the one or more computers, perform one or more operations comprising: obtaining feature data describing a transaction initiated by a user of a transaction service, wherein the feature data comprises a set of features belonging to respective feature categories; for each feature category: selecting, from the features belonging to the feature category and in accordance with a respective feature retention rate, a plurality of sampled features; and generating, based on processing the sampled features using a machine learning model, an output specifying a predicted classification of the transaction.
16. The computer-implemented system of claim 15, wherein the machine learning model is a deep forest model.
17. The computer-implemented system of claim 16, wherein the deep forest model includes multiple levels of respective base classifiers.
18. The computer-implemented system of claim 17, further comprising training the deep forest model on training data that specifies a plurality of transaction samples.
19. The computer-implemented system of claim 18, wherein training the deep forest network comprises: collecting a plurality of black samples and white samples, wherein the each black sample relates to a risky transaction, and wherein each white sample relates to a normal transaction; extracting feature data from data associated with the black samples and data associated with the white samples; performing a dimension reduction process on the feature data having a first dimension to obtain sampled feature data; and iteratively performing a training process on the deep forest model.
20. The computer-implemented system of claim 19, wherein iteratively performing a training process comprises, for a current level of respective base classifiers: training each base classifier included in the current level on the sampled feature data; concatenating one or more output features of the current level to features from the sampled feature data; training each base classifier included in a next level by using the concatenated features; and terminating the training process upon determining that a predetermined termination condition is satisfied.
</claims>
</document>
