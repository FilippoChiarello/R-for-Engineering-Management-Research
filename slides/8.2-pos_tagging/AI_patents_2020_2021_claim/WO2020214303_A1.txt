<document>

<filing_date>
2020-03-17
</filing_date>

<publication_date>
2020-10-22
</publication_date>

<priority_date>
2019-04-19
</priority_date>

<ipc_classes>
G06T19/00
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
BLEYER, MICHAEL
PRICE, RAYMOND KIRK
PEKELNY, YURI
</inventors>

<docdb_family_id>
70285866
</docdb_family_id>

<title>
2D OBSTACLE BOUNDARY DETECTION
</title>

<abstract>
Techniques are provided to dynamically generate and render an object bounding fence in a mixed-reality scene. Initially, a sparse spatial mapping is accessed. The sparse spatial mapping beneficially includes perimeter edge data describing an object's edge perimeters. A gravity vector is also generated. Based on the perimeter edge data and the gravity vector, two-dimensional (2D) boundaries of the object are determined and a bounding fence mesh of the environment is generated. A virtual object is then rendered, where the virtual object is representative of at least a portion of the bounding fence mesh and visually illustrates a bounding fence around the object.
</abstract>

<claims>
1. A method for dynamically generating and rendering an object bounding fence in a mixed-reality scene, the method comprising:
accessing a spatial mapping of an environment, the spatial mapping including perimeter edge data describing one or more perimeter edge(s) of an object located within the environment;
generating a gravity vector of a head-mounted device (HMD) that is operating in the environment and that is displaying a mixed-reality scene;
based on the perimeter edge data and the gravity vector, determining twodimensional (2D) boundaries of the object within the environment;
generating a bounding fence mesh of the environment, the bounding fence mesh identifying the 2D boundaries of the object within the environment; and
rendering, within the mixed-reality scene, a virtual object that is representative of at least a portion of the bounding fence mesh and that visually illustrates a bounding fence around the object.
2. The method of claim 1, wherein the perimeter edge data describes a portion, but not all, of the one or more perimeter edge(s) of the object such that the perimeter edge data constitutes incomplete data, and wherein the spatial mapping is a sparse spatial mapping as a result of including the incomplete data for the object.
3. The method of claim 1, wherein the virtual object includes a visualization of a 2D bird's eye view of the environment.
4. The method of claim 1, wherein the spatial mapping is a sparse spatial mapping and is generated using a passive stereo camera system.
5. The method of claim 4, wherein the passive stereo camera system is included as a part of a head-tracking system of the HMD.
6. The method of claim 1, wherein the gravity vector is generated based on data obtained from an inertial measurement unit (EMU).
7. The method of claim 1, wherein the bounding fence is defined by the 2D boundaries of the object to form a 2D planar area surrounding the object relative to the gravity vector.
8. The method of claim 7, wherein the bounding fence further includes a rectangular cuboid whose length and width are defined by the 2D boundaries of the object and whose height extends upwardly in an unbounded direction perpendicular to the 2D planar area and parallel to the gravity vector.
9. The method of claim 7, wherein the bounding fence further includes a rectangular cuboid whose length and width are defined by the 2D boundaries of the object and whose height extends upwardly in an bounded direction perpendicular to the 2D planar area and parallel to the gravity vector, the height extending at least to a height of the object such that the rectangular cuboid entirely envelopes the object.
10. The method of claim 7, wherein a buffer is provided between the bounding fence and the 2D boundaries of the object such that an area defined by the bounding fence is larger than an area defined by the 2D boundaries of the object.
</claims>
</document>
