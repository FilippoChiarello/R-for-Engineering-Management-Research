<document>

<filing_date>
2020-04-08
</filing_date>

<publication_date>
2020-11-19
</publication_date>

<priority_date>
2019-05-16
</priority_date>

<ipc_classes>
G06F12/0897,G06N3/04,G06N3/08
</ipc_classes>

<assignee>
SAMSUNG ELECTRONICS COMPANY
</assignee>

<inventors>
LEE, DONGSOO
KWON, SEJUNG
PARK, Baeseong
</inventors>

<docdb_family_id>
73228678
</docdb_family_id>

<title>
ELECTRONIC APPARATUS AND CONTROLLING METHOD THEREOF
</title>

<abstract>
Provided is an electronic apparatus. The electronic apparatus includes a memory and a processor. The processor is configured to apply a low rank approximation using a matrix decomposition for a first square matrix among a plurality of square matrices based on parameter values of a deep learning model, and obtain a first approximated matrix and a second approximated matrix for the first square matrix, obtain second approximated matrices for each of a plurality of remaining square matrices other than the first square matrix among the plurality of square matrices, based on the first approximated matrix for the first square matrix, and store the first approximated matrix the first square matrix and the second approximated matrices for each of the plurality of square matrices in the memory.
</abstract>

<claims>
1. An electronic apparatus comprising: a memory; and a processor configured to: apply a low rank approximation using a matrix decomposition for a first square matrix among a plurality of square matrices based on parameter values of a deep learning model, and obtain a first approximated matrix and a second approximated matrix for the first square matrix, obtain second approximated matrices for each of a plurality of remaining square matrices other than the first square matrix among the plurality of square matrices, based on the first approximated matrix for the first square matrix, and store the first approximated matrix for the first square matrix and the second approximated matrices for each of the plurality of square matrices in the memory.
2. The electronic apparatus of claim 1, wherein the matrix decomposition comprises a singular value decomposition (SVD).
3. The electronic apparatus of claim 2, wherein the processor is further configured to obtain a first matrix and a second matrix by applying the singular value decomposition to the first square matrix, and obtain the first approximated matrix and the second approximated matrix for the first square matrix by applying the low rank approximation to the first matrix and the second matrix.
4. The electronic apparatus of claim 3, wherein the first approximated matrix for the first square matrix is a right-singular value vector matrix approximated for the first square matrix, and wherein the second approximated matrices for each of the plurality of square matrices are left-singular value vector matrices approximated for each of the plurality of square matrices.
5. The electronic apparatus of claim 3, wherein the processor is further configured to obtain the second approximated matrices for each of the plurality of remaining square matrices by multiplying each of the plurality of remaining square matrices by an inverse matrix of a transposed first matrix to which the first matrix is transposed.
6. The electronic apparatus of claim 1, wherein the processor is further configured to obtain a second approximated matrix for a second square matrix through an algorithm to minimize a difference between the second square matrix and a product of the second approximated matrix for the second square matrix and the first approximated matrix for the first square matrix, and wherein the second square matrix is any one of the plurality of remaining square matrices.
7. An electronic apparatus comprising: a memory storing a first approximated matrix and second approximated matrices forming a compressed deep learning model; and a processor configured to load the compressed deep learning model stored in the memory and obtain an output value based on the compressed deep learning model, wherein the first approximated matrix is one of two matrices obtained by applying a low rank approximation using a matrix decomposition for a first square matrix among a plurality of square matrices based on parameter values of a deep learning model, and wherein the second approximated matrices comprise another approximated matrix other than the first approximated matrix among the two matrices and approximated matrices obtained based on the first approximated matrix and a plurality of remaining square matrices other than the first square matrix among the plurality of square matrices.
8. The electronic apparatus of claim 7, wherein the first approximated matrix is a right-singular value vector matrix obtained by applying the low rank approximation through singular value decomposition (SVD) to the first square matrix, and wherein the second approximated matrices are left-singular value vector matrices for each of the plurality of square matrices.
9. The electronic apparatus of claim 7, wherein the processor further comprises a calculator, a primary cache memory, and a second cache memory, wherein the first approximated matrix is loaded into the primary cache memory from the memory, and at least one second approximated matrix among the second approximated matrices is loaded into the secondary cache memory from the memory, and the calculator is configured to obtain the output value based on the approximated matrices loaded into the primary cache memory and the secondary cache memory.
10. The electronic apparatus of claim 9, wherein, while the calculator is performing a first algorithm based on an input value and the first approximated matrix loaded into the primary cache memory, the at least one second approximated matrix is loaded into the secondary cache memory from the memory, and wherein the calculator is configured to perform a second algorithm based on a first algorithm result and the at least one second approximated matrix loaded into the secondary cache memory.
11. A method for controlling an electronic apparatus, the method comprising: applying a low rank approximation using a matrix decomposition for a first square matrix among a plurality of square matrices based on parameter values of a deep learning model, and obtaining a first approximated matrix and a second approximated matrix for the first square matrix; obtaining second approximated matrices for each of a plurality of remaining square matrices other than the first square matrix among the plurality of square matrices based on the first approximated matrix for the first square matrix; and storing the first approximated matrix for the first square matrix and the second approximated matrices for each of the plurality of square matrices in the memory.
12. The method of claim 11, wherein the matrix decomposition comprises a singular value decomposition (SVD).
13. The method of claim 12, wherein the obtaining the first approximated matrix and the second approximated matrix for the first square matrix comprises obtaining a first matrix and a second matrix by applying the singular value decomposition to the first square matrix, and obtaining the first approximated matrix and the second approximated matrix for the first square matrix by applying the low rank approximation to the first matrix and the second matrix.
14. The method of claim 13, wherein the first approximated matrix for the first square matrix is a right-singular value vector matrix approximated for the first square matrix, and wherein the second approximated matrices for each of the plurality of square matrices are left-singular value vector matrices approximated for each of the plurality of square matrices.
15. The method of claim 13, wherein the obtaining the second approximated matrices for each of the plurality of remaining square matrices comprises obtaining the second approximated matrices for each of the plurality of remaining square matrices by multiplying each of the plurality of remaining square matrices by an inverse matrix of a transposed first matrix to which the first matrix is transposed.
16. The method of claim 11, wherein the obtaining the approximated second matrices for each of the plurality of remaining square matrices comprises: obtaining a second approximated matrix for a second square matrix through an algorithm to minimize a difference between the second square matrix and a product of the second approximated matrix for the second square matrix and the first approximated matrix for the first square matrix, wherein the second square matrix is any one of the plurality of remaining square matrices.
17. A method for controlling an electronic apparatus, the method comprising: loading a compressed deep learning model stored in a memory; and obtaining an output value based on the compressed deep learning model, wherein the compressed deep learning model comprises a first approximated matrix and second approximated matrices, wherein the first approximated matrix is one of two matrices obtained by applying a low rank approximation using a matrix decomposition for a first square matrix among a plurality of square matrices based on parameter values of a deep learning model, and wherein the second approximated matrices comprise another approximated matrix other than the first approximated matrix among the two matrices and approximated matrices obtained based on the first approximated matrix and a plurality of remaining square matrices other than the first square matrix among the plurality of square matrices.
18. The method of claim 17, wherein the first approximated matrix is a right-singular value vector matrix obtained by applying the low rank approximation through singular value decomposition (SVD) to the first square matrix, and wherein the second approximated matrices are left-singular value vector matrices for each of the plurality of square matrices.
19. The method of claim 17, wherein the loading further comprises: loading the first approximated matrix into a primary cache memory from the memory; and loading at least one second approximated matrix among the second approximated matrices into a secondary cache memory from the memory, wherein the obtaining the output value comprises obtaining the output value based on the approximated matrices loaded into the primary cache memory and the secondary cache memory.
20. The method of claim 19, wherein the loading further comprises: while performing a first algorithm based on an input value and the first approximated matrix loaded into the primary cache memory, loading the at least one second approximated matrix into the secondary cache memory, wherein the obtaining the output value further comprises performing a second algorithm based on a first algorithm result and the at least one second approximated matrix loaded into the secondary cache memory.
</claims>
</document>
