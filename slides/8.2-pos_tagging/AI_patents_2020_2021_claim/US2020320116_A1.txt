<document>

<filing_date>
2017-11-24
</filing_date>

<publication_date>
2020-10-08
</publication_date>

<priority_date>
2017-11-24
</priority_date>

<ipc_classes>
G06F16/438,G06F16/45,G06F16/483,G06F16/9032,G06N5/02,H04L12/58
</ipc_classes>

<assignee>
MICROSOFT TECHNOLOGY LICENSING
</assignee>

<inventors>
WU, XIANCHAO
</inventors>

<docdb_family_id>
66631401
</docdb_family_id>

<title>
PROVIDING A SUMMARY OF A MULTIMEDIA DOCUMENT IN A SESSION
</title>

<abstract>
The present disclosure provides method and apparatus for providing a summary of a multimedia document in a session. In some implementations, a message may be received from a user in a session and the session is between the user and an electronic conversational agent. The multimedia document may be obtained based at least on the message. The emotion informaion in the multimedia document may be extracted. The summary of the multimedia document may be generated based at least on the message and the extracted emotion information. A response including the generated summary of the multimedia document may be provided to the user.
</abstract>

<claims>
1. A method for providing a summary of a multimedia document in a session, comprising: receiving a message from a user in the session, the session being between the user and an electronic conversational agent; obtaining the multimedia document based at least on the message; extracting emotion information in the multimedia document; generating the summary of the multimedia document based at least on the message and the extracted emotion information; and providing a response including the generated summary of the multimedia document to the user.
2. The method of claim 1, wherein generating the summary is further based on context information of the multimedia document, wherein the context information is obtained from at least one of a user log and a knowledge graph.
3. The method of claim 1, wherein the multimedia document comprises a text document associated with the session, the text document comprises multiple topics in the session, and each topic is appended with a lifecycle flag indicating a current state of the topic.
4. The method of claim 3, wherein the lifecycle flag for each topic is determined based at least on a change of emotion information for the topic in the session.
5. The method of claim 1, wherein the multimedia document comprises an image document, and generating the summary of the image document further comprises: identifying at least one region of interest (RoI) in the image document; determining emotion information of each RoI; determining a user intent from the message; and generating the summary of the image document based at least on the emotion information of each RoI and the user intent.
6. The method of claim 5, wherein generating the summary of the image document is further based on fact information of each RoI.
7. The method of claim 1, wherein the multimedia document comprises a voice document, and generating the summary of the voice document further comprises: identifying at least one voice segment in the voice document; determining emotion information of each voice segment; determining a user intent from the message; and generating the summary of the voice document based at least on the emotion information of each voice segment and the user intent.
8. The method of claim 7, wherein generating the summary of the voice document is further based on fact information of each voice segment.
9. The method of claim 1, wherein the multimedia document comprises a video document, and generating the summary of the video document further comprises: identifying at least one video clip in the video document; determining emotion information of each video clip; determining a user intent from the message; and generating the summary of the video document based at least on the emotion information of each video clip and the user intent.
10. The method of claim 9, wherein each video clip comprises at least one of image part and voice part, and wherein determining emotion information of each video clip further comprises determining emotion information of at least one of the image part and the voice part in the video clip.
11. The method of claim 10, wherein determining the emotion information of the image part in the video clip further comprises identifying at least one region of interest (RoI) in the image part, determining emotion information of each RoI, and obtaining the emotion information of the image part by combining the emotion information of each RoI in the image part, and wherein determining the emotion information of the voice part in the video clip further comprises identifying at least one voice segment in the voice part, determining emotion information of each voice segment, and obtaining the emotion information of the voice part by combining the emotion information of each voice segment in the voice part.
12. The method of claim 10, further comprising: predicting a next state of at least one object in the video document based at least on the user intent, a current state of the at least one object and relationship among regions of interest (RoIs) in the image part of a current video clip.
13. The method of claim 1, wherein the message comprises at least one of text message, image message, voice message and video message.
14. An apparatus for providing a summary of a multimedia document in a session, comprising: a message receiving module, for receiving a message from a user in the session, the session being between the user and an electronic conversational agent; a multimedia document obtaining module, for obtaining the multimedia document based at least on the message; an emotion information extraction module, for extracting emotion information in the multimedia document; a summary generation module, for generating the summary of the multimedia document based at least on the message and the extracted emotion information; and a response providing module, for providing a response including the generated summary of the multimedia document to the user.
15. The apparatus of claim 14, wherein the summary generation module generates the summary of the multimedia document further based on context information of the multimedia document, wherein the context information is obtained from at least one of a user log and a knowledge graph.
16. The apparatus of claim 14, wherein the multimedia document comprises a text document associated with the session, the text document comprises multiple topics in the session, and each topic is appended with a lifecycle flag indicating a current state of the topic.
17. The apparatus of claim 14, wherein the multimedia document comprises an image document, and the summary generation module is further configured for: identifying at least one region of interest (RoI) in the image document; determining emotion information of each RoI; determining a user intent from the message; and generating the summary of the image document based at least on the emotion information of each RoI and the user intent.
18. The apparatus of claim 14, wherein the multimedia document comprises a voice document, and the summary generation module is further configured for: identifying at least one voice segment in the voice document; determining emotion information of each voice segment; determining a user intent from the message; and generating the summary of the voice document based at least on the emotion information of each voice segment and the user intent.
19. The apparatus of claim 14, wherein the multimedia document comprises a video document, and the summary generation module is further configured for: identifying at least one video clip in the video document; determining emotion information of each video clip; determining a user intent from the message; and generating the summary of the video document based at least on the emotion information of each video clip and the user intent.
20. An apparatus for providing a summary of a multimedia document in a session, comprising: one or more processors; and a memory storing computer-executable instructions that, when executed, cause the one or more processors to: receive a message from a user in the session, the session being between the user and an electronic conversational agent; obtain the multimedia document based at least on the message; extract emotion information in the multimedia document; generate the summary of the multimedia document based at least on the message and the extracted emotion information; and provide a response including the generated summary of the multimedia document to the user.
</claims>
</document>
