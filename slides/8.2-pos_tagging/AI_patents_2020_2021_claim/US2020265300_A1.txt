<document>

<filing_date>
2020-03-26
</filing_date>

<publication_date>
2020-08-20
</publication_date>

<priority_date>
2017-10-24
</priority_date>

<ipc_classes>
G06F9/30,G06N3/06,G06N3/063,H03M7/42
</ipc_classes>

<assignee>
SHANGHAI CAMBRICON INFORMATION TECHNOLOGY COMPANY
</assignee>

<inventors>
DU, ZIDONG
ZHOU, XUDA
LIU, SHAOLI
LIU, DAOFU
</inventors>

<docdb_family_id>
68695469
</docdb_family_id>

<title>
PROCESSING METHOD AND DEVICE, OPERATION METHOD AND DEVICE
</title>

<abstract>
The application provides an operation method and device. Quantized data is looked up to realize an operation, which simplifies the structure and reduces the computation energy consumption of the data, meanwhile, a plurality of operations are realized.
</abstract>

<claims>
1. An operation device, comprising: an instruction control unit configured to decode a received instruction and generate lookup control information; and a lookup table unit configured to identify output neurons in an operation codebook according to the lookup control information and a received weight dictionary, a neuron dictionary, an operation codebook, weights, and input neurons.
2. The operation device of claim 1, wherein the weight dictionary includes weight positions and weight indexes, wherein the neuron dictionary includes the input neurons and neuron indexes, and wherein the operation codebook includes the weight indexes, the neuron indexes, and operation results of the input neurons and the weights.
3. The operation device of claim 2, further comprising: a preprocessing unit configured to preprocess input information to obtain the weights, the input neurons, the instruction, the weight dictionary, the neuron dictionary, and the operation codebook; a storage unit configured to store the input neurons, the weights, the weight dictionary, the neuron dictionary, the operation codebook, and the instruction, and receive the output neurons; a cache unit configured to cache the instruction, the input neurons, the weights, the weight indexes, the neuron indexes, and the output neurons; and a DMA configured to read and write data or instructions between the storage unit and the cache unit.
4. The operation device of claim 3, wherein the cache unit includes: an instruction cache configured to cache the instruction and output the cached instruction to the instruction control unit; a weight cache configured to cache the weights; an input neuron cache configured to cache the input neurons; and an output neuron cache configured to cache the output neurons output by the lookup table unit.
5. The operation device of claim 4, wherein the cache unit further includes: a weight index cache configured to cache the weight indexes; and a neuron index cache configured to cache the neuron indexes.
6. The operation device of claim 3, wherein the preprocessing unit is configured to preprocess the input information according to one or more algorithms that include segmentation, Gaussian filtering, binarization, regularization, and/or normalization.
7. The operation device of claim 2, wherein the lookup table unit includes at least one lookup table selected from a group consisting of: a multiplication lookup table that includes one or more multiplication results, wherein each of the one or more multiplication results respectively corresponds to a central weight and a central neuron, wherein the central weight corresponds to one of the weight indexes, and wherein the central neuron corresponds to one of the neuron indexes; an addition lookup table that includes one or more addition results, wherein each of the one or more addition results respectively corresponds to the central weight and the central neuron; and a pooling lookup table that includes one or more pooling results that respectively correspond to a central data, wherein the one or more pooling results are determined based on pooling operations including average pooling, maximum pooling, and median pooling.
8. The operation device of claim 4, wherein the instruction is a neural network-dedicated instruction, and the neural network-dedicated instruction includes: a control instruction configured to control a neural network execution process; a data transfer instruction configured to complete data transfer between different storage media, a data format including a matrix, a vector, and a scalar; an computation instruction configured to complete arithmetic operation of a neural network, wherein the computation instruction includes at least one of a matrix computation instruction, a vector computation instruction, a scalar computation instruction, a convolutional neural network computation instruction, a fully connected neural network computation instruction, a pooling neural network computation instruction, an RBM neural network computation instruction, an LRN neural network computation instruction, an LCN neural network computation instruction, an LSTM neural network computation instruction, an RNN computation instruction, an ReLU neural network computation instruction, a PReLU neural network computation instruction, a sigmoid neural network computation instruction, a tanh neural network computation instruction, or a maxout neural network computation instruction; and a logical instruction configured to complete logical operation of the neural network and including a vector logical computation instruction and a scalar logical computation instruction.
9. The operation device of claim 8, wherein the neural network-dedicated instruction includes at least one Cambricon instruction including an operation code and an operand, and the Cambricon instruction includes: a Cambricon control instruction configured to control the execution process, including a JUMP instruction and a conditional branch instruction; a Cam bricon data transfer instruction configured to complete data transfer between different storage media and including a load instruction, a store instruction, and a move instruction, where the load instruction is configured to load data from a main memory to a cache, the store instruction is configured to store the data from the cache to the main memory, and the move instruction is configured to move the data between the cache and another cache, between the cache and a register, or between the register and another register; a Cam bricon computation instruction configured to complete the arithmetic operation of the neural network and including a Cambricon matrix computation instruction, a Cambricon vector computation instruction, and a Cambricon scalar computation instruction, where the Cambricon matrix computation instruction is configured to complete matrix operation in the neural network, including matrix multiply vector operation, vector multiply matrix operation, matrix multiply scalar operation, outer product operation, matrix add matrix operation, and matrix subtract matrix operation, the Cambricon vector computation instruction is configured to complete vector operation in the neural network, including vector basic operations, vector transcendental functions operation, dot product operation, random vector generator operation, and operation of maximum/minimum of a vector, and the Cambricon scalar computation instruction is configured to complete scalar operation in the neural network, including scalar basic operations, and scalar transcendental functions operation; and a Cambricon logical instruction configured for the logical operation of the neural network, including a Cambricon vector logical computation instruction and a Cambricon scalar logical computation instruction, where the Cambricon vector logical computation instruction includes vector compare operation, vector logical operation, and vector greater than merge operation, the vector logical operation includes AND, OR, and NOT, and the Cambricon scalar logical operation includes scalar compare operation and scalar logical operation.
10. The operation device of claim 9, wherein the Cambricon data transfer instruction supports one or more of the following data organization manners: matrix, vector, and scalar, the vector basic operations include vector addition, subtraction, multiplication, and division, the vector transcendental functions refer to functions which do not meet any polynomial equations taking polynomials as coefficients, and include an exponential function, a logarithmic function, a trigonometric function, and an anti-trigonometric function, the scalar basic operations include scalar addition, subtraction, multiplication, and division, the scalar transcendental functions refer to functions which do not meet any polynomial equations taking polynomials as coefficients, and include an exponential function, a logarithmic function, a trigonometric function, and an anti-trigonometric function, the vector compare includes greater than, smaller than, equal to, more than or equal to, less than or equal to, and unequal to, the vector logical operations include AND, OR, and NOT, the scalar compare includes greater than, smaller than, equal to, more than or equal to, less than or equal to, and unequal to, and the scalar logical operations include AND, OR, and NOT.
11. An operation method, comprising: receiving weights, input neurons, an instruction, a weight dictionary, a neuron dictionary, and an operation codebook; decoding the instruction to determine lookup control information; and identifying output neurons in the operation codebook according to the lookup control information, the weights, the weight dictionary, the neuron dictionary, and the input neurons.
12. The operation method of claim 11, wherein the weight dictionary includes the weight positions and the weight indexes; the neuron dictionary includes the input neurons and the neuron indexes; and the operation codebook includes the weight indexes, the neuron indexes, and operation results of the weights and the input neurons.
13. The operation method of claim 12, wherein the identifying the output neurons in the operation codebook according to the lookup control information, the weights, and the input neurons includes: determining neuron ranges to determine the neuron indexes in the neuron dictionary; determining the weight positions to determine the weight indexes in the weight dictionary according to the weights, the input neurons, the weight dictionary, and the neuron dictionary; and identifying the operation results in the operation codebook according to the weight indexes and the neuron indexes to determine the output neurons.
14. The operation method of claim 13, wherein the operation results include a result of at least one of the following operations: addition, multiplication, and pooling, where pooling includes average pooling, maximum pooling, and median pooling.
15. The operation method of claim 14, further comprising: preprocessing input information to obtain the weights, the input neurons, the instruction, the weight dictionary, the neuron dictionary, and the operation codebook; storing, after receiving the weights, the input neurons, the instruction, the weight dictionary, the neuron dictionary and the operation codebook, the weights, the input neurons, the instruction, the weight dictionary, the neuron dictionary, and the operation codebook; receiving the output neurons; and caching the instruction, the input neurons, the weights, and the output neurons.
16. The operation method of claim 15, further comprising caching the weight indexes and the neuron indexes.
17. The operation method of claim 16, wherein the preprocessing includes segmentation, Gaussian filtering, binarization, regularization, and/or normalization.
18. The operation method of claim 17, wherein the instruction is a neural network-dedicated instruction, and the neural network-dedicated instruction includes: a control instruction configured to control a neural network execution process; a data transfer instruction configured to complete data transfer between different storage media, a data format including a matrix, a vector, and a scalar; an computation instruction configured to complete arithmetic operation of a neural network, wherein the computation instruction includes at least one of a matrix computation instruction, a vector computation instruction, a scalar computation instruction, a convolutional neural network computation instruction, a fully connected neural network computation instruction, a pooling neural network computation instruction, an RBM neural network computation instruction, an LRN neural network computation instruction, an LCN neural network computation instruction, an LSTM neural network computation instruction, an RNN computation instruction, an ReLU neural network computation instruction, a PReLU neural network computation instruction, a sigmoid neural network computation instruction, a tanh neural network computation instruction, or a maxout neural network computation instruction; and a logical instruction configured to complete logical operation of the neural network and including a vector logical computation instruction and a scalar logical computation instruction.
19. The operation method of claim 18, wherein the neural network-dedicated instruction includes at least one Cambricon instruction including an operation code and an operand, and the Cambricon instruction includes: a Cambricon control instruction configured to control the execution process, including a JUMP instruction and a conditional branch instruction; a Cam bricon data transfer instruction configured to complete data transfer between different storage media and including a load instruction, a store instruction and a move instruction, where the load instruction is configured to load data from a main memory to a cache, the store instruction is configured to store the data from the cache to the main memory, and the move instruction is configured to move the data between the cache and another cache or between the cache and a register or between the register and another register; a Cam bricon computation instruction configured to complete the arithmetic operation of the neural network and including a Cam bricon matrix computation instruction, a Cambricon vector computation instruction, and a Cambricon scalar computation instruction, where the Cambricon matrix computation instruction is configured to complete matrix operation in the neural network, including matrix multiply vector operation, vector multiply matrix operation, matrix multiply scalar operation, outer product operation, matrix add matrix operation, and matrix subtract matrix operation, the Cambricon vector computation instruction is configured to complete vector operation in the neural network, including vector basic operations, vector transcendental functions operation, dot product operation, random vector generator operation, and operation of maximum/minimum of a vector, and the Cambricon scalar computation instruction is configured to complete scalar operation in the neural network, including scalar basic operations and scalar transcendental functions operation; and a Cambricon logical instruction configured for the logical operation of the neural network, including a Cambricon vector logical computation instruction and a Cambricon scalar logical computation instruction, where the Cambricon vector logical computation instruction includes vector compare operation, vector logical operations, and vector greater than merge operation, the vector logical operations include AND, OR, and NOT, and the Cambricon scalar logical operation includes scalar compare operation and scalar logical operations.
20. The operation method of claim 19, wherein the Cam bricon data transfer instruction supports one or more of the following data organization manners: the matrix, the vector, and the scalar, the vector basic operations include vector addition, subtraction, multiplication, and division, the vector transcendental functions refer to functions which do not meet any polynomial equations taking polynomials as coefficients, and include an exponential function, a logarithmic function, a trigonometric function, and an anti-trigonometric function, the scalar basic operations include scalar addition, subtraction, multiplication, and division, the scalar transcendental functions refer to functions which do not meet any polynomial equations taking polynomials as coefficients, and include an exponential function, a logarithmic function, a trigonometric function, and an anti-trigonometric function, vector compare includes greater than, smaller than, equal to, more than or equal to, less than or equal to, and unequal to, the vector logical operations include AND, OR, and NOT, the scalar compare includes greater than, smaller than, equal to, more than or equal to, less than or equal to, and unequal to, and the scalar logical operations include AND, OR, and NOT.
</claims>
</document>
