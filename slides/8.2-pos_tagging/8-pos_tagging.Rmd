---
title: "POS-tagging with UDPipe"
subtitle: ""
author: "Filippo Chiarello, Ph.D."
output:
  xaringan::moon_reader:
    css: ["xaringan-themer.css", "slides.css"]
    lib_dir: libs
    anchor_sections: FALSE
    nature:
      ratio: "16:9"
      highlightLines: false
      highlightStyle: solarized-ligth
      countIncrementalSlides: true
---

```{r child = "../setup.Rmd"}
```

# UDPipe Introduction
- [UDPipe](http://ufal.mff.cuni.cz/udpipe, https://github.com/ufal/udpipe) is an R package which is an Rcpp wrapper around the UDPipe C++ library

- UDPipe provides language-agnostic tokenization, tagging, lemmatization and dependency parsing of raw text, which is an essential part in natural language processing.

- The techniques used are explained in detail in the paper: "Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe", available at http://ufal.mff.cuni.cz/~straka/papers/2017-conll_udpipe.pdf. I

- In that paper, you'll also find accuracies on different languages and process flow speed (measured in words per second).

---

# General Features 

The udpipe R package was designed with the following things in mind when building the Rcpp wrapper around the UDPipe C++ library:

- Give R users simple access in order to easily tokenize, tag, lemmatize or perform dependency parsing on text in any language
- Provide easy access to pre-trained annotation models
- Allow R users to easily construct your own annotation model based on data in CONLL-U format as provided in more than 100 treebanks available at http://universaldependencies.org/#ud-treebanks
- Don't rely on Python or Java so that R users can easily install this package without configuration hassle
- No external R package dependencies except the strict necessary (Rcpp and data.table, no tidyverse)

---

# Installation & License

The package is available under the Mozilla Public License Version 2.0. Installation can be done as follows. Please visit the package documentation at https://bnosac.github.io/udpipe/en and look at the R package vignettes for further details.

```{r}

# install.packages("udpipe")
# vignette("udpipe-tryitout", package = "udpipe")
# vignette("udpipe-annotation", package = "udpipe")
# vignette("udpipe-universe", package = "udpipe")
# vignette("udpipe-usecase-postagging-lemmatisation", package = "udpipe")
# # An overview of keyword extraction techniques: https://bnosac.github.io/udpipe/docs/doc7.html
# vignette("udpipe-usecase-topicmodelling", package = "udpipe")
# vignette("udpipe-parallel", package = "udpipe")
# vignette("udpipe-train", package = "udpipe")

```

---

# A first example

Currently the package allows you to do tokenisation, tagging, lemmatization and dependency parsing with one convenient function called udpipe.

```{r}
library(udpipe)
udmodel <- udpipe_download_model(language = "english")
# udmodel

x <- udpipe(x = "Competitive intelligence (CI) is the process and forward-looking 
            practices used in producing knowledge about the competitive environment
            to improve organizational performance.",
            object = udmodel)

```

---

# Inspect the content

```{r}

str(x)

```



---
# Pre-trained models

- Pre-trained models build on Universal Dependencies treebanks are made available for more than 65 languages based on 101 treebanks. 

- These have been made available easily to users of the package by using udpipe_download_model

---

# How good are these models?
- Accuracy statistics of models provided by the UDPipe authors which you download with udpipe_download_model from the default repository are available at this link.

- Accuracy statistics of models trained using this R package which you download with udpipe_download_model from the bnosac/udpipe.models.ud repository are available at https://github.com/bnosac/udpipe.models.ud.

- For a comparison between UDPipe and spaCy visit https://github.com/jwijffels/udpipe-spacy-comparison

---

# UDPipe - Basic Analytics

In order to get the most out of the package, let's enumerate a few things one can now easily do with your text annotated using the udpipe package using merely the Parts of Speech tags & the Lemma of each word.

## Improved exploratory text visualisations
- Due to richer features
- Allowing to select easily words which you like to plot (e.g. nouns/adjectives or the subject of the text)
- look for co-occurrences between words which are relevant based on the POS tag
- look for correlations between words which are relevant based on the POS tag

---

## Easy summarisation of text
- automatic keyword detection
- noun phrase extraction or chunking
- automatic text summarisation (e.g. using the textrank R package)

---

## Improved topic modelling by
- taking only words with specific parts-of-speech tags in the topic model
- automation of topic modelling for all languages by using the right pos tags instead of working with stopwords
- using lemmatisation as a better replacement than stemming in topic modelling

---

## Further processing
- Improved sentence or document similarities by using only the words of a specific POS tag
- Identification of authors based on grammatical patterns used

---

# Let's Apply the tools

Let's start by reading some text. We have a set of  patents on AI.

```{r}
library(udpipe)
library(tidyverse)

# Read in the folder of txt, saving in list_of_files all the names of the file we want to read
list_of_files <- list.files(path = "AI_patents_2020_2021_claim/",
                            pattern = "\\.txt$", 
                            full.names = TRUE)

```

---

```{r}

# set how many patents we want to read (for time purposes)
n_patents <- 100

# Create an empty tibble, that will store all the information
patents <-tibble(id = rep("", n_patents), 
                 title = rep("", n_patents), 
                 abstract = rep("", n_patents))
```

---

# Read in all files from a folder

```{r}


for(i in 1:n_patents){
  
  raw_text <- read_file(list_of_files[[i]])
  
  patents[[i, 1]] <- str_remove_all(string = list_of_files[[i]], 
                                pattern = "AI_patents_2020_2021_claim//|.txt")
  
  patents[[i, 2]] <- str_extract(string = raw_text, 
                                pattern = "<title>\n(.*?)\n</title>") %>% 
    str_remove_all("<title>\n|\n</title>")
  
  patents[[i, 3]] <- str_extract(string = raw_text, 
                                pattern = "<abstract>\n(.*?)\n</abstract>") %>% 
    str_remove_all("<abstract>\n|\n</abstract>")
  
}

```

---

# Have a look at the table

```{r}

head(patents)

```

---

# Tag the text

```{r}
ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(ud_model$file_model)

patents_tagged <- udpipe_annotate(ud_model, x = patents$abstract, doc_id = patents$id) %>% 
  as_tibble()

```

---

# Inspect the results 

The resulting data.frame has a field called upos which is the Universal Parts of Speech tag and also a field called lemma which is the root form of each token in the text. These 2 fields give us a broad range of analytical possibilities.

```{r}

head(patents_tagged)

```

---

# Basic frequency statistics

In most languages, nouns (NOUN) are the most common types of words, next to verbs (VERB) and these are the most relevant for analytical purposes, next to the adjectives (ADJ) and proper nouns (PROPN). 

For a detailed list of all POS tags: visit https://universaldependencies.org/u/pos/index.html.

---

```{r}

patents_tagged %>% 
  count(upos) %>% 
  ggplot(aes(x = reorder(upos, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  theme_bw()


```

---

# Using POS to filter

Parts of Speech tags are really interesting to extract easily the words you like to plot. You really don't need stopwords for doing this, just select nouns / verbs or adjectives and you have already the most relevant parts for basic frequency analysis.

```{r}

graph_noun <- patents_tagged %>% 
  filter(upos == "NOUN") %>% 
  count(lemma) %>% 
  filter(n > 30) %>% 
  ggplot(aes(x = reorder(lemma, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  theme_bw()


```

---

```{r}

graph_noun

```

---

```{r}

graph_adj <- patents_tagged %>% 
  filter(upos == "ADJ") %>% 
  count(lemma) %>% 
  filter(n > 10) %>% 
  ggplot(aes(x = reorder(lemma, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  theme_bw()


```

---

```{r}

graph_adj

```

---

```{r}

graph_vb <- patents_tagged %>% 
  filter(upos == "VERB") %>% 
  count(lemma) %>% 
  filter(n > 30) %>% 
  ggplot(aes(x = reorder(lemma, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  theme_bw()


```

---

```{r}

graph_vb

```

---

# Finding keywords

Frequency statistics of words are nice but most of the time, you are getting stuck in words which only make sense in combination with other words. This is typical of technical documents, where the most of the key concepts are multi-words. Hence you want to find keywords which are a combination of words.

Currently, udpipe provides 3 methods to identify keywords in text:

- RAKE (Rapid Automatic Keyword Extraction)
- Collocation ordering using Pointwise Mutual Information
- Parts of Speech phrase sequence detection

---

## Using RAKE

```{r}

pat_stats <- keywords_rake(x = patents_tagged, term = "lemma", group = "doc_id", 
                       relevant = patents_tagged$upos %in% c("NOUN", "ADJ")) %>% 
  as_tibble()

head(pat_stats)

```

---

```{r}

pat_stats %>% 
  top_n(10, rake) %>% 
  ggplot(aes(x = reorder(keyword, rake), y = rake)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  theme_bw()

```

---

# Using Pointwise Mutual Information Collocations

```{r}
patents_tagged$word <- tolower(patents_tagged$token)
pat_stats <- keywords_collocation(x = patents_tagged, term = "word", group = "doc_id")
pat_stats$key <- factor(pat_stats$keyword, levels = rev(pat_stats$keyword))

head(pat_stats)
```

---

```{r}

pat_stats %>% 
  top_n(10, pmi) %>% 
  ggplot(aes(x = reorder(key, pmi), y = pmi)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  theme_bw()



```

---

## Using a sequence of POS tags (noun phrases / verb phrases)


```{r}

# transform the pos in letters, to be readed by the tool
patents_tagged$phrase_tag <- as_phrasemachine(patents_tagged$upos, type = "upos")

# extract exact patterns of POS, using regex
stats <- keywords_phrases(x = patents_tagged$phrase_tag, term = tolower(patents_tagged$token), 
                          pattern = "(A|N)*N(P+D*(A|N)*N)*", 
                          is_regex = TRUE, detailed = FALSE)

head(stats)

```


---

```{r}

chunked_graph <- stats %>% 
  filter(ngram >1 ) %>% 
  top_n(10, freq) %>% 
  ggplot(aes(x = reorder(keyword, freq), y = freq)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  theme_bw()


```

---

```{r}

chunked_graph

```


